{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optuna GP-based Hyperparameter Optimization for Grok scRNA-seq Translation Model\n",
    "Implements curriculum learning with adaptive loss normalization\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers._gp import GPSampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import optuna.visualization as vis\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "# Import your existing modules\n",
    "#sys.path.append('/content/drive/MyDrive/Colab_Notebooks/translation/')\n",
    "from GAN_functions import *\n",
    "\n",
    "# Set up GPU memory management\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrokOptimizer:\n",
    "    \"\"\"Optuna-based optimizer for Grok CycleGAN model with curriculum learning\"\"\"\n",
    "    \n",
    "    def __init__(self, mouse_adata, human_adata, output_dir='optuna_results'):\n",
    "        self.mouse_adata = mouse_adata\n",
    "        self.human_adata = human_adata\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Setup device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Prepare data scalers and dimensionality reduction\n",
    "        self.setup_data_preprocessing()\n",
    "        \n",
    "        # Initialize best score tracking\n",
    "        self.best_score = float('inf')\n",
    "        self.best_config = None\n",
    "        \n",
    "    def setup_data_preprocessing(self):\n",
    "        \"\"\"Setup scalers and dimensionality reduction models\"\"\"\n",
    "        # Preprocess data using your existing function\n",
    "        self.mouse_tensor = preprocess_data(\n",
    "            self.mouse_adata, 'counts', library_size=10000\n",
    "        )\n",
    "        self.human_tensor = preprocess_data(\n",
    "            self.human_adata, 'counts', library_size=10000\n",
    "        )\n",
    "        \n",
    "        # Setup scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        human_data_np = self.human_tensor.numpy()\n",
    "        self.scaler.fit(human_data_np)\n",
    "        \n",
    "        # Setup PCA and UMAP\n",
    "        self.pca_model = PCA(n_components=50)\n",
    "        human_pca = self.pca_model.fit_transform(self.scaler.transform(human_data_np))\n",
    "        self.umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "        self.human_umap_coords = self.umap_model.fit_transform(human_pca)\n",
    "        \n",
    "        # Setup categorical covariates\n",
    "        self.setup_categorical_covariates()\n",
    "        \n",
    "    def setup_categorical_covariates(self):\n",
    "        \"\"\"Setup cell type mappings\"\"\"\n",
    "        unique_cell_types = sorted(\n",
    "            set(self.mouse_adata.obs['myannotations']) | \n",
    "            set(self.human_adata.obs['myannotations'])\n",
    "        )\n",
    "        self.value_to_idx = {val: idx for idx, val in enumerate(unique_cell_types)}\n",
    "        self.idx_to_cell_type = {idx: val for val, idx in self.value_to_idx.items()}\n",
    "        \n",
    "        mouse_cat = np.array([\n",
    "            self.value_to_idx[val] for val in self.mouse_adata.obs['myannotations'].values\n",
    "        ])\n",
    "        human_cat = np.array([\n",
    "            self.value_to_idx[val] for val in self.human_adata.obs['myannotations'].values\n",
    "        ])\n",
    "        \n",
    "        self.cat_tensor = np.concatenate([mouse_cat, human_cat])[:, None]\n",
    "        self.vocab_sizes = [len(unique_cell_types)]\n",
    "        \n",
    "    def create_curriculum_schedule(self, trial):\n",
    "        \"\"\"Create curriculum learning schedule based on trial parameters\"\"\"\n",
    "        curriculum_config = {\n",
    "            'warmup_epochs': trial.suggest_int('warmup_epochs', 10, 50),\n",
    "            'cycle_focus_epochs': trial.suggest_int('cycle_focus_epochs', 20, 80),\n",
    "            'adversarial_rampup': trial.suggest_int('adversarial_rampup', 10, 40),\n",
    "            'full_training_start': trial.suggest_int('full_training_start', 50, 150),\n",
    "            \n",
    "            # Initial weights during warmup\n",
    "            'initial_cycle_weight': trial.suggest_float('initial_cycle_weight', 5.0, 20.0),\n",
    "            'initial_celltype_weight': trial.suggest_float('initial_celltype_weight', 0.1, 1.0),\n",
    "            \n",
    "            # Final weights\n",
    "            'final_adv_weight': trial.suggest_float('final_adv_weight', 0.5, 2.0),\n",
    "            'final_fm_weight': trial.suggest_float('final_fm_weight', 0.1, 0.5),\n",
    "            'final_kl_weight': trial.suggest_float('final_kl_weight', 0.05, 0.2),\n",
    "        }\n",
    "        \n",
    "        return curriculum_config\n",
    "    \n",
    "    def get_curriculum_weights(self, epoch, curriculum_config, base_config):\n",
    "        \"\"\"Get loss weights based on curriculum schedule\"\"\"\n",
    "        weights = {}\n",
    "        \n",
    "        if epoch < curriculum_config['warmup_epochs']:\n",
    "            # Phase 1: Focus on reconstruction\n",
    "            progress = epoch / curriculum_config['warmup_epochs']\n",
    "            weights['lambda_adv'] = 0.0\n",
    "            weights['lambda_cycle'] = curriculum_config['initial_cycle_weight']\n",
    "            weights['lambda_celltype'] = curriculum_config['initial_celltype_weight'] * progress\n",
    "            weights['lambda_fm'] = 0.0\n",
    "            weights['lambda_kl'] = base_config['lambda_kl'] * 0.1  # Small KL from start\n",
    "            \n",
    "        elif epoch < curriculum_config['cycle_focus_epochs']:\n",
    "            # Phase 2: Strong cycle consistency\n",
    "            weights['lambda_adv'] = 0.1  # Very small adversarial\n",
    "            weights['lambda_cycle'] = base_config['lambda_cycle']\n",
    "            weights['lambda_celltype'] = base_config['lambda_celltype']\n",
    "            weights['lambda_fm'] = 0.0\n",
    "            weights['lambda_kl'] = base_config['lambda_kl'] * 0.5\n",
    "            \n",
    "        elif epoch < curriculum_config['full_training_start']:\n",
    "            # Phase 3: Gradual adversarial introduction\n",
    "            progress = (epoch - curriculum_config['cycle_focus_epochs']) / \\\n",
    "                      (curriculum_config['full_training_start'] - curriculum_config['cycle_focus_epochs'])\n",
    "            weights['lambda_adv'] = curriculum_config['final_adv_weight'] * progress\n",
    "            weights['lambda_cycle'] = base_config['lambda_cycle']\n",
    "            weights['lambda_celltype'] = base_config['lambda_celltype']\n",
    "            weights['lambda_fm'] = curriculum_config['final_fm_weight'] * progress\n",
    "            weights['lambda_kl'] = base_config['lambda_kl']\n",
    "            \n",
    "        else:\n",
    "            # Phase 4: Full training\n",
    "            weights['lambda_adv'] = curriculum_config['final_adv_weight']\n",
    "            weights['lambda_cycle'] = base_config['lambda_cycle']\n",
    "            weights['lambda_celltype'] = base_config['lambda_celltype']\n",
    "            weights['lambda_fm'] = curriculum_config['final_fm_weight']\n",
    "            weights['lambda_kl'] = base_config['lambda_kl']\n",
    "            \n",
    "        return weights\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Objective function for Optuna optimization\"\"\"\n",
    "        try:\n",
    "            # Suggest hyperparameters\n",
    "            config = self.suggest_hyperparameters(trial)\n",
    "            \n",
    "            # Create curriculum schedule\n",
    "            curriculum_config = self.create_curriculum_schedule(trial)\n",
    "            \n",
    "            # Build models\n",
    "            models = self.build_models(config)\n",
    "            \n",
    "            # Train with curriculum learning\n",
    "            metrics = self.train_with_curriculum(\n",
    "                models, config, curriculum_config, trial\n",
    "            )\n",
    "            \n",
    "            # Return multiple objectives\n",
    "            return [\n",
    "                metrics['final_fid'],           # Minimize FID\n",
    "                metrics['final_cycle_loss'],    # Minimize cycle loss\n",
    "                metrics['biological_score'],    # Minimize bio score (negative correlation)\n",
    "                -metrics['cell_type_preservation']  # Maximize cell type preservation\n",
    "            ]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {str(e)}\")\n",
    "            # Clean up GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    def suggest_hyperparameters(self, trial):\n",
    "        \"\"\"Suggest hyperparameters for the trial\"\"\"\n",
    "        config = {\n",
    "            # Training parameters\n",
    "            'epochs': 300,  # Fixed for fair comparison\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "            'lr': trial.suggest_float('lr', 1e-5, 5e-4, log=True),\n",
    "            \n",
    "            # Architecture parameters\n",
    "            'hidden_dims': trial.suggest_categorical(\n",
    "                'hidden_dims',\n",
    "                [[2048, 1024, 512], [1024, 512, 256], [4096, 2048, 1024]]\n",
    "            ),\n",
    "            'latent_dim': trial.suggest_categorical('latent_dim', [128, 256, 512]),\n",
    "            'num_attention_heads': trial.suggest_int('num_attention_heads', 4, 16, step=4),\n",
    "            'residual_blocks': trial.suggest_int('residual_blocks', 3, 6),\n",
    "            \n",
    "            # Loss weights (base values, will be modified by curriculum)\n",
    "            'lambda_cycle': trial.suggest_float('lambda_cycle', 5.0, 20.0),\n",
    "            'lambda_celltype': trial.suggest_float('lambda_celltype', 0.1, 0.5),\n",
    "            'lambda_gp': trial.suggest_float('lambda_gp', 5.0, 20.0),\n",
    "            'lambda_kl': trial.suggest_float('lambda_kl', 0.05, 0.2),\n",
    "            \n",
    "            # Training strategies\n",
    "            'nb_critic': trial.suggest_int('nb_critic', 3, 7),\n",
    "            'use_self_attention': trial.suggest_categorical('use_self_attention', [True, False]),\n",
    "            'use_sparse_attention': False,  # Keep false for stability\n",
    "            \n",
    "            # Optimizer parameters\n",
    "            'beta1': trial.suggest_float('beta1', 0.0, 0.9),\n",
    "            'beta2': trial.suggest_float('beta2', 0.9, 0.999),\n",
    "            \n",
    "            # Learning rate decay\n",
    "            'use_lr_decay': trial.suggest_categorical('use_lr_decay', [True, False]),\n",
    "            'decay_start_epoch': 100,\n",
    "            'decay_factor': trial.suggest_float('decay_factor', 0.1, 0.5),\n",
    "            \n",
    "            # Other fixed parameters\n",
    "            'library_size': 10000,\n",
    "            'preprocess_type': 'counts',\n",
    "            'temperature': 0.5,\n",
    "            'sparsity_target': 0.9,\n",
    "            'device': str(self.device),\n",
    "            'use_wandb': False  # Disable for optimization\n",
    "        }\n",
    "        \n",
    "        # Conditional parameters\n",
    "        if config['use_lr_decay']:\n",
    "            config['decay_type'] = trial.suggest_categorical('decay_type', ['linear', 'cosine'])\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def build_models(self, config):\n",
    "        \"\"\"Build generator and discriminator models\"\"\"\n",
    "        # Generators\n",
    "        g_m2h = GeneExpressionGenerator(\n",
    "            self.mouse_tensor.shape[1], \n",
    "            self.human_tensor.shape[1],\n",
    "            self.vocab_sizes, \n",
    "            config\n",
    "        ).to(self.device)\n",
    "        \n",
    "        g_h2m = GeneExpressionGenerator(\n",
    "            self.human_tensor.shape[1],\n",
    "            self.mouse_tensor.shape[1],\n",
    "            self.vocab_sizes,\n",
    "            config\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Discriminators\n",
    "        d_m = GeneExpressionDiscriminator(\n",
    "            self.mouse_tensor.shape[1],\n",
    "            self.vocab_sizes,\n",
    "            config\n",
    "        ).to(self.device)\n",
    "        \n",
    "        d_h = GeneExpressionDiscriminator(\n",
    "            self.human_tensor.shape[1],\n",
    "            self.vocab_sizes,\n",
    "            config\n",
    "        ).to(self.device)\n",
    "        \n",
    "        return {\n",
    "            'g_m2h': g_m2h,\n",
    "            'g_h2m': g_h2m,\n",
    "            'd_m': d_m,\n",
    "            'd_h': d_h\n",
    "        }\n",
    "    \n",
    "    def train_with_curriculum(self, models, config, curriculum_config, trial):\n",
    "        \"\"\"Train models with curriculum learning and adaptive loss normalization\"\"\"\n",
    "        # Setup optimizers\n",
    "        g_optimizer = torch.optim.Adam(\n",
    "            itertools.chain(models['g_m2h'].parameters(), models['g_h2m'].parameters()),\n",
    "            lr=config['lr'],\n",
    "            betas=(config['beta1'], config['beta2'])\n",
    "        )\n",
    "        \n",
    "        d_m_optimizer = torch.optim.Adam(\n",
    "            models['d_m'].parameters(),\n",
    "            lr=config['lr'],\n",
    "            betas=(config['beta1'], config['beta2'])\n",
    "        )\n",
    "        \n",
    "        d_h_optimizer = torch.optim.Adam(\n",
    "            models['d_h'].parameters(),\n",
    "            lr=config['lr'],\n",
    "            betas=(config['beta1'], config['beta2'])\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        if config['use_lr_decay']:\n",
    "            lr_scheduler = LRDecayScheduler(\n",
    "                [g_optimizer, d_m_optimizer, d_h_optimizer],\n",
    "                config['lr'],\n",
    "                config['decay_start_epoch'],\n",
    "                config['epochs'],\n",
    "                config['decay_factor'],\n",
    "                config.get('decay_type', 'cosine')\n",
    "            )\n",
    "        else:\n",
    "            lr_scheduler = None\n",
    "        \n",
    "        # Prepare data loaders\n",
    "        n_mouse = self.mouse_tensor.size(0)\n",
    "        mouse_cat_tensor = torch.tensor(self.cat_tensor[:n_mouse], dtype=torch.long).to(self.device)\n",
    "        human_cat_tensor = torch.tensor(self.cat_tensor[n_mouse:], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        mouse_dataset = TensorDataset(self.mouse_tensor, mouse_cat_tensor)\n",
    "        human_dataset = TensorDataset(self.human_tensor, human_cat_tensor)\n",
    "        \n",
    "        mouse_loader = DataLoader(mouse_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        human_loader = DataLoader(human_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        \n",
    "        # Training metrics\n",
    "        best_biological_score = float('inf')\n",
    "        metrics_history = []\n",
    "        \n",
    "        # Initialize adaptive loss scaling (from your original code)\n",
    "        moving_avg_adv = 0\n",
    "        moving_avg_cycle = 0\n",
    "        moving_avg_celltype = 0\n",
    "        moving_avg_fm = 0\n",
    "        moving_avg_kl = 0\n",
    "        alpha = 0.3  # Smoothing factor\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(config['epochs']):\n",
    "            # Get curriculum weights\n",
    "            curriculum_weights = self.get_curriculum_weights(\n",
    "                epoch, curriculum_config, config\n",
    "            )\n",
    "            \n",
    "            # Train one epoch\n",
    "            epoch_metrics = self.train_epoch(\n",
    "                models, mouse_loader, human_loader,\n",
    "                g_optimizer, d_m_optimizer, d_h_optimizer,\n",
    "                config, curriculum_weights,\n",
    "                moving_avg_adv, moving_avg_cycle, moving_avg_celltype,\n",
    "                moving_avg_fm, moving_avg_kl, alpha\n",
    "            )\n",
    "            \n",
    "            # Update moving averages\n",
    "            moving_avg_adv = alpha * epoch_metrics['avg_adv_loss'] + (1 - alpha) * moving_avg_adv\n",
    "            moving_avg_cycle = alpha * epoch_metrics['avg_cycle_loss'] + (1 - alpha) * moving_avg_cycle\n",
    "            moving_avg_celltype = alpha * epoch_metrics['avg_celltype_loss'] + (1 - alpha) * moving_avg_celltype\n",
    "            moving_avg_fm = alpha * epoch_metrics['avg_fm_loss'] + (1 - alpha) * moving_avg_fm\n",
    "            moving_avg_kl = alpha * epoch_metrics['avg_kl_loss'] + (1 - alpha) * moving_avg_kl\n",
    "            \n",
    "            # Update learning rate\n",
    "            if lr_scheduler:\n",
    "                current_lr = lr_scheduler.step(epoch)\n",
    "                epoch_metrics['lr'] = current_lr\n",
    "            \n",
    "            metrics_history.append(epoch_metrics)\n",
    "            \n",
    "            # Evaluate and report to Optuna (every 10 epochs)\n",
    "            if epoch % 10 == 0 and epoch > 0:\n",
    "                eval_metrics = self.evaluate_models(models, epoch)\n",
    "                \n",
    "                # Report primary metric for pruning\n",
    "                trial.report(eval_metrics['biological_score'], epoch)\n",
    "                \n",
    "                # Check for pruning\n",
    "                if trial.should_prune():\n",
    "                    print(f\"Trial {trial.number} pruned at epoch {epoch}\")\n",
    "                    raise optuna.TrialPruned()\n",
    "                \n",
    "                # Check for GAN-specific failures\n",
    "                if self.check_gan_failure(epoch_metrics):\n",
    "                    print(f\"Trial {trial.number} failed due to GAN instability\")\n",
    "                    raise optuna.TrialPruned()\n",
    "                \n",
    "                # Update best score\n",
    "                if eval_metrics['biological_score'] < best_biological_score:\n",
    "                    best_biological_score = eval_metrics['biological_score']\n",
    "                    self.save_checkpoint(models, config, trial.number, epoch)\n",
    "            \n",
    "            # Progress logging\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Trial {trial.number}, Epoch {epoch}: \"\n",
    "                      f\"G Loss: {epoch_metrics['avg_g_loss']:.4f}, \"\n",
    "                      f\"Cycle: {epoch_metrics['avg_cycle_loss']:.4f}, \"\n",
    "                      f\"Bio Score: {best_biological_score:.4f}\")\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_metrics = self.evaluate_models(models, config['epochs'])\n",
    "        final_metrics['training_history'] = metrics_history\n",
    "        \n",
    "        # Save trial results\n",
    "        self.save_trial_results(trial, config, final_metrics)\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    def train_epoch(self, models, mouse_loader, human_loader, \n",
    "                   g_optimizer, d_m_optimizer, d_h_optimizer,\n",
    "                   config, curriculum_weights,\n",
    "                   moving_avg_adv, moving_avg_cycle, moving_avg_celltype,\n",
    "                   moving_avg_fm, moving_avg_kl, alpha):\n",
    "        \"\"\"Train one epoch with adaptive loss normalization\"\"\"\n",
    "        # Tracking metrics\n",
    "        g_losses, d_m_losses, d_h_losses = [], [], []\n",
    "        cycle_losses, celltype_losses, fm_losses, kl_losses = [], [], [], []\n",
    "        adv_losses = []\n",
    "        \n",
    "        mouse_iter = iter(mouse_loader)\n",
    "        human_iter = iter(human_loader)\n",
    "        \n",
    "        for i in range(max(len(mouse_loader), len(human_loader))):\n",
    "            # Get batches\n",
    "            try:\n",
    "                mouse_data, mouse_cat = next(mouse_iter)\n",
    "            except StopIteration:\n",
    "                mouse_iter = iter(mouse_loader)\n",
    "                mouse_data, mouse_cat = next(mouse_iter)\n",
    "            \n",
    "            try:\n",
    "                human_data, human_cat = next(human_iter)\n",
    "            except StopIteration:\n",
    "                human_iter = iter(human_loader)\n",
    "                human_data, human_cat = next(human_iter)\n",
    "            \n",
    "            # Synchronize batch sizes\n",
    "            min_batch_size = min(mouse_data.size(0), human_data.size(0))\n",
    "            mouse_data = mouse_data[:min_batch_size].to(self.device)\n",
    "            human_data = human_data[:min_batch_size].to(self.device)\n",
    "            mouse_cat = mouse_cat[:min_batch_size].to(self.device)\n",
    "            human_cat = human_cat[:min_batch_size].to(self.device)\n",
    "            \n",
    "            # Train discriminators\n",
    "            if curriculum_weights['lambda_adv'] > 0:\n",
    "                d_m_optimizer.zero_grad()\n",
    "                d_h_optimizer.zero_grad()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    fake_human, _, _ = models['g_m2h'](mouse_data, mouse_cat)\n",
    "                    fake_mouse, _, _ = models['g_h2m'](human_data, human_cat)\n",
    "                \n",
    "                # Discriminator losses\n",
    "                real_m_validity, real_m_features = models['d_m'](mouse_data, mouse_cat, return_features=True)\n",
    "                fake_m_validity, fake_m_features = models['d_m'](fake_mouse.detach(), human_cat, return_features=True)\n",
    "                gp_m = compute_gradient_penalty(models['d_m'], mouse_data, fake_mouse, self.device, human_cat)\n",
    "                d_m_loss = -torch.mean(real_m_validity) + torch.mean(fake_m_validity) + config['lambda_gp'] * gp_m\n",
    "                \n",
    "                real_h_validity, real_h_features = models['d_h'](human_data, human_cat, return_features=True)\n",
    "                fake_h_validity, fake_h_features = models['d_h'](fake_human.detach(), mouse_cat, return_features=True)\n",
    "                gp_h = compute_gradient_penalty(models['d_h'], human_data, fake_human, self.device, mouse_cat)\n",
    "                d_h_loss = -torch.mean(real_h_validity) + torch.mean(fake_h_validity) + config['lambda_gp'] * gp_h\n",
    "                \n",
    "                d_m_loss.backward()\n",
    "                d_h_loss.backward()\n",
    "                d_m_optimizer.step()\n",
    "                d_h_optimizer.step()\n",
    "                \n",
    "                d_m_losses.append(d_m_loss.item())\n",
    "                d_h_losses.append(d_h_loss.item())\n",
    "            \n",
    "            # Train generators\n",
    "            if i % config['nb_critic'] == 0:\n",
    "                g_optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                fake_human, mu_h, log_var_h = models['g_m2h'](mouse_data, mouse_cat)\n",
    "                fake_mouse, mu_m, log_var_m = models['g_h2m'](human_data, human_cat)\n",
    "                \n",
    "                # Calculate losses\n",
    "                g_adv = 0\n",
    "                if curriculum_weights['lambda_adv'] > 0:\n",
    "                    g_adv = -torch.mean(models['d_h'](fake_human, human_cat)) - \\\n",
    "                            torch.mean(models['d_m'](fake_mouse, mouse_cat))\n",
    "                    adv_losses.append(g_adv.item())\n",
    "                \n",
    "                # Cycle consistency loss\n",
    "                if config['preprocess_type'] == 'counts':\n",
    "                    cycle_mouse, _, _ = models['g_h2m'](fake_human, mouse_cat)\n",
    "                    cycle_human, _, _ = models['g_m2h'](fake_mouse, human_cat)\n",
    "                    g_cycle = compute_poisson_loss(mouse_data, cycle_mouse) + \\\n",
    "                             compute_poisson_loss(human_data, cycle_human)\n",
    "                else:\n",
    "                    cycle_mouse, _, _ = models['g_h2m'](fake_human, mouse_cat)\n",
    "                    cycle_human, _, _ = models['g_m2h'](fake_mouse, human_cat)\n",
    "                    g_cycle = F.l1_loss(mouse_data, cycle_mouse) + F.l1_loss(human_data, cycle_human)\n",
    "                \n",
    "                # Cell type loss\n",
    "                g_celltype = F.cross_entropy(models['d_h'](fake_human, mouse_cat), mouse_cat.squeeze()) + \\\n",
    "                            F.cross_entropy(models['d_m'](fake_mouse, human_cat), human_cat.squeeze())\n",
    "                \n",
    "                # Feature matching loss\n",
    "                g_fm = 0\n",
    "                if curriculum_weights['lambda_fm'] > 0:\n",
    "                    _, real_h_feat = models['d_h'](human_data, human_cat, return_features=True)\n",
    "                    _, fake_h_feat = models['d_h'](fake_human, mouse_cat, return_features=True)\n",
    "                    _, real_m_feat = models['d_m'](mouse_data, mouse_cat, return_features=True)\n",
    "                    _, fake_m_feat = models['d_m'](fake_mouse, human_cat, return_features=True)\n",
    "                    \n",
    "                    for rf, ff in zip(real_h_feat, fake_h_feat):\n",
    "                        g_fm += F.l1_loss(rf.detach(), ff)\n",
    "                    for rf, ff in zip(real_m_feat, fake_m_feat):\n",
    "                        g_fm += F.l1_loss(rf.detach(), ff)\n",
    "                \n",
    "                # KL loss\n",
    "                kl_loss = torch.mean(-0.5 * torch.sum(1 + log_var_h - mu_h ** 2 - log_var_h.exp(), dim=1)) + \\\n",
    "                         torch.mean(-0.5 * torch.sum(1 + log_var_m - mu_m ** 2 - log_var_m.exp(), dim=1))\n",
    "                \n",
    "                # Adaptive loss scaling (from your original code)\n",
    "                if moving_avg_adv > 0:\n",
    "                    scale_adv = 1.0 / (1.0 + moving_avg_adv)\n",
    "                else:\n",
    "                    scale_adv = 1.0\n",
    "                scale_cycle = 1.0 / (1.0 + moving_avg_cycle) if moving_avg_cycle > 0 else 1.0\n",
    "                scale_celltype = 1.0 / (1.0 + moving_avg_celltype) if moving_avg_celltype > 0 else 1.0\n",
    "                scale_fm = 1.0 / (1.0 + moving_avg_fm) if moving_avg_fm > 0 else 1.0\n",
    "                scale_kl = 1.0 / (1.0 + moving_avg_kl) if moving_avg_kl > 0 else 1.0\n",
    "                \n",
    "                # Total loss with curriculum weights and adaptive scaling\n",
    "                g_loss = (curriculum_weights['lambda_adv'] * scale_adv * g_adv +\n",
    "                         curriculum_weights['lambda_cycle'] * scale_cycle * g_cycle +\n",
    "                         curriculum_weights['lambda_celltype'] * scale_celltype * g_celltype +\n",
    "                         curriculum_weights['lambda_fm'] * scale_fm * g_fm +\n",
    "                         curriculum_weights['lambda_kl'] * scale_kl * kl_loss)\n",
    "                \n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "                \n",
    "                # Track losses\n",
    "                g_losses.append(g_loss.item())\n",
    "                cycle_losses.append(g_cycle.item())\n",
    "                celltype_losses.append(g_celltype.item())\n",
    "                fm_losses.append(g_fm.item() if curriculum_weights['lambda_fm'] > 0 else 0)\n",
    "                kl_losses.append(kl_loss.item())\n",
    "        \n",
    "        # Return epoch metrics\n",
    "        return {\n",
    "            'avg_g_loss': np.mean(g_losses) if g_losses else 0,\n",
    "            'avg_d_m_loss': np.mean(d_m_losses) if d_m_losses else 0,\n",
    "            'avg_d_h_loss': np.mean(d_h_losses) if d_h_losses else 0,\n",
    "            'avg_cycle_loss': np.mean(cycle_losses) if cycle_losses else 0,\n",
    "            'avg_celltype_loss': np.mean(celltype_losses) if celltype_losses else 0,\n",
    "            'avg_fm_loss': np.mean(fm_losses) if fm_losses else 0,\n",
    "            'avg_kl_loss': np.mean(kl_losses) if kl_losses else 0,\n",
    "            'avg_adv_loss': np.mean(adv_losses) if adv_losses else 0,\n",
    "        }\n",
    "    \n",
    "    def evaluate_models(self, models, epoch):\n",
    "        \"\"\"Evaluate models with biological metrics\"\"\"\n",
    "        models['g_m2h'].eval()\n",
    "        models['g_h2m'].eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Sample translations\n",
    "            n_samples = min(1000, self.mouse_tensor.size(0))\n",
    "            indices = np.random.choice(self.mouse_tensor.size(0), n_samples, replace=False)\n",
    "            \n",
    "            mouse_sample = self.mouse_tensor[indices].to(self.device)\n",
    "            n_mouse = self.mouse_tensor.size(0)\n",
    "            cat_sample = torch.tensor(\n",
    "                self.cat_tensor[:n_mouse][indices], \n",
    "                dtype=torch.long\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Generate translations\n",
    "            fake_human, _, _ = models['g_m2h'](mouse_sample, cat_sample)\n",
    "            fake_human_np = fake_human.cpu().numpy()\n",
    "            \n",
    "            # Biological evaluation metrics\n",
    "            \n",
    "            # 1. Cell type preservation (correlation of cell type centroids)\n",
    "            real_centroids = compute_cell_type_centroids(\n",
    "                self.human_adata, self.human_tensor, self.device\n",
    "            )\n",
    "            fake_adata = ad.AnnData(fake_human_np)\n",
    "            fake_adata.obs['myannotations'] = [\n",
    "                self.idx_to_cell_type[idx.item()] for idx in cat_sample\n",
    "            ]\n",
    "            fake_centroids = compute_cell_type_centroids(\n",
    "                fake_adata, fake_human, self.device\n",
    "            )\n",
    "            \n",
    "            correlations = []\n",
    "            for ct in real_centroids:\n",
    "                if ct in fake_centroids:\n",
    "                    corr = np.corrcoef(\n",
    "                        real_centroids[ct].cpu().numpy(),\n",
    "                        fake_centroids[ct].cpu().numpy()\n",
    "                    )[0, 1]\n",
    "                    if not np.isnan(corr):\n",
    "                        correlations.append(corr)\n",
    "            \n",
    "            avg_correlation = np.mean(correlations) if correlations else 0\n",
    "            \n",
    "            # 2. UMAP-based evaluation\n",
    "            if self.pca_model and self.scaler:\n",
    "                fake_human_scaled = self.scaler.transform(fake_human_np)\n",
    "                fake_human_pca = self.pca_model.transform(fake_human_scaled)\n",
    "                fake_human_umap = self.umap_model.transform(fake_human_pca)\n",
    "                \n",
    "                # Calculate silhouette score\n",
    "                labels = [self.idx_to_cell_type[idx.item()] for idx in cat_sample]\n",
    "                if len(set(labels)) > 1:\n",
    "                    silhouette = silhouette_score(fake_human_umap, labels)\n",
    "                else:\n",
    "                    silhouette = 0\n",
    "            else:\n",
    "                silhouette = 0\n",
    "            \n",
    "            # 3. Simple FID approximation (using PCA distances)\n",
    "            if hasattr(self, 'human_pca_mean'):\n",
    "                fake_pca_mean = np.mean(fake_human_pca, axis=0)\n",
    "                fake_pca_cov = np.cov(fake_human_pca, rowvar=False)\n",
    "                \n",
    "                # Simplified FID calculation\n",
    "                mean_diff = np.sum((self.human_pca_mean - fake_pca_mean) ** 2)\n",
    "                cov_diff = np.trace(self.human_pca_cov + fake_pca_cov - \n",
    "                                   2 * np.sqrt(self.human_pca_cov @ fake_pca_cov))\n",
    "                fid_score = mean_diff + cov_diff\n",
    "            else:\n",
    "                # Cache human statistics for future use\n",
    "                human_pca = self.pca_model.transform(\n",
    "                    self.scaler.transform(self.human_tensor.numpy())\n",
    "                )\n",
    "                self.human_pca_mean = np.mean(human_pca, axis=0)\n",
    "                self.human_pca_cov = np.cov(human_pca, rowvar=False)\n",
    "                fid_score = 100.0  # Default high value\n",
    "            \n",
    "            # 4. Cycle consistency check\n",
    "            cycle_mouse, _, _ = models['g_h2m'](fake_human, cat_sample)\n",
    "            cycle_loss = F.l1_loss(mouse_sample, cycle_mouse).item()\n",
    "        \n",
    "        models['g_m2h'].train()\n",
    "        models['g_h2m'].train()\n",
    "        \n",
    "        # Combine metrics into biological score\n",
    "        biological_score = (\n",
    "            -avg_correlation * 100 +  # Negative because we want high correlation\n",
    "            (1 - silhouette) * 50 +   # Low silhouette is bad\n",
    "            cycle_loss * 10           # Low cycle loss is good\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'biological_score': biological_score,\n",
    "            'cell_type_preservation': avg_correlation,\n",
    "            'silhouette_score': silhouette,\n",
    "            'final_fid': fid_score,\n",
    "            'final_cycle_loss': cycle_loss\n",
    "        }\n",
    "    \n",
    "    def check_gan_failure(self, metrics):\n",
    "        \"\"\"Check for GAN-specific failure modes\"\"\"\n",
    "        # Mode collapse detection\n",
    "        if metrics['avg_g_loss'] < 0.1 and metrics['avg_d_m_loss'] > 5.0:\n",
    "            return True\n",
    "        \n",
    "        # Training divergence\n",
    "        if (metrics['avg_g_loss'] > 10.0 or \n",
    "            metrics['avg_d_m_loss'] > 10.0 or\n",
    "            np.isnan(metrics['avg_g_loss'])):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, models, config, trial_number, epoch):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint_dir = os.path.join(self.output_dir, f'trial_{trial_number}')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'config': config,\n",
    "            'g_m2h_state': models['g_m2h'].state_dict(),\n",
    "            'g_h2m_state': models['g_h2m'].state_dict(),\n",
    "            'd_m_state': models['d_m'].state_dict(),\n",
    "            'd_h_state': models['d_h'].state_dict(),\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt'))\n",
    "    \n",
    "    def save_trial_results(self, trial, config, metrics):\n",
    "        \"\"\"Save detailed trial results\"\"\"\n",
    "        results = {\n",
    "            'trial_number': trial.number,\n",
    "            'config': config,\n",
    "            'metrics': metrics,\n",
    "            'params': trial.params,\n",
    "            'values': trial.values if hasattr(trial, 'values') else None,\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, f'trial_{trial.number}_results.pkl'), 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    def run_optimization(self, n_trials=100, n_jobs=1):\n",
    "        \"\"\"Run the optimization study\"\"\"\n",
    "        # Create study with GP sampler\n",
    "        sampler = GPSampler(\n",
    "            n_startup_trials=20,  # More startup trials for GP\n",
    "            deterministic_objective=False,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        pruner = MedianPruner(\n",
    "            n_startup_trials=5,\n",
    "            n_warmup_steps=30,\n",
    "            interval_steps=10\n",
    "        )\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            directions=['minimize', 'minimize', 'minimize', 'minimize'],\n",
    "            sampler=sampler,\n",
    "            pruner=pruner,\n",
    "            study_name='grok_cyclegan_optimization'\n",
    "        )\n",
    "        \n",
    "        # Add callbacks\n",
    "        study.set_user_attr(\"best_biological_score\", float('inf'))\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(\n",
    "            self.objective,\n",
    "            n_trials=n_trials,\n",
    "            n_jobs=n_jobs,\n",
    "            gc_after_trial=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Save study\n",
    "        with open(os.path.join(self.output_dir, 'study.pkl'), 'wb') as f:\n",
    "            pickle.dump(study, f)\n",
    "        \n",
    "        # Analyze results\n",
    "        self.analyze_results(study)\n",
    "        \n",
    "        return study\n",
    "    \n",
    "    def analyze_results(self, study):\n",
    "        \"\"\"Analyze and visualize optimization results\"\"\"\n",
    "        # Get Pareto front trials\n",
    "        pareto_trials = study.best_trials\n",
    "        \n",
    "        print(f\"\\nOptimization completed with {len(study.trials)} trials\")\n",
    "        print(f\"Number of Pareto optimal solutions: {len(pareto_trials)}\")\n",
    "        \n",
    "        # Print best trials\n",
    "        print(\"\\nTop 5 Pareto optimal configurations:\")\n",
    "        for i, trial in enumerate(pareto_trials[:5]):\n",
    "            print(f\"\\nTrial {trial.number}:\")\n",
    "            print(f\"  Objectives: {trial.values}\")\n",
    "            print(f\"  Key parameters:\")\n",
    "            for key in ['lr', 'batch_size', 'lambda_cycle', 'lambda_celltype']:\n",
    "                if key in trial.params:\n",
    "                    print(f\"    {key}: {trial.params[key]}\")\n",
    "        \n",
    "        # Visualizations\n",
    "        self.create_optimization_plots(study)\n",
    "        \n",
    "        # Save best configuration\n",
    "        best_trial = min(pareto_trials, key=lambda t: t.values[2])  # Best biological score\n",
    "        best_config = self.suggest_hyperparameters(best_trial)\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'best_config.json'), 'w') as f:\n",
    "            json.dump(best_config, f, indent=4)\n",
    "        \n",
    "        print(f\"\\nBest configuration saved to {self.output_dir}/best_config.json\")\n",
    "    \n",
    "    def create_optimization_plots(self, study):\n",
    "        \"\"\"Create visualization plots for the optimization results\"\"\"\n",
    "        \n",
    "        # Parameter importance\n",
    "        fig = vis.plot_param_importances(study)\n",
    "        fig.write_html(os.path.join(self.output_dir, 'param_importances.html'))\n",
    "        \n",
    "        # Optimization history\n",
    "        fig = vis.plot_optimization_history(study)\n",
    "        fig.write_html(os.path.join(self.output_dir, 'optimization_history.html'))\n",
    "        \n",
    "        # Pareto front\n",
    "        if len(study.best_trials) > 1:\n",
    "            fig = vis.plot_pareto_front(study, targets=['FID', 'Biological Score'])\n",
    "            fig.write_html(os.path.join(self.output_dir, 'pareto_front.html'))\n",
    "        \n",
    "        print(f\"Visualization plots saved to {self.output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scRNA-seq data...\n",
      "Initializing Grok optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/torch_env/lib/python3.11/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Load your data\n",
    "    print(\"Loading scRNA-seq data...\")\n",
    "    mouse_adata = ad.read_h5ad(\"/Users/guyshani/Documents/PHD/Aim_2/PBMC_data/mouse/train_data_library_counts_PBMC.h5ad\")\n",
    "    human_adata = ad.read_h5ad(\"/Users/guyshani/Documents/PHD/Aim_2/PBMC_data/human/320k_test/train_data_library_counts_PBMC_human.h5ad\")\n",
    "    \n",
    "    # Create optimizer\n",
    "    print(\"Initializing Grok optimizer...\")\n",
    "    optimizer = GrokOptimizer(mouse_adata, human_adata, output_dir='optuna_grok_results')\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"Starting hyperparameter optimization...\")\n",
    "    study = optimizer.run_optimization(\n",
    "        n_trials=100,  # Adjust based on your computational budget\n",
    "        n_jobs=1       # Set to number of GPUs for parallel trials\n",
    "    )\n",
    "    \n",
    "    print(\"\\nOptimization completed!\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nTraining final model with best parameters...\")\n",
    "    best_trial = min(study.best_trials, key=lambda t: t.values[2])\n",
    "    best_config = optimizer.suggest_hyperparameters(best_trial)\n",
    "    \n",
    "    # You can now use best_config to train your final model\n",
    "    print(f\"Best configuration found:\")\n",
    "    print(json.dumps(best_config, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
