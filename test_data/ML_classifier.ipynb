{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "from cross_validation import CrossValidationFramework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "-----Real data-----\n",
      "Original number of samples: 11854\n",
      "Samples after filtering: 11854\n",
      "Removed classes with < 10 samples\n",
      "-----Generated data-----\n",
      "Original number of samples: 11854\n",
      "Samples after filtering: 11854\n",
      "Removed classes with < 10 samples\n",
      "\n",
      "Performing cross-validation for tree-based models...\n",
      "\n",
      "Cross-validating random_forest...\n",
      "  Scenario: generated_only\n",
      "    Fold 1: Balanced accuracy = 0.0853, Macro F1 = 0.0640,Weighted ROC AUC = 0.5112, Time = 152.38s\n",
      "    Fold 2: Balanced accuracy = 0.0848, Macro F1 = 0.0635,Weighted ROC AUC = 0.4934, Time = 151.56s\n",
      "    Fold 3: Balanced accuracy = 0.0843, Macro F1 = 0.0623,Weighted ROC AUC = 0.5100, Time = 158.91s\n",
      "    Fold 4: Balanced accuracy = 0.0829, Macro F1 = 0.0605,Weighted ROC AUC = 0.4947, Time = 211.71s\n",
      "\n",
      "Final Results:\n",
      "\n",
      "Tree-based Models:\n",
      "\n",
      "random_forest:\n",
      "  generated_only:\n",
      "    Mean Accuracy: 0.0843 ± 0.0009\n",
      "    Mean Training Time: 168.64s\n",
      "\n",
      "Neural Network:\n"
     ]
    }
   ],
   "source": [
    "def main(selected_models=None, selected_scenarios=None):\n",
    "    # Device configuration\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set directory paths\n",
    "    output_dir = \"/Users/guyshani/Documents/PHD/Aim_2/test_models/full_data_counts/run_20250302_091205_dataset+myannotations/\"\n",
    "    data_path = \"/Users/guyshani/Documents/PHD/Aim_2/PBMC_data/mouse/\"\n",
    "\n",
    "    # Load files\n",
    "    generated_h5ad = os.path.join(output_dir, \"generated_data.h5ad\")  # The newly generated h5ad file\n",
    "    real_h5ad = os.path.join(data_path, \"test_data_library_counts_PBMC.h5ad\")  # The original h5ad file\n",
    "    adataGen = ad.read_h5ad(generated_h5ad)\n",
    "    adataReal = ad.read_h5ad(real_h5ad)\n",
    "\n",
    "    real_data = adataReal.X.toarray()\n",
    "    labels_real = adataReal.obs['myannotations']\n",
    "    gen_data = adataGen.X\n",
    "    labels_gen = adataGen.obs['myannotations']\n",
    "\n",
    "    ### Real data ###\n",
    "    # Convert cluster labels to numeric\n",
    "    unique_clusters = sorted(set(labels_real.values))\n",
    "    cluster_dict = {t: i for i, t in enumerate(unique_clusters)}\n",
    "    labels_real = np.vectorize(lambda t: cluster_dict[t])(labels_real).flatten()\n",
    "    # Count samples per class\n",
    "    class_counts = np.bincount(labels_real)\n",
    "    # Keep only classes with at least n_splits samples\n",
    "    min_samples = 10  \n",
    "    valid_classes = np.where(class_counts >= min_samples)[0]\n",
    "    mask = np.isin(labels_real, valid_classes)\n",
    "    # Filter data and labels\n",
    "    real_data = real_data[mask]\n",
    "    labels_real = labels_real[mask]\n",
    "    \n",
    "    print(\"-----Real data-----\")\n",
    "    print(f\"Original number of samples: {len(mask)}\")\n",
    "    print(f\"Samples after filtering: {len(labels_real)}\")\n",
    "    print(f\"Removed classes with < {min_samples} samples\")\n",
    "\n",
    "    ### Generated data ###\n",
    "    # Convert cluster labels to numeric\n",
    "    unique_clusters = sorted(set(labels_gen.values))\n",
    "    cluster_dict = {t: i for i, t in enumerate(unique_clusters)}\n",
    "    labels_gen = np.vectorize(lambda t: cluster_dict[t])(labels_gen).flatten()\n",
    "    # Count samples per class\n",
    "    class_counts = np.bincount(labels_gen)\n",
    "    # Keep only classes with at least n_splits samples\n",
    "    valid_classes = np.where(class_counts >= min_samples)[0]\n",
    "    mask = np.isin(labels_gen, valid_classes)\n",
    "    # Filter data and labels\n",
    "    gen_data = gen_data[mask]\n",
    "    labels_gen = labels_gen[mask]\n",
    "    \n",
    "    print(\"-----Generated data-----\")\n",
    "    print(f\"Original number of samples: {len(mask)}\")\n",
    "    print(f\"Samples after filtering: {len(labels_gen)}\")\n",
    "    print(f\"Removed classes with < {min_samples} samples\")\n",
    "    \n",
    "    # Create framework instance with selected models\n",
    "    cv_framework = CrossValidationFramework(\n",
    "        n_splits=4,\n",
    "        nn_hidden_dims=[512, 256, 128],\n",
    "        selected_models=selected_models,\n",
    "        selected_scenarios=selected_scenarios\n",
    "    )\n",
    "    \n",
    "    # Perform cross-validation for tree-based models if any are selected\n",
    "    tree_based = ['decision_tree', 'random_forest', 'gradient_boosting', 'xgboost']\n",
    "    if any(model in selected_models for model in tree_based):\n",
    "        print(\"\\nPerforming cross-validation for tree-based models...\")\n",
    "        tree_results = cv_framework.cross_validate_trees(\n",
    "            real_data=real_data,\n",
    "            real_labels=labels_real,\n",
    "            generated_data=gen_data,\n",
    "            generated_labels=labels_gen\n",
    "        )\n",
    "    else:\n",
    "        tree_results = {}\n",
    "    \n",
    "    # Perform cross-validation for neural network if selected\n",
    "    if 'neural_network' in selected_models:\n",
    "        print(\"\\nPerforming cross-validation for neural network...\")\n",
    "        nn_results = cv_framework.cross_validate_nn(\n",
    "            real_data=real_data,\n",
    "            real_labels=labels_real,\n",
    "            generated_data=gen_data,\n",
    "            generated_labels=labels_gen,\n",
    "            batch_size=32,\n",
    "            epochs=50,\n",
    "            device=device\n",
    "        )\n",
    "    else:\n",
    "        nn_results = {}\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    \n",
    "    print(\"\\nTree-based Models:\")\n",
    "    for model_name, model_results in tree_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for scenario, metrics in model_results.items():\n",
    "            print(f\"  {scenario}:\")\n",
    "            print(f\"    Mean Accuracy: {metrics['mean_accuracy']:.4f} ± {metrics['std_accuracy']:.4f}\")\n",
    "            print(f\"    Mean Training Time: {metrics['mean_time']:.2f}s\")\n",
    "    \n",
    "    print(\"\\nNeural Network:\")\n",
    "    for scenario, metrics in nn_results.items():\n",
    "        print(f\"  {scenario}:\")\n",
    "        print(f\"    Mean Accuracy: {metrics['mean_accuracy']:.4f} ± {metrics['std_accuracy']:.4f}\")\n",
    "        print(f\"    Mean Training Time: {metrics['mean_time']:.2f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    selected_models = ['random_forest']\n",
    "    selected_scenarios = ['generated_only']\n",
    "    main(selected_models=selected_models, selected_scenarios=selected_scenarios)\n",
    "\n",
    "    # Run only random forest and neural network\n",
    "    # main(['random_forest', 'neural_network'])\n",
    "    # Run all models\n",
    "    # main()  # or main(None)\n",
    "    # Run only tree-based models\n",
    "    # ['real_only','generated_only', 'mixed']\n",
    "    # main(['decision_tree', 'random_forest', 'gradient_boosting', 'xgboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_G = \"/Users/guyshani/Documents/PHD/Aim_2/test_models/run_20250201_161547_dataset_singler_label/\"\n",
    "# Load expression matrix\n",
    "with h5py.File(data_path_G+'_generated_data.h5', 'r') as f:\n",
    "    gen_data = f['matrix'][:]\n",
    "# Load cluster info\n",
    "meta_data_gen = pd.read_csv(data_path_G+'_generated_labels.csv')\n",
    "labels_gen = meta_data_gen['cell_type']\n",
    "# Convert cluster labels to numeric\n",
    "unique_clusters = sorted(set(labels_gen.values.flatten()))\n",
    "cluster_dict = {t: i for i, t in enumerate(unique_clusters)}\n",
    "labels_gen = np.vectorize(lambda t: cluster_dict[t])(labels_gen).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ..., 13, 13, 13])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_gen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
