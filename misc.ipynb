{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time as t\n",
    "import random\n",
    "from itertools import product\n",
    "import concurrent.futures\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load real and generated scRNA-seq data\n",
    "run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/test_models/run_20250201_161547_dataset_singler_label/\"\n",
    "data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/30_1_2025__normalized/\"\n",
    "\n",
    "df_gen = pd.read_csv(f'{run_dir}_generated_data.csv')   # load generated data\n",
    "cell_type = df_gen['cell_type']   # get cell type column\n",
    "dataset = df_gen['dataset'] # get dataset column\n",
    "df_gen = df_gen.drop(df_gen.columns[-2:], axis=1)   # drop label columns\n",
    "df_real = pd.read_csv(f'{data_path}combined_data.csv', sep=\";\")  # load real data\n",
    "df_gen.columns = df_real.columns # add gene names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape: (41588, 2000)\n",
      "x_gen shape: (41588, 2000)\n"
     ]
    }
   ],
   "source": [
    "# process data\n",
    "## replace \",\" with \".\" and convert string to float for the real data we loaded\n",
    "df_real = df_real.replace(',', '.', regex=True)\n",
    "df_real = df_real.apply(pd.to_numeric)\n",
    "df_real = df_real.to_numpy()\n",
    "df_gen = df_gen.to_numpy()\n",
    "# test that both matrices have the same size\n",
    "print(\"x_test shape:\", df_real.shape)\n",
    "print(\"x_gen shape:\", df_gen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation matrix calculations\n",
    "def upper_diag_list(m_):\n",
    "   \"\"\"Returns upper triangular values of matrix, excluding diagonal\"\"\"\n",
    "   # np.triu_indices gets indices of upper triangle, k=1 excludes diagonal\n",
    "   return m_[np.triu_indices(len(m_), k=1)]\n",
    "\n",
    "def standardize(a):\n",
    "    \"\"\"Standardizes data by subtracting mean and dividing by std dev (z-score)\"\"\"\n",
    "    a_off = np.mean(a, axis=0)  # Calculate mean\n",
    "    a_std = np.std(a, axis=0)   # Calculate standard deviation\n",
    "    S = (a - a_off) / a_std     # Z-score standardization\n",
    "    S[np.isnan(S)] = (a - a_off)[np.isnan(S)]  # Handle zeros in std\n",
    "    return S\n",
    "def pearson_correlation(x: np.array, y: np.array):\n",
    "    \"\"\"Computes correlation between each pair of vectors in x and y matrices\"\"\"\n",
    "    assert x.shape[0] == y.shape[0] # Check input matrices have same number of samples\n",
    "    x_ = standardize(x)\n",
    "    y_ = standardize(y)\n",
    "    return np.dot(x_.T, y_) / x.shape[0]  # Compute correlation as dot product of standardized values\n",
    "\n",
    "def correlations_list(x: np.array, y: np.array):\n",
    "    \"\"\"Gets list of correlations between all pairs\"\"\"\n",
    "    corr = pearson_correlation(x, y)\n",
    "    return upper_diag_list(corr)\n",
    "\n",
    "def gamma_coeff_score(x_test: np.array, x_gen: np.array):\n",
    "    \"\"\"Calculates correlation between distance matrices of test and generated data\"\"\"\n",
    "    assert x_test.shape[1] == x_gen.shape[1]    # Verify matrices have same number of features\n",
    "    dists_x = 1 - correlations_list(x_test, x_test)\n",
    "    dists_y = 1 - correlations_list(x_gen, x_gen)\n",
    "    # Reshape for pearson_correlation, to column vectors and compute correlation between distances\n",
    "    return pearson_correlation(dists_x.reshape(-1, 1), dists_y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6233977]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pearson correlation between correlation matrices\n",
    "g = gamma_coeff_score(df_real, df_gen)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_pairwise_distances(U: torch.tensor, V: torch.tensor):\n",
    "    \"\"\"Compute pairwise distances between two batches of feature vectors.\n",
    "    ----\n",
    "    Parameters:\n",
    "        U (torch.tensor): first feature vector\n",
    "        V (torch.tensor): second feature vector\n",
    "    Returns:\n",
    "        tensor of pairwise distances \"\"\"\n",
    "\n",
    "    # Squared norms of each row in U and V.\n",
    "    norm_u = torch.sum(torch.square(U), 1)\n",
    "    norm_v = torch.sum(torch.square(V), 1)\n",
    "\n",
    "    # norm_u as a column and norm_v as a row vectors.\n",
    "    norm_u = torch.reshape(norm_u, (-1, 1))\n",
    "    norm_v = torch.reshape(norm_v, (1, -1))\n",
    "\n",
    "    # Pairwise squared Euclidean distances.\n",
    "    D = torch.maximum(norm_u - 2 * torch.matmul(U, V.T) +\n",
    "                      norm_v, torch.zeros((1,)))\n",
    "\n",
    "    return D\n",
    "\n",
    "\n",
    "class ManifoldEstimator():\n",
    "    \"\"\"Estimates the manifold of given feature vectors.\"\"\"\n",
    "\n",
    "    def __init__(self, features, row_batch_size=25000, col_batch_size=50000,\n",
    "                 nhood_sizes=[3], clamp_to_percentile=None, eps=1e-5):\n",
    "        \"\"\"Estimate the manifold of given feature vectors.\n",
    "\n",
    "            Args:\n",
    "                features (np.array/tf.Tensor): Matrix of feature vectors to estimate their manifold.\n",
    "                row_batch_size (int): Row batch size to compute pairwise distances\n",
    "                    (parameter to trade-off between memory usage and performance).\n",
    "                col_batch_size (int): Column batch size to compute pairwise distances.\n",
    "                nhood_sizes (list): Number of neighbors used to estimate the manifold.\n",
    "                clamp_to_percentile (float): Prune hyperspheres that have radius larger than\n",
    "                    the given percentile.\n",
    "                eps (float): Small number for numerical stability.\n",
    "        \"\"\"\n",
    "        batch_size = features.shape[0]\n",
    "        self.nhood_sizes = nhood_sizes\n",
    "        self.num_nhoods = len(nhood_sizes)\n",
    "        self.eps = eps\n",
    "        self.row_batch_size = row_batch_size\n",
    "        self.col_batch_size = col_batch_size\n",
    "        self._ref_features = features\n",
    "\n",
    "        # Estimate manifold of features by calculating distances to k-NN of\n",
    "        # each sample.\n",
    "        self.D = np.zeros([batch_size, self.num_nhoods], dtype=np.float32)\n",
    "        distance_batch = np.zeros(\n",
    "            [row_batch_size, batch_size], dtype=np.float32)\n",
    "        seq = np.arange(max(self.nhood_sizes) + 1, dtype=np.int32)\n",
    "\n",
    "        for begin1 in range(0, batch_size, row_batch_size):\n",
    "            end1 = min(begin1 + row_batch_size, batch_size)\n",
    "            row_batch = features[begin1:end1]\n",
    "\n",
    "            for begin2 in range(0, batch_size, col_batch_size):\n",
    "                end2 = min(begin2 + col_batch_size, batch_size)\n",
    "                col_batch = features[begin2:end2]\n",
    "\n",
    "                # Compute distances between batches.\n",
    "                distance_batch[0:end1 - begin1,\n",
    "                               begin2:end2] = batch_pairwise_distances(row_batch,\n",
    "                                                                       col_batch)\n",
    "\n",
    "            # Find the k-nearest neighbor from the current batch.\n",
    "            self.D[begin1:end1, :] = np.partition(\n",
    "                distance_batch[0:end1 - begin1, :], seq, axis=1)[:, self.nhood_sizes]\n",
    "\n",
    "        if clamp_to_percentile is not None:\n",
    "            max_distances = np.percentile(self.D, clamp_to_percentile, axis=0)\n",
    "            self.D[self.D > max_distances] = 0\n",
    "\n",
    "    def evaluate(\n",
    "            self,\n",
    "            eval_features,\n",
    "            return_realism=False,\n",
    "            return_neighbors=False):\n",
    "        \"\"\"Evaluate if new feature vectors are at the manifold.\n",
    "        \"\"\"\n",
    "        num_eval = eval_features.shape[0]\n",
    "        num_ref = self.D.shape[0]\n",
    "        distance_batch = np.zeros(\n",
    "            [self.row_batch_size, num_ref], dtype=np.float32)\n",
    "        batch_predictions = np.zeros(\n",
    "            [num_eval, self.num_nhoods], dtype=np.int32)\n",
    "        max_realism_score = np.zeros([num_eval,], dtype=np.float32)\n",
    "        nearest_indices = np.zeros([num_eval,], dtype=np.int32)\n",
    "\n",
    "        for begin1 in range(0, num_eval, self.row_batch_size):\n",
    "            end1 = min(begin1 + self.row_batch_size, num_eval)\n",
    "            feature_batch = eval_features[begin1:end1]\n",
    "\n",
    "            for begin2 in range(0, num_ref, self.col_batch_size):\n",
    "                end2 = min(begin2 + self.col_batch_size, num_ref)\n",
    "                ref_batch = self._ref_features[begin2:end2]\n",
    "\n",
    "                distance_batch[0:end1 - begin1,\n",
    "                               begin2:end2] = batch_pairwise_distances(feature_batch,\n",
    "                                                                       ref_batch)\n",
    "\n",
    "            # From the minibatch of new feature vectors, determine if they are in the estimated manifold.\n",
    "            # If a feature vector is inside a hypersphere of some reference sample, then\n",
    "            # the new sample lies at the estimated manifold.\n",
    "            # The radii of the hyperspheres are determined from distances of\n",
    "            # neighborhood size k.\n",
    "            samples_in_manifold = distance_batch[0:end1 -\n",
    "                                                 begin1, :, None] <= self.D\n",
    "            batch_predictions[begin1:end1] = np.any(\n",
    "                samples_in_manifold, axis=1).astype(np.int32)\n",
    "\n",
    "            max_realism_score[begin1:end1] = np.max(\n",
    "                self.D[:, 0] / (distance_batch[0:end1 - begin1, :] + self.eps), axis=1)\n",
    "            nearest_indices[begin1:end1] = np.argmin(\n",
    "                distance_batch[0:end1 - begin1, :], axis=1)\n",
    "\n",
    "        if return_realism and return_neighbors:\n",
    "            return batch_predictions, max_realism_score, nearest_indices\n",
    "        elif return_realism:\n",
    "            return batch_predictions, max_realism_score\n",
    "        elif return_neighbors:\n",
    "            return batch_predictions, nearest_indices\n",
    "\n",
    "        return batch_predictions\n",
    "\n",
    "\n",
    "def knn_precision_recall_features(\n",
    "        ref_features,\n",
    "        eval_features,\n",
    "        nhood_sizes=[3],\n",
    "        row_batch_size=10000,\n",
    "        col_batch_size=50000,\n",
    "        num_gpus=1):\n",
    "    \"\"\"Calculates k-NN precision and recall for two sets of feature vectors.\n",
    "\n",
    "        Args:\n",
    "            ref_features (np.array/tf.Tensor): Feature vectors of reference samples.\n",
    "            eval_features (np.array/tf.Tensor): Feature vectors of generated samples.\n",
    "            nhood_sizes (list): Number of neighbors used to estimate the manifold.\n",
    "            row_batch_size (int): Row batch size to compute pairwise distances\n",
    "                (parameter to trade-off between memory usage and performance).\n",
    "            col_batch_size (int): Column batch size to compute pairwise distances.\n",
    "            num_gpus (int): Number of GPUs used to evaluate precision and recall.\n",
    "        Returns:\n",
    "            State (dict): Dict that contains precision and recall calculated from\n",
    "            ref_features and eval_features.\n",
    "    \"\"\"\n",
    "    state = dict()\n",
    "    num_data = ref_features.shape[0]\n",
    "    num_features = ref_features.shape[1]\n",
    "\n",
    "    # Initialize ManifoldEstimators.\n",
    "    ref_manifold = ManifoldEstimator(\n",
    "        ref_features,\n",
    "        row_batch_size,\n",
    "        col_batch_size,\n",
    "        nhood_sizes)\n",
    "    eval_manifold = ManifoldEstimator(\n",
    "        eval_features,\n",
    "        row_batch_size,\n",
    "        col_batch_size,\n",
    "        nhood_sizes)\n",
    "\n",
    "    # Evaluate precision and recall using k-nearest neighbors.\n",
    "    #print('Evaluating k-NN precision and recall with %i samples...' % num_data)\n",
    "    start = t.time()\n",
    "\n",
    "    # Precision: How many points from eval_features are in ref_features\n",
    "    # manifold.\n",
    "    precision = ref_manifold.evaluate(eval_features)\n",
    "    state['precision'] = precision.mean(axis=0)\n",
    "\n",
    "    # Recall: How many points from ref_features are in eval_features manifold.\n",
    "    recall = eval_manifold.evaluate(ref_features)\n",
    "    state['recall'] = recall.mean(axis=0)\n",
    "\n",
    "    #print('Evaluated k-NN precision and recall in: %gs' % (t.time() - start))\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def get_precision_recall(\n",
    "        real_data: torch.tensor,\n",
    "        fake_data: torch.tensor,\n",
    "        nb_nn: list = [50]):\n",
    "    \"\"\"\n",
    "    Compute precision and recall between datasets.\n",
    "    ----\n",
    "    Parameters:\n",
    "        real_data (torch.tensor): First data set of comparison.\n",
    "        fake_data (torch.tensor): Second dataset to use for comparison.\n",
    "        nb_nn (list): Number of neighbors used to estimate the data manifold.\n",
    "    Returns:\n",
    "        tuple with precision and recall.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate k-NN precision and recall.\n",
    "    state = knn_precision_recall_features(\n",
    "        real_data, fake_data, nhood_sizes=nb_nn)\n",
    "\n",
    "    precision = state['precision'][0]\n",
    "    recall = state['recall'][0]\n",
    "\n",
    "    return (precision, recall)\n",
    "\n",
    "\n",
    "def get_realism_score(real_data: torch.tensor, fake_data: torch.tensor):\n",
    "    \"\"\"\n",
    "    Compute realism score between datasets.\n",
    "    ----\n",
    "    Parameters:\n",
    "        real_data (torch.tensor): First data set of comparison.\n",
    "        fake_data (torch.tensor): Second dataset to use for comparison.\n",
    "    Returns:\n",
    "        Maximum realism score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Estimate manifold of real images.\n",
    "    print('Estimating manifold of real data...')\n",
    "    real_manifold = ManifoldEstimator(real_data, clamp_to_percentile=50)\n",
    "\n",
    "    # Estimate quality of individual samples.\n",
    "    _, realism_scores = real_manifold.evaluate(fake_data, return_realism=True)\n",
    "\n",
    "    return realism_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating manifold of real data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g_/c_f20pc566n31j9p36lcdc0w0000gn/T/ipykernel_12416/4189303324.py:66: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  distance_batch[0:end1 - begin1,\n",
      "/var/folders/g_/c_f20pc566n31j9p36lcdc0w0000gn/T/ipykernel_12416/4189303324.py:102: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  distance_batch[0:end1 - begin1,\n"
     ]
    }
   ],
   "source": [
    "rc = get_realism_score(torch.tensor(df_real), torch.tensor(df_gen))\n",
    "pr = get_precision_recall(torch.tensor(df_real), torch.tensor(df_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realism score: [1.2204499 1.3169284 1.4032382 ... 1.6623068 1.6903054 1.4691069]\n",
      "Precision-Recall: (np.float64(0.8418053284601328), np.float64(0.0028373569298836203))\n"
     ]
    }
   ],
   "source": [
    "print(\"Realism score: \"+str(rc))\n",
    "print(\"Precision-Recall: \"+str(pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED\n",
    "np.random.seed(42)\n",
    "\n",
    "# AA class object\n",
    "class NearestNeighborMetrics():\n",
    "    \"\"\"Calculate nearest neighbors and metrics\"\"\"\n",
    "\n",
    "    def __init__(self, real_data, synths):\n",
    "        self.data = {'train': real_data}\n",
    "\n",
    "        # add all synthetics\n",
    "        # for i, s in enumerate(synths):\n",
    "        #self.data[f'synth_{i}'] = s.reshape(1,-1)\n",
    "        #self.synth_keys = [f'synth_{i}' for i in range(len(synths))]\n",
    "        self.data[f'synth_0'] = synths\n",
    "        self.synth_keys = [f'synth_0']\n",
    "\n",
    "        # pre allocate distances\n",
    "        self.dists = {}\n",
    "\n",
    "    def nearest_neighbors(self, t, s):\n",
    "        \"\"\"Find nearest neighbors d_ts and d_ss\"\"\"\n",
    "        # fit to S\n",
    "        nn_s = NearestNeighbors(n_neighbors=1).fit(self.data[s])\n",
    "        if t == s:\n",
    "            # find distances from s to s\n",
    "            d = nn_s.kneighbors()[0]\n",
    "        else:\n",
    "            # find distances from t to s\n",
    "            d = nn_s.kneighbors(self.data[t])[0]\n",
    "        return t, s, d\n",
    "\n",
    "    def compute_nn(self):\n",
    "        \"\"\"run all the nearest neighbors calculations\"\"\"\n",
    "        tasks = product(self.data.keys(), repeat=2)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = [\n",
    "                executor.submit(self.nearest_neighbors, t, s)\n",
    "                for (t, s) in tasks\n",
    "            ]\n",
    "            # wait for each job to finish\n",
    "            for future in tqdm(\n",
    "                    concurrent.futures.as_completed(futures),\n",
    "                    total=len(futures)):\n",
    "                t, s, d = future.result()\n",
    "                self.dists[(t, s)] = d\n",
    "\n",
    "    def divergence(self, t, s):\n",
    "        \"\"\"calculate the NN divergence\"\"\"\n",
    "        left = np.mean(np.log(self.dists[(t, s)] / self.dists[(t, t)]))\n",
    "        right = np.mean(np.log(self.dists[(s, t)] / self.dists[(s, s)]))\n",
    "        return 0.5 * (left + right)\n",
    "\n",
    "    def discrepancy_score(self, t, s):\n",
    "        \"\"\"calculate the NN discrepancy score\"\"\"\n",
    "        left = np.mean(self.dists[(t, s)])\n",
    "        right = np.mean(self.dists[(s, t)])\n",
    "        return 0.5 * (left + right)\n",
    "\n",
    "    def adversarial_accuracy(self, t, s):\n",
    "        \"\"\"calculate the NN adversarial accuracy\"\"\"\n",
    "        left = np.mean(self.dists[(t, s)] > self.dists[(t, t)])\n",
    "        right = np.mean(self.dists[(s, t)] > self.dists[(s, s)])\n",
    "        return 0.5 * (left + right)\n",
    "\n",
    "    def compute_discrepancy(self):\n",
    "        \"\"\"compute the standard discrepancy scores\"\"\"\n",
    "        j_rr = self.discrepancy_score('train', 'train')\n",
    "        j_ra = []\n",
    "        j_aa = []\n",
    "        # for all of the synthetic datasets\n",
    "        for k in self.synth_keys:\n",
    "            j_ra.append(self.discrepancy_score('train', k))\n",
    "            # comparison to other synthetics\n",
    "            for k_2 in self.synth_keys:\n",
    "                if k != k_2:\n",
    "                    j_aa.append(self.discrepancy_score(k, k_2))\n",
    "\n",
    "        # average accross synthetics\n",
    "        j_ra = np.mean(np.array(j_ra))\n",
    "        j_aa = np.mean(np.array(j_aa))\n",
    "        return j_rr, j_ra, j_aa\n",
    "\n",
    "    def compute_divergence(self):\n",
    "        \"\"\"compute the standard divergence scores\"\"\"\n",
    "        d_tr_a = []\n",
    "        for k in self.synth_keys:\n",
    "            d_tr_a.append(self.divergence('train', k))\n",
    "\n",
    "        training = np.mean(np.array(d_tr_a))\n",
    "        return training\n",
    "\n",
    "    def compute_adversarial_accuracy(self):\n",
    "        \"\"\"compute the standarad adversarial accuracy scores\"\"\"\n",
    "        a_tr_a = []\n",
    "        for k in self.synth_keys:\n",
    "            a_tr_a.append(self.adversarial_accuracy('train', k))\n",
    "\n",
    "        a_tr = np.mean(np.array(a_tr_a))\n",
    "        return a_tr\n",
    "\n",
    "# Compute metric\n",
    "\n",
    "\n",
    "def compute_AAts(real_data: np.array = None, fake_data: np.array = None):\n",
    "    \"\"\" Compute similarity scores based on nearest neighbors distances.\n",
    "    ----\n",
    "    Parameters:\n",
    "        real_data (np.array): array of real data\n",
    "        fake_data (np.array): array of synthetic data\n",
    "    Returns:\n",
    "        discrepancy score, divergence score, adversarial accuracy\n",
    "    \"\"\"\n",
    "    nnm = NearestNeighborMetrics(real_data, fake_data)\n",
    "    nnm.compute_nn()\n",
    "\n",
    "    # run discrepancy score, divergence, adversarial accuracy\n",
    "    discrepancy = nnm.compute_discrepancy()\n",
    "    #divergence = nnm.compute_divergence()\n",
    "    adversarial = nnm.compute_adversarial_accuracy()\n",
    "\n",
    "    return discrepancy[0], discrepancy[1], adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:48<00:00, 27.14s/it] \n",
      "/Users/guyshani/Documents/PHD/Aim_2/cycle_GAN/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/guyshani/Documents/PHD/Aim_2/cycle_GAN/.venv/lib/python3.11/site-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "AAts = compute_AAts(df_real, df_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbour Adversarial Accuracy: (np.float64(30.742639678692225), np.float64(25.531596366894455), np.float64(0.5050976243147062))\n"
     ]
    }
   ],
   "source": [
    "print(\"Nearest Neighbour Adversarial Accuracy: \"+str(AAts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(15.69700849603785),\n",
       " np.float64(13.35955319773705),\n",
       " np.float64(0.5131768779455612))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AAts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
