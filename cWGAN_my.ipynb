{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, z_dim, library_size=10000):\n",
    "        \"\"\"\n",
    "        Generator network for conditional GAN with fixed architecture and library size normalization\n",
    "        Args:\n",
    "            x_dim: Dimension of output data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            z_dim: Dimension of latent noise vector\n",
    "            library_size: Target sum for the generated expression values (default: 10000)\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Store library size\n",
    "        self.library_size = library_size\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size)) \n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is latent dim + embedding dim + numeric covariates\n",
    "        input_dim = z_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Fixed architecture with 3 layers: 256 -> 512 -> 1024\n",
    "        self.network = nn.Sequential(\n",
    "            # First layer: input_dim -> 256\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Second layer: 256 -> 512\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Third layer: 512 -> 1024\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Output layer: 1024 -> x_dim\n",
    "            nn.Linear(1024, x_dim),\n",
    "            # No activation here as we'll normalize in forward pass\n",
    "        )\n",
    "\n",
    "    def normalize_to_library_size(self, x):\n",
    "        \"\"\"\n",
    "        Normalize the output tensor so the sum equals the target library size\n",
    "        while ensuring all values are non-negative\n",
    "        \"\"\"\n",
    "        # Apply ReLU to ensure non-negative values\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Add small epsilon to avoid division by zero\n",
    "        epsilon = 1e-10\n",
    "        \n",
    "        # Calculate current sum for each sample\n",
    "        current_sums = x.sum(dim=1, keepdim=True) + epsilon\n",
    "        \n",
    "        # Scale to target library size\n",
    "        normalized = x * (self.library_size / current_sums)\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "    def forward(self, z, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output through network\n",
    "        output = self.network(gen_input)\n",
    "        \n",
    "        # Normalize to library size\n",
    "        normalized_output = self.normalize_to_library_size(output)\n",
    "        \n",
    "        return normalized_output\n",
    "\n",
    "    def get_negative_penalty(self, generated_data):\n",
    "        \"\"\"Calculate penalty for negative values\"\"\"\n",
    "        negative_mask = (generated_data < 0).float()\n",
    "        negative_proportion = negative_mask.mean()\n",
    "        negative_magnitude = (generated_data * negative_mask).abs().mean()\n",
    "        return negative_magnitude, negative_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, use_neg_detector=False):\n",
    "        \"\"\"\n",
    "        Discriminator network with fixed architecture: 1024 -> 512 -> 256 -> 1\n",
    "        Args:\n",
    "            x_dim: Dimension of input data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            use_neg_detector: Whether to use negative value detection\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Store use_neg_detector flag\n",
    "        self.use_neg_detector = use_neg_detector\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size))\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is data dim + embedding dim + numeric covariates\n",
    "        input_dim = x_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Fixed discriminator architecture\n",
    "        self.main_network = nn.Sequential(\n",
    "            # First layer: input_dim -> 1024\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Second layer: 1024 -> 512\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Third layer: 512 -> 256\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Output layer: 256 -> 1\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Add negative value detection branch if enabled\n",
    "        if use_neg_detector:\n",
    "            self.negative_detector = nn.Sequential(\n",
    "                nn.Linear(x_dim, 1024),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(1024, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate inputs for main discrimination\n",
    "        disc_input = torch.cat([x, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Main discrimination score\n",
    "        validity = self.main_network(disc_input)\n",
    "        \n",
    "        # Add negative value detection if enabled\n",
    "        if self.use_neg_detector:\n",
    "            neg_score = self.negative_detector(torch.relu(-x))  # Only pass negative values\n",
    "            return validity - 0.1 * neg_score  # Penalize negative values\n",
    "        \n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, cat_covs, num_covs, device):\n",
    "    \"\"\"\n",
    "    Calculate gradient penalty for WGAN-GP\n",
    "    \"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand((real_samples.size(0), 1), device=device)\n",
    "    \n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    \n",
    "    # Calculate discriminator output for interpolated samples\n",
    "    d_interpolates = discriminator(interpolates, cat_covs, num_covs)\n",
    "    \n",
    "    # Get gradients w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # Calculate gradient penalty\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, cat_covs, num_covs, \n",
    "              config, device, score_fn=None, save_fn=None):\n",
    "    \"\"\"\n",
    "    Train the conditional GAN with progress tracking and proper device handling\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    '''\n",
    "    g_optimizer = optim.RMSprop(generator.parameters(), lr=config['lr'])\n",
    "    d_optimizer = optim.RMSprop(discriminator.parameters(), lr=config['lr'])\n",
    "    '''\n",
    "    # New optimizers with AMSGrad:\n",
    "    g_optimizer = optim.Adam(\n",
    "        generator.parameters(),\n",
    "        lr=config['lr'],\n",
    "        betas=(config.get('beta1', 0.5), config.get('beta2', 0.9)),\n",
    "        eps=config.get('eps', 1e-8),\n",
    "        amsgrad=True)\n",
    "\n",
    "    d_optimizer = optim.Adam(\n",
    "        discriminator.parameters(),\n",
    "        lr=config['lr'],\n",
    "        betas=(config.get('beta1', 0.5), config.get('beta2', 0.9)),\n",
    "        eps=config.get('eps', 1e-8),\n",
    "        amsgrad=True)\n",
    "    \n",
    "    # Training parameters\n",
    "    lambda_gp = config.get('lambda_gp', 10)\n",
    "    grad_clip_value = config.get('grad_clip_value', 1.0)\n",
    "    neg_penalty_start = config.get('negative_penalty_start_epoch', 50)\n",
    "    neg_penalty_ramp = config.get('negative_penalty_ramp_epochs', 50)\n",
    "    max_neg_penalty = config.get('max_negative_penalty', 10.0)\n",
    "    \n",
    "    # Convert covariates to tensors and move to device\n",
    "    cat_covs = torch.tensor(cat_covs, dtype=torch.long).to(device)\n",
    "    num_covs = torch.tensor(num_covs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "    print(f\"Total batches per epoch: {total_batches}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Using negative detector: {discriminator.use_neg_detector}\")\n",
    "    print(f\"Negative penalty starts at epoch: {neg_penalty_start}\")\n",
    "    \n",
    "    def get_negative_penalty_weight(epoch):\n",
    "        \"\"\"Calculate curriculum learning weight for negative penalty\"\"\"\n",
    "        if epoch < neg_penalty_start:\n",
    "            return 0.0\n",
    "        \n",
    "        ramp_progress = (epoch - neg_penalty_start) / neg_penalty_ramp\n",
    "        ramp_progress = min(1.0, max(0.0, ramp_progress))\n",
    "        return max_neg_penalty * ramp_progress\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        g_losses_main = []  # Track main generator loss without penalty\n",
    "        neg_metrics = []\n",
    "        \n",
    "        print(f\"\\nEpoch [{epoch+1}/{config['epochs']}]\")\n",
    "        curr_neg_weight = get_negative_penalty_weight(epoch)\n",
    "        \n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Move real data to device\n",
    "            real_data = real_data.to(device)\n",
    "            \n",
    "            # Get random batch of categorical and numerical covariates\n",
    "            batch_indices = torch.randint(0, cat_covs.size(0), (batch_size,))\n",
    "            batch_cat_covs = cat_covs[batch_indices]\n",
    "            batch_num_covs = num_covs[batch_indices]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(config['nb_critic']):\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "                fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate discriminator outputs\n",
    "                real_validity = discriminator(real_data, batch_cat_covs, batch_num_covs)\n",
    "                fake_validity = discriminator(fake_data.detach(), batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate gradient penalty\n",
    "                gp = compute_gradient_penalty(\n",
    "                    discriminator,\n",
    "                    real_data,\n",
    "                    fake_data.detach(),\n",
    "                    batch_cat_covs,\n",
    "                    batch_num_covs,\n",
    "                    device)\n",
    "                \n",
    "                # Calculate discriminator loss with gradient penalty\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
    "                \n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "            fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "            \n",
    "            # Calculate standard generator loss\n",
    "            fake_validity = discriminator(fake_data, batch_cat_covs, batch_num_covs)\n",
    "            g_loss_main = -torch.mean(fake_validity)\n",
    "            \n",
    "            # Calculate negative penalty\n",
    "            neg_magnitude, neg_proportion = generator.get_negative_penalty(fake_data)\n",
    "            g_loss = g_loss_main + curr_neg_weight * neg_magnitude\n",
    "            \n",
    "            g_loss.backward()\n",
    "            \n",
    "            g_optimizer.step()\n",
    "            \n",
    "            # Track losses and metrics\n",
    "            g_losses.append(g_loss.item())\n",
    "            g_losses_main.append(g_loss_main.item())\n",
    "            neg_metrics.append({\n",
    "                'proportion': neg_proportion.item(),\n",
    "                'magnitude': neg_magnitude.item()\n",
    "            })\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                progress_msg = (\n",
    "                    f\"  Batch [{batch_idx}/{total_batches}] \"\n",
    "                    f\"D_loss: {d_loss.item():.4f}, \"\n",
    "                    f\"G_loss: {g_loss.item():.4f}, \"\n",
    "                    f\"G_main: {g_loss_main.item():.4f}, \"\n",
    "                    f\"Neg_prop: {neg_proportion.item():.3f}, \"\n",
    "                    f\"Neg_mag: {neg_magnitude.item():.3f}, \"\n",
    "                    f\"Neg_weight: {curr_neg_weight:.3f}\"\n",
    "                )\n",
    "                print(progress_msg)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        avg_g_main = np.mean(g_losses_main)\n",
    "        avg_neg_prop = np.mean([m['proportion'] for m in neg_metrics])\n",
    "        avg_neg_mag = np.mean([m['magnitude'] for m in neg_metrics])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average D_loss: {avg_d_loss:.4f}\")\n",
    "        print(f\"  Average G_loss: {avg_g_loss:.4f}\")\n",
    "        print(f\"  Average G_main: {avg_g_main:.4f}\")\n",
    "        print(f\"  Average Neg_prop: {avg_neg_prop:.4f}\")\n",
    "        print(f\"  Average Neg_mag: {avg_neg_mag:.4f}\")\n",
    "        print(f\"  Negative Penalty Weight: {curr_neg_weight:.4f}\")\n",
    "        \n",
    "        # Log metrics\n",
    "        if wandb.run is not None:\n",
    "            metrics = {\n",
    "                'epoch': epoch,\n",
    "                'd_loss': avg_d_loss,\n",
    "                'g_loss': avg_g_loss,\n",
    "                'g_loss_main': avg_g_main,\n",
    "                'negative_proportion': avg_neg_prop,\n",
    "                'negative_magnitude': avg_neg_mag,\n",
    "                'negative_penalty_weight': curr_neg_weight\n",
    "            }\n",
    "            \n",
    "            if discriminator.use_neg_detector:\n",
    "                metrics.update({\n",
    "                    'discriminator_neg_proportion': avg_neg_prop,\n",
    "                    'discriminator_neg_magnitude': avg_neg_mag\n",
    "                })\n",
    "            \n",
    "            wandb.log(metrics)\n",
    "        \n",
    "        # Evaluate and save model if needed\n",
    "        if score_fn is not None and epoch % 10 == 0:\n",
    "            score = score_fn(generator)\n",
    "            print(f'Epoch {epoch}: Score = {score:.4f}')\n",
    "        \n",
    "        if save_fn is not None and epoch % 20 == 0:\n",
    "            save_fn(generator, discriminator, epoch)\n",
    "        \n",
    "        # Save model from last epoch\n",
    "        if epoch == config['epochs'] - 1 and save_fn is not None:\n",
    "            save_fn(generator, discriminator, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(selected_categories=None):\n",
    "    \"\"\"\n",
    "    Train the GAN with selected categorical variables\n",
    "    Args:\n",
    "        selected_categories: List of column names to use as categorical variables.\n",
    "                           If None, uses all columns except 'cell_id'\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'epochs': 300,\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 32,\n",
    "        'lr': 1e-4,\n",
    "        'beta1': 0.5,      # First moment coefficient\n",
    "        'beta2': 0.9,    # Second moment coefficient\n",
    "        'eps': 1e-8,       # Small constant for numerical stability\n",
    "        'nb_critic': 5,\n",
    "        'lambda_gp': 10,\n",
    "        'negative_penalty_start_epoch': 50,\n",
    "        'negative_penalty_ramp_epochs': 50,\n",
    "        'max_negative_penalty': 5.0,\n",
    "        'library_size': 10000 \n",
    "    }\n",
    "    neg_detector = False # True - use negative value detector in the discriminator\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/\"\n",
    "    # data_path = \"/content/drive/MyDrive/Colab_Notebooks/data/5000_genes/\"\n",
    "    \n",
    "    # Load expression matrix\n",
    "    # matrix with cells as columns and genes as rows\n",
    "    with h5py.File(data_path+'combined_normalized_data.h5', 'r') as f:\n",
    "        x_train = f['matrix'][:]\n",
    "    '''\n",
    "    with h5py.File(data_path+'full_matrix_top5000.h5', 'r') as f:\n",
    "        x_train = np.array(f['matrix/data'])\n",
    "    '''\n",
    "\n",
    "    # Load all categorical variables from single file\n",
    "    cat_data = pd.read_csv(data_path+'combined_metadata.csv', sep=';')\n",
    "    print(\"Categorical data shape:\", cat_data.shape)\n",
    "    print(\"Available categorical variables:\", [col for col in cat_data.columns if col != 'cell_id'])\n",
    "    \n",
    "    # Determine which categories to use\n",
    "    if selected_categories is None:\n",
    "        # Use all columns except cell_id\n",
    "        categories_to_use = [col for col in cat_data.columns if col != 'cell_id']\n",
    "    else:\n",
    "        # Validate selected categories\n",
    "        invalid_categories = [cat for cat in selected_categories if cat not in cat_data.columns]\n",
    "        if invalid_categories:\n",
    "            raise ValueError(f\"Invalid categories: {invalid_categories}\")\n",
    "        categories_to_use = selected_categories\n",
    "    \n",
    "    print(f\"\\nUsing categorical variables: {categories_to_use}\")\n",
    "    \n",
    "    # Create dictionaries and inverse mappings for categorical variables\n",
    "    cat_dicts = []\n",
    "    encoded_covs = []\n",
    "    \n",
    "    # Process each selected column as a categorical variable\n",
    "    for column in categories_to_use:\n",
    "        # Get the column data\n",
    "        cat_vec = cat_data[column]\n",
    "        print(f\"\\nProcessing categorical variable: {column}\")\n",
    "        \n",
    "        # Create list of unique category names, sorted\n",
    "        dict_inv = np.array(list(sorted(set(cat_vec.values))))\n",
    "        dict_map = {t: i for i, t in enumerate(dict_inv)}\n",
    "        cat_dicts.append(dict_inv)\n",
    "        \n",
    "        # Convert categorical variables to integers\n",
    "        encoded = np.vectorize(lambda t: dict_map[t])(cat_vec)\n",
    "        encoded = encoded.reshape(-1, 1)  # Reshape to column vector\n",
    "        encoded_covs.append(encoded)\n",
    "        \n",
    "        print(f\"Categories in {column}:\", dict_inv)\n",
    "        print(f\"Number of categories:\", len(dict_inv))\n",
    "    \n",
    "    # Combine all categorical covariates\n",
    "    cat_covs = np.hstack(encoded_covs)\n",
    "    print(\"\\nCombined categorical covariates shape:\", cat_covs.shape)\n",
    "    \n",
    "    # Load numerical covariates (currently empty)\n",
    "    num_covs = np.zeros((x_train.shape[0], 0))\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to device\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)  # Keep on CPU for DataLoader\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(x_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    ############# Initialize models\n",
    "    # Generator\n",
    "    vocab_sizes = [len(c) for c in cat_dicts]\n",
    "    print(\"\\nVocabulary sizes for categorical variables:\", vocab_sizes)\n",
    "    nb_numeric = num_covs.shape[-1]\n",
    "    x_dim = x_train.shape[-1]\n",
    "    \n",
    "    generator = Generator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        z_dim=CONFIG['latent_dim'],\n",
    "        library_size=CONFIG['library_size']).to(device)\n",
    "    \n",
    "    # Discriminator\n",
    "    discriminator = Discriminator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        use_neg_detector=False).to(device)\n",
    "    \n",
    "    # Define save function\n",
    "    def save_models(generator, discriminator, epoch):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # create save directory\n",
    "        categories_str = \"+\".join(categories_to_use)\n",
    "        save_dir = os.path.join(data_path, \"saved_models\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Create run folder\n",
    "        run_dir = os.path.join(save_dir, f\"run_{timestamp}_{categories_str}\")\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "        # Save model initialization parameters\n",
    "        model_config = {\n",
    "            'x_dim': x_dim,\n",
    "            'vocab_sizes': vocab_sizes,\n",
    "            'nb_numeric': nb_numeric,\n",
    "            'h_dims': [CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "            'z_dim': CONFIG['latent_dim'],\n",
    "            'categories': categories_to_use,\n",
    "            'training_config': CONFIG}\n",
    "        config_path = os.path.join(run_dir, 'model_config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(model_config, f, indent=4)\n",
    "        \n",
    "        # Save generator\n",
    "        generator_path = os.path.join(run_dir, f\"generator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(generator.state_dict(), generator_path)\n",
    "        \n",
    "        \n",
    "        # Save discriminator\n",
    "        discriminator_path = os.path.join(run_dir, f\"discriminator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(discriminator.state_dict(), discriminator_path)\n",
    "        \n",
    "        print(f\"\\nModels saved at epoch {epoch + 1}:\")\n",
    "        print(f\"Generator: {generator_path}\")\n",
    "        print(f\"Discriminator: {discriminator_path}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        if wandb.run is not None:\n",
    "            wandb.save(generator_path)\n",
    "            wandb.save(discriminator_path)\n",
    "\n",
    "    # Initialize wandb with unique run name\n",
    "    run_name = f\"run_{int(time.time())}\"  # Uses timestamp for unique name\n",
    "    wandb.init(\n",
    "        project='adversarial_gene_expr',\n",
    "        config=CONFIG,\n",
    "        name=run_name,\n",
    "        reinit=True  # Ensures new run each time\n",
    "    )\n",
    "    \n",
    "    # Add selected categories to wandb config\n",
    "    wandb.config.update({'selected_categories': categories_to_use})\n",
    "    \n",
    "    # Train model\n",
    "    train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=train_loader,\n",
    "        cat_covs=cat_covs,\n",
    "        num_covs=num_covs,\n",
    "        config=CONFIG,\n",
    "        device=device,\n",
    "        save_fn=save_models\n",
    "        #save_fn=None\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    # Use specific categories:\n",
    "    main(selected_categories=['dataset','singler_label'])\n",
    "    \n",
    "    # Or use all available categories:\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for data gneration\n",
    "def inspect_generator_dims(generator):\n",
    "    \"\"\"\n",
    "    Inspect the generator's dimensions and architecture\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Generator model\n",
    "    \n",
    "    Returns:\n",
    "        dict containing dimension information\n",
    "    \"\"\"\n",
    "    # Get embedding dimensions\n",
    "    embedding_dims = [emb.embedding_dim for emb in generator.embeddings]\n",
    "    total_embedding_dim = sum(embedding_dims)\n",
    "    \n",
    "    # Get first layer dimension\n",
    "    first_layer_in_dim = generator.network[0].in_features\n",
    "    \n",
    "    return {\n",
    "        'embedding_dims': embedding_dims,\n",
    "        'total_embedding_dim': total_embedding_dim,\n",
    "        'first_layer_in_dim': first_layer_in_dim,\n",
    "        'recommended_latent_dim': first_layer_in_dim - total_embedding_dim\n",
    "    }\n",
    "\n",
    "def generate_expression_profiles(generator, n_samples, dataset_category, cell_type_category, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate gene expression profiles using the trained cWGAN generator\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained Generator model\n",
    "        n_samples: Number of profiles to generate\n",
    "        dataset_category: Integer indicating which dataset category to generate\n",
    "        cell_type_category: Integer indicating which cell type to generate\n",
    "        device: Device to run generation on ('cuda', 'mps', or 'cpu')\n",
    "        debug: If True, print debugging information\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of generated expression profiles with shape (n_samples, n_genes)\n",
    "    \"\"\"\n",
    "    # Set generator to eval mode\n",
    "    generator.eval()\n",
    "    \n",
    "    # Inspect dimensions\n",
    "    dims = inspect_generator_dims(generator)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Generator dimensions:\")\n",
    "        for k, v in dims.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    \n",
    "    # Create latent vectors\n",
    "    latent_dim = dims['recommended_latent_dim']\n",
    "    z = torch.randn(n_samples, latent_dim, device=device)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nLatent vector shape: {z.shape}\")\n",
    "    \n",
    "    # Create categorical condition tensor with dataset and cell type\n",
    "    num_embeddings = len(generator.embeddings)\n",
    "    cat_covs = torch.zeros((n_samples, num_embeddings), dtype=torch.long, device=device)\n",
    "    cat_covs[:, 0] = dataset_category  # Set dataset category\n",
    "    cat_covs[:, 1] = cell_type_category  # Set cell type category\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Categorical covariates shape: {cat_covs.shape}\")\n",
    "        print(f\"Number of embedding layers: {num_embeddings}\")\n",
    "    \n",
    "    # Create empty numeric covariates tensor\n",
    "    num_covs = torch.zeros((n_samples, 0), device=device)\n",
    "    \n",
    "    # Generate samples\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings\n",
    "            embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(generator.embeddings)]\n",
    "            embedded = torch.cat(embeddings, dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Embedded shape: {embedded.shape}\")\n",
    "            \n",
    "            # Concatenate inputs\n",
    "            gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Generator input shape: {gen_input.shape}\")\n",
    "                print(f\"First layer input dim: {generator.network[0].in_features}\")\n",
    "                print(f\"First layer weight shape: {generator.network[0].weight.shape}\")\n",
    "            \n",
    "            # Generate samples\n",
    "            fake_samples = generator.network(gen_input)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(\"\\nError during generation:\")\n",
    "        print(e)\n",
    "        print(\"\\nGenerator architecture:\")\n",
    "        print(generator)\n",
    "        raise\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    return fake_samples.cpu().numpy()\n",
    "\n",
    "def generate_and_save_profiles(generator, samples_per_combination, save_path, cell_type_names, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate expression profiles using the trained generator.\n",
    "    \n",
    "    Args:\n",
    "        generator: Trained generator model\n",
    "        samples_per_combination: Dictionary with (dataset_num, cell_type_num) keys and number of samples as values\n",
    "        save_path: Where to save the generated data\n",
    "        cell_type_names: Dictionary mapping cell type indices to their names\n",
    "        device: Device to use for generation\n",
    "        debug: Whether to print debug information\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "    all_categories = []\n",
    "    \n",
    "    # Generate samples for specified combinations\n",
    "    for (dataset_num, cell_type_num), n_samples in samples_per_combination.items():\n",
    "        dataset_category = dataset_num - 1  # Convert dataset number (1-7) to category index (0-6)\n",
    "        cell_type_category = cell_type_num\n",
    "        cell_type_name = cell_type_names[cell_type_num]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nGenerating {n_samples} samples for dataset{dataset_num}, {cell_type_name}\")\n",
    "        \n",
    "        samples = generate_expression_profiles(\n",
    "            generator, \n",
    "            n_samples, \n",
    "            dataset_category,\n",
    "            cell_type_category,\n",
    "            device,\n",
    "            debug=debug\n",
    "        )\n",
    "        all_samples.append(samples)\n",
    "        all_categories.extend([f'dataset{dataset_num}_{cell_type_name}'] * n_samples)\n",
    "\n",
    "    print(\"Save location: \"+str(save_path))\n",
    "\n",
    "    # Combine all samples\n",
    "    all_samples = np.vstack(all_samples)\n",
    "    \n",
    "    # Save generated profiles\n",
    "    np.save(f'{save_path}_profiles.npy', all_samples)\n",
    "    \n",
    "    # Save category labels\n",
    "    with open(f'{save_path}_categories.txt', 'w') as f:\n",
    "        for category in all_categories:\n",
    "            f.write(f'{category}\\n')\n",
    "\n",
    "    \n",
    "            \n",
    "    return all_samples, all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "# Set directories\n",
    "data_path = \"~/Documents/PHD/Aim_2/test_models/5000_genes/\"\n",
    "#run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/test_models/5000_genes/run_20250206_184657_dataset_singler_label/\"\n",
    "#generator_model = \"generator_20250206_184657_dataset_singler_label_epoch_281.pt\"\n",
    "####\n",
    "run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/test_models/5000_genes/run_20250206_160000_dataset_singler_label/\"\n",
    "generator_model = \"generator_20250206_160000_dataset_singler_label_epoch_121.pt\"\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(run_dir, 'model_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "# Initialize models with saved configuration\n",
    "generator = Generator(\n",
    "    x_dim=model_config['x_dim'],\n",
    "    vocab_sizes=model_config['vocab_sizes'],\n",
    "    nb_numeric=model_config['nb_numeric'],\n",
    "    h_dims=model_config['h_dims'],\n",
    "    z_dim=model_config['z_dim']).to(device)\n",
    "\n",
    "generator_path = os.path.join(run_dir, generator_model)\n",
    "generator.load_state_dict(torch.load(generator_path, map_location=device, weights_only=True))\n",
    "\n",
    "# Load metadata\n",
    "meta = pd.read_csv(data_path+'metadata_top5000.csv')\n",
    "\n",
    "# Get counts of each dataset-celltype combination\n",
    "counts = meta.groupby(['dataset', 'singler_label']).size().to_dict()\n",
    "\n",
    "# Get unique cell types from the data\n",
    "unique_cell_types = sorted(meta['singler_label'].unique())\n",
    "\n",
    "# Create cell type mapping automatically\n",
    "cell_type_map = {cell_type: idx for idx, cell_type in enumerate(unique_cell_types)}\n",
    "\n",
    "print(\"Detected cell types:\")\n",
    "for cell_type, idx in cell_type_map.items():\n",
    "    print(f\"{cell_type}: {idx}\")\n",
    "\n",
    "# Convert the dict keys from tuple of strings to tuple of numbers\n",
    "samples_dict = {}\n",
    "for (dataset, cell_type), count in counts.items():\n",
    "    # Extract dataset number\n",
    "    dataset_num = int(dataset.replace('dataset', ''))\n",
    "    # Get cell type number from our automatic mapping\n",
    "    cell_type_num = cell_type_map[cell_type]\n",
    "    # Add to new dictionary with numerical tuple as key\n",
    "    samples_dict[(dataset_num, cell_type_num)] = count\n",
    "\n",
    "# Get the reverse mapping for cell type names\n",
    "cell_type_names = {v: k for k, v in cell_type_map.items()}\n",
    "\n",
    "all_samples, categories = generate_and_save_profiles(\n",
    "    generator,\n",
    "    samples_per_combination=samples_dict,\n",
    "    save_path=run_dir+'_generated_data',\n",
    "    cell_type_names=cell_type_names,  # Pass the mapping to the function\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and analyze generated data\n",
    "# Load the generated profiles\n",
    "profiles = np.load(run_dir + '_generated_data_profiles.npy')\n",
    "\n",
    "# Load categories\n",
    "with open(run_dir + '_generated_data_categories.txt', 'r') as f:\n",
    "    categories = [line.strip() for line in f]\n",
    "\n",
    "# Load gene names\n",
    "gene_names = pd.read_csv(data_path+\"full_matrix_top5000.csv\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(profiles)\n",
    "print(df.shape)\n",
    "# Set gene names as header\n",
    "#df.columns = gene_names.iloc[0]\n",
    "df.columns = gene_names['Unnamed: 0']\n",
    "\n",
    "\n",
    "# Add categories as a column\n",
    "df['labels'] = categories\n",
    "\n",
    "## \n",
    "# Add separate dataset and cell_type columns\n",
    "df['dataset'] = df['labels'].apply(lambda x: x.split('_')[0])\n",
    "df['cell_type'] = df['labels'].apply(lambda x: x.split('_')[1])\n",
    "# drop the combined labels colum\n",
    "df=df.drop(['labels'], axis=1)\n",
    "\n",
    "# Save a csv file\n",
    "df.to_csv(f'{run_dir}_generated_data.csv', index=False)\n",
    "#\n",
    "with h5py.File(f'{run_dir}_generated_data.h5', 'w') as f:\n",
    "    # Save as matrix - select only numerical values\n",
    "    f.create_dataset('matrix', data=df.select_dtypes(include=[np.number]).values)  # or df.to_numpy()\n",
    "\n",
    "# Save the categorical information separately:\n",
    "df_labels = df.select_dtypes(exclude=[np.number])\n",
    "df_labels.to_csv(f'{run_dir}_generated_labels.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/guyshani/Documents/PHD/Aim_2/test_models/5000_genes/\"\n",
    "# data_path = \"/content/drive/MyDrive/Colab_Notebooks/data/\"\n",
    "\n",
    "# Load expression matrix\n",
    "# matrix with cells as columns and genes as rows\n",
    "with h5py.File(data_path+'full_matrix_top5000.h5', 'r') as f:\n",
    "    x_train = np.array(f['matrix/data'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
