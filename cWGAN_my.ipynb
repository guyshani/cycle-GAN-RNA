{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims, z_dim):\n",
    "        \"\"\"\n",
    "        Generator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of output data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "            z_dim: Dimension of latent noise vector\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size)) \n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is latent dim + embedding dim + numeric covariates\n",
    "        input_dim = z_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build generator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, x_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims):\n",
    "        \"\"\"\n",
    "        Discriminator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of input data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size))\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is data dim + embedding dim + numeric covariates\n",
    "        input_dim = x_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build discriminator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        disc_input = torch.cat([x, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(disc_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, dataloader, cat_covs, num_covs, \n",
    "              config, device, score_fn=None, save_fn=None):\n",
    "    \"\"\"\n",
    "    Train the conditional GAN with progress tracking and proper device handling\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optimizer = optim.RMSprop(generator.parameters(), lr=config['lr'])\n",
    "    d_optimizer = optim.RMSprop(discriminator.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Lambda for gradient penalty\n",
    "    lambda_gp = 10\n",
    "    \n",
    "    # Convert covariates to tensors and move to device\n",
    "    cat_covs = torch.tensor(cat_covs, dtype=torch.long).to(device)\n",
    "    num_covs = torch.tensor(num_covs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "    print(f\"Total batches per epoch: {total_batches}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        print(f\"\\nEpoch [{epoch+1}/{config['epochs']}]\")\n",
    "        \n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Move real data to device\n",
    "            real_data = real_data.to(device)\n",
    "            \n",
    "            # Get random batch of categorical and numerical covariates\n",
    "            batch_indices = torch.randint(0, cat_covs.size(0), (batch_size,))\n",
    "            batch_cat_covs = cat_covs[batch_indices]\n",
    "            batch_num_covs = num_covs[batch_indices]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(config['nb_critic']):\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "                fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate discriminator output for real and fake data\n",
    "                real_validity = discriminator(real_data, batch_cat_covs, batch_num_covs)\n",
    "                fake_validity = discriminator(fake_data.detach(), batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate gradient penalty\n",
    "                gp = compute_gradient_penalty(\n",
    "                    discriminator,\n",
    "                    real_data,\n",
    "                    fake_data.detach(),\n",
    "                    batch_cat_covs,\n",
    "                    batch_num_covs,\n",
    "                    device)\n",
    "                \n",
    "                # Calculate discriminator loss with gradient penalty\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
    "                \n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "            fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "            \n",
    "            # Calculate generator loss\n",
    "            fake_validity = discriminator(fake_data, batch_cat_covs, batch_num_covs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            g_losses.append(g_loss.item())\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Batch [{batch_idx}/{total_batches}] \" \\\n",
    "                      f\"D_loss: {d_loss.item():.4f}, \" \\\n",
    "                      f\"G_loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average D_loss: {avg_d_loss:.4f}\")\n",
    "        print(f\"  Average G_loss: {avg_g_loss:.4f}\")\n",
    "        \n",
    "        # Log metrics\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'd_loss': np.mean(d_losses),\n",
    "                'g_loss': np.mean(g_losses)\n",
    "            })\n",
    "        \n",
    "        # Evaluate and save model if needed\n",
    "        if score_fn is not None and epoch % 10 == 0:\n",
    "            score = score_fn(generator)\n",
    "            print(f'Epoch {epoch}: Score = {score:.4f}')\n",
    "        \n",
    "        if save_fn is not None and epoch % 20 == 0:\n",
    "            save_fn(generator, discriminator, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Categorical data shape: (41588, 4)\n",
      "Available categorical variables: ['dataset', 'cluster', 'cell_type']\n",
      "\n",
      "Using categorical variables: ['dataset', 'cell_type']\n",
      "\n",
      "Processing categorical variable: dataset\n",
      "Categories in dataset: ['dataset1' 'dataset2' 'dataset3' 'dataset4' 'dataset5' 'dataset6'\n",
      " 'dataset7']\n",
      "Number of categories: 7\n",
      "\n",
      "Processing categorical variable: cell_type\n",
      "Categories in cell_type: ['B cells' 'Dendritic cells' 'Endothelial cells' 'Erythrocytes'\n",
      " 'Fibroblasts' 'Granulocytes' 'Macrophages' 'Monocytes' 'NK cells'\n",
      " 'T cells']\n",
      "Number of categories: 10\n",
      "\n",
      "Combined categorical covariates shape: (41588, 2)\n",
      "\n",
      "Vocabulary sizes for categorical variables: [7, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguyshani3\u001b[0m (\u001b[33mguyshani-tel-aviv-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/guyshani/Documents/PHD/Aim_2/cycle_GAN/wandb/run-20250120_142012-sgfzdf1i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/sgfzdf1i' target=\"_blank\">run_1737375612</a></strong> to <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/sgfzdf1i' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/sgfzdf1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 100 epochs...\n",
      "Total batches per epoch: 1299\n",
      "Using device: mps\n",
      "\n",
      "Epoch [1/100]\n",
      "  Batch [0/1299] D_loss: -0.5605, G_loss: 0.3044\n",
      "  Batch [10/1299] D_loss: -2.8216, G_loss: 1.5951\n",
      "  Batch [20/1299] D_loss: -2.2083, G_loss: 1.3289\n",
      "  Batch [30/1299] D_loss: -2.0722, G_loss: 1.2758\n",
      "  Batch [40/1299] D_loss: -1.0944, G_loss: 0.0772\n",
      "  Batch [50/1299] D_loss: -1.0580, G_loss: -0.3197\n",
      "  Batch [60/1299] D_loss: -0.9674, G_loss: 0.3070\n",
      "  Batch [70/1299] D_loss: -1.5179, G_loss: 0.1291\n",
      "  Batch [80/1299] D_loss: -0.6571, G_loss: 0.1424\n",
      "  Batch [90/1299] D_loss: -1.0714, G_loss: -0.0338\n",
      "  Batch [100/1299] D_loss: -1.0053, G_loss: -0.4887\n",
      "  Batch [110/1299] D_loss: -1.5703, G_loss: -0.2438\n",
      "  Batch [120/1299] D_loss: -1.1552, G_loss: 1.2693\n",
      "  Batch [130/1299] D_loss: -1.2954, G_loss: 0.7968\n",
      "  Batch [140/1299] D_loss: -1.1264, G_loss: 1.4317\n",
      "  Batch [150/1299] D_loss: -0.9933, G_loss: 1.5298\n",
      "  Batch [160/1299] D_loss: -0.9129, G_loss: -0.2407\n",
      "  Batch [170/1299] D_loss: -0.8028, G_loss: 0.1508\n",
      "  Batch [180/1299] D_loss: -0.8641, G_loss: 0.5502\n",
      "  Batch [190/1299] D_loss: -0.4719, G_loss: -0.8083\n",
      "  Batch [200/1299] D_loss: -0.3070, G_loss: -0.6620\n",
      "  Batch [210/1299] D_loss: -0.8349, G_loss: 0.0998\n",
      "  Batch [220/1299] D_loss: -0.8931, G_loss: 0.7648\n",
      "  Batch [230/1299] D_loss: -0.9242, G_loss: 0.4965\n",
      "  Batch [240/1299] D_loss: -0.7488, G_loss: 1.5513\n",
      "  Batch [250/1299] D_loss: -0.8412, G_loss: -0.3053\n",
      "  Batch [260/1299] D_loss: -0.4403, G_loss: -2.3430\n",
      "  Batch [270/1299] D_loss: -0.0244, G_loss: 0.7012\n",
      "  Batch [280/1299] D_loss: -0.6850, G_loss: 0.6140\n",
      "  Batch [290/1299] D_loss: -1.0604, G_loss: 0.6750\n",
      "  Batch [300/1299] D_loss: -0.8858, G_loss: -2.4575\n",
      "  Batch [310/1299] D_loss: -0.2962, G_loss: 0.2726\n",
      "  Batch [320/1299] D_loss: -0.3563, G_loss: 0.9043\n",
      "  Batch [330/1299] D_loss: -0.7888, G_loss: 0.3788\n",
      "  Batch [340/1299] D_loss: -1.6877, G_loss: 1.7520\n",
      "  Batch [350/1299] D_loss: -0.9923, G_loss: 0.2192\n",
      "  Batch [360/1299] D_loss: -1.0358, G_loss: 0.6699\n",
      "  Batch [370/1299] D_loss: -0.5287, G_loss: 0.2216\n",
      "  Batch [380/1299] D_loss: -1.2145, G_loss: 0.9811\n",
      "  Batch [390/1299] D_loss: -0.7600, G_loss: 0.5134\n",
      "  Batch [400/1299] D_loss: -1.1702, G_loss: -1.3131\n",
      "  Batch [410/1299] D_loss: -0.4755, G_loss: 0.6692\n",
      "  Batch [420/1299] D_loss: -0.7724, G_loss: 0.0424\n",
      "  Batch [430/1299] D_loss: -0.4668, G_loss: -2.0247\n",
      "  Batch [440/1299] D_loss: -0.8354, G_loss: 1.3719\n",
      "  Batch [450/1299] D_loss: -0.5389, G_loss: -0.2618\n",
      "  Batch [460/1299] D_loss: -1.5390, G_loss: 1.0937\n",
      "  Batch [470/1299] D_loss: -1.1984, G_loss: 1.8816\n",
      "  Batch [480/1299] D_loss: -0.5999, G_loss: 0.7882\n",
      "  Batch [490/1299] D_loss: -0.7658, G_loss: 0.9842\n",
      "  Batch [500/1299] D_loss: -0.8865, G_loss: 1.0958\n",
      "  Batch [510/1299] D_loss: -0.2392, G_loss: -1.2638\n",
      "  Batch [520/1299] D_loss: -0.2603, G_loss: 1.8537\n",
      "  Batch [530/1299] D_loss: -0.1206, G_loss: -0.0041\n",
      "  Batch [540/1299] D_loss: -1.1927, G_loss: -1.1637\n",
      "  Batch [550/1299] D_loss: -1.0523, G_loss: 1.1138\n",
      "  Batch [560/1299] D_loss: 0.1314, G_loss: -0.9189\n",
      "  Batch [570/1299] D_loss: -1.1037, G_loss: 1.6544\n",
      "  Batch [580/1299] D_loss: -0.0437, G_loss: 0.3285\n",
      "  Batch [590/1299] D_loss: -0.0649, G_loss: -0.6428\n",
      "  Batch [600/1299] D_loss: -0.7709, G_loss: 0.6527\n",
      "  Batch [610/1299] D_loss: -1.0477, G_loss: -2.4098\n",
      "  Batch [620/1299] D_loss: -1.1065, G_loss: 1.6813\n",
      "  Batch [630/1299] D_loss: -0.1929, G_loss: -0.9410\n",
      "  Batch [640/1299] D_loss: -0.4668, G_loss: 1.6565\n",
      "  Batch [650/1299] D_loss: -0.0646, G_loss: -0.3019\n",
      "  Batch [660/1299] D_loss: -1.7706, G_loss: -0.2308\n",
      "  Batch [670/1299] D_loss: -1.4833, G_loss: 1.9907\n",
      "  Batch [680/1299] D_loss: 0.0310, G_loss: -0.5462\n",
      "  Batch [690/1299] D_loss: -0.1830, G_loss: 0.8810\n",
      "  Batch [700/1299] D_loss: -1.3522, G_loss: 1.7006\n",
      "  Batch [710/1299] D_loss: -0.5772, G_loss: -0.2965\n",
      "  Batch [720/1299] D_loss: -0.1779, G_loss: -0.1922\n",
      "  Batch [730/1299] D_loss: -0.0394, G_loss: 0.7513\n",
      "  Batch [740/1299] D_loss: -0.1866, G_loss: -1.3821\n",
      "  Batch [750/1299] D_loss: -0.5891, G_loss: -0.7578\n",
      "  Batch [760/1299] D_loss: -0.5456, G_loss: 0.2981\n",
      "  Batch [770/1299] D_loss: -0.5560, G_loss: -1.7335\n",
      "  Batch [780/1299] D_loss: 0.0066, G_loss: -0.1037\n",
      "  Batch [790/1299] D_loss: -0.6347, G_loss: -2.5416\n",
      "  Batch [800/1299] D_loss: -0.0126, G_loss: 0.6743\n",
      "  Batch [810/1299] D_loss: -0.6425, G_loss: -0.3519\n",
      "  Batch [820/1299] D_loss: -0.4951, G_loss: 2.2098\n",
      "  Batch [830/1299] D_loss: -1.2520, G_loss: -1.0007\n",
      "  Batch [840/1299] D_loss: -0.1087, G_loss: 0.5792\n",
      "  Batch [850/1299] D_loss: -0.0171, G_loss: -0.6240\n",
      "  Batch [860/1299] D_loss: -0.7033, G_loss: -0.2136\n",
      "  Batch [870/1299] D_loss: -0.6521, G_loss: 3.0675\n",
      "  Batch [880/1299] D_loss: -0.5489, G_loss: 0.8864\n",
      "  Batch [890/1299] D_loss: -0.5259, G_loss: 0.6474\n",
      "  Batch [900/1299] D_loss: -0.3083, G_loss: -1.5577\n",
      "  Batch [910/1299] D_loss: -0.2419, G_loss: 1.0273\n",
      "  Batch [920/1299] D_loss: -0.8763, G_loss: 0.8481\n",
      "  Batch [930/1299] D_loss: -1.0090, G_loss: 1.3894\n",
      "  Batch [940/1299] D_loss: -0.0436, G_loss: -0.0143\n",
      "  Batch [950/1299] D_loss: -0.7240, G_loss: 1.3879\n",
      "  Batch [960/1299] D_loss: -0.5803, G_loss: 1.0207\n",
      "  Batch [970/1299] D_loss: -0.5410, G_loss: -0.0538\n",
      "  Batch [980/1299] D_loss: -0.4942, G_loss: 1.7372\n",
      "  Batch [990/1299] D_loss: -0.3234, G_loss: -1.1707\n",
      "  Batch [1000/1299] D_loss: -0.3303, G_loss: 1.4686\n",
      "  Batch [1010/1299] D_loss: -0.2815, G_loss: -1.4542\n",
      "  Batch [1020/1299] D_loss: -0.1208, G_loss: 1.0781\n",
      "  Batch [1030/1299] D_loss: -0.6236, G_loss: -1.2863\n",
      "  Batch [1040/1299] D_loss: -0.3081, G_loss: 0.6265\n",
      "  Batch [1050/1299] D_loss: -0.2153, G_loss: 0.0364\n",
      "  Batch [1060/1299] D_loss: -0.1968, G_loss: 0.2480\n",
      "  Batch [1070/1299] D_loss: -0.9820, G_loss: 1.8134\n",
      "  Batch [1080/1299] D_loss: -0.3498, G_loss: -1.4131\n",
      "  Batch [1090/1299] D_loss: -0.7115, G_loss: 0.4617\n",
      "  Batch [1100/1299] D_loss: -0.4522, G_loss: 0.8022\n",
      "  Batch [1110/1299] D_loss: -0.7578, G_loss: 0.6717\n",
      "  Batch [1120/1299] D_loss: -0.2669, G_loss: -0.0853\n",
      "  Batch [1130/1299] D_loss: -0.2182, G_loss: 0.3771\n",
      "  Batch [1140/1299] D_loss: -0.9144, G_loss: 0.8647\n",
      "  Batch [1150/1299] D_loss: -0.8156, G_loss: 0.7361\n",
      "  Batch [1160/1299] D_loss: -0.8453, G_loss: 0.8425\n",
      "  Batch [1170/1299] D_loss: -0.0887, G_loss: -0.6498\n",
      "  Batch [1180/1299] D_loss: -0.8061, G_loss: 1.0303\n",
      "  Batch [1190/1299] D_loss: -0.2595, G_loss: -0.7899\n",
      "  Batch [1200/1299] D_loss: -0.2296, G_loss: 0.8925\n",
      "  Batch [1210/1299] D_loss: -0.4352, G_loss: 0.6654\n",
      "  Batch [1220/1299] D_loss: -0.5013, G_loss: 0.4957\n",
      "  Batch [1230/1299] D_loss: -0.8084, G_loss: 0.5793\n",
      "  Batch [1240/1299] D_loss: -0.8365, G_loss: 0.2557\n",
      "  Batch [1250/1299] D_loss: -0.2768, G_loss: -0.8150\n",
      "  Batch [1260/1299] D_loss: -0.7059, G_loss: 0.5703\n",
      "  Batch [1270/1299] D_loss: 0.0626, G_loss: -0.4070\n",
      "  Batch [1280/1299] D_loss: -0.2556, G_loss: 1.5705\n",
      "  Batch [1290/1299] D_loss: -0.6965, G_loss: 1.5281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Average D_loss: -0.2844\n",
      "  Average G_loss: 0.2044\n",
      "\n",
      "Models saved at epoch 1:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250120_142110_dataset+cell_type/generator_20250120_142110_dataset+cell_type_epoch_1.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250120_142110_dataset+cell_type/discriminator_20250120_142110_dataset+cell_type_epoch_1.pt\n",
      "\n",
      "Epoch [2/100]\n",
      "  Batch [0/1299] D_loss: -1.7853, G_loss: -1.0287\n",
      "  Batch [10/1299] D_loss: -0.2461, G_loss: 0.5622\n",
      "  Batch [20/1299] D_loss: 0.0149, G_loss: 0.2563\n",
      "  Batch [30/1299] D_loss: -0.0716, G_loss: -0.6569\n",
      "  Batch [40/1299] D_loss: -0.0680, G_loss: 0.3703\n",
      "  Batch [50/1299] D_loss: -0.1452, G_loss: -0.9772\n",
      "  Batch [60/1299] D_loss: -0.4741, G_loss: -0.7263\n",
      "  Batch [70/1299] D_loss: -0.6283, G_loss: 0.1565\n",
      "  Batch [80/1299] D_loss: -0.3867, G_loss: -0.5935\n",
      "  Batch [90/1299] D_loss: -0.6718, G_loss: -0.4842\n",
      "  Batch [100/1299] D_loss: -0.2476, G_loss: 0.1887\n",
      "  Batch [110/1299] D_loss: -1.0037, G_loss: 0.5091\n",
      "  Batch [120/1299] D_loss: -0.1079, G_loss: -0.8394\n",
      "  Batch [130/1299] D_loss: 0.1587, G_loss: 1.6374\n",
      "  Batch [140/1299] D_loss: -0.3426, G_loss: -1.9842\n",
      "  Batch [150/1299] D_loss: -1.0532, G_loss: -0.8246\n",
      "  Batch [160/1299] D_loss: -0.4567, G_loss: 0.1732\n",
      "  Batch [170/1299] D_loss: -0.1777, G_loss: -0.7841\n",
      "  Batch [180/1299] D_loss: -0.2308, G_loss: 0.4451\n",
      "  Batch [190/1299] D_loss: -0.0955, G_loss: -0.5536\n",
      "  Batch [200/1299] D_loss: -0.6077, G_loss: 0.4912\n",
      "  Batch [210/1299] D_loss: -0.6606, G_loss: 0.5211\n",
      "  Batch [220/1299] D_loss: -0.9119, G_loss: -0.9961\n",
      "  Batch [230/1299] D_loss: -0.1905, G_loss: -0.4781\n",
      "  Batch [240/1299] D_loss: -1.4884, G_loss: -0.9428\n",
      "  Batch [250/1299] D_loss: -0.0289, G_loss: 0.4174\n",
      "  Batch [260/1299] D_loss: -0.0870, G_loss: -1.4162\n",
      "  Batch [270/1299] D_loss: -0.6016, G_loss: -0.0755\n",
      "  Batch [280/1299] D_loss: -1.0030, G_loss: 1.3826\n",
      "  Batch [290/1299] D_loss: -1.5088, G_loss: -1.1445\n",
      "  Batch [300/1299] D_loss: -0.6464, G_loss: -0.0800\n",
      "  Batch [310/1299] D_loss: 0.0004, G_loss: -0.4402\n",
      "  Batch [320/1299] D_loss: 0.3251, G_loss: 0.5733\n",
      "  Batch [330/1299] D_loss: 0.1721, G_loss: -1.4842\n",
      "  Batch [340/1299] D_loss: -0.6914, G_loss: 1.3912\n",
      "  Batch [350/1299] D_loss: -0.3408, G_loss: -0.2951\n",
      "  Batch [360/1299] D_loss: -0.3414, G_loss: 1.1672\n",
      "  Batch [370/1299] D_loss: -0.7377, G_loss: 0.0585\n",
      "  Batch [380/1299] D_loss: -0.5053, G_loss: 0.5220\n",
      "  Batch [390/1299] D_loss: -0.1461, G_loss: 0.2184\n",
      "  Batch [400/1299] D_loss: -0.7207, G_loss: -0.2218\n",
      "  Batch [410/1299] D_loss: -0.6589, G_loss: -1.0983\n",
      "  Batch [420/1299] D_loss: -0.0243, G_loss: 0.9736\n",
      "  Batch [430/1299] D_loss: -0.1660, G_loss: -1.4329\n",
      "  Batch [440/1299] D_loss: -0.0392, G_loss: 0.0254\n",
      "  Batch [450/1299] D_loss: -1.2837, G_loss: 2.1479\n",
      "  Batch [460/1299] D_loss: -0.4397, G_loss: -0.5765\n",
      "  Batch [470/1299] D_loss: -0.8058, G_loss: 0.7006\n",
      "  Batch [480/1299] D_loss: -0.4422, G_loss: -1.5384\n",
      "  Batch [490/1299] D_loss: -0.1830, G_loss: 1.3542\n",
      "  Batch [500/1299] D_loss: -0.3551, G_loss: -0.7474\n",
      "  Batch [510/1299] D_loss: -0.2745, G_loss: 0.1669\n",
      "  Batch [520/1299] D_loss: -0.5440, G_loss: -0.9155\n",
      "  Batch [530/1299] D_loss: -0.7862, G_loss: 1.9941\n",
      "  Batch [540/1299] D_loss: -0.7898, G_loss: -2.0194\n",
      "  Batch [550/1299] D_loss: -1.0510, G_loss: 1.3695\n",
      "  Batch [560/1299] D_loss: 0.0236, G_loss: -1.3437\n",
      "  Batch [570/1299] D_loss: -0.3033, G_loss: 1.5479\n",
      "  Batch [580/1299] D_loss: -0.8396, G_loss: -0.1870\n",
      "  Batch [590/1299] D_loss: -0.6861, G_loss: -0.6874\n",
      "  Batch [600/1299] D_loss: -1.2790, G_loss: -1.9118\n",
      "  Batch [610/1299] D_loss: -0.7015, G_loss: 1.0633\n",
      "  Batch [620/1299] D_loss: -0.2228, G_loss: -0.9293\n",
      "  Batch [630/1299] D_loss: -0.6195, G_loss: 0.8861\n",
      "  Batch [640/1299] D_loss: -0.2601, G_loss: -0.4712\n",
      "  Batch [650/1299] D_loss: -0.3165, G_loss: -0.0161\n",
      "  Batch [660/1299] D_loss: -0.4162, G_loss: 1.8598\n",
      "  Batch [670/1299] D_loss: -0.9411, G_loss: 0.5072\n",
      "  Batch [680/1299] D_loss: -0.5091, G_loss: 1.6780\n",
      "  Batch [690/1299] D_loss: -0.1994, G_loss: -1.2768\n",
      "  Batch [700/1299] D_loss: -0.4233, G_loss: -1.5307\n",
      "  Batch [710/1299] D_loss: -0.5598, G_loss: 2.2924\n",
      "  Batch [720/1299] D_loss: -0.5157, G_loss: 0.8331\n",
      "  Batch [730/1299] D_loss: -0.7607, G_loss: 0.1967\n",
      "  Batch [740/1299] D_loss: -0.8137, G_loss: 0.8150\n",
      "  Batch [750/1299] D_loss: -0.7485, G_loss: -0.7233\n",
      "  Batch [760/1299] D_loss: -0.2468, G_loss: -0.2053\n",
      "  Batch [770/1299] D_loss: -0.4358, G_loss: 2.3961\n",
      "  Batch [780/1299] D_loss: -0.3291, G_loss: -1.2411\n",
      "  Batch [790/1299] D_loss: -0.5060, G_loss: 0.9393\n",
      "  Batch [800/1299] D_loss: -0.7067, G_loss: 0.1559\n",
      "  Batch [810/1299] D_loss: -0.2773, G_loss: -0.0280\n",
      "  Batch [820/1299] D_loss: -0.9383, G_loss: 1.0495\n",
      "  Batch [830/1299] D_loss: -0.3754, G_loss: 0.2394\n",
      "  Batch [840/1299] D_loss: -0.4412, G_loss: -0.6354\n",
      "  Batch [850/1299] D_loss: -1.0273, G_loss: 0.1549\n",
      "  Batch [860/1299] D_loss: -0.5194, G_loss: 1.3532\n",
      "  Batch [870/1299] D_loss: -0.8837, G_loss: -1.8080\n",
      "  Batch [880/1299] D_loss: -0.9060, G_loss: 0.8979\n",
      "  Batch [890/1299] D_loss: -0.5930, G_loss: -1.2919\n",
      "  Batch [900/1299] D_loss: -0.0029, G_loss: 0.5305\n",
      "  Batch [910/1299] D_loss: -0.2954, G_loss: -1.4468\n",
      "  Batch [920/1299] D_loss: -0.6587, G_loss: 0.2759\n",
      "  Batch [930/1299] D_loss: -0.9056, G_loss: 0.3017\n",
      "  Batch [940/1299] D_loss: -0.0496, G_loss: 0.2897\n",
      "  Batch [950/1299] D_loss: -0.1721, G_loss: -0.9320\n",
      "  Batch [960/1299] D_loss: -0.1808, G_loss: 0.5101\n",
      "  Batch [970/1299] D_loss: -0.9444, G_loss: 0.9129\n",
      "  Batch [980/1299] D_loss: -1.2303, G_loss: -0.2744\n",
      "  Batch [990/1299] D_loss: -0.7177, G_loss: 0.1476\n",
      "  Batch [1000/1299] D_loss: -0.2154, G_loss: 0.9317\n",
      "  Batch [1010/1299] D_loss: -0.6792, G_loss: 0.0092\n",
      "  Batch [1020/1299] D_loss: -1.2790, G_loss: 2.0887\n",
      "  Batch [1030/1299] D_loss: -0.3565, G_loss: -1.3452\n",
      "  Batch [1040/1299] D_loss: 0.0192, G_loss: 1.3394\n",
      "  Batch [1050/1299] D_loss: -0.2777, G_loss: 0.0352\n",
      "  Batch [1060/1299] D_loss: -0.9982, G_loss: -0.3559\n",
      "  Batch [1070/1299] D_loss: -0.5975, G_loss: -0.5173\n",
      "  Batch [1080/1299] D_loss: -0.6824, G_loss: -2.1272\n",
      "  Batch [1090/1299] D_loss: -0.1084, G_loss: 0.9547\n",
      "  Batch [1100/1299] D_loss: -0.2619, G_loss: -1.6336\n",
      "  Batch [1110/1299] D_loss: -1.1060, G_loss: 2.0402\n",
      "  Batch [1120/1299] D_loss: -1.0021, G_loss: -0.0930\n",
      "  Batch [1130/1299] D_loss: -0.7908, G_loss: 0.6286\n",
      "  Batch [1140/1299] D_loss: -1.0045, G_loss: -0.7386\n",
      "  Batch [1150/1299] D_loss: -0.2280, G_loss: -0.8463\n",
      "  Batch [1160/1299] D_loss: -0.4966, G_loss: 0.9845\n",
      "  Batch [1170/1299] D_loss: -0.0744, G_loss: 0.3813\n",
      "  Batch [1180/1299] D_loss: -0.2777, G_loss: -2.2155\n",
      "  Batch [1190/1299] D_loss: -0.2100, G_loss: 1.4338\n",
      "  Batch [1200/1299] D_loss: -0.4308, G_loss: -1.5170\n",
      "  Batch [1210/1299] D_loss: -0.2615, G_loss: 0.0221\n",
      "  Batch [1220/1299] D_loss: -0.0668, G_loss: -0.6118\n",
      "  Batch [1230/1299] D_loss: -0.4877, G_loss: 1.5108\n",
      "  Batch [1240/1299] D_loss: -0.3368, G_loss: -1.3641\n",
      "  Batch [1250/1299] D_loss: -0.3014, G_loss: 1.0163\n",
      "  Batch [1260/1299] D_loss: -0.0641, G_loss: -1.3831\n",
      "  Batch [1270/1299] D_loss: -0.9559, G_loss: 1.5572\n",
      "  Batch [1280/1299] D_loss: -0.4502, G_loss: -1.1694\n",
      "  Batch [1290/1299] D_loss: -0.6688, G_loss: 0.8009\n",
      "\n",
      "Epoch 2 Summary:\n",
      "  Average D_loss: -0.1981\n",
      "  Average G_loss: 0.1254\n",
      "\n",
      "Epoch [3/100]\n",
      "  Batch [0/1299] D_loss: -1.0465, G_loss: 0.2354\n",
      "  Batch [10/1299] D_loss: -0.7434, G_loss: -0.0714\n",
      "  Batch [20/1299] D_loss: -0.4275, G_loss: -2.4551\n",
      "  Batch [30/1299] D_loss: -0.5672, G_loss: 0.6004\n",
      "  Batch [40/1299] D_loss: -0.3538, G_loss: -1.8216\n",
      "  Batch [50/1299] D_loss: -0.2515, G_loss: 1.5972\n",
      "  Batch [60/1299] D_loss: -0.2719, G_loss: -2.2938\n",
      "  Batch [70/1299] D_loss: -0.1215, G_loss: -0.7060\n",
      "  Batch [80/1299] D_loss: -0.7732, G_loss: 1.4936\n",
      "  Batch [90/1299] D_loss: -0.8490, G_loss: 1.1240\n",
      "  Batch [100/1299] D_loss: -0.4864, G_loss: 0.6419\n",
      "  Batch [110/1299] D_loss: -1.3937, G_loss: 0.3834\n",
      "  Batch [120/1299] D_loss: -0.6658, G_loss: 0.9099\n",
      "  Batch [130/1299] D_loss: -0.5265, G_loss: 0.3837\n",
      "  Batch [140/1299] D_loss: -0.4812, G_loss: -0.7546\n",
      "  Batch [150/1299] D_loss: -0.5353, G_loss: -0.8554\n",
      "  Batch [160/1299] D_loss: -0.2532, G_loss: 1.2234\n",
      "  Batch [170/1299] D_loss: -0.4720, G_loss: -1.5998\n",
      "  Batch [180/1299] D_loss: -0.8813, G_loss: 1.2885\n",
      "  Batch [190/1299] D_loss: -0.1801, G_loss: 0.3252\n",
      "  Batch [200/1299] D_loss: -0.0391, G_loss: -0.3981\n",
      "  Batch [210/1299] D_loss: -0.5389, G_loss: 0.6778\n",
      "  Batch [220/1299] D_loss: -0.4265, G_loss: -1.1649\n",
      "  Batch [230/1299] D_loss: -0.6904, G_loss: 1.7875\n",
      "  Batch [240/1299] D_loss: -0.1656, G_loss: -1.0535\n",
      "  Batch [250/1299] D_loss: -0.6851, G_loss: 0.2337\n",
      "  Batch [260/1299] D_loss: -0.7035, G_loss: 0.9605\n",
      "  Batch [270/1299] D_loss: -0.9742, G_loss: 0.2227\n",
      "  Batch [280/1299] D_loss: -0.9906, G_loss: 0.6930\n",
      "  Batch [290/1299] D_loss: -0.0882, G_loss: -1.8460\n",
      "  Batch [300/1299] D_loss: -0.5906, G_loss: 0.7374\n",
      "  Batch [310/1299] D_loss: -0.6159, G_loss: 1.4961\n",
      "  Batch [320/1299] D_loss: -0.3275, G_loss: -2.2301\n",
      "  Batch [330/1299] D_loss: -0.8450, G_loss: 0.6983\n",
      "  Batch [340/1299] D_loss: -0.0126, G_loss: -0.3433\n",
      "  Batch [350/1299] D_loss: -0.6896, G_loss: 0.6378\n",
      "  Batch [360/1299] D_loss: -0.1848, G_loss: -0.2108\n",
      "  Batch [370/1299] D_loss: -0.8358, G_loss: 0.6526\n",
      "  Batch [380/1299] D_loss: -0.3678, G_loss: 0.0167\n",
      "  Batch [390/1299] D_loss: -0.2340, G_loss: 1.7101\n",
      "  Batch [400/1299] D_loss: 0.0016, G_loss: -1.4402\n",
      "  Batch [410/1299] D_loss: -0.4683, G_loss: 0.3997\n",
      "  Batch [420/1299] D_loss: -0.8466, G_loss: 2.2650\n",
      "  Batch [430/1299] D_loss: -0.1254, G_loss: -0.6527\n",
      "  Batch [440/1299] D_loss: -0.7458, G_loss: 0.0577\n",
      "  Batch [450/1299] D_loss: -0.1544, G_loss: -0.4331\n",
      "  Batch [460/1299] D_loss: -0.8087, G_loss: 0.4305\n",
      "  Batch [470/1299] D_loss: -0.5957, G_loss: -0.3003\n",
      "  Batch [480/1299] D_loss: -0.2766, G_loss: -0.2065\n",
      "  Batch [490/1299] D_loss: -0.7772, G_loss: 0.0640\n",
      "  Batch [500/1299] D_loss: -0.6144, G_loss: -1.2933\n",
      "  Batch [510/1299] D_loss: -0.8447, G_loss: 0.8936\n",
      "  Batch [520/1299] D_loss: -0.3227, G_loss: -0.3181\n",
      "  Batch [530/1299] D_loss: -0.1295, G_loss: -1.6221\n",
      "  Batch [540/1299] D_loss: -0.8020, G_loss: 1.6118\n",
      "  Batch [550/1299] D_loss: -0.5171, G_loss: 0.2174\n",
      "  Batch [560/1299] D_loss: -0.8650, G_loss: 0.4745\n",
      "  Batch [570/1299] D_loss: -1.0807, G_loss: 0.9342\n",
      "  Batch [580/1299] D_loss: -0.6271, G_loss: -0.1545\n",
      "  Batch [590/1299] D_loss: -0.4521, G_loss: -0.9322\n",
      "  Batch [600/1299] D_loss: 0.0339, G_loss: 0.4740\n",
      "  Batch [610/1299] D_loss: 0.0468, G_loss: -1.3382\n",
      "  Batch [620/1299] D_loss: 0.2452, G_loss: 1.8556\n",
      "  Batch [630/1299] D_loss: -0.4622, G_loss: -1.6861\n",
      "  Batch [640/1299] D_loss: -1.3509, G_loss: 2.9808\n",
      "  Batch [650/1299] D_loss: -0.7400, G_loss: 1.3134\n",
      "  Batch [660/1299] D_loss: -0.9546, G_loss: -2.5036\n",
      "  Batch [670/1299] D_loss: -0.9842, G_loss: 1.9889\n",
      "  Batch [680/1299] D_loss: -0.4607, G_loss: 0.4044\n",
      "  Batch [690/1299] D_loss: -0.0828, G_loss: -0.9748\n",
      "  Batch [700/1299] D_loss: -0.5736, G_loss: 2.2328\n",
      "  Batch [710/1299] D_loss: -0.2802, G_loss: 0.0020\n",
      "  Batch [720/1299] D_loss: -0.5512, G_loss: -2.0596\n",
      "  Batch [730/1299] D_loss: -0.6159, G_loss: 2.5794\n",
      "  Batch [740/1299] D_loss: -0.0544, G_loss: 0.0241\n",
      "  Batch [750/1299] D_loss: -0.3064, G_loss: -2.2972\n",
      "  Batch [760/1299] D_loss: -2.2380, G_loss: 5.2981\n",
      "  Batch [770/1299] D_loss: -0.0843, G_loss: -2.6915\n",
      "  Batch [780/1299] D_loss: -0.1563, G_loss: -0.2908\n",
      "  Batch [790/1299] D_loss: 0.1612, G_loss: 0.8888\n",
      "  Batch [800/1299] D_loss: 0.0130, G_loss: -0.1682\n",
      "  Batch [810/1299] D_loss: 0.0191, G_loss: -0.2897\n",
      "  Batch [820/1299] D_loss: -0.2061, G_loss: -0.3343\n",
      "  Batch [830/1299] D_loss: -0.3543, G_loss: 0.6256\n",
      "  Batch [840/1299] D_loss: -0.5553, G_loss: -0.8259\n",
      "  Batch [850/1299] D_loss: -0.9560, G_loss: 1.7392\n",
      "  Batch [860/1299] D_loss: -0.8692, G_loss: 0.8867\n",
      "  Batch [870/1299] D_loss: -0.9096, G_loss: 1.4222\n",
      "  Batch [880/1299] D_loss: -0.1926, G_loss: 1.3368\n",
      "  Batch [890/1299] D_loss: -0.3397, G_loss: -0.2976\n",
      "  Batch [900/1299] D_loss: -0.4676, G_loss: 0.2302\n",
      "  Batch [910/1299] D_loss: -0.5380, G_loss: -0.9326\n",
      "  Batch [920/1299] D_loss: -0.5127, G_loss: -0.1975\n",
      "  Batch [930/1299] D_loss: -0.1821, G_loss: -0.2887\n",
      "  Batch [940/1299] D_loss: -0.1738, G_loss: 1.5695\n",
      "  Batch [950/1299] D_loss: -0.6027, G_loss: -0.0994\n",
      "  Batch [960/1299] D_loss: -0.3218, G_loss: -0.0503\n",
      "  Batch [970/1299] D_loss: -1.1142, G_loss: 1.1029\n",
      "  Batch [980/1299] D_loss: -0.2278, G_loss: -0.7429\n",
      "  Batch [990/1299] D_loss: -0.7530, G_loss: -0.5217\n",
      "  Batch [1000/1299] D_loss: -0.1878, G_loss: -1.0100\n",
      "  Batch [1010/1299] D_loss: -0.6861, G_loss: 0.0809\n",
      "  Batch [1020/1299] D_loss: 0.1710, G_loss: -1.2215\n",
      "  Batch [1030/1299] D_loss: -0.2219, G_loss: -0.0078\n",
      "  Batch [1040/1299] D_loss: -0.0437, G_loss: 0.4767\n",
      "  Batch [1050/1299] D_loss: -0.6389, G_loss: -0.7836\n",
      "  Batch [1060/1299] D_loss: -0.6958, G_loss: 0.4740\n",
      "  Batch [1070/1299] D_loss: -0.7071, G_loss: 0.6842\n",
      "  Batch [1080/1299] D_loss: -0.8175, G_loss: 1.5190\n",
      "  Batch [1090/1299] D_loss: -0.4626, G_loss: -1.2490\n",
      "  Batch [1100/1299] D_loss: -0.3699, G_loss: 2.0539\n",
      "  Batch [1110/1299] D_loss: -0.7680, G_loss: 0.4252\n",
      "  Batch [1120/1299] D_loss: -0.7540, G_loss: -2.1420\n",
      "  Batch [1130/1299] D_loss: -1.2995, G_loss: 0.8280\n",
      "  Batch [1140/1299] D_loss: -0.4517, G_loss: 0.8010\n",
      "  Batch [1150/1299] D_loss: -0.8769, G_loss: -1.3073\n",
      "  Batch [1160/1299] D_loss: -0.3045, G_loss: 2.7098\n",
      "  Batch [1170/1299] D_loss: -0.9132, G_loss: -3.4590\n",
      "  Batch [1180/1299] D_loss: -0.7474, G_loss: 2.0862\n",
      "  Batch [1190/1299] D_loss: -1.0301, G_loss: -2.2865\n",
      "  Batch [1200/1299] D_loss: -0.1942, G_loss: 1.1255\n",
      "  Batch [1210/1299] D_loss: -0.4825, G_loss: 0.8506\n",
      "  Batch [1220/1299] D_loss: -0.3995, G_loss: 0.3062\n",
      "  Batch [1230/1299] D_loss: -0.6292, G_loss: -0.5493\n",
      "  Batch [1240/1299] D_loss: -0.6278, G_loss: -1.4412\n",
      "  Batch [1250/1299] D_loss: -0.1564, G_loss: -2.0268\n",
      "  Batch [1260/1299] D_loss: -0.5256, G_loss: 1.2861\n",
      "  Batch [1270/1299] D_loss: -0.0546, G_loss: 0.0197\n",
      "  Batch [1280/1299] D_loss: -0.1741, G_loss: -0.8169\n",
      "  Batch [1290/1299] D_loss: -0.0958, G_loss: 0.4222\n",
      "\n",
      "Epoch 3 Summary:\n",
      "  Average D_loss: -0.1997\n",
      "  Average G_loss: 0.1040\n",
      "\n",
      "Epoch [4/100]\n",
      "  Batch [0/1299] D_loss: -0.2911, G_loss: -0.9781\n",
      "  Batch [10/1299] D_loss: -0.1064, G_loss: 0.4773\n",
      "  Batch [20/1299] D_loss: -0.3814, G_loss: 0.4088\n",
      "  Batch [30/1299] D_loss: -0.6611, G_loss: 0.6115\n",
      "  Batch [40/1299] D_loss: -0.5532, G_loss: -1.2396\n",
      "  Batch [50/1299] D_loss: -0.5837, G_loss: -0.0521\n",
      "  Batch [60/1299] D_loss: -0.6482, G_loss: 0.0987\n",
      "  Batch [70/1299] D_loss: -0.7285, G_loss: 0.8953\n",
      "  Batch [80/1299] D_loss: -1.2086, G_loss: -0.3504\n",
      "  Batch [90/1299] D_loss: -0.3286, G_loss: -0.2491\n",
      "  Batch [100/1299] D_loss: -0.6232, G_loss: 1.3066\n",
      "  Batch [110/1299] D_loss: -0.6232, G_loss: 0.9704\n",
      "  Batch [120/1299] D_loss: -0.8708, G_loss: -2.8978\n",
      "  Batch [130/1299] D_loss: -0.2773, G_loss: 1.8429\n",
      "  Batch [140/1299] D_loss: -0.3624, G_loss: 0.3549\n",
      "  Batch [150/1299] D_loss: -1.0391, G_loss: -2.9801\n",
      "  Batch [160/1299] D_loss: 0.5467, G_loss: -0.8949\n",
      "  Batch [170/1299] D_loss: 0.0259, G_loss: 0.8108\n",
      "  Batch [180/1299] D_loss: -0.1380, G_loss: 0.0182\n",
      "  Batch [190/1299] D_loss: 0.2981, G_loss: 0.5695\n",
      "  Batch [200/1299] D_loss: 0.2573, G_loss: -0.8709\n",
      "  Batch [210/1299] D_loss: -0.1577, G_loss: 0.5617\n",
      "  Batch [220/1299] D_loss: -0.5165, G_loss: -1.8281\n",
      "  Batch [230/1299] D_loss: -0.3183, G_loss: 0.3678\n",
      "  Batch [240/1299] D_loss: -0.4537, G_loss: 0.7957\n",
      "  Batch [250/1299] D_loss: -1.4647, G_loss: -1.1921\n",
      "  Batch [260/1299] D_loss: -0.8064, G_loss: 1.5950\n",
      "  Batch [270/1299] D_loss: -0.1102, G_loss: -0.8577\n",
      "  Batch [280/1299] D_loss: -0.8396, G_loss: 0.2228\n",
      "  Batch [290/1299] D_loss: -0.2712, G_loss: -0.4019\n",
      "  Batch [300/1299] D_loss: -0.5817, G_loss: 2.0876\n",
      "  Batch [310/1299] D_loss: -0.1584, G_loss: -0.3732\n",
      "  Batch [320/1299] D_loss: -0.8238, G_loss: -2.8100\n",
      "  Batch [330/1299] D_loss: -0.3882, G_loss: 0.5473\n",
      "  Batch [340/1299] D_loss: -0.4608, G_loss: -0.1716\n",
      "  Batch [350/1299] D_loss: -1.2211, G_loss: -0.6709\n",
      "  Batch [360/1299] D_loss: -0.4793, G_loss: -0.7641\n",
      "  Batch [370/1299] D_loss: -0.7388, G_loss: 0.0195\n",
      "  Batch [380/1299] D_loss: -0.8094, G_loss: -1.3421\n",
      "  Batch [390/1299] D_loss: -0.6863, G_loss: 0.4381\n",
      "  Batch [400/1299] D_loss: -0.0154, G_loss: 0.4857\n",
      "  Batch [410/1299] D_loss: -0.6783, G_loss: -2.4819\n",
      "  Batch [420/1299] D_loss: -0.5000, G_loss: 0.8713\n",
      "  Batch [430/1299] D_loss: -0.6543, G_loss: -0.7850\n",
      "  Batch [440/1299] D_loss: -0.3053, G_loss: 2.1088\n",
      "  Batch [450/1299] D_loss: -0.0762, G_loss: -0.1595\n",
      "  Batch [460/1299] D_loss: -0.2898, G_loss: -1.1138\n",
      "  Batch [470/1299] D_loss: -0.0128, G_loss: 1.8411\n",
      "  Batch [480/1299] D_loss: -0.2405, G_loss: -1.6503\n",
      "  Batch [490/1299] D_loss: -0.1652, G_loss: 0.7370\n",
      "  Batch [500/1299] D_loss: -0.0273, G_loss: 0.5886\n",
      "  Batch [510/1299] D_loss: -0.3556, G_loss: -0.2993\n",
      "  Batch [520/1299] D_loss: -0.5233, G_loss: 1.0137\n",
      "  Batch [530/1299] D_loss: -0.0912, G_loss: -0.0725\n",
      "  Batch [540/1299] D_loss: -0.0244, G_loss: 1.1385\n",
      "  Batch [550/1299] D_loss: -0.7834, G_loss: -2.3222\n",
      "  Batch [560/1299] D_loss: -0.5598, G_loss: 0.6983\n",
      "  Batch [570/1299] D_loss: -0.0280, G_loss: 1.0153\n",
      "  Batch [580/1299] D_loss: -0.5807, G_loss: -2.8471\n",
      "  Batch [590/1299] D_loss: -0.6177, G_loss: 1.0601\n",
      "  Batch [600/1299] D_loss: -0.2993, G_loss: -0.3586\n",
      "  Batch [610/1299] D_loss: -0.5022, G_loss: 0.4372\n",
      "  Batch [620/1299] D_loss: -0.2583, G_loss: -0.0206\n",
      "  Batch [630/1299] D_loss: -0.5817, G_loss: 1.6333\n",
      "  Batch [640/1299] D_loss: -0.3168, G_loss: -1.2129\n",
      "  Batch [650/1299] D_loss: -0.0852, G_loss: 0.5217\n",
      "  Batch [660/1299] D_loss: -0.5150, G_loss: -0.1407\n",
      "  Batch [670/1299] D_loss: -0.4688, G_loss: 0.1032\n",
      "  Batch [680/1299] D_loss: -0.2633, G_loss: -1.3784\n",
      "  Batch [690/1299] D_loss: -0.7804, G_loss: 1.1775\n",
      "  Batch [700/1299] D_loss: -0.6347, G_loss: 0.4485\n",
      "  Batch [710/1299] D_loss: -0.8190, G_loss: 0.1037\n",
      "  Batch [720/1299] D_loss: -0.6875, G_loss: -0.6200\n",
      "  Batch [730/1299] D_loss: -1.0796, G_loss: 1.4358\n",
      "  Batch [740/1299] D_loss: -0.8499, G_loss: 2.5123\n",
      "  Batch [750/1299] D_loss: -0.1127, G_loss: -1.4637\n",
      "  Batch [760/1299] D_loss: -0.3320, G_loss: 0.6501\n",
      "  Batch [770/1299] D_loss: -0.1151, G_loss: 1.0379\n",
      "  Batch [780/1299] D_loss: -1.0531, G_loss: 0.2298\n",
      "  Batch [790/1299] D_loss: -0.7456, G_loss: 0.9520\n",
      "  Batch [800/1299] D_loss: -0.4540, G_loss: 0.5057\n",
      "  Batch [810/1299] D_loss: -0.6289, G_loss: -2.1209\n",
      "  Batch [820/1299] D_loss: -0.6679, G_loss: 2.2960\n",
      "  Batch [830/1299] D_loss: -0.8571, G_loss: 0.4377\n",
      "  Batch [840/1299] D_loss: -0.4946, G_loss: -1.3021\n",
      "  Batch [850/1299] D_loss: -0.5233, G_loss: 0.0073\n",
      "  Batch [860/1299] D_loss: -0.1735, G_loss: -0.0158\n",
      "  Batch [870/1299] D_loss: -0.2672, G_loss: -1.6702\n",
      "  Batch [880/1299] D_loss: -0.8220, G_loss: 0.6072\n",
      "  Batch [890/1299] D_loss: -0.0034, G_loss: 1.9044\n",
      "  Batch [900/1299] D_loss: -0.0104, G_loss: -0.9578\n",
      "  Batch [910/1299] D_loss: -0.4815, G_loss: 1.0933\n",
      "  Batch [920/1299] D_loss: -0.0816, G_loss: 0.5498\n",
      "  Batch [930/1299] D_loss: -1.1492, G_loss: -2.7737\n",
      "  Batch [940/1299] D_loss: -0.0349, G_loss: 0.0664\n",
      "  Batch [950/1299] D_loss: -0.3445, G_loss: 1.1689\n",
      "  Batch [960/1299] D_loss: -0.6858, G_loss: -2.0326\n",
      "  Batch [970/1299] D_loss: -0.8940, G_loss: 1.7572\n",
      "  Batch [980/1299] D_loss: -0.0619, G_loss: 0.0468\n",
      "  Batch [990/1299] D_loss: -0.3549, G_loss: 1.3946\n",
      "  Batch [1000/1299] D_loss: -0.9167, G_loss: -1.0426\n",
      "  Batch [1010/1299] D_loss: -0.8008, G_loss: 0.2973\n",
      "  Batch [1020/1299] D_loss: -0.3168, G_loss: -1.2447\n",
      "  Batch [1030/1299] D_loss: -0.5998, G_loss: 0.1935\n",
      "  Batch [1040/1299] D_loss: -0.8331, G_loss: -0.8918\n",
      "  Batch [1050/1299] D_loss: -0.2309, G_loss: -1.1252\n",
      "  Batch [1060/1299] D_loss: -0.2969, G_loss: 1.1633\n",
      "  Batch [1070/1299] D_loss: -0.7960, G_loss: 0.5296\n",
      "  Batch [1080/1299] D_loss: -1.1013, G_loss: 0.2393\n",
      "  Batch [1090/1299] D_loss: -0.7190, G_loss: 0.7191\n",
      "  Batch [1100/1299] D_loss: -0.3475, G_loss: 0.0897\n",
      "  Batch [1110/1299] D_loss: -0.5365, G_loss: 0.7186\n",
      "  Batch [1120/1299] D_loss: 0.0578, G_loss: -1.8051\n",
      "  Batch [1130/1299] D_loss: -0.3870, G_loss: 0.1815\n",
      "  Batch [1140/1299] D_loss: -1.0166, G_loss: 0.6956\n",
      "  Batch [1150/1299] D_loss: -1.2668, G_loss: -0.6005\n",
      "  Batch [1160/1299] D_loss: 0.0769, G_loss: 1.2212\n",
      "  Batch [1170/1299] D_loss: -0.5486, G_loss: -2.0179\n",
      "  Batch [1180/1299] D_loss: -0.5023, G_loss: 0.0413\n",
      "  Batch [1190/1299] D_loss: -0.3905, G_loss: -0.4653\n",
      "  Batch [1200/1299] D_loss: -0.9839, G_loss: -3.5747\n",
      "  Batch [1210/1299] D_loss: -0.4734, G_loss: 1.7082\n",
      "  Batch [1220/1299] D_loss: -0.6603, G_loss: -1.6038\n",
      "  Batch [1230/1299] D_loss: -0.5790, G_loss: 0.6660\n",
      "  Batch [1240/1299] D_loss: -0.9012, G_loss: 1.2831\n",
      "  Batch [1250/1299] D_loss: -0.8461, G_loss: 1.0159\n",
      "  Batch [1260/1299] D_loss: -0.7582, G_loss: 0.3890\n",
      "  Batch [1270/1299] D_loss: -0.5465, G_loss: -0.3526\n",
      "  Batch [1280/1299] D_loss: -0.4932, G_loss: 0.4690\n",
      "  Batch [1290/1299] D_loss: -0.6007, G_loss: 0.3044\n",
      "\n",
      "Epoch 4 Summary:\n",
      "  Average D_loss: -0.2169\n",
      "  Average G_loss: 0.0785\n",
      "\n",
      "Epoch [5/100]\n",
      "  Batch [0/1299] D_loss: -0.3243, G_loss: 0.4702\n",
      "  Batch [10/1299] D_loss: -0.4482, G_loss: -1.3747\n",
      "  Batch [20/1299] D_loss: -0.2631, G_loss: 0.3064\n",
      "  Batch [30/1299] D_loss: -0.0205, G_loss: 1.0429\n",
      "  Batch [40/1299] D_loss: -0.7547, G_loss: 1.0516\n",
      "  Batch [50/1299] D_loss: -1.3337, G_loss: -1.3290\n",
      "  Batch [60/1299] D_loss: -0.3386, G_loss: -0.2002\n",
      "  Batch [70/1299] D_loss: 0.0888, G_loss: -1.4514\n",
      "  Batch [80/1299] D_loss: -0.3118, G_loss: 0.8080\n",
      "  Batch [90/1299] D_loss: -0.9238, G_loss: -1.2978\n",
      "  Batch [100/1299] D_loss: -0.9526, G_loss: 1.6390\n",
      "  Batch [110/1299] D_loss: -0.9942, G_loss: 1.5286\n",
      "  Batch [120/1299] D_loss: 0.1705, G_loss: 0.3849\n",
      "  Batch [130/1299] D_loss: -0.4625, G_loss: 2.6561\n",
      "  Batch [140/1299] D_loss: -0.4227, G_loss: -0.0725\n",
      "  Batch [150/1299] D_loss: -0.1075, G_loss: -0.7569\n",
      "  Batch [160/1299] D_loss: -0.3513, G_loss: 0.5296\n",
      "  Batch [170/1299] D_loss: -0.6374, G_loss: 0.3003\n",
      "  Batch [180/1299] D_loss: -0.6179, G_loss: 0.6876\n",
      "  Batch [190/1299] D_loss: -0.0647, G_loss: -0.1925\n",
      "  Batch [200/1299] D_loss: -0.6806, G_loss: 1.0905\n",
      "  Batch [210/1299] D_loss: -0.5446, G_loss: 0.7365\n",
      "  Batch [220/1299] D_loss: -0.3132, G_loss: -0.6714\n",
      "  Batch [230/1299] D_loss: 0.0897, G_loss: 1.6074\n",
      "  Batch [240/1299] D_loss: -0.2738, G_loss: -1.6160\n",
      "  Batch [250/1299] D_loss: -0.1016, G_loss: 1.5858\n",
      "  Batch [260/1299] D_loss: -1.6609, G_loss: -2.6977\n",
      "  Batch [270/1299] D_loss: -0.0371, G_loss: 0.6051\n",
      "  Batch [280/1299] D_loss: -0.8267, G_loss: 1.3936\n",
      "  Batch [290/1299] D_loss: -0.0163, G_loss: -0.3764\n",
      "  Batch [300/1299] D_loss: 0.1998, G_loss: 0.4750\n",
      "  Batch [310/1299] D_loss: -0.5690, G_loss: -0.6022\n",
      "  Batch [320/1299] D_loss: -0.5198, G_loss: 1.1161\n",
      "  Batch [330/1299] D_loss: -0.5608, G_loss: 0.2521\n",
      "  Batch [340/1299] D_loss: -0.7354, G_loss: -0.4997\n",
      "  Batch [350/1299] D_loss: -0.2664, G_loss: 1.6887\n",
      "  Batch [360/1299] D_loss: -0.7008, G_loss: 1.1428\n",
      "  Batch [370/1299] D_loss: -0.1110, G_loss: 1.0932\n",
      "  Batch [380/1299] D_loss: -1.2080, G_loss: -2.0312\n",
      "  Batch [390/1299] D_loss: -0.3551, G_loss: 0.7834\n",
      "  Batch [400/1299] D_loss: 0.1604, G_loss: 0.5837\n",
      "  Batch [410/1299] D_loss: -0.3158, G_loss: 1.0935\n",
      "  Batch [420/1299] D_loss: -0.9594, G_loss: 1.8970\n",
      "  Batch [430/1299] D_loss: -0.4896, G_loss: 0.2319\n",
      "  Batch [440/1299] D_loss: -0.6607, G_loss: 0.0389\n",
      "  Batch [450/1299] D_loss: -0.0362, G_loss: -1.3184\n",
      "  Batch [460/1299] D_loss: -1.6403, G_loss: 4.7273\n",
      "  Batch [470/1299] D_loss: 0.0500, G_loss: 0.1111\n",
      "  Batch [480/1299] D_loss: -0.3676, G_loss: -0.4859\n",
      "  Batch [490/1299] D_loss: -0.2563, G_loss: -0.2354\n",
      "  Batch [500/1299] D_loss: -1.1385, G_loss: -4.7851\n",
      "  Batch [510/1299] D_loss: -0.4983, G_loss: 1.3242\n",
      "  Batch [520/1299] D_loss: -0.6123, G_loss: 1.7795\n",
      "  Batch [530/1299] D_loss: 0.1912, G_loss: -1.9979\n",
      "  Batch [540/1299] D_loss: -0.7336, G_loss: 1.2331\n",
      "  Batch [550/1299] D_loss: -0.0101, G_loss: 0.4235\n",
      "  Batch [560/1299] D_loss: -0.2312, G_loss: 0.0345\n",
      "  Batch [570/1299] D_loss: -0.3930, G_loss: 0.0968\n",
      "  Batch [580/1299] D_loss: -0.6093, G_loss: 0.4490\n",
      "  Batch [590/1299] D_loss: -0.7906, G_loss: 0.1794\n",
      "  Batch [600/1299] D_loss: -0.3696, G_loss: -0.4846\n",
      "  Batch [610/1299] D_loss: -0.5361, G_loss: 0.9426\n",
      "  Batch [620/1299] D_loss: -0.4507, G_loss: -1.7273\n",
      "  Batch [630/1299] D_loss: -0.7524, G_loss: 2.7597\n",
      "  Batch [640/1299] D_loss: -0.4385, G_loss: -0.9400\n",
      "  Batch [650/1299] D_loss: -0.2268, G_loss: 0.2434\n",
      "  Batch [660/1299] D_loss: -0.3954, G_loss: 0.2308\n",
      "  Batch [670/1299] D_loss: -0.4214, G_loss: 0.5150\n",
      "  Batch [680/1299] D_loss: -0.5251, G_loss: -1.8251\n",
      "  Batch [690/1299] D_loss: -0.2991, G_loss: 1.4000\n",
      "  Batch [700/1299] D_loss: -0.3461, G_loss: -0.1916\n",
      "  Batch [710/1299] D_loss: -0.1341, G_loss: 0.5901\n",
      "  Batch [720/1299] D_loss: -0.5676, G_loss: -1.7457\n",
      "  Batch [730/1299] D_loss: -0.2786, G_loss: 1.3431\n",
      "  Batch [740/1299] D_loss: -0.0901, G_loss: 0.0938\n",
      "  Batch [750/1299] D_loss: 0.2577, G_loss: -0.7474\n",
      "  Batch [760/1299] D_loss: -1.0987, G_loss: 3.0833\n",
      "  Batch [770/1299] D_loss: -0.9066, G_loss: -0.7158\n",
      "  Batch [780/1299] D_loss: -0.7512, G_loss: -1.4640\n",
      "  Batch [790/1299] D_loss: -0.8892, G_loss: -0.1803\n",
      "  Batch [800/1299] D_loss: -0.4974, G_loss: 0.4431\n",
      "  Batch [810/1299] D_loss: -0.6218, G_loss: -2.2428\n",
      "  Batch [820/1299] D_loss: -0.4139, G_loss: 0.9460\n",
      "  Batch [830/1299] D_loss: -0.4974, G_loss: -3.2597\n",
      "  Batch [840/1299] D_loss: -0.6986, G_loss: 2.8045\n",
      "  Batch [850/1299] D_loss: -0.4666, G_loss: -2.8769\n",
      "  Batch [860/1299] D_loss: -0.8675, G_loss: 1.2373\n",
      "  Batch [870/1299] D_loss: -0.3687, G_loss: -2.0045\n",
      "  Batch [880/1299] D_loss: -0.3934, G_loss: 0.5497\n",
      "  Batch [890/1299] D_loss: -0.2291, G_loss: -2.5439\n",
      "  Batch [900/1299] D_loss: -0.2663, G_loss: 1.7236\n",
      "  Batch [910/1299] D_loss: -0.1473, G_loss: -0.3273\n",
      "  Batch [920/1299] D_loss: -0.0449, G_loss: -0.7239\n",
      "  Batch [930/1299] D_loss: -0.3743, G_loss: 2.5774\n",
      "  Batch [940/1299] D_loss: -1.3477, G_loss: -2.3974\n",
      "  Batch [950/1299] D_loss: -0.1361, G_loss: 0.6552\n",
      "  Batch [960/1299] D_loss: -0.1089, G_loss: 0.3565\n",
      "  Batch [970/1299] D_loss: -0.3165, G_loss: 0.5630\n",
      "  Batch [980/1299] D_loss: -0.4963, G_loss: 1.0916\n",
      "  Batch [990/1299] D_loss: -0.1332, G_loss: -0.2211\n",
      "  Batch [1000/1299] D_loss: -0.6088, G_loss: 1.9070\n",
      "  Batch [1010/1299] D_loss: -0.1499, G_loss: -1.1717\n",
      "  Batch [1020/1299] D_loss: -1.1199, G_loss: -1.2916\n",
      "  Batch [1030/1299] D_loss: -0.5954, G_loss: 1.3070\n",
      "  Batch [1040/1299] D_loss: -0.7039, G_loss: 0.6542\n",
      "  Batch [1050/1299] D_loss: -0.4123, G_loss: -0.1984\n",
      "  Batch [1060/1299] D_loss: -0.3757, G_loss: 1.0907\n",
      "  Batch [1070/1299] D_loss: -0.2442, G_loss: -2.0368\n",
      "  Batch [1080/1299] D_loss: -0.2497, G_loss: 0.2346\n",
      "  Batch [1090/1299] D_loss: -0.0007, G_loss: 0.2680\n",
      "  Batch [1100/1299] D_loss: -0.0766, G_loss: -2.7485\n",
      "  Batch [1110/1299] D_loss: -0.4706, G_loss: 2.0816\n",
      "  Batch [1120/1299] D_loss: -0.5158, G_loss: 0.3371\n",
      "  Batch [1130/1299] D_loss: -0.0282, G_loss: -0.0978\n",
      "  Batch [1140/1299] D_loss: -0.4034, G_loss: -2.5790\n",
      "  Batch [1150/1299] D_loss: -0.9514, G_loss: -0.6922\n",
      "  Batch [1160/1299] D_loss: -0.9286, G_loss: -1.9710\n",
      "  Batch [1170/1299] D_loss: -0.2207, G_loss: -0.1586\n",
      "  Batch [1180/1299] D_loss: -0.2141, G_loss: -0.1631\n",
      "  Batch [1190/1299] D_loss: -0.2744, G_loss: 0.6881\n",
      "  Batch [1200/1299] D_loss: -0.7447, G_loss: 0.8796\n",
      "  Batch [1210/1299] D_loss: -1.1489, G_loss: -1.6094\n",
      "  Batch [1220/1299] D_loss: -0.7054, G_loss: -0.0122\n",
      "  Batch [1230/1299] D_loss: -0.5361, G_loss: -0.0140\n",
      "  Batch [1240/1299] D_loss: -0.0317, G_loss: -0.4265\n",
      "  Batch [1250/1299] D_loss: -0.2358, G_loss: 2.0198\n",
      "  Batch [1260/1299] D_loss: -0.7688, G_loss: 0.3853\n",
      "  Batch [1270/1299] D_loss: -1.4619, G_loss: -0.0491\n",
      "  Batch [1280/1299] D_loss: -0.4470, G_loss: -0.4409\n",
      "  Batch [1290/1299] D_loss: -0.3654, G_loss: 0.1223\n",
      "\n",
      "Epoch 5 Summary:\n",
      "  Average D_loss: -0.2087\n",
      "  Average G_loss: 0.0787\n",
      "\n",
      "Epoch [6/100]\n",
      "  Batch [0/1299] D_loss: -0.1848, G_loss: 0.1347\n",
      "  Batch [10/1299] D_loss: -1.4797, G_loss: -0.2315\n",
      "  Batch [20/1299] D_loss: -0.8235, G_loss: -2.5190\n",
      "  Batch [30/1299] D_loss: -0.8453, G_loss: 2.3072\n",
      "  Batch [40/1299] D_loss: -0.3276, G_loss: -0.2852\n",
      "  Batch [50/1299] D_loss: -0.1481, G_loss: 1.9452\n",
      "  Batch [60/1299] D_loss: -0.8508, G_loss: -1.1154\n",
      "  Batch [70/1299] D_loss: -0.6070, G_loss: 1.2560\n",
      "  Batch [80/1299] D_loss: -0.6428, G_loss: -0.6469\n",
      "  Batch [90/1299] D_loss: -0.0651, G_loss: -1.6355\n",
      "  Batch [100/1299] D_loss: -0.7513, G_loss: 1.5177\n",
      "  Batch [110/1299] D_loss: -1.4850, G_loss: 1.5126\n",
      "  Batch [120/1299] D_loss: -1.2070, G_loss: 0.9522\n",
      "  Batch [130/1299] D_loss: -0.5789, G_loss: -2.2816\n",
      "  Batch [140/1299] D_loss: -0.9657, G_loss: 1.6244\n",
      "  Batch [150/1299] D_loss: -0.7138, G_loss: 0.3191\n",
      "  Batch [160/1299] D_loss: -0.8558, G_loss: 1.8134\n",
      "  Batch [170/1299] D_loss: -0.9283, G_loss: 0.3299\n",
      "  Batch [180/1299] D_loss: -0.5606, G_loss: -1.6910\n",
      "  Batch [190/1299] D_loss: -0.3882, G_loss: 1.5720\n",
      "  Batch [200/1299] D_loss: -1.1214, G_loss: -2.4505\n",
      "  Batch [210/1299] D_loss: -0.4373, G_loss: 1.2311\n",
      "  Batch [220/1299] D_loss: -0.2908, G_loss: -2.4647\n",
      "  Batch [230/1299] D_loss: -0.7338, G_loss: 4.1665\n",
      "  Batch [240/1299] D_loss: -0.4691, G_loss: -0.0869\n",
      "  Batch [250/1299] D_loss: -1.0728, G_loss: -1.8235\n",
      "  Batch [260/1299] D_loss: -0.2633, G_loss: -2.7804\n",
      "  Batch [270/1299] D_loss: -0.7432, G_loss: 3.3152\n",
      "  Batch [280/1299] D_loss: -0.2779, G_loss: -3.1421\n",
      "  Batch [290/1299] D_loss: -0.7892, G_loss: 3.3700\n",
      "  Batch [300/1299] D_loss: -0.7102, G_loss: 1.3117\n",
      "  Batch [310/1299] D_loss: 0.2399, G_loss: -0.7597\n",
      "  Batch [320/1299] D_loss: -0.1578, G_loss: -0.7804\n",
      "  Batch [330/1299] D_loss: -0.1073, G_loss: 1.7843\n",
      "  Batch [340/1299] D_loss: -0.3521, G_loss: 0.1725\n",
      "  Batch [350/1299] D_loss: -0.8978, G_loss: -2.6004\n",
      "  Batch [360/1299] D_loss: -0.5215, G_loss: 0.7353\n",
      "  Batch [370/1299] D_loss: -0.1493, G_loss: 0.9024\n",
      "  Batch [380/1299] D_loss: -0.3627, G_loss: -1.8750\n",
      "  Batch [390/1299] D_loss: -0.4412, G_loss: 0.4535\n",
      "  Batch [400/1299] D_loss: -0.8007, G_loss: 1.7925\n",
      "  Batch [410/1299] D_loss: -0.6569, G_loss: 0.0358\n",
      "  Batch [420/1299] D_loss: -0.9369, G_loss: 0.1252\n",
      "  Batch [430/1299] D_loss: -0.3391, G_loss: -2.3910\n",
      "  Batch [440/1299] D_loss: -0.7561, G_loss: 0.0352\n",
      "  Batch [450/1299] D_loss: -0.1439, G_loss: 0.9232\n",
      "  Batch [460/1299] D_loss: -0.6084, G_loss: -3.5137\n",
      "  Batch [470/1299] D_loss: -0.2054, G_loss: 1.5338\n",
      "  Batch [480/1299] D_loss: 0.2463, G_loss: 0.7959\n",
      "  Batch [490/1299] D_loss: -0.5691, G_loss: -1.5538\n",
      "  Batch [500/1299] D_loss: -0.1307, G_loss: -1.3433\n",
      "  Batch [510/1299] D_loss: -0.3923, G_loss: -0.5600\n",
      "  Batch [520/1299] D_loss: -0.3545, G_loss: 1.3344\n",
      "  Batch [530/1299] D_loss: 0.1346, G_loss: -1.9726\n",
      "  Batch [540/1299] D_loss: -1.8138, G_loss: 2.5940\n",
      "  Batch [550/1299] D_loss: -0.2210, G_loss: 0.0848\n",
      "  Batch [560/1299] D_loss: -0.7090, G_loss: 0.7155\n",
      "  Batch [570/1299] D_loss: -0.5583, G_loss: -0.0912\n",
      "  Batch [580/1299] D_loss: -0.8229, G_loss: 0.1015\n",
      "  Batch [590/1299] D_loss: -1.0887, G_loss: -0.6911\n",
      "  Batch [600/1299] D_loss: -1.0374, G_loss: 0.8376\n",
      "  Batch [610/1299] D_loss: -0.5186, G_loss: 0.1290\n",
      "  Batch [620/1299] D_loss: -0.0400, G_loss: 1.2468\n",
      "  Batch [630/1299] D_loss: -0.4869, G_loss: -0.9676\n",
      "  Batch [640/1299] D_loss: -0.4305, G_loss: -2.4505\n",
      "  Batch [650/1299] D_loss: 0.0766, G_loss: 1.5256\n",
      "  Batch [660/1299] D_loss: -0.7434, G_loss: -0.6319\n",
      "  Batch [670/1299] D_loss: 0.0474, G_loss: -0.5052\n",
      "  Batch [680/1299] D_loss: -0.1810, G_loss: -1.4503\n",
      "  Batch [690/1299] D_loss: -1.1550, G_loss: 4.7771\n",
      "  Batch [700/1299] D_loss: -0.3223, G_loss: 1.0078\n",
      "  Batch [710/1299] D_loss: 0.0218, G_loss: -1.0872\n",
      "  Batch [720/1299] D_loss: -0.0144, G_loss: 0.7889\n",
      "  Batch [730/1299] D_loss: -0.0214, G_loss: -0.0590\n",
      "  Batch [740/1299] D_loss: -0.0570, G_loss: 0.0751\n",
      "  Batch [750/1299] D_loss: 0.3584, G_loss: -0.6952\n",
      "  Batch [760/1299] D_loss: -1.6466, G_loss: -4.8471\n",
      "  Batch [770/1299] D_loss: -1.2340, G_loss: 3.4064\n",
      "  Batch [780/1299] D_loss: -0.3298, G_loss: 0.1745\n",
      "  Batch [790/1299] D_loss: 0.0024, G_loss: -0.1926\n",
      "  Batch [800/1299] D_loss: -1.8779, G_loss: 1.1086\n",
      "  Batch [810/1299] D_loss: 0.9620, G_loss: -1.0449\n",
      "  Batch [820/1299] D_loss: 0.0214, G_loss: -0.0993\n",
      "  Batch [830/1299] D_loss: -0.2738, G_loss: 1.2673\n",
      "  Batch [840/1299] D_loss: -1.3571, G_loss: -0.9988\n",
      "  Batch [850/1299] D_loss: -0.0867, G_loss: 0.6335\n",
      "  Batch [860/1299] D_loss: 0.0014, G_loss: -0.9122\n",
      "  Batch [870/1299] D_loss: 0.0000, G_loss: 0.6048\n",
      "  Batch [880/1299] D_loss: 0.0472, G_loss: -1.1873\n",
      "  Batch [890/1299] D_loss: -0.8736, G_loss: 2.3479\n",
      "  Batch [900/1299] D_loss: -0.4003, G_loss: -0.7121\n",
      "  Batch [910/1299] D_loss: -0.4680, G_loss: 1.5915\n",
      "  Batch [920/1299] D_loss: 0.0572, G_loss: 0.3389\n",
      "  Batch [930/1299] D_loss: -0.0907, G_loss: -0.4286\n",
      "  Batch [940/1299] D_loss: -0.1568, G_loss: 0.4068\n",
      "  Batch [950/1299] D_loss: -1.1468, G_loss: -3.2574\n",
      "  Batch [960/1299] D_loss: -0.5138, G_loss: 2.8347\n",
      "  Batch [970/1299] D_loss: -0.0797, G_loss: -0.5019\n",
      "  Batch [980/1299] D_loss: -0.1296, G_loss: 1.3548\n",
      "  Batch [990/1299] D_loss: -0.3180, G_loss: 0.1944\n",
      "  Batch [1000/1299] D_loss: 0.0094, G_loss: -1.7875\n",
      "  Batch [1010/1299] D_loss: -0.7852, G_loss: 1.9968\n",
      "  Batch [1020/1299] D_loss: -0.4440, G_loss: 0.5370\n",
      "  Batch [1030/1299] D_loss: -0.1193, G_loss: 0.1246\n",
      "  Batch [1040/1299] D_loss: 0.5175, G_loss: 1.2002\n",
      "  Batch [1050/1299] D_loss: -2.1825, G_loss: -3.1907\n",
      "  Batch [1060/1299] D_loss: 0.1281, G_loss: -0.5587\n",
      "  Batch [1070/1299] D_loss: -0.0316, G_loss: 0.9565\n",
      "  Batch [1080/1299] D_loss: -0.1003, G_loss: -0.9101\n",
      "  Batch [1090/1299] D_loss: -0.1437, G_loss: 1.7240\n",
      "  Batch [1100/1299] D_loss: -0.2712, G_loss: 0.3577\n",
      "  Batch [1110/1299] D_loss: -0.8152, G_loss: -0.7539\n",
      "  Batch [1120/1299] D_loss: -0.2977, G_loss: -1.7800\n",
      "  Batch [1130/1299] D_loss: -0.9273, G_loss: 3.1271\n",
      "  Batch [1140/1299] D_loss: -0.3595, G_loss: -0.8734\n",
      "  Batch [1150/1299] D_loss: -0.2455, G_loss: 0.3578\n",
      "  Batch [1160/1299] D_loss: -0.4684, G_loss: 2.6770\n",
      "  Batch [1170/1299] D_loss: -0.7527, G_loss: 0.1139\n",
      "  Batch [1180/1299] D_loss: -0.5088, G_loss: 0.3158\n",
      "  Batch [1190/1299] D_loss: -0.8894, G_loss: 2.5249\n",
      "  Batch [1200/1299] D_loss: -1.1166, G_loss: -2.8194\n",
      "  Batch [1210/1299] D_loss: -0.0742, G_loss: -0.7029\n",
      "  Batch [1220/1299] D_loss: -0.7957, G_loss: 2.0526\n",
      "  Batch [1230/1299] D_loss: -0.7736, G_loss: -0.3824\n",
      "  Batch [1240/1299] D_loss: -0.2272, G_loss: 0.8049\n",
      "  Batch [1250/1299] D_loss: -0.2201, G_loss: 0.6538\n",
      "  Batch [1260/1299] D_loss: -0.4496, G_loss: -0.2235\n",
      "  Batch [1270/1299] D_loss: -0.5923, G_loss: 1.9000\n",
      "  Batch [1280/1299] D_loss: -0.7671, G_loss: -3.4156\n",
      "  Batch [1290/1299] D_loss: -0.3068, G_loss: 2.4184\n",
      "\n",
      "Epoch 6 Summary:\n",
      "  Average D_loss: -0.2090\n",
      "  Average G_loss: 0.0618\n",
      "\n",
      "Epoch [7/100]\n",
      "  Batch [0/1299] D_loss: -1.0389, G_loss: -2.1419\n",
      "  Batch [10/1299] D_loss: -0.0953, G_loss: -0.1179\n",
      "  Batch [20/1299] D_loss: -0.4890, G_loss: 2.5301\n",
      "  Batch [30/1299] D_loss: -0.6417, G_loss: -2.6251\n",
      "  Batch [40/1299] D_loss: -0.4114, G_loss: 1.4530\n",
      "  Batch [50/1299] D_loss: -0.3148, G_loss: -1.6104\n",
      "  Batch [60/1299] D_loss: -0.3937, G_loss: 0.2564\n",
      "  Batch [70/1299] D_loss: -0.1590, G_loss: 1.5162\n",
      "  Batch [80/1299] D_loss: -0.7248, G_loss: -4.0021\n",
      "  Batch [90/1299] D_loss: -0.3304, G_loss: 1.9406\n",
      "  Batch [100/1299] D_loss: -0.3227, G_loss: -2.2844\n",
      "  Batch [110/1299] D_loss: -0.3618, G_loss: 0.6261\n",
      "  Batch [120/1299] D_loss: 0.0428, G_loss: 1.6340\n",
      "  Batch [130/1299] D_loss: -0.1289, G_loss: -2.5320\n",
      "  Batch [140/1299] D_loss: -0.7383, G_loss: -0.0714\n",
      "  Batch [150/1299] D_loss: -0.2118, G_loss: -0.3536\n",
      "  Batch [160/1299] D_loss: 0.0075, G_loss: -1.6796\n",
      "  Batch [170/1299] D_loss: -0.0897, G_loss: -0.0676\n",
      "  Batch [180/1299] D_loss: -1.0285, G_loss: 2.3945\n",
      "  Batch [190/1299] D_loss: -0.6190, G_loss: 0.5905\n",
      "  Batch [200/1299] D_loss: -0.4251, G_loss: -2.2183\n",
      "  Batch [210/1299] D_loss: -0.0150, G_loss: 1.0721\n",
      "  Batch [220/1299] D_loss: -0.1927, G_loss: 0.1315\n",
      "  Batch [230/1299] D_loss: -0.6937, G_loss: -1.5723\n",
      "  Batch [240/1299] D_loss: -0.5442, G_loss: 0.3905\n",
      "  Batch [250/1299] D_loss: -0.3957, G_loss: -0.8975\n",
      "  Batch [260/1299] D_loss: -0.2701, G_loss: 0.2007\n",
      "  Batch [270/1299] D_loss: -0.9958, G_loss: -0.9596\n",
      "  Batch [280/1299] D_loss: -0.1629, G_loss: -1.2156\n",
      "  Batch [290/1299] D_loss: -0.2761, G_loss: 0.2534\n",
      "  Batch [300/1299] D_loss: -0.5319, G_loss: -1.3876\n",
      "  Batch [310/1299] D_loss: -0.3583, G_loss: -0.0079\n",
      "  Batch [320/1299] D_loss: -0.4142, G_loss: 1.7085\n",
      "  Batch [330/1299] D_loss: -0.5412, G_loss: 0.8717\n",
      "  Batch [340/1299] D_loss: -0.7507, G_loss: 1.3933\n",
      "  Batch [350/1299] D_loss: 0.1949, G_loss: -1.4799\n",
      "  Batch [360/1299] D_loss: -0.8428, G_loss: 2.4196\n",
      "  Batch [370/1299] D_loss: -0.8264, G_loss: -1.2728\n",
      "  Batch [380/1299] D_loss: -0.3903, G_loss: -3.1593\n",
      "  Batch [390/1299] D_loss: -0.7094, G_loss: 3.0206\n",
      "  Batch [400/1299] D_loss: -0.0873, G_loss: -1.5608\n",
      "  Batch [410/1299] D_loss: -0.2196, G_loss: 1.1887\n",
      "  Batch [420/1299] D_loss: -0.4409, G_loss: 1.1497\n",
      "  Batch [430/1299] D_loss: -0.2397, G_loss: -0.8528\n",
      "  Batch [440/1299] D_loss: -0.1518, G_loss: 1.4064\n",
      "  Batch [450/1299] D_loss: -0.1378, G_loss: 0.6467\n",
      "  Batch [460/1299] D_loss: -0.4859, G_loss: -1.1070\n",
      "  Batch [470/1299] D_loss: -0.2493, G_loss: 0.2863\n",
      "  Batch [480/1299] D_loss: -0.5244, G_loss: 2.7985\n",
      "  Batch [490/1299] D_loss: -1.3258, G_loss: 1.4728\n",
      "  Batch [500/1299] D_loss: -1.1363, G_loss: -2.4577\n",
      "  Batch [510/1299] D_loss: -0.3687, G_loss: -0.1024\n",
      "  Batch [520/1299] D_loss: -0.3051, G_loss: 1.3900\n",
      "  Batch [530/1299] D_loss: -0.3773, G_loss: -2.5846\n",
      "  Batch [540/1299] D_loss: -1.0817, G_loss: 5.0640\n",
      "  Batch [550/1299] D_loss: 0.0758, G_loss: -4.3709\n",
      "  Batch [560/1299] D_loss: -0.0439, G_loss: -0.8571\n",
      "  Batch [570/1299] D_loss: 0.0963, G_loss: 0.8580\n",
      "  Batch [580/1299] D_loss: -0.6634, G_loss: -3.2654\n",
      "  Batch [590/1299] D_loss: 0.0724, G_loss: -0.1172\n",
      "  Batch [600/1299] D_loss: -0.2676, G_loss: 3.2957\n",
      "  Batch [610/1299] D_loss: -0.0003, G_loss: -0.0161\n",
      "  Batch [620/1299] D_loss: 0.0005, G_loss: -0.0188\n",
      "  Batch [630/1299] D_loss: -0.0001, G_loss: -0.0195\n",
      "  Batch [640/1299] D_loss: -0.0021, G_loss: -0.0208\n",
      "  Batch [650/1299] D_loss: -0.0015, G_loss: -0.0195\n",
      "  Batch [660/1299] D_loss: -0.0033, G_loss: -0.0236\n",
      "  Batch [670/1299] D_loss: -0.0073, G_loss: -0.0285\n",
      "  Batch [680/1299] D_loss: -0.0123, G_loss: -0.0376\n",
      "  Batch [690/1299] D_loss: -1.2382, G_loss: 4.3017\n",
      "  Batch [700/1299] D_loss: -0.0480, G_loss: -0.0343\n",
      "  Batch [710/1299] D_loss: -0.0724, G_loss: -0.1606\n",
      "  Batch [720/1299] D_loss: -0.0308, G_loss: -0.4878\n",
      "  Batch [730/1299] D_loss: -0.0781, G_loss: -0.5472\n",
      "  Batch [740/1299] D_loss: -0.1703, G_loss: -0.6513\n",
      "  Batch [750/1299] D_loss: -0.0205, G_loss: -0.5019\n",
      "  Batch [760/1299] D_loss: -0.0126, G_loss: -0.0801\n",
      "  Batch [770/1299] D_loss: -2.2277, G_loss: 5.5365\n",
      "  Batch [780/1299] D_loss: -0.0636, G_loss: -0.1269\n",
      "  Batch [790/1299] D_loss: -0.0749, G_loss: -0.2410\n",
      "  Batch [800/1299] D_loss: -0.1346, G_loss: -0.3558\n",
      "  Batch [810/1299] D_loss: -0.1506, G_loss: -0.4172\n",
      "  Batch [820/1299] D_loss: -0.0103, G_loss: -0.2170\n",
      "  Batch [830/1299] D_loss: -0.0194, G_loss: -0.1803\n",
      "  Batch [840/1299] D_loss: -0.2953, G_loss: 4.6533\n",
      "  Batch [850/1299] D_loss: -0.0136, G_loss: -0.0865\n",
      "  Batch [860/1299] D_loss: -0.1004, G_loss: -0.1881\n",
      "  Batch [870/1299] D_loss: -0.0129, G_loss: -0.2172\n",
      "  Batch [880/1299] D_loss: -0.0933, G_loss: -0.3717\n",
      "  Batch [890/1299] D_loss: -0.2316, G_loss: -0.4105\n",
      "  Batch [900/1299] D_loss: -0.0295, G_loss: -0.1797\n",
      "  Batch [910/1299] D_loss: -1.3103, G_loss: 3.7172\n",
      "  Batch [920/1299] D_loss: -0.0585, G_loss: -0.1466\n",
      "  Batch [930/1299] D_loss: -0.0931, G_loss: -0.2640\n",
      "  Batch [940/1299] D_loss: -0.0497, G_loss: -0.3880\n",
      "  Batch [950/1299] D_loss: -0.0590, G_loss: -0.3689\n",
      "  Batch [960/1299] D_loss: 0.0115, G_loss: -0.1531\n",
      "  Batch [970/1299] D_loss: 0.0089, G_loss: -0.0548\n",
      "  Batch [980/1299] D_loss: -0.5816, G_loss: 1.8316\n",
      "  Batch [990/1299] D_loss: -0.0096, G_loss: -0.1493\n",
      "  Batch [1000/1299] D_loss: -0.1218, G_loss: -0.3379\n",
      "  Batch [1010/1299] D_loss: -0.0238, G_loss: -0.4664\n",
      "  Batch [1020/1299] D_loss: -0.0565, G_loss: -0.3911\n",
      "  Batch [1030/1299] D_loss: -0.0247, G_loss: -0.1665\n",
      "  Batch [1040/1299] D_loss: -0.5902, G_loss: 1.7063\n",
      "  Batch [1050/1299] D_loss: -0.0724, G_loss: -0.1241\n",
      "  Batch [1060/1299] D_loss: -0.0215, G_loss: -0.1716\n",
      "  Batch [1070/1299] D_loss: -0.0877, G_loss: -0.3498\n",
      "  Batch [1080/1299] D_loss: -0.2835, G_loss: -0.4494\n",
      "  Batch [1090/1299] D_loss: -0.0463, G_loss: -0.3676\n",
      "  Batch [1100/1299] D_loss: -0.0355, G_loss: -0.2600\n",
      "  Batch [1110/1299] D_loss: -0.0384, G_loss: -0.1670\n",
      "  Batch [1120/1299] D_loss: -0.0150, G_loss: -0.0462\n",
      "  Batch [1130/1299] D_loss: -0.0918, G_loss: -0.1263\n",
      "  Batch [1140/1299] D_loss: -0.1559, G_loss: -0.4019\n",
      "  Batch [1150/1299] D_loss: -0.1380, G_loss: -0.4231\n",
      "  Batch [1160/1299] D_loss: -0.0249, G_loss: -0.3399\n",
      "  Batch [1170/1299] D_loss: -0.0062, G_loss: -0.1655\n",
      "  Batch [1180/1299] D_loss: 0.0334, G_loss: -0.2152\n",
      "  Batch [1190/1299] D_loss: -3.1615, G_loss: 7.2898\n",
      "  Batch [1200/1299] D_loss: -0.0634, G_loss: -0.1147\n",
      "  Batch [1210/1299] D_loss: -0.1595, G_loss: -0.3166\n",
      "  Batch [1220/1299] D_loss: -0.1009, G_loss: -0.5147\n",
      "  Batch [1230/1299] D_loss: -0.1113, G_loss: -0.4433\n",
      "  Batch [1240/1299] D_loss: -0.0297, G_loss: -0.3627\n",
      "  Batch [1250/1299] D_loss: -0.0825, G_loss: -0.3731\n",
      "  Batch [1260/1299] D_loss: -0.0007, G_loss: -0.1484\n",
      "  Batch [1270/1299] D_loss: -0.0311, G_loss: -0.0651\n",
      "  Batch [1280/1299] D_loss: -0.1077, G_loss: 1.3750\n",
      "  Batch [1290/1299] D_loss: -0.0089, G_loss: -0.0325\n",
      "\n",
      "Epoch 7 Summary:\n",
      "  Average D_loss: -0.1286\n",
      "  Average G_loss: 0.0545\n",
      "\n",
      "Epoch [8/100]\n",
      "  Batch [0/1299] D_loss: -0.0216, G_loss: -0.1635\n",
      "  Batch [10/1299] D_loss: -0.1980, G_loss: -0.2709\n",
      "  Batch [20/1299] D_loss: -0.0786, G_loss: -0.3835\n",
      "  Batch [30/1299] D_loss: -0.0987, G_loss: -0.2314\n",
      "  Batch [40/1299] D_loss: -0.0567, G_loss: -0.3825\n",
      "  Batch [50/1299] D_loss: -0.0060, G_loss: -0.2505\n",
      "  Batch [60/1299] D_loss: -0.0310, G_loss: -0.0760\n",
      "  Batch [70/1299] D_loss: -0.1090, G_loss: 0.7235\n",
      "  Batch [80/1299] D_loss: -0.0124, G_loss: -0.0965\n",
      "  Batch [90/1299] D_loss: -0.0902, G_loss: -0.1584\n",
      "  Batch [100/1299] D_loss: -0.1296, G_loss: -0.3687\n",
      "  Batch [110/1299] D_loss: -0.0059, G_loss: -0.2785\n",
      "  Batch [120/1299] D_loss: -0.0201, G_loss: -0.1604\n",
      "  Batch [130/1299] D_loss: -0.5338, G_loss: 2.1930\n",
      "  Batch [140/1299] D_loss: -0.0444, G_loss: -0.1010\n",
      "  Batch [150/1299] D_loss: -0.0109, G_loss: -0.2982\n",
      "  Batch [160/1299] D_loss: -0.2216, G_loss: -0.4743\n",
      "  Batch [170/1299] D_loss: -0.0237, G_loss: -0.6047\n",
      "  Batch [180/1299] D_loss: -0.1091, G_loss: -0.5003\n",
      "  Batch [190/1299] D_loss: 0.0178, G_loss: -0.0778\n",
      "  Batch [200/1299] D_loss: -0.1516, G_loss: 0.6120\n",
      "  Batch [210/1299] D_loss: -0.0291, G_loss: -0.0807\n",
      "  Batch [220/1299] D_loss: -0.0382, G_loss: -0.1616\n",
      "  Batch [230/1299] D_loss: -0.1371, G_loss: -0.2584\n",
      "  Batch [240/1299] D_loss: -0.1592, G_loss: -0.4143\n",
      "  Batch [250/1299] D_loss: 0.0510, G_loss: -0.3008\n",
      "  Batch [260/1299] D_loss: 0.0151, G_loss: -0.0861\n",
      "  Batch [270/1299] D_loss: -2.3696, G_loss: 5.8304\n",
      "  Batch [280/1299] D_loss: -0.0095, G_loss: -0.0433\n",
      "  Batch [290/1299] D_loss: -0.0238, G_loss: -0.0793\n",
      "  Batch [300/1299] D_loss: 0.0172, G_loss: -0.2231\n",
      "  Batch [310/1299] D_loss: -0.0138, G_loss: -0.3230\n",
      "  Batch [320/1299] D_loss: -0.0040, G_loss: -0.4767\n",
      "  Batch [330/1299] D_loss: 0.0572, G_loss: -0.3703\n",
      "  Batch [340/1299] D_loss: -0.5730, G_loss: 3.0773\n",
      "  Batch [350/1299] D_loss: -0.0592, G_loss: -0.1440\n",
      "  Batch [360/1299] D_loss: -0.0239, G_loss: -0.2516\n",
      "  Batch [370/1299] D_loss: -0.1025, G_loss: -0.2556\n",
      "  Batch [380/1299] D_loss: -0.0311, G_loss: -0.2676\n",
      "  Batch [390/1299] D_loss: -0.0070, G_loss: -0.1074\n",
      "  Batch [400/1299] D_loss: -0.1860, G_loss: 0.5082\n",
      "  Batch [410/1299] D_loss: -0.0456, G_loss: -0.0787\n",
      "  Batch [420/1299] D_loss: -0.1385, G_loss: -0.2294\n",
      "  Batch [430/1299] D_loss: -0.0959, G_loss: -0.3803\n",
      "  Batch [440/1299] D_loss: -0.0957, G_loss: -0.5874\n",
      "  Batch [450/1299] D_loss: -0.0437, G_loss: -0.2950\n",
      "  Batch [460/1299] D_loss: -0.1011, G_loss: 0.4128\n",
      "  Batch [470/1299] D_loss: -0.0246, G_loss: -0.0660\n",
      "  Batch [480/1299] D_loss: -0.1097, G_loss: -0.2687\n",
      "  Batch [490/1299] D_loss: -0.1206, G_loss: -0.3367\n",
      "  Batch [500/1299] D_loss: -0.0544, G_loss: -0.5908\n",
      "  Batch [510/1299] D_loss: -0.0404, G_loss: -0.4582\n",
      "  Batch [520/1299] D_loss: -0.0320, G_loss: -0.2551\n",
      "  Batch [530/1299] D_loss: -0.1010, G_loss: 0.3308\n",
      "  Batch [540/1299] D_loss: -0.0408, G_loss: -0.0746\n",
      "  Batch [550/1299] D_loss: -0.1100, G_loss: -0.1733\n",
      "  Batch [560/1299] D_loss: -0.0408, G_loss: -0.2882\n",
      "  Batch [570/1299] D_loss: -0.0684, G_loss: -0.2804\n",
      "  Batch [580/1299] D_loss: -0.0338, G_loss: -0.2003\n",
      "  Batch [590/1299] D_loss: -0.2005, G_loss: 0.4219\n",
      "  Batch [600/1299] D_loss: -0.0046, G_loss: -0.1004\n",
      "  Batch [610/1299] D_loss: -0.0612, G_loss: -0.1927\n",
      "  Batch [620/1299] D_loss: -0.0601, G_loss: -0.2716\n",
      "  Batch [630/1299] D_loss: -0.0953, G_loss: -0.2454\n",
      "  Batch [640/1299] D_loss: -0.0714, G_loss: -0.1847\n",
      "  Batch [650/1299] D_loss: -0.0014, G_loss: -0.1415\n",
      "  Batch [660/1299] D_loss: -0.0249, G_loss: -0.0869\n",
      "  Batch [670/1299] D_loss: -1.4177, G_loss: 4.0418\n",
      "  Batch [680/1299] D_loss: -0.0465, G_loss: -0.1453\n",
      "  Batch [690/1299] D_loss: -0.1124, G_loss: -0.3251\n",
      "  Batch [700/1299] D_loss: -0.1248, G_loss: -0.4457\n",
      "  Batch [710/1299] D_loss: -0.1235, G_loss: -0.5281\n",
      "  Batch [720/1299] D_loss: -0.2685, G_loss: -0.5822\n",
      "  Batch [730/1299] D_loss: -0.0944, G_loss: -0.4016\n",
      "  Batch [740/1299] D_loss: -0.0234, G_loss: -0.1277\n",
      "  Batch [750/1299] D_loss: 0.0097, G_loss: -0.0425\n",
      "  Batch [760/1299] D_loss: -0.0114, G_loss: -0.0347\n",
      "  Batch [770/1299] D_loss: -1.4339, G_loss: 3.5260\n",
      "  Batch [780/1299] D_loss: -0.0077, G_loss: -0.1598\n",
      "  Batch [790/1299] D_loss: -0.2293, G_loss: -0.3836\n",
      "  Batch [800/1299] D_loss: -0.0496, G_loss: -0.5066\n",
      "  Batch [810/1299] D_loss: -0.0956, G_loss: -0.5764\n",
      "  Batch [820/1299] D_loss: -0.0250, G_loss: -0.1610\n",
      "  Batch [830/1299] D_loss: -0.3994, G_loss: 1.2515\n",
      "  Batch [840/1299] D_loss: -0.0225, G_loss: -0.1425\n",
      "  Batch [850/1299] D_loss: -0.0490, G_loss: -0.3245\n",
      "  Batch [860/1299] D_loss: -0.1334, G_loss: -0.3569\n",
      "  Batch [870/1299] D_loss: -0.0207, G_loss: -0.3182\n",
      "  Batch [880/1299] D_loss: 0.0264, G_loss: -0.2764\n",
      "  Batch [890/1299] D_loss: -0.0316, G_loss: 0.0593\n",
      "  Batch [900/1299] D_loss: -0.0200, G_loss: -0.0747\n",
      "  Batch [910/1299] D_loss: -0.0076, G_loss: -0.1025\n",
      "  Batch [920/1299] D_loss: -0.0332, G_loss: -0.1456\n",
      "  Batch [930/1299] D_loss: -0.1165, G_loss: -0.2355\n",
      "  Batch [940/1299] D_loss: -0.0566, G_loss: -0.3464\n",
      "  Batch [950/1299] D_loss: -0.0728, G_loss: -0.2117\n",
      "  Batch [960/1299] D_loss: -0.0295, G_loss: -0.2034\n",
      "  Batch [970/1299] D_loss: 0.0756, G_loss: 0.3636\n",
      "  Batch [980/1299] D_loss: -0.0287, G_loss: -0.0670\n",
      "  Batch [990/1299] D_loss: -0.0389, G_loss: -0.1316\n",
      "  Batch [1000/1299] D_loss: -0.0790, G_loss: -0.1811\n",
      "  Batch [1010/1299] D_loss: 0.0338, G_loss: -0.2339\n",
      "  Batch [1020/1299] D_loss: 0.0132, G_loss: -0.2162\n",
      "  Batch [1030/1299] D_loss: -0.0942, G_loss: -0.2928\n",
      "  Batch [1040/1299] D_loss: -0.0256, G_loss: -0.1290\n",
      "  Batch [1050/1299] D_loss: -0.0012, G_loss: -0.2546\n",
      "  Batch [1060/1299] D_loss: -0.6971, G_loss: 2.2356\n",
      "  Batch [1070/1299] D_loss: -0.0476, G_loss: -0.0810\n",
      "  Batch [1080/1299] D_loss: -0.1508, G_loss: -0.2392\n",
      "  Batch [1090/1299] D_loss: -0.0200, G_loss: -0.3607\n",
      "  Batch [1100/1299] D_loss: -0.1664, G_loss: -0.6483\n",
      "  Batch [1110/1299] D_loss: -0.0406, G_loss: -0.4371\n",
      "  Batch [1120/1299] D_loss: -0.0105, G_loss: -0.2403\n",
      "  Batch [1130/1299] D_loss: -0.0754, G_loss: -0.2070\n",
      "  Batch [1140/1299] D_loss: -0.0151, G_loss: -0.0796\n",
      "  Batch [1150/1299] D_loss: -0.0115, G_loss: -0.0301\n",
      "  Batch [1160/1299] D_loss: -1.2965, G_loss: 3.6090\n",
      "  Batch [1170/1299] D_loss: -0.0377, G_loss: -0.1301\n",
      "  Batch [1180/1299] D_loss: -0.1480, G_loss: -0.2963\n",
      "  Batch [1190/1299] D_loss: -0.0189, G_loss: -0.2875\n",
      "  Batch [1200/1299] D_loss: -0.0833, G_loss: -0.3640\n",
      "  Batch [1210/1299] D_loss: -0.0472, G_loss: -0.3818\n",
      "  Batch [1220/1299] D_loss: -0.0132, G_loss: -0.2414\n",
      "  Batch [1230/1299] D_loss: -0.0562, G_loss: -0.2375\n",
      "  Batch [1240/1299] D_loss: -0.6928, G_loss: 2.1389\n",
      "  Batch [1250/1299] D_loss: 0.0066, G_loss: -0.1183\n",
      "  Batch [1260/1299] D_loss: -0.0648, G_loss: -0.1929\n",
      "  Batch [1270/1299] D_loss: -0.0865, G_loss: -0.1830\n",
      "  Batch [1280/1299] D_loss: -0.0946, G_loss: -0.1652\n",
      "  Batch [1290/1299] D_loss: -0.0462, G_loss: -0.1561\n",
      "\n",
      "Epoch 8 Summary:\n",
      "  Average D_loss: -0.0599\n",
      "  Average G_loss: 0.0454\n",
      "\n",
      "Epoch [9/100]\n",
      "  Batch [0/1299] D_loss: -0.0324, G_loss: -0.0787\n",
      "  Batch [10/1299] D_loss: 0.0005, G_loss: -0.1216\n",
      "  Batch [20/1299] D_loss: -0.6261, G_loss: 3.7681\n",
      "  Batch [30/1299] D_loss: 0.0089, G_loss: -0.1032\n",
      "  Batch [40/1299] D_loss: -0.0307, G_loss: -0.1408\n",
      "  Batch [50/1299] D_loss: -0.1165, G_loss: -0.3275\n",
      "  Batch [60/1299] D_loss: 0.0108, G_loss: -0.3164\n",
      "  Batch [70/1299] D_loss: -0.0899, G_loss: -0.3147\n",
      "  Batch [80/1299] D_loss: -0.0450, G_loss: -0.1873\n",
      "  Batch [90/1299] D_loss: -0.0021, G_loss: -0.0704\n",
      "  Batch [100/1299] D_loss: 0.0485, G_loss: -0.1229\n",
      "  Batch [110/1299] D_loss: -0.0620, G_loss: -0.2420\n",
      "  Batch [120/1299] D_loss: -0.0868, G_loss: -0.2279\n",
      "  Batch [130/1299] D_loss: -0.0476, G_loss: -0.1430\n",
      "  Batch [140/1299] D_loss: -0.0022, G_loss: -0.2640\n",
      "  Batch [150/1299] D_loss: -0.0343, G_loss: -0.0934\n",
      "  Batch [160/1299] D_loss: -0.0074, G_loss: -0.0668\n",
      "  Batch [170/1299] D_loss: -0.0132, G_loss: -0.0595\n",
      "  Batch [180/1299] D_loss: -0.0062, G_loss: -0.1044\n",
      "  Batch [190/1299] D_loss: -0.0658, G_loss: -0.2307\n",
      "  Batch [200/1299] D_loss: -0.0672, G_loss: -0.2265\n",
      "  Batch [210/1299] D_loss: -0.0726, G_loss: -0.3214\n",
      "  Batch [220/1299] D_loss: -0.0242, G_loss: -0.3791\n",
      "  Batch [230/1299] D_loss: -0.0353, G_loss: -0.1099\n",
      "  Batch [240/1299] D_loss: -4.4473, G_loss: 10.4550\n",
      "  Batch [250/1299] D_loss: -0.0261, G_loss: -0.0711\n",
      "  Batch [260/1299] D_loss: -0.0917, G_loss: -0.1682\n",
      "  Batch [270/1299] D_loss: -0.2589, G_loss: -0.3886\n",
      "  Batch [280/1299] D_loss: -0.1149, G_loss: -0.4706\n",
      "  Batch [290/1299] D_loss: -0.1190, G_loss: -0.3927\n",
      "  Batch [300/1299] D_loss: -0.0064, G_loss: -0.0780\n",
      "  Batch [310/1299] D_loss: 0.0061, G_loss: -0.0952\n",
      "  Batch [320/1299] D_loss: -0.1756, G_loss: 0.5947\n",
      "  Batch [330/1299] D_loss: -0.0446, G_loss: -0.0915\n",
      "  Batch [340/1299] D_loss: -0.0680, G_loss: -0.2227\n",
      "  Batch [350/1299] D_loss: 0.0200, G_loss: -0.3684\n",
      "  Batch [360/1299] D_loss: -0.0328, G_loss: -0.4940\n",
      "  Batch [370/1299] D_loss: 0.0262, G_loss: -0.0926\n",
      "  Batch [380/1299] D_loss: 0.0010, G_loss: -0.0227\n",
      "  Batch [390/1299] D_loss: -0.0011, G_loss: -0.0265\n",
      "  Batch [400/1299] D_loss: -0.0188, G_loss: -0.0565\n",
      "  Batch [410/1299] D_loss: -0.0570, G_loss: -0.1262\n",
      "  Batch [420/1299] D_loss: -0.0906, G_loss: -0.2457\n",
      "  Batch [430/1299] D_loss: -0.1070, G_loss: -0.3941\n",
      "  Batch [440/1299] D_loss: -0.0232, G_loss: -0.2386\n",
      "  Batch [450/1299] D_loss: 0.0127, G_loss: -0.1901\n",
      "  Batch [460/1299] D_loss: -0.1461, G_loss: 0.1958\n",
      "  Batch [470/1299] D_loss: -0.0748, G_loss: -0.1435\n",
      "  Batch [480/1299] D_loss: -0.1261, G_loss: -0.2468\n",
      "  Batch [490/1299] D_loss: -0.0769, G_loss: -0.3407\n",
      "  Batch [500/1299] D_loss: -0.0507, G_loss: -0.3811\n",
      "  Batch [510/1299] D_loss: -0.0530, G_loss: -0.4601\n",
      "  Batch [520/1299] D_loss: 0.0020, G_loss: -0.1603\n",
      "  Batch [530/1299] D_loss: -0.1189, G_loss: 0.3370\n",
      "  Batch [540/1299] D_loss: -0.0171, G_loss: -0.0581\n",
      "  Batch [550/1299] D_loss: -0.0562, G_loss: -0.1775\n",
      "  Batch [560/1299] D_loss: -0.3048, G_loss: -0.3872\n",
      "  Batch [570/1299] D_loss: -0.0946, G_loss: -0.5298\n",
      "  Batch [580/1299] D_loss: -0.0736, G_loss: -0.4307\n",
      "  Batch [590/1299] D_loss: 0.0048, G_loss: -0.1668\n",
      "  Batch [600/1299] D_loss: -1.3022, G_loss: 3.9271\n",
      "  Batch [610/1299] D_loss: -0.0209, G_loss: -0.1075\n",
      "  Batch [620/1299] D_loss: -0.0415, G_loss: -0.2401\n",
      "  Batch [630/1299] D_loss: -0.0735, G_loss: -0.2837\n",
      "  Batch [640/1299] D_loss: -0.2405, G_loss: -0.5355\n",
      "  Batch [650/1299] D_loss: 0.0696, G_loss: -0.4004\n",
      "  Batch [660/1299] D_loss: 0.0551, G_loss: -0.3259\n",
      "  Batch [670/1299] D_loss: -0.0081, G_loss: -0.2000\n",
      "  Batch [680/1299] D_loss: -0.0019, G_loss: -0.0930\n",
      "  Batch [690/1299] D_loss: -0.2103, G_loss: 1.4791\n",
      "  Batch [700/1299] D_loss: -0.0174, G_loss: -0.0509\n",
      "  Batch [710/1299] D_loss: -0.0757, G_loss: -0.1329\n",
      "  Batch [720/1299] D_loss: -0.1583, G_loss: -0.2779\n",
      "  Batch [730/1299] D_loss: -0.2301, G_loss: -0.4240\n",
      "  Batch [740/1299] D_loss: -0.1060, G_loss: -0.4027\n",
      "  Batch [750/1299] D_loss: 0.0281, G_loss: -0.2974\n",
      "  Batch [760/1299] D_loss: -0.0255, G_loss: -0.2169\n",
      "  Batch [770/1299] D_loss: -0.9884, G_loss: 3.2809\n",
      "  Batch [780/1299] D_loss: -0.0153, G_loss: -0.0275\n",
      "  Batch [790/1299] D_loss: -0.0679, G_loss: -0.1377\n",
      "  Batch [800/1299] D_loss: -0.1388, G_loss: -0.3206\n",
      "  Batch [810/1299] D_loss: -0.1910, G_loss: -0.4286\n",
      "  Batch [820/1299] D_loss: -0.0772, G_loss: -0.6130\n",
      "  Batch [830/1299] D_loss: 0.0039, G_loss: -0.4216\n",
      "  Batch [840/1299] D_loss: 0.0002, G_loss: -0.2034\n",
      "  Batch [850/1299] D_loss: -0.0367, G_loss: -0.2075\n",
      "  Batch [860/1299] D_loss: -0.2998, G_loss: 2.1981\n",
      "  Batch [870/1299] D_loss: -0.0455, G_loss: -0.0893\n",
      "  Batch [880/1299] D_loss: -0.1220, G_loss: -0.3428\n",
      "  Batch [890/1299] D_loss: -0.1562, G_loss: -0.4835\n",
      "  Batch [900/1299] D_loss: -0.0160, G_loss: -0.5677\n",
      "  Batch [910/1299] D_loss: -0.0014, G_loss: -0.5154\n",
      "  Batch [920/1299] D_loss: -0.0466, G_loss: -0.2837\n",
      "  Batch [930/1299] D_loss: -0.0246, G_loss: -0.2557\n",
      "  Batch [940/1299] D_loss: -0.0377, G_loss: -0.0847\n",
      "  Batch [950/1299] D_loss: -0.0334, G_loss: -0.1101\n",
      "  Batch [960/1299] D_loss: -0.6840, G_loss: 2.6314\n",
      "  Batch [970/1299] D_loss: -0.0272, G_loss: -0.1425\n",
      "  Batch [980/1299] D_loss: -0.1197, G_loss: -0.2897\n",
      "  Batch [990/1299] D_loss: -0.0092, G_loss: -0.3439\n",
      "  Batch [1000/1299] D_loss: -0.1492, G_loss: -0.4458\n",
      "  Batch [1010/1299] D_loss: -0.1601, G_loss: -0.5264\n",
      "  Batch [1020/1299] D_loss: 0.0290, G_loss: -0.2511\n",
      "  Batch [1030/1299] D_loss: -0.0033, G_loss: -0.0777\n",
      "  Batch [1040/1299] D_loss: -2.0050, G_loss: 7.4986\n",
      "  Batch [1050/1299] D_loss: -0.0051, G_loss: -0.0676\n",
      "  Batch [1060/1299] D_loss: -0.0640, G_loss: -0.2160\n",
      "  Batch [1070/1299] D_loss: -0.1331, G_loss: -0.3153\n",
      "  Batch [1080/1299] D_loss: -0.0656, G_loss: -0.2566\n",
      "  Batch [1090/1299] D_loss: -0.0073, G_loss: -0.1955\n",
      "  Batch [1100/1299] D_loss: -0.2731, G_loss: 0.9575\n",
      "  Batch [1110/1299] D_loss: -0.0128, G_loss: -0.1505\n",
      "  Batch [1120/1299] D_loss: -0.0118, G_loss: -0.2493\n",
      "  Batch [1130/1299] D_loss: -0.0313, G_loss: -0.2862\n",
      "  Batch [1140/1299] D_loss: -0.0407, G_loss: -0.3032\n",
      "  Batch [1150/1299] D_loss: -0.0307, G_loss: -0.1688\n",
      "  Batch [1160/1299] D_loss: -0.0016, G_loss: -0.1146\n",
      "  Batch [1170/1299] D_loss: -0.0668, G_loss: 1.0600\n",
      "  Batch [1180/1299] D_loss: -0.0491, G_loss: -0.0782\n",
      "  Batch [1190/1299] D_loss: -0.0936, G_loss: -0.2210\n",
      "  Batch [1200/1299] D_loss: -0.1693, G_loss: -0.3740\n",
      "  Batch [1210/1299] D_loss: -0.0995, G_loss: -0.3423\n",
      "  Batch [1220/1299] D_loss: -0.0721, G_loss: -0.4249\n",
      "  Batch [1230/1299] D_loss: -0.1156, G_loss: -0.3235\n",
      "  Batch [1240/1299] D_loss: -1.3218, G_loss: 4.7923\n",
      "  Batch [1250/1299] D_loss: -0.0118, G_loss: -0.0500\n",
      "  Batch [1260/1299] D_loss: -0.0342, G_loss: -0.0932\n",
      "  Batch [1270/1299] D_loss: -0.1023, G_loss: -0.2556\n",
      "  Batch [1280/1299] D_loss: -0.0471, G_loss: -0.3699\n",
      "  Batch [1290/1299] D_loss: -0.0720, G_loss: -0.4479\n",
      "\n",
      "Epoch 9 Summary:\n",
      "  Average D_loss: -0.0612\n",
      "  Average G_loss: 0.0497\n",
      "\n",
      "Epoch [10/100]\n",
      "  Batch [0/1299] D_loss: -0.0531, G_loss: -0.3782\n",
      "  Batch [10/1299] D_loss: -0.0843, G_loss: 0.2334\n",
      "  Batch [20/1299] D_loss: -0.0109, G_loss: -0.0962\n",
      "  Batch [30/1299] D_loss: -0.0659, G_loss: -0.2904\n",
      "  Batch [40/1299] D_loss: -0.1100, G_loss: -0.3382\n",
      "  Batch [50/1299] D_loss: -1.3161, G_loss: 3.7544\n",
      "  Batch [60/1299] D_loss: -0.6063, G_loss: 1.2562\n",
      "  Batch [70/1299] D_loss: -0.0992, G_loss: -0.2485\n",
      "  Batch [80/1299] D_loss: -0.0212, G_loss: -0.5478\n",
      "  Batch [90/1299] D_loss: -0.1652, G_loss: -0.7340\n",
      "  Batch [100/1299] D_loss: -0.0353, G_loss: -0.6265\n",
      "  Batch [110/1299] D_loss: 0.0066, G_loss: -0.4183\n",
      "  Batch [120/1299] D_loss: -0.3946, G_loss: 1.5547\n",
      "  Batch [130/1299] D_loss: -0.0126, G_loss: -0.1140\n",
      "  Batch [140/1299] D_loss: -0.0871, G_loss: -0.1738\n",
      "  Batch [150/1299] D_loss: -0.1450, G_loss: -0.3609\n",
      "  Batch [160/1299] D_loss: -0.0445, G_loss: -0.2877\n",
      "  Batch [170/1299] D_loss: -0.1062, G_loss: -0.3909\n",
      "  Batch [180/1299] D_loss: -0.0235, G_loss: -0.1906\n",
      "  Batch [190/1299] D_loss: -0.2418, G_loss: 3.1767\n",
      "  Batch [200/1299] D_loss: -0.0098, G_loss: -0.0395\n",
      "  Batch [210/1299] D_loss: -0.0119, G_loss: -0.0710\n",
      "  Batch [220/1299] D_loss: -0.1828, G_loss: -0.2216\n",
      "  Batch [230/1299] D_loss: -0.0673, G_loss: -0.3911\n",
      "  Batch [240/1299] D_loss: -0.1069, G_loss: -0.3796\n",
      "  Batch [250/1299] D_loss: -0.0139, G_loss: -0.2576\n",
      "  Batch [260/1299] D_loss: -0.0128, G_loss: -0.1262\n",
      "  Batch [270/1299] D_loss: -0.2045, G_loss: 0.6543\n",
      "  Batch [280/1299] D_loss: -0.0346, G_loss: -0.0796\n",
      "  Batch [290/1299] D_loss: -0.1035, G_loss: -0.2222\n",
      "  Batch [300/1299] D_loss: -0.2324, G_loss: -0.3983\n",
      "  Batch [310/1299] D_loss: -0.1398, G_loss: -0.5135\n",
      "  Batch [320/1299] D_loss: -0.0144, G_loss: -0.3800\n",
      "  Batch [330/1299] D_loss: 0.0389, G_loss: -0.2968\n",
      "  Batch [340/1299] D_loss: -0.0088, G_loss: -0.1731\n",
      "  Batch [350/1299] D_loss: 0.0056, G_loss: -0.0680\n",
      "  Batch [360/1299] D_loss: -1.1759, G_loss: 3.3265\n",
      "  Batch [370/1299] D_loss: -0.0260, G_loss: -0.1148\n",
      "  Batch [380/1299] D_loss: -0.0963, G_loss: -0.2642\n",
      "  Batch [390/1299] D_loss: -0.0258, G_loss: -0.3929\n",
      "  Batch [400/1299] D_loss: 0.0120, G_loss: -0.4113\n",
      "  Batch [410/1299] D_loss: -0.0191, G_loss: -0.4241\n",
      "  Batch [420/1299] D_loss: 0.0489, G_loss: -0.2069\n",
      "  Batch [430/1299] D_loss: -0.6067, G_loss: 4.7092\n",
      "  Batch [440/1299] D_loss: -0.7083, G_loss: 0.8668\n",
      "  Batch [450/1299] D_loss: -0.0540, G_loss: -0.0911\n",
      "  Batch [460/1299] D_loss: -0.2164, G_loss: -0.3487\n",
      "  Batch [470/1299] D_loss: -0.1187, G_loss: -0.4071\n",
      "  Batch [480/1299] D_loss: -0.2114, G_loss: -0.6723\n",
      "  Batch [490/1299] D_loss: 0.0388, G_loss: -0.4594\n",
      "  Batch [500/1299] D_loss: -0.0308, G_loss: -0.2435\n",
      "  Batch [510/1299] D_loss: -0.0222, G_loss: -0.1306\n",
      "  Batch [520/1299] D_loss: -0.0262, G_loss: -0.0219\n",
      "  Batch [530/1299] D_loss: -0.0140, G_loss: -0.0929\n",
      "  Batch [540/1299] D_loss: -0.0370, G_loss: -0.1538\n",
      "  Batch [550/1299] D_loss: -0.0914, G_loss: -0.2670\n",
      "  Batch [560/1299] D_loss: -0.1365, G_loss: -0.3614\n",
      "  Batch [570/1299] D_loss: -0.1359, G_loss: -0.2199\n",
      "  Batch [580/1299] D_loss: -0.0292, G_loss: -0.2543\n",
      "  Batch [590/1299] D_loss: -0.2790, G_loss: 0.9466\n",
      "  Batch [600/1299] D_loss: -0.0519, G_loss: -0.1143\n",
      "  Batch [610/1299] D_loss: -0.1939, G_loss: -0.2472\n",
      "  Batch [620/1299] D_loss: -0.0803, G_loss: -0.4229\n",
      "  Batch [630/1299] D_loss: -0.1010, G_loss: -0.5796\n",
      "  Batch [640/1299] D_loss: 0.0107, G_loss: -0.4164\n",
      "  Batch [650/1299] D_loss: -0.0166, G_loss: -0.2804\n",
      "  Batch [660/1299] D_loss: 0.0082, G_loss: -0.1005\n",
      "  Batch [670/1299] D_loss: -0.3087, G_loss: 0.4186\n",
      "  Batch [680/1299] D_loss: -0.2729, G_loss: 1.0520\n",
      "  Batch [690/1299] D_loss: -0.0555, G_loss: -0.1443\n",
      "  Batch [700/1299] D_loss: -0.1947, G_loss: -0.3736\n",
      "  Batch [710/1299] D_loss: -0.2259, G_loss: -0.4558\n",
      "  Batch [720/1299] D_loss: -0.0655, G_loss: -0.7899\n",
      "  Batch [730/1299] D_loss: -0.0067, G_loss: -0.3700\n",
      "  Batch [740/1299] D_loss: -0.0312, G_loss: -0.1635\n",
      "  Batch [750/1299] D_loss: -0.1729, G_loss: 1.0005\n",
      "  Batch [760/1299] D_loss: -0.0132, G_loss: -0.0532\n",
      "  Batch [770/1299] D_loss: -0.1635, G_loss: -0.2295\n",
      "  Batch [780/1299] D_loss: -0.1910, G_loss: -0.3950\n",
      "  Batch [790/1299] D_loss: -0.1742, G_loss: -0.6450\n",
      "  Batch [800/1299] D_loss: -0.0823, G_loss: -0.6305\n",
      "  Batch [810/1299] D_loss: 0.0215, G_loss: -0.2203\n",
      "  Batch [820/1299] D_loss: -0.0606, G_loss: 0.1688\n",
      "  Batch [830/1299] D_loss: -0.0222, G_loss: -0.0972\n",
      "  Batch [840/1299] D_loss: -0.0692, G_loss: -0.1724\n",
      "  Batch [850/1299] D_loss: -0.0352, G_loss: -0.2624\n",
      "  Batch [860/1299] D_loss: -0.0391, G_loss: -0.3087\n",
      "  Batch [870/1299] D_loss: -0.0726, G_loss: -0.3399\n",
      "  Batch [880/1299] D_loss: -0.0392, G_loss: -0.2122\n",
      "  Batch [890/1299] D_loss: -0.0406, G_loss: -0.1394\n",
      "  Batch [900/1299] D_loss: 0.0014, G_loss: 0.0984\n",
      "  Batch [910/1299] D_loss: -0.0086, G_loss: -0.0567\n",
      "  Batch [920/1299] D_loss: -0.0240, G_loss: -0.0736\n",
      "  Batch [930/1299] D_loss: -1.9825, G_loss: 6.8401\n",
      "  Batch [940/1299] D_loss: -0.0305, G_loss: -0.1368\n",
      "  Batch [950/1299] D_loss: -0.0737, G_loss: -0.3327\n",
      "  Batch [960/1299] D_loss: -0.2120, G_loss: -0.4680\n",
      "  Batch [970/1299] D_loss: -0.1280, G_loss: -0.5248\n",
      "  Batch [980/1299] D_loss: 0.0338, G_loss: -0.3909\n",
      "  Batch [990/1299] D_loss: -0.0125, G_loss: -0.3086\n",
      "  Batch [1000/1299] D_loss: -0.7828, G_loss: 3.8484\n",
      "  Batch [1010/1299] D_loss: -0.0164, G_loss: 0.0500\n",
      "  Batch [1020/1299] D_loss: -0.0170, G_loss: -0.0871\n",
      "  Batch [1030/1299] D_loss: -0.2647, G_loss: -0.3703\n",
      "  Batch [1040/1299] D_loss: -0.1825, G_loss: -0.5032\n",
      "  Batch [1050/1299] D_loss: -0.1125, G_loss: -0.7501\n",
      "  Batch [1060/1299] D_loss: 0.0227, G_loss: -0.4155\n",
      "  Batch [1070/1299] D_loss: -0.0504, G_loss: -0.4628\n",
      "  Batch [1080/1299] D_loss: 0.0052, G_loss: -0.1115\n",
      "  Batch [1090/1299] D_loss: -0.4367, G_loss: 2.9879\n",
      "  Batch [1100/1299] D_loss: -1.2231, G_loss: 3.2586\n",
      "  Batch [1110/1299] D_loss: -0.0274, G_loss: -0.1227\n",
      "  Batch [1120/1299] D_loss: -0.0796, G_loss: -0.2146\n",
      "  Batch [1130/1299] D_loss: -0.0830, G_loss: -0.3228\n",
      "  Batch [1140/1299] D_loss: -0.0792, G_loss: -0.5541\n",
      "  Batch [1150/1299] D_loss: 0.0247, G_loss: -0.5641\n",
      "  Batch [1160/1299] D_loss: -0.0078, G_loss: -0.2538\n",
      "  Batch [1170/1299] D_loss: -0.0140, G_loss: -0.1245\n",
      "  Batch [1180/1299] D_loss: -0.0246, G_loss: 0.0386\n",
      "  Batch [1190/1299] D_loss: -0.0915, G_loss: 0.3029\n",
      "  Batch [1200/1299] D_loss: -0.0252, G_loss: -0.1140\n",
      "  Batch [1210/1299] D_loss: -0.0917, G_loss: -0.2118\n",
      "  Batch [1220/1299] D_loss: -0.0956, G_loss: -0.3636\n",
      "  Batch [1230/1299] D_loss: -0.0692, G_loss: -0.4028\n",
      "  Batch [1240/1299] D_loss: -0.0647, G_loss: -0.4374\n",
      "  Batch [1250/1299] D_loss: -0.0189, G_loss: -0.2277\n",
      "  Batch [1260/1299] D_loss: -0.0048, G_loss: -0.1371\n",
      "  Batch [1270/1299] D_loss: -0.0086, G_loss: -0.1225\n",
      "  Batch [1280/1299] D_loss: -0.0717, G_loss: -0.2613\n",
      "  Batch [1290/1299] D_loss: -0.0839, G_loss: -0.2818\n",
      "\n",
      "Epoch 10 Summary:\n",
      "  Average D_loss: -0.0717\n",
      "  Average G_loss: 0.0677\n",
      "\n",
      "Epoch [11/100]\n",
      "  Batch [0/1299] D_loss: -0.1018, G_loss: -0.2831\n",
      "  Batch [10/1299] D_loss: -0.0533, G_loss: -0.2513\n",
      "  Batch [20/1299] D_loss: -0.0828, G_loss: -0.2277\n",
      "  Batch [30/1299] D_loss: -0.0469, G_loss: -0.2658\n",
      "  Batch [40/1299] D_loss: -0.0394, G_loss: 0.1870\n",
      "  Batch [50/1299] D_loss: 0.0053, G_loss: -0.0709\n",
      "  Batch [60/1299] D_loss: -0.0096, G_loss: -0.0606\n",
      "  Batch [70/1299] D_loss: 0.0033, G_loss: -0.0528\n",
      "  Batch [80/1299] D_loss: -0.9234, G_loss: 2.5476\n",
      "  Batch [90/1299] D_loss: -0.0996, G_loss: -0.1465\n",
      "  Batch [100/1299] D_loss: -0.1951, G_loss: -0.2731\n",
      "  Batch [110/1299] D_loss: -0.1265, G_loss: -0.3276\n",
      "  Batch [120/1299] D_loss: -0.0576, G_loss: -0.3482\n",
      "  Batch [130/1299] D_loss: -0.1229, G_loss: -0.3610\n",
      "  Batch [140/1299] D_loss: -0.1176, G_loss: -0.2514\n",
      "  Batch [150/1299] D_loss: -0.5291, G_loss: 1.1371\n",
      "  Batch [160/1299] D_loss: -0.0582, G_loss: -0.1828\n",
      "  Batch [170/1299] D_loss: -1.3865, G_loss: 4.1465\n",
      "  Batch [180/1299] D_loss: -0.3570, G_loss: 1.1459\n",
      "  Batch [190/1299] D_loss: -0.0675, G_loss: -0.1450\n",
      "  Batch [200/1299] D_loss: -0.0774, G_loss: -0.3344\n",
      "  Batch [210/1299] D_loss: -0.1742, G_loss: -0.5204\n",
      "  Batch [220/1299] D_loss: -0.2026, G_loss: -0.6937\n",
      "  Batch [230/1299] D_loss: 0.0168, G_loss: -0.6017\n",
      "  Batch [240/1299] D_loss: 0.0119, G_loss: -0.4774\n",
      "  Batch [250/1299] D_loss: -0.0711, G_loss: -0.2405\n",
      "  Batch [260/1299] D_loss: -0.7918, G_loss: 3.0570\n",
      "  Batch [270/1299] D_loss: -0.0398, G_loss: -0.1148\n",
      "  Batch [280/1299] D_loss: -0.1002, G_loss: -0.2211\n",
      "  Batch [290/1299] D_loss: -0.1030, G_loss: -0.2665\n",
      "  Batch [300/1299] D_loss: -0.0879, G_loss: -0.3180\n",
      "  Batch [310/1299] D_loss: -0.0909, G_loss: -0.3589\n",
      "  Batch [320/1299] D_loss: -0.0188, G_loss: -0.2276\n",
      "  Batch [330/1299] D_loss: -0.0113, G_loss: -0.1710\n",
      "  Batch [340/1299] D_loss: -0.0390, G_loss: -0.1082\n",
      "  Batch [350/1299] D_loss: -0.4763, G_loss: 1.0681\n",
      "  Batch [360/1299] D_loss: -0.0661, G_loss: -0.0956\n",
      "  Batch [370/1299] D_loss: -0.1362, G_loss: -0.3443\n",
      "  Batch [380/1299] D_loss: -0.1927, G_loss: -0.5698\n",
      "  Batch [390/1299] D_loss: -0.1409, G_loss: -0.7481\n",
      "  Batch [400/1299] D_loss: -0.0971, G_loss: -0.7210\n",
      "  Batch [410/1299] D_loss: -0.0843, G_loss: -0.6104\n",
      "  Batch [420/1299] D_loss: -0.0075, G_loss: -0.1414\n",
      "  Batch [430/1299] D_loss: -2.2452, G_loss: 6.4952\n",
      "  Batch [440/1299] D_loss: -0.1396, G_loss: 0.2884\n",
      "  Batch [450/1299] D_loss: -0.0337, G_loss: -0.0885\n",
      "  Batch [460/1299] D_loss: -0.1776, G_loss: -0.2201\n",
      "  Batch [470/1299] D_loss: -0.0607, G_loss: -0.3244\n",
      "  Batch [480/1299] D_loss: -0.2077, G_loss: -0.5143\n",
      "  Batch [490/1299] D_loss: -0.0635, G_loss: -0.6611\n",
      "  Batch [500/1299] D_loss: -0.1359, G_loss: -0.6328\n",
      "  Batch [510/1299] D_loss: -0.0290, G_loss: -0.3518\n",
      "  Batch [520/1299] D_loss: -0.0153, G_loss: -0.1336\n",
      "  Batch [530/1299] D_loss: -0.2494, G_loss: 0.8732\n",
      "  Batch [540/1299] D_loss: -0.3379, G_loss: 0.3036\n",
      "  Batch [550/1299] D_loss: -0.0342, G_loss: -0.0660\n",
      "  Batch [560/1299] D_loss: -0.0885, G_loss: -0.2256\n",
      "  Batch [570/1299] D_loss: -0.2305, G_loss: -0.3901\n",
      "  Batch [580/1299] D_loss: -0.0596, G_loss: -0.4487\n",
      "  Batch [590/1299] D_loss: -0.1330, G_loss: -0.6038\n",
      "  Batch [600/1299] D_loss: -0.0619, G_loss: -0.6147\n",
      "  Batch [610/1299] D_loss: -0.0215, G_loss: -0.3233\n",
      "  Batch [620/1299] D_loss: -0.0015, G_loss: -0.1614\n",
      "  Batch [630/1299] D_loss: -1.9407, G_loss: 5.7730\n",
      "  Batch [640/1299] D_loss: -0.0395, G_loss: -0.0788\n",
      "  Batch [650/1299] D_loss: -0.0830, G_loss: -0.1835\n",
      "  Batch [660/1299] D_loss: -0.0553, G_loss: -0.2975\n",
      "  Batch [670/1299] D_loss: -0.0892, G_loss: -0.5089\n",
      "  Batch [680/1299] D_loss: -0.1070, G_loss: -0.5497\n",
      "  Batch [690/1299] D_loss: -0.1061, G_loss: -0.4466\n",
      "  Batch [700/1299] D_loss: -0.0202, G_loss: -0.3120\n",
      "  Batch [710/1299] D_loss: 0.0061, G_loss: -0.1016\n",
      "  Batch [720/1299] D_loss: 0.0021, G_loss: -0.0575\n",
      "  Batch [730/1299] D_loss: -0.0226, G_loss: -0.1040\n",
      "  Batch [740/1299] D_loss: -0.0333, G_loss: -0.1508\n",
      "  Batch [750/1299] D_loss: -0.1054, G_loss: -0.2427\n",
      "  Batch [760/1299] D_loss: 0.0240, G_loss: -0.0909\n",
      "  Batch [770/1299] D_loss: -0.0019, G_loss: -0.0851\n",
      "  Batch [780/1299] D_loss: -0.0101, G_loss: -0.1304\n",
      "  Batch [790/1299] D_loss: -0.1047, G_loss: -0.2353\n",
      "  Batch [800/1299] D_loss: -0.0734, G_loss: -0.2515\n",
      "  Batch [810/1299] D_loss: -0.0402, G_loss: -0.2056\n",
      "  Batch [820/1299] D_loss: -0.0260, G_loss: -0.1571\n",
      "  Batch [830/1299] D_loss: -0.0182, G_loss: 0.2442\n",
      "  Batch [840/1299] D_loss: -0.0253, G_loss: 0.0057\n",
      "  Batch [850/1299] D_loss: -0.1011, G_loss: -0.1950\n",
      "  Batch [860/1299] D_loss: -0.0941, G_loss: -0.3566\n",
      "  Batch [870/1299] D_loss: -0.1271, G_loss: -0.4698\n",
      "  Batch [880/1299] D_loss: -0.0153, G_loss: -0.4586\n",
      "  Batch [890/1299] D_loss: -0.0482, G_loss: -0.2665\n",
      "  Batch [900/1299] D_loss: 0.0140, G_loss: -0.1901\n",
      "  Batch [910/1299] D_loss: -0.0064, G_loss: -0.0472\n",
      "  Batch [920/1299] D_loss: -0.7146, G_loss: 2.4849\n",
      "  Batch [930/1299] D_loss: -0.0173, G_loss: -0.0954\n",
      "  Batch [940/1299] D_loss: -0.0635, G_loss: -0.2472\n",
      "  Batch [950/1299] D_loss: -0.0967, G_loss: -0.4054\n",
      "  Batch [960/1299] D_loss: -0.0777, G_loss: -0.5362\n",
      "  Batch [970/1299] D_loss: -0.0000, G_loss: -0.2188\n",
      "  Batch [980/1299] D_loss: -0.0322, G_loss: -0.1874\n",
      "  Batch [990/1299] D_loss: -0.0244, G_loss: -0.1356\n",
      "  Batch [1000/1299] D_loss: -1.5530, G_loss: 4.5338\n",
      "  Batch [1010/1299] D_loss: -0.0116, G_loss: -0.0727\n",
      "  Batch [1020/1299] D_loss: -0.0310, G_loss: -0.1168\n",
      "  Batch [1030/1299] D_loss: -0.0969, G_loss: -0.2848\n",
      "  Batch [1040/1299] D_loss: -0.0907, G_loss: -0.3379\n",
      "  Batch [1050/1299] D_loss: -0.0570, G_loss: -0.3253\n",
      "  Batch [1060/1299] D_loss: -0.0204, G_loss: -0.2289\n",
      "  Batch [1070/1299] D_loss: -0.0229, G_loss: -0.0629\n",
      "  Batch [1080/1299] D_loss: -0.8105, G_loss: 1.4448\n",
      "  Batch [1090/1299] D_loss: -0.0385, G_loss: -0.1201\n",
      "  Batch [1100/1299] D_loss: -0.0706, G_loss: -0.2543\n",
      "  Batch [1110/1299] D_loss: -0.1072, G_loss: -0.4267\n",
      "  Batch [1120/1299] D_loss: -0.0800, G_loss: -0.4356\n",
      "  Batch [1130/1299] D_loss: -0.1248, G_loss: -0.6277\n",
      "  Batch [1140/1299] D_loss: 0.0003, G_loss: -0.2772\n",
      "  Batch [1150/1299] D_loss: 0.0151, G_loss: -0.0954\n",
      "  Batch [1160/1299] D_loss: -0.5414, G_loss: 3.3239\n",
      "  Batch [1170/1299] D_loss: -0.0038, G_loss: -0.1128\n",
      "  Batch [1180/1299] D_loss: -0.0823, G_loss: -0.2308\n",
      "  Batch [1190/1299] D_loss: -0.0904, G_loss: -0.2297\n",
      "  Batch [1200/1299] D_loss: -0.0646, G_loss: -0.3243\n",
      "  Batch [1210/1299] D_loss: -0.0425, G_loss: -0.3465\n",
      "  Batch [1220/1299] D_loss: -0.0357, G_loss: -0.1543\n",
      "  Batch [1230/1299] D_loss: -0.0183, G_loss: -0.1138\n",
      "  Batch [1240/1299] D_loss: -0.0333, G_loss: -0.1882\n",
      "  Batch [1250/1299] D_loss: -1.2182, G_loss: 1.6868\n",
      "  Batch [1260/1299] D_loss: -0.0314, G_loss: -0.0198\n",
      "  Batch [1270/1299] D_loss: -0.0556, G_loss: -0.1725\n",
      "  Batch [1280/1299] D_loss: -0.0331, G_loss: -0.3417\n",
      "  Batch [1290/1299] D_loss: -0.0933, G_loss: -0.4199\n",
      "\n",
      "Epoch 11 Summary:\n",
      "  Average D_loss: -0.0710\n",
      "  Average G_loss: 0.0648\n",
      "\n",
      "Epoch [12/100]\n",
      "  Batch [0/1299] D_loss: -0.0972, G_loss: -0.4797\n",
      "  Batch [10/1299] D_loss: -0.0854, G_loss: -0.4988\n",
      "  Batch [20/1299] D_loss: -0.0008, G_loss: -0.3481\n",
      "  Batch [30/1299] D_loss: -0.0183, G_loss: -0.1151\n",
      "  Batch [40/1299] D_loss: -5.4702, G_loss: 9.8091\n",
      "  Batch [50/1299] D_loss: -0.0256, G_loss: -0.0786\n",
      "  Batch [60/1299] D_loss: -0.0976, G_loss: -0.1788\n",
      "  Batch [70/1299] D_loss: -0.1474, G_loss: -0.3469\n",
      "  Batch [80/1299] D_loss: -0.1234, G_loss: -0.5228\n",
      "  Batch [90/1299] D_loss: -0.0613, G_loss: -0.3720\n",
      "  Batch [100/1299] D_loss: -0.1282, G_loss: -0.4002\n",
      "  Batch [110/1299] D_loss: -0.0308, G_loss: -0.1147\n",
      "  Batch [120/1299] D_loss: -0.0320, G_loss: -0.1566\n",
      "  Batch [130/1299] D_loss: -0.0286, G_loss: -0.1482\n",
      "  Batch [140/1299] D_loss: -0.2228, G_loss: 0.6704\n",
      "  Batch [150/1299] D_loss: -0.4441, G_loss: 1.0031\n",
      "  Batch [160/1299] D_loss: -0.0458, G_loss: -0.2154\n",
      "  Batch [170/1299] D_loss: -0.2789, G_loss: -0.4576\n",
      "  Batch [180/1299] D_loss: -0.1118, G_loss: -0.5477\n",
      "  Batch [190/1299] D_loss: -0.2820, G_loss: -0.7610\n",
      "  Batch [200/1299] D_loss: 0.0117, G_loss: -0.4788\n",
      "  Batch [210/1299] D_loss: -0.0126, G_loss: -0.4238\n",
      "  Batch [220/1299] D_loss: -0.0298, G_loss: -0.1514\n",
      "  Batch [230/1299] D_loss: -0.0267, G_loss: 0.2373\n",
      "  Batch [240/1299] D_loss: -0.0029, G_loss: -0.0666\n",
      "  Batch [250/1299] D_loss: -0.0572, G_loss: -0.1784\n",
      "  Batch [260/1299] D_loss: -0.1101, G_loss: -0.4017\n",
      "  Batch [270/1299] D_loss: -0.0984, G_loss: -0.4713\n",
      "  Batch [280/1299] D_loss: -0.1070, G_loss: -0.5434\n",
      "  Batch [290/1299] D_loss: 0.0027, G_loss: -0.4769\n",
      "  Batch [300/1299] D_loss: -0.0428, G_loss: -0.3693\n",
      "  Batch [310/1299] D_loss: -0.0133, G_loss: -0.2133\n",
      "  Batch [320/1299] D_loss: -0.7053, G_loss: 2.6323\n",
      "  Batch [330/1299] D_loss: -0.3967, G_loss: 0.9384\n",
      "  Batch [340/1299] D_loss: -0.0238, G_loss: -0.1023\n",
      "  Batch [350/1299] D_loss: -0.1042, G_loss: -0.2534\n",
      "  Batch [360/1299] D_loss: -0.0652, G_loss: -0.4289\n",
      "  Batch [370/1299] D_loss: -0.1552, G_loss: -0.6565\n",
      "  Batch [380/1299] D_loss: -0.0606, G_loss: -0.6945\n",
      "  Batch [390/1299] D_loss: -0.0765, G_loss: -0.8907\n",
      "  Batch [400/1299] D_loss: 0.0148, G_loss: -0.2525\n",
      "  Batch [410/1299] D_loss: 0.0205, G_loss: -0.1175\n",
      "  Batch [420/1299] D_loss: -0.4422, G_loss: 1.7187\n",
      "  Batch [430/1299] D_loss: -0.0124, G_loss: -0.1096\n",
      "  Batch [440/1299] D_loss: -0.0293, G_loss: -0.1787\n",
      "  Batch [450/1299] D_loss: -0.0946, G_loss: -0.2982\n",
      "  Batch [460/1299] D_loss: -0.0439, G_loss: -0.4670\n",
      "  Batch [470/1299] D_loss: -0.0671, G_loss: -0.4401\n",
      "  Batch [480/1299] D_loss: -0.0369, G_loss: -0.2723\n",
      "  Batch [490/1299] D_loss: -0.0175, G_loss: -0.2496\n",
      "  Batch [500/1299] D_loss: -0.7928, G_loss: 2.9849\n",
      "  Batch [510/1299] D_loss: 0.0014, G_loss: -0.0357\n",
      "  Batch [520/1299] D_loss: -0.0021, G_loss: -0.0338\n",
      "  Batch [530/1299] D_loss: -0.3909, G_loss: 2.2917\n",
      "  Batch [540/1299] D_loss: -0.3990, G_loss: 1.2799\n",
      "  Batch [550/1299] D_loss: -0.0348, G_loss: -0.0922\n",
      "  Batch [560/1299] D_loss: -0.1177, G_loss: -0.2967\n",
      "  Batch [570/1299] D_loss: -0.1525, G_loss: -0.4737\n",
      "  Batch [580/1299] D_loss: -0.1619, G_loss: -0.6164\n",
      "  Batch [590/1299] D_loss: -0.0686, G_loss: -0.4465\n",
      "  Batch [600/1299] D_loss: -0.0475, G_loss: -0.2697\n",
      "  Batch [610/1299] D_loss: -0.0351, G_loss: -0.1327\n",
      "  Batch [620/1299] D_loss: -0.0508, G_loss: -0.1988\n",
      "  Batch [630/1299] D_loss: -2.9019, G_loss: 7.7737\n",
      "  Batch [640/1299] D_loss: -0.0398, G_loss: -0.0949\n",
      "  Batch [650/1299] D_loss: -0.0612, G_loss: -0.2091\n",
      "  Batch [660/1299] D_loss: -0.1167, G_loss: -0.3405\n",
      "  Batch [670/1299] D_loss: -0.2117, G_loss: -0.4887\n",
      "  Batch [680/1299] D_loss: -0.1047, G_loss: -0.4107\n",
      "  Batch [690/1299] D_loss: -0.0327, G_loss: -0.2941\n",
      "  Batch [700/1299] D_loss: -0.0339, G_loss: -0.2019\n",
      "  Batch [710/1299] D_loss: -1.1292, G_loss: 1.5293\n",
      "  Batch [720/1299] D_loss: -0.0345, G_loss: -0.1500\n",
      "  Batch [730/1299] D_loss: -0.0805, G_loss: -0.3337\n",
      "  Batch [740/1299] D_loss: -0.0332, G_loss: -0.4940\n",
      "  Batch [750/1299] D_loss: -0.0444, G_loss: -0.5870\n",
      "  Batch [760/1299] D_loss: -0.0629, G_loss: -0.4420\n",
      "  Batch [770/1299] D_loss: -0.0113, G_loss: -0.4142\n",
      "  Batch [780/1299] D_loss: 0.0386, G_loss: -0.2730\n",
      "  Batch [790/1299] D_loss: -0.0269, G_loss: -0.2367\n",
      "  Batch [800/1299] D_loss: -0.0239, G_loss: -0.1044\n",
      "  Batch [810/1299] D_loss: -1.6396, G_loss: 7.4535\n",
      "  Batch [820/1299] D_loss: -0.5643, G_loss: 1.5288\n",
      "  Batch [830/1299] D_loss: -0.0953, G_loss: -0.2018\n",
      "  Batch [840/1299] D_loss: -0.1233, G_loss: -0.3486\n",
      "  Batch [850/1299] D_loss: -0.1252, G_loss: -0.4342\n",
      "  Batch [860/1299] D_loss: -0.1017, G_loss: -0.4785\n",
      "  Batch [870/1299] D_loss: -0.0097, G_loss: -0.6260\n",
      "  Batch [880/1299] D_loss: -0.1209, G_loss: -0.5526\n",
      "  Batch [890/1299] D_loss: -0.0115, G_loss: -0.1363\n",
      "  Batch [900/1299] D_loss: -0.0138, G_loss: -0.0600\n",
      "  Batch [910/1299] D_loss: -0.7869, G_loss: 2.8061\n",
      "  Batch [920/1299] D_loss: -0.4958, G_loss: 1.3020\n",
      "  Batch [930/1299] D_loss: -0.0815, G_loss: -0.1774\n",
      "  Batch [940/1299] D_loss: -0.1087, G_loss: -0.2893\n",
      "  Batch [950/1299] D_loss: -0.1888, G_loss: -0.4158\n",
      "  Batch [960/1299] D_loss: 0.0099, G_loss: -0.5145\n",
      "  Batch [970/1299] D_loss: -0.0215, G_loss: -0.4715\n",
      "  Batch [980/1299] D_loss: -0.0182, G_loss: -0.3229\n",
      "  Batch [990/1299] D_loss: -0.0255, G_loss: -0.1868\n",
      "  Batch [1000/1299] D_loss: -0.0246, G_loss: -0.1190\n",
      "  Batch [1010/1299] D_loss: -1.1845, G_loss: 2.5340\n",
      "  Batch [1020/1299] D_loss: -0.7486, G_loss: 1.5096\n",
      "  Batch [1030/1299] D_loss: -0.0497, G_loss: -0.1206\n",
      "  Batch [1040/1299] D_loss: -0.1144, G_loss: -0.2378\n",
      "  Batch [1050/1299] D_loss: -0.1011, G_loss: -0.4357\n",
      "  Batch [1060/1299] D_loss: -0.0601, G_loss: -0.4890\n",
      "  Batch [1070/1299] D_loss: -0.1149, G_loss: -0.7053\n",
      "  Batch [1080/1299] D_loss: -0.0370, G_loss: -0.4921\n",
      "  Batch [1090/1299] D_loss: -0.0637, G_loss: -0.6783\n",
      "  Batch [1100/1299] D_loss: -0.0149, G_loss: -0.2364\n",
      "  Batch [1110/1299] D_loss: 0.0007, G_loss: -0.0832\n",
      "  Batch [1120/1299] D_loss: 0.0496, G_loss: 0.0029\n",
      "  Batch [1130/1299] D_loss: -0.0232, G_loss: -0.0858\n",
      "  Batch [1140/1299] D_loss: -0.0310, G_loss: -0.1451\n",
      "  Batch [1150/1299] D_loss: -0.0716, G_loss: -0.1591\n",
      "  Batch [1160/1299] D_loss: -0.0825, G_loss: -0.2251\n",
      "  Batch [1170/1299] D_loss: -0.0235, G_loss: -0.1591\n",
      "  Batch [1180/1299] D_loss: 0.0029, G_loss: -0.1674\n",
      "  Batch [1190/1299] D_loss: -0.0575, G_loss: -0.1745\n",
      "  Batch [1200/1299] D_loss: -0.0274, G_loss: -0.1287\n",
      "  Batch [1210/1299] D_loss: -0.0746, G_loss: 0.2755\n",
      "  Batch [1220/1299] D_loss: -0.0176, G_loss: -0.0482\n",
      "  Batch [1230/1299] D_loss: -0.0602, G_loss: -0.1737\n",
      "  Batch [1240/1299] D_loss: -0.0689, G_loss: -0.3378\n",
      "  Batch [1250/1299] D_loss: -0.2439, G_loss: -0.4935\n",
      "  Batch [1260/1299] D_loss: -0.0908, G_loss: -0.6055\n",
      "  Batch [1270/1299] D_loss: -0.0122, G_loss: -0.5781\n",
      "  Batch [1280/1299] D_loss: 0.0238, G_loss: -0.2606\n",
      "  Batch [1290/1299] D_loss: 0.0099, G_loss: -0.0787\n",
      "\n",
      "Epoch 12 Summary:\n",
      "  Average D_loss: -0.0746\n",
      "  Average G_loss: 0.0660\n",
      "\n",
      "Epoch [13/100]\n",
      "  Batch [0/1299] D_loss: -0.5005, G_loss: 2.1127\n",
      "  Batch [10/1299] D_loss: -0.0227, G_loss: -0.0761\n",
      "  Batch [20/1299] D_loss: -0.0629, G_loss: -0.1595\n",
      "  Batch [30/1299] D_loss: -0.1653, G_loss: -0.3565\n",
      "  Batch [40/1299] D_loss: -0.3366, G_loss: -0.5318\n",
      "  Batch [50/1299] D_loss: -0.1155, G_loss: -0.5905\n",
      "  Batch [60/1299] D_loss: -0.1035, G_loss: -0.4889\n",
      "  Batch [70/1299] D_loss: 0.0243, G_loss: -0.3846\n",
      "  Batch [80/1299] D_loss: -0.0149, G_loss: -0.0973\n",
      "  Batch [90/1299] D_loss: -0.1257, G_loss: 1.2986\n",
      "  Batch [100/1299] D_loss: -0.0869, G_loss: 0.3017\n",
      "  Batch [110/1299] D_loss: -0.0380, G_loss: -0.1528\n",
      "  Batch [120/1299] D_loss: -0.0816, G_loss: -0.3243\n",
      "  Batch [130/1299] D_loss: -0.1340, G_loss: -0.5111\n",
      "  Batch [140/1299] D_loss: -0.0124, G_loss: -0.3940\n",
      "  Batch [150/1299] D_loss: -0.0227, G_loss: -0.4563\n",
      "  Batch [160/1299] D_loss: -0.0230, G_loss: -0.3313\n",
      "  Batch [170/1299] D_loss: -0.0431, G_loss: -0.1277\n",
      "  Batch [180/1299] D_loss: -1.4385, G_loss: 3.5434\n",
      "  Batch [190/1299] D_loss: -0.4917, G_loss: 0.7453\n",
      "  Batch [200/1299] D_loss: -0.0624, G_loss: -0.2662\n",
      "  Batch [210/1299] D_loss: -0.1383, G_loss: -0.4158\n",
      "  Batch [220/1299] D_loss: -0.0208, G_loss: -0.4549\n",
      "  Batch [230/1299] D_loss: -0.1307, G_loss: -0.5615\n",
      "  Batch [240/1299] D_loss: -0.0341, G_loss: -0.3700\n",
      "  Batch [250/1299] D_loss: -0.0671, G_loss: -0.4434\n",
      "  Batch [260/1299] D_loss: 0.0058, G_loss: -0.2159\n",
      "  Batch [270/1299] D_loss: 0.0107, G_loss: -0.0566\n",
      "  Batch [280/1299] D_loss: -1.5175, G_loss: 3.7353\n",
      "  Batch [290/1299] D_loss: -0.0290, G_loss: -0.1203\n",
      "  Batch [300/1299] D_loss: -0.0738, G_loss: -0.2393\n",
      "  Batch [310/1299] D_loss: -0.0760, G_loss: -0.4246\n",
      "  Batch [320/1299] D_loss: -0.0360, G_loss: -0.4147\n",
      "  Batch [330/1299] D_loss: 0.0399, G_loss: -0.4607\n",
      "  Batch [340/1299] D_loss: -0.0238, G_loss: -0.4284\n",
      "  Batch [350/1299] D_loss: 0.0099, G_loss: -0.3279\n",
      "  Batch [360/1299] D_loss: -0.0342, G_loss: -0.1343\n",
      "  Batch [370/1299] D_loss: -0.4883, G_loss: 3.0751\n",
      "  Batch [380/1299] D_loss: -0.0836, G_loss: 0.0052\n",
      "  Batch [390/1299] D_loss: -0.0713, G_loss: -0.1719\n",
      "  Batch [400/1299] D_loss: -0.0187, G_loss: -0.3576\n",
      "  Batch [410/1299] D_loss: -0.1570, G_loss: -0.5510\n",
      "  Batch [420/1299] D_loss: -0.2891, G_loss: -0.7496\n",
      "  Batch [430/1299] D_loss: -0.1219, G_loss: -0.6670\n",
      "  Batch [440/1299] D_loss: 0.0444, G_loss: -0.5103\n",
      "  Batch [450/1299] D_loss: -0.0119, G_loss: -0.2195\n",
      "  Batch [460/1299] D_loss: -0.0340, G_loss: -0.0961\n",
      "  Batch [470/1299] D_loss: -1.3139, G_loss: 4.8633\n",
      "  Batch [480/1299] D_loss: -0.2683, G_loss: 0.7734\n",
      "  Batch [490/1299] D_loss: -0.0478, G_loss: -0.1087\n",
      "  Batch [500/1299] D_loss: -0.0141, G_loss: -0.2547\n",
      "  Batch [510/1299] D_loss: -0.1437, G_loss: -0.4169\n",
      "  Batch [520/1299] D_loss: -0.1541, G_loss: -0.5683\n",
      "  Batch [530/1299] D_loss: -0.0450, G_loss: -0.5702\n",
      "  Batch [540/1299] D_loss: 0.0086, G_loss: -0.5042\n",
      "  Batch [550/1299] D_loss: 0.0170, G_loss: -0.5648\n",
      "  Batch [560/1299] D_loss: -0.0061, G_loss: -0.1172\n",
      "  Batch [570/1299] D_loss: -1.6337, G_loss: 4.2836\n",
      "  Batch [580/1299] D_loss: 0.3407, G_loss: 0.6715\n",
      "  Batch [590/1299] D_loss: -0.0326, G_loss: -0.1564\n",
      "  Batch [600/1299] D_loss: -0.1009, G_loss: -0.3472\n",
      "  Batch [610/1299] D_loss: -0.0930, G_loss: -0.5616\n",
      "  Batch [620/1299] D_loss: -0.2229, G_loss: -0.5641\n",
      "  Batch [630/1299] D_loss: -0.1822, G_loss: -0.6442\n",
      "  Batch [640/1299] D_loss: -0.0546, G_loss: -0.5841\n",
      "  Batch [650/1299] D_loss: -0.0308, G_loss: -0.3399\n",
      "  Batch [660/1299] D_loss: 0.0030, G_loss: -0.0966\n",
      "  Batch [670/1299] D_loss: -0.0879, G_loss: 1.5624\n",
      "  Batch [680/1299] D_loss: -0.0033, G_loss: -0.0521\n",
      "  Batch [690/1299] D_loss: -0.0630, G_loss: -0.2083\n",
      "  Batch [700/1299] D_loss: -0.1157, G_loss: -0.3278\n",
      "  Batch [710/1299] D_loss: -0.0945, G_loss: -0.4241\n",
      "  Batch [720/1299] D_loss: 0.0025, G_loss: -0.4648\n",
      "  Batch [730/1299] D_loss: -0.1380, G_loss: -0.6114\n",
      "  Batch [740/1299] D_loss: -0.0741, G_loss: -0.3587\n",
      "  Batch [750/1299] D_loss: -0.0407, G_loss: -0.1734\n",
      "  Batch [760/1299] D_loss: -1.4434, G_loss: 4.9900\n",
      "  Batch [770/1299] D_loss: -0.4354, G_loss: 2.2196\n",
      "  Batch [780/1299] D_loss: -0.0538, G_loss: -0.1533\n",
      "  Batch [790/1299] D_loss: -0.0841, G_loss: -0.2598\n",
      "  Batch [800/1299] D_loss: -0.1468, G_loss: -0.4906\n",
      "  Batch [810/1299] D_loss: -0.1545, G_loss: -0.5802\n",
      "  Batch [820/1299] D_loss: -0.0821, G_loss: -0.6656\n",
      "  Batch [830/1299] D_loss: -0.1880, G_loss: -0.6563\n",
      "  Batch [840/1299] D_loss: 0.0033, G_loss: -0.3633\n",
      "  Batch [850/1299] D_loss: -0.9998, G_loss: 5.1620\n",
      "  Batch [860/1299] D_loss: -2.0491, G_loss: 4.6727\n",
      "  Batch [870/1299] D_loss: -0.0043, G_loss: -0.1294\n",
      "  Batch [880/1299] D_loss: -0.0234, G_loss: -0.1577\n",
      "  Batch [890/1299] D_loss: -0.0386, G_loss: -0.3713\n",
      "  Batch [900/1299] D_loss: -0.0555, G_loss: -0.4023\n",
      "  Batch [910/1299] D_loss: -0.0654, G_loss: -0.3945\n",
      "  Batch [920/1299] D_loss: -0.1037, G_loss: -0.3557\n",
      "  Batch [930/1299] D_loss: 0.0005, G_loss: -0.4159\n",
      "  Batch [940/1299] D_loss: 0.0190, G_loss: -0.2159\n",
      "  Batch [950/1299] D_loss: -0.0342, G_loss: -0.1798\n",
      "  Batch [960/1299] D_loss: -0.2966, G_loss: 0.7829\n",
      "  Batch [970/1299] D_loss: -1.0925, G_loss: 1.7693\n",
      "  Batch [980/1299] D_loss: -0.0867, G_loss: -0.1671\n",
      "  Batch [990/1299] D_loss: -0.1348, G_loss: -0.3456\n",
      "  Batch [1000/1299] D_loss: -0.2109, G_loss: -0.4767\n",
      "  Batch [1010/1299] D_loss: -0.1034, G_loss: -0.5562\n",
      "  Batch [1020/1299] D_loss: -0.1026, G_loss: -0.6367\n",
      "  Batch [1030/1299] D_loss: -0.0540, G_loss: -0.5263\n",
      "  Batch [1040/1299] D_loss: -0.0046, G_loss: -0.5180\n",
      "  Batch [1050/1299] D_loss: -0.0171, G_loss: -0.2522\n",
      "  Batch [1060/1299] D_loss: -4.2253, G_loss: 9.1637\n",
      "  Batch [1070/1299] D_loss: -0.0085, G_loss: -0.1230\n",
      "  Batch [1080/1299] D_loss: -0.0467, G_loss: -0.2039\n",
      "  Batch [1090/1299] D_loss: -0.1390, G_loss: -0.2494\n",
      "  Batch [1100/1299] D_loss: 0.0171, G_loss: -0.2044\n",
      "  Batch [1110/1299] D_loss: 0.0234, G_loss: -0.2468\n",
      "  Batch [1120/1299] D_loss: -0.0012, G_loss: -0.2346\n",
      "  Batch [1130/1299] D_loss: -0.0229, G_loss: -0.2400\n",
      "  Batch [1140/1299] D_loss: 0.0188, G_loss: -0.2348\n",
      "  Batch [1150/1299] D_loss: -0.0343, G_loss: -0.2190\n",
      "  Batch [1160/1299] D_loss: 0.0217, G_loss: -0.1335\n",
      "  Batch [1170/1299] D_loss: -0.8183, G_loss: 2.6333\n",
      "  Batch [1180/1299] D_loss: -0.0467, G_loss: -0.1714\n",
      "  Batch [1190/1299] D_loss: -0.1185, G_loss: -0.2214\n",
      "  Batch [1200/1299] D_loss: -0.0587, G_loss: -0.3368\n",
      "  Batch [1210/1299] D_loss: -0.0850, G_loss: -0.3339\n",
      "  Batch [1220/1299] D_loss: 0.0057, G_loss: -0.3223\n",
      "  Batch [1230/1299] D_loss: -0.0611, G_loss: -0.2688\n",
      "  Batch [1240/1299] D_loss: -0.0287, G_loss: -0.0872\n",
      "  Batch [1250/1299] D_loss: 0.1767, G_loss: 0.3480\n",
      "  Batch [1260/1299] D_loss: -0.0099, G_loss: -0.0803\n",
      "  Batch [1270/1299] D_loss: -0.0983, G_loss: -0.1391\n",
      "  Batch [1280/1299] D_loss: -0.2183, G_loss: 0.8486\n",
      "  Batch [1290/1299] D_loss: -0.1928, G_loss: -0.2492\n",
      "\n",
      "Epoch 13 Summary:\n",
      "  Average D_loss: -0.0835\n",
      "  Average G_loss: 0.0544\n",
      "\n",
      "Epoch [14/100]\n",
      "  Batch [0/1299] D_loss: -0.0062, G_loss: -0.3587\n",
      "  Batch [10/1299] D_loss: -0.1970, G_loss: -0.5206\n",
      "  Batch [20/1299] D_loss: -0.0788, G_loss: -0.5390\n",
      "  Batch [30/1299] D_loss: -0.1711, G_loss: -0.5047\n",
      "  Batch [40/1299] D_loss: -0.1489, G_loss: -0.4406\n",
      "  Batch [50/1299] D_loss: -0.1194, G_loss: 2.1049\n",
      "  Batch [60/1299] D_loss: -0.0229, G_loss: -0.0437\n",
      "  Batch [70/1299] D_loss: -0.0486, G_loss: -0.1953\n",
      "  Batch [80/1299] D_loss: -0.0759, G_loss: -0.2469\n",
      "  Batch [90/1299] D_loss: -0.0796, G_loss: -0.3252\n",
      "  Batch [100/1299] D_loss: -0.1291, G_loss: -0.3210\n",
      "  Batch [110/1299] D_loss: -0.0254, G_loss: -0.2983\n",
      "  Batch [120/1299] D_loss: -0.0573, G_loss: -0.1791\n",
      "  Batch [130/1299] D_loss: 0.0646, G_loss: -0.2976\n",
      "  Batch [140/1299] D_loss: 0.0042, G_loss: -0.0706\n",
      "  Batch [150/1299] D_loss: -0.0097, G_loss: -0.0539\n",
      "  Batch [160/1299] D_loss: -0.0008, G_loss: -0.0429\n",
      "  Batch [170/1299] D_loss: -1.1160, G_loss: 2.5518\n",
      "  Batch [180/1299] D_loss: -0.0024, G_loss: -0.1406\n",
      "  Batch [190/1299] D_loss: -0.1146, G_loss: -0.3249\n",
      "  Batch [200/1299] D_loss: -0.1738, G_loss: -0.6271\n",
      "  Batch [210/1299] D_loss: -0.2248, G_loss: -0.6488\n",
      "  Batch [220/1299] D_loss: -0.1528, G_loss: -0.5012\n",
      "  Batch [230/1299] D_loss: 0.0750, G_loss: -0.5044\n",
      "  Batch [240/1299] D_loss: -0.0023, G_loss: -0.0743\n",
      "  Batch [250/1299] D_loss: -0.6411, G_loss: 2.1421\n",
      "  Batch [260/1299] D_loss: -0.0117, G_loss: -0.0704\n",
      "  Batch [270/1299] D_loss: -0.0469, G_loss: -0.2538\n",
      "  Batch [280/1299] D_loss: -0.0789, G_loss: -0.3411\n",
      "  Batch [290/1299] D_loss: 0.0259, G_loss: -0.3685\n",
      "  Batch [300/1299] D_loss: -0.0990, G_loss: -0.5855\n",
      "  Batch [310/1299] D_loss: 0.0029, G_loss: -0.5091\n",
      "  Batch [320/1299] D_loss: -0.0149, G_loss: -0.2005\n",
      "  Batch [330/1299] D_loss: -0.0161, G_loss: 0.0499\n",
      "  Batch [340/1299] D_loss: -0.0027, G_loss: -0.0675\n",
      "  Batch [350/1299] D_loss: -0.0615, G_loss: -0.1660\n",
      "  Batch [360/1299] D_loss: -0.1035, G_loss: -0.2488\n",
      "  Batch [370/1299] D_loss: -0.0783, G_loss: -0.3458\n",
      "  Batch [380/1299] D_loss: 0.0066, G_loss: -0.4042\n",
      "  Batch [390/1299] D_loss: -0.0031, G_loss: -0.3538\n",
      "  Batch [400/1299] D_loss: -0.0413, G_loss: -0.2572\n",
      "  Batch [410/1299] D_loss: -0.0436, G_loss: -0.2680\n",
      "  Batch [420/1299] D_loss: -0.0077, G_loss: -0.0728\n",
      "  Batch [430/1299] D_loss: -0.5527, G_loss: 1.2796\n",
      "  Batch [440/1299] D_loss: -0.0359, G_loss: -0.1291\n",
      "  Batch [450/1299] D_loss: -0.0884, G_loss: -0.2335\n",
      "  Batch [460/1299] D_loss: -0.0738, G_loss: -0.3725\n",
      "  Batch [470/1299] D_loss: -0.0486, G_loss: -0.3997\n",
      "  Batch [480/1299] D_loss: 0.0323, G_loss: -0.3987\n",
      "  Batch [490/1299] D_loss: -0.0611, G_loss: -0.2920\n",
      "  Batch [500/1299] D_loss: 0.0340, G_loss: -0.3199\n",
      "  Batch [510/1299] D_loss: 0.0014, G_loss: -0.0616\n",
      "  Batch [520/1299] D_loss: -0.2319, G_loss: 0.9304\n",
      "  Batch [530/1299] D_loss: -0.0064, G_loss: -0.0785\n",
      "  Batch [540/1299] D_loss: -0.0428, G_loss: -0.2417\n",
      "  Batch [550/1299] D_loss: -0.1185, G_loss: -0.3833\n",
      "  Batch [560/1299] D_loss: -0.0566, G_loss: -0.4196\n",
      "  Batch [570/1299] D_loss: -0.0899, G_loss: -0.6236\n",
      "  Batch [580/1299] D_loss: -0.0092, G_loss: -0.5236\n",
      "  Batch [590/1299] D_loss: -0.0315, G_loss: -0.4113\n",
      "  Batch [600/1299] D_loss: -0.0110, G_loss: -0.1228\n",
      "  Batch [610/1299] D_loss: -1.7531, G_loss: 4.9868\n",
      "  Batch [620/1299] D_loss: -0.1932, G_loss: 0.2997\n",
      "  Batch [630/1299] D_loss: -0.0657, G_loss: -0.0996\n",
      "  Batch [640/1299] D_loss: -0.0213, G_loss: -0.1931\n",
      "  Batch [650/1299] D_loss: -0.1309, G_loss: -0.3885\n",
      "  Batch [660/1299] D_loss: -0.1496, G_loss: -0.4473\n",
      "  Batch [670/1299] D_loss: -0.1250, G_loss: -0.5106\n",
      "  Batch [680/1299] D_loss: 0.0191, G_loss: -0.3586\n",
      "  Batch [690/1299] D_loss: -0.0140, G_loss: -0.2343\n",
      "  Batch [700/1299] D_loss: -0.0007, G_loss: -0.1458\n",
      "  Batch [710/1299] D_loss: -0.0381, G_loss: -0.0819\n",
      "  Batch [720/1299] D_loss: -0.0107, G_loss: -0.0671\n",
      "  Batch [730/1299] D_loss: -0.0070, G_loss: -0.1131\n",
      "  Batch [740/1299] D_loss: -0.0687, G_loss: -0.2117\n",
      "  Batch [750/1299] D_loss: -0.1505, G_loss: -0.4187\n",
      "  Batch [760/1299] D_loss: -0.0279, G_loss: -0.4181\n",
      "  Batch [770/1299] D_loss: -0.1526, G_loss: -0.4557\n",
      "  Batch [780/1299] D_loss: -0.0293, G_loss: -0.2469\n",
      "  Batch [790/1299] D_loss: -0.9368, G_loss: 1.7810\n",
      "  Batch [800/1299] D_loss: -0.1794, G_loss: 0.6027\n",
      "  Batch [810/1299] D_loss: -0.0540, G_loss: -0.0675\n",
      "  Batch [820/1299] D_loss: -0.0559, G_loss: -0.2021\n",
      "  Batch [830/1299] D_loss: -0.2947, G_loss: -0.3714\n",
      "  Batch [840/1299] D_loss: -0.2367, G_loss: -0.5214\n",
      "  Batch [850/1299] D_loss: -0.0850, G_loss: -0.5919\n",
      "  Batch [860/1299] D_loss: -0.2611, G_loss: -0.8114\n",
      "  Batch [870/1299] D_loss: -0.2292, G_loss: -0.8033\n",
      "  Batch [880/1299] D_loss: 0.0783, G_loss: -0.5011\n",
      "  Batch [890/1299] D_loss: 0.0332, G_loss: -0.4055\n",
      "  Batch [900/1299] D_loss: -0.0656, G_loss: -0.2028\n",
      "  Batch [910/1299] D_loss: 0.0204, G_loss: -0.1240\n",
      "  Batch [920/1299] D_loss: -0.3821, G_loss: 2.0764\n",
      "  Batch [930/1299] D_loss: -0.1851, G_loss: 0.8881\n",
      "  Batch [940/1299] D_loss: -0.0121, G_loss: -0.0039\n",
      "  Batch [950/1299] D_loss: -0.1232, G_loss: 0.4075\n",
      "  Batch [960/1299] D_loss: -0.2678, G_loss: 0.4979\n",
      "  Batch [970/1299] D_loss: -0.0675, G_loss: -0.1507\n",
      "  Batch [980/1299] D_loss: -0.1376, G_loss: -0.3532\n",
      "  Batch [990/1299] D_loss: -0.1022, G_loss: -0.6089\n",
      "  Batch [1000/1299] D_loss: -0.0467, G_loss: -0.6350\n",
      "  Batch [1010/1299] D_loss: -0.1180, G_loss: -0.9136\n",
      "  Batch [1020/1299] D_loss: -0.0895, G_loss: -0.7494\n",
      "  Batch [1030/1299] D_loss: -0.0556, G_loss: -0.4410\n",
      "  Batch [1040/1299] D_loss: 0.0047, G_loss: -0.1292\n",
      "  Batch [1050/1299] D_loss: -0.0216, G_loss: -0.0622\n",
      "  Batch [1060/1299] D_loss: -0.6503, G_loss: 2.8371\n",
      "  Batch [1070/1299] D_loss: -0.0551, G_loss: -0.1424\n",
      "  Batch [1080/1299] D_loss: -0.0851, G_loss: -0.2046\n",
      "  Batch [1090/1299] D_loss: -0.1301, G_loss: -0.3639\n",
      "  Batch [1100/1299] D_loss: -0.1944, G_loss: -0.3224\n",
      "  Batch [1110/1299] D_loss: -0.0763, G_loss: -0.3832\n",
      "  Batch [1120/1299] D_loss: -0.0319, G_loss: -0.2745\n",
      "  Batch [1130/1299] D_loss: -0.0650, G_loss: -0.2825\n",
      "  Batch [1140/1299] D_loss: -0.0326, G_loss: -0.2448\n",
      "  Batch [1150/1299] D_loss: -0.0203, G_loss: -0.1492\n",
      "  Batch [1160/1299] D_loss: -0.0130, G_loss: -0.0752\n",
      "  Batch [1170/1299] D_loss: -1.3745, G_loss: 5.0879\n",
      "  Batch [1180/1299] D_loss: -0.2660, G_loss: 0.8959\n",
      "  Batch [1190/1299] D_loss: -0.0460, G_loss: -0.1424\n",
      "  Batch [1200/1299] D_loss: -0.1163, G_loss: -0.2469\n",
      "  Batch [1210/1299] D_loss: 0.0182, G_loss: -0.3715\n",
      "  Batch [1220/1299] D_loss: -0.1370, G_loss: -0.6522\n",
      "  Batch [1230/1299] D_loss: 0.0091, G_loss: -0.5954\n",
      "  Batch [1240/1299] D_loss: -0.0079, G_loss: -0.3712\n",
      "  Batch [1250/1299] D_loss: -0.0252, G_loss: -0.1890\n",
      "  Batch [1260/1299] D_loss: -0.0046, G_loss: -0.1289\n",
      "  Batch [1270/1299] D_loss: -0.0162, G_loss: -0.0933\n",
      "  Batch [1280/1299] D_loss: -0.0099, G_loss: -0.0544\n",
      "  Batch [1290/1299] D_loss: -0.8948, G_loss: 1.0634\n",
      "\n",
      "Epoch 14 Summary:\n",
      "  Average D_loss: -0.0814\n",
      "  Average G_loss: 0.0674\n",
      "\n",
      "Epoch [15/100]\n",
      "  Batch [0/1299] D_loss: -0.0379, G_loss: -0.0906\n",
      "  Batch [10/1299] D_loss: -0.0613, G_loss: -0.2003\n",
      "  Batch [20/1299] D_loss: -0.1725, G_loss: -0.4040\n",
      "  Batch [30/1299] D_loss: -0.2394, G_loss: -0.5910\n",
      "  Batch [40/1299] D_loss: -0.0465, G_loss: -0.5534\n",
      "  Batch [50/1299] D_loss: -0.0615, G_loss: -0.3116\n",
      "  Batch [60/1299] D_loss: 0.0216, G_loss: -0.2458\n",
      "  Batch [70/1299] D_loss: -0.0088, G_loss: -0.1177\n",
      "  Batch [80/1299] D_loss: -0.0480, G_loss: 0.2076\n",
      "  Batch [90/1299] D_loss: -0.3534, G_loss: 0.5698\n",
      "  Batch [100/1299] D_loss: -0.0500, G_loss: -0.2130\n",
      "  Batch [110/1299] D_loss: -0.1392, G_loss: -0.3361\n",
      "  Batch [120/1299] D_loss: -0.0661, G_loss: -0.4181\n",
      "  Batch [130/1299] D_loss: -0.1843, G_loss: -0.4569\n",
      "  Batch [140/1299] D_loss: 0.0215, G_loss: -0.6148\n",
      "  Batch [150/1299] D_loss: 0.0206, G_loss: -0.4148\n",
      "  Batch [160/1299] D_loss: -0.0406, G_loss: -0.1056\n",
      "  Batch [170/1299] D_loss: -0.8133, G_loss: 2.3610\n",
      "  Batch [180/1299] D_loss: -0.0902, G_loss: 1.1506\n",
      "  Batch [190/1299] D_loss: -0.0583, G_loss: -0.1086\n",
      "  Batch [200/1299] D_loss: -0.1086, G_loss: -0.2452\n",
      "  Batch [210/1299] D_loss: -0.1678, G_loss: -0.4459\n",
      "  Batch [220/1299] D_loss: -0.1899, G_loss: -0.5714\n",
      "  Batch [230/1299] D_loss: -0.1351, G_loss: -0.7037\n",
      "  Batch [240/1299] D_loss: -0.0221, G_loss: -0.6747\n",
      "  Batch [250/1299] D_loss: -0.0316, G_loss: -0.4274\n",
      "  Batch [260/1299] D_loss: 0.0159, G_loss: -0.2148\n",
      "  Batch [270/1299] D_loss: -0.0248, G_loss: -0.0947\n",
      "  Batch [280/1299] D_loss: -0.4977, G_loss: 0.8709\n",
      "  Batch [290/1299] D_loss: -1.3827, G_loss: 2.0535\n",
      "  Batch [300/1299] D_loss: -0.0171, G_loss: -0.0361\n",
      "  Batch [310/1299] D_loss: -0.0744, G_loss: -0.1716\n",
      "  Batch [320/1299] D_loss: -0.1120, G_loss: -0.2865\n",
      "  Batch [330/1299] D_loss: -0.2725, G_loss: -0.5090\n",
      "  Batch [340/1299] D_loss: -0.0999, G_loss: -0.6868\n",
      "  Batch [350/1299] D_loss: -0.2654, G_loss: -0.7073\n",
      "  Batch [360/1299] D_loss: 0.0136, G_loss: -0.6126\n",
      "  Batch [370/1299] D_loss: -0.0758, G_loss: -0.3534\n",
      "  Batch [380/1299] D_loss: -0.0296, G_loss: -0.1039\n",
      "  Batch [390/1299] D_loss: -0.6655, G_loss: 3.2274\n",
      "  Batch [400/1299] D_loss: -0.0431, G_loss: -0.0225\n",
      "  Batch [410/1299] D_loss: -0.0285, G_loss: -0.1981\n",
      "  Batch [420/1299] D_loss: -0.1591, G_loss: -0.3116\n",
      "  Batch [430/1299] D_loss: -0.2031, G_loss: -0.5928\n",
      "  Batch [440/1299] D_loss: -0.1210, G_loss: -0.6639\n",
      "  Batch [450/1299] D_loss: -0.0650, G_loss: -0.6837\n",
      "  Batch [460/1299] D_loss: -0.0964, G_loss: -0.7141\n",
      "  Batch [470/1299] D_loss: 0.0070, G_loss: -0.1395\n",
      "  Batch [480/1299] D_loss: -0.0353, G_loss: -0.1009\n",
      "  Batch [490/1299] D_loss: -0.0260, G_loss: -0.1157\n",
      "  Batch [500/1299] D_loss: -0.6351, G_loss: 1.7603\n",
      "  Batch [510/1299] D_loss: -0.0298, G_loss: -0.0939\n",
      "  Batch [520/1299] D_loss: -0.0939, G_loss: -0.2256\n",
      "  Batch [530/1299] D_loss: -0.1172, G_loss: -0.3512\n",
      "  Batch [540/1299] D_loss: -0.0691, G_loss: -0.5019\n",
      "  Batch [550/1299] D_loss: -0.1578, G_loss: -0.4925\n",
      "  Batch [560/1299] D_loss: -0.0852, G_loss: -0.4921\n",
      "  Batch [570/1299] D_loss: -0.0098, G_loss: -0.4335\n",
      "  Batch [580/1299] D_loss: -0.0160, G_loss: -0.3106\n",
      "  Batch [590/1299] D_loss: -0.0098, G_loss: -0.1280\n",
      "  Batch [600/1299] D_loss: -0.9805, G_loss: 4.2568\n",
      "  Batch [610/1299] D_loss: -0.0156, G_loss: -0.0654\n",
      "  Batch [620/1299] D_loss: -0.0708, G_loss: -0.2010\n",
      "  Batch [630/1299] D_loss: -0.0403, G_loss: -0.2657\n",
      "  Batch [640/1299] D_loss: -0.1044, G_loss: -0.3578\n",
      "  Batch [650/1299] D_loss: -0.0459, G_loss: -0.3670\n",
      "  Batch [660/1299] D_loss: -0.0856, G_loss: -0.2311\n",
      "  Batch [670/1299] D_loss: -0.0457, G_loss: -0.2159\n",
      "  Batch [680/1299] D_loss: 0.0035, G_loss: -0.2231\n",
      "  Batch [690/1299] D_loss: 0.0004, G_loss: -0.0795\n",
      "  Batch [700/1299] D_loss: -0.0225, G_loss: -0.0779\n",
      "  Batch [710/1299] D_loss: -0.0790, G_loss: 0.4736\n",
      "  Batch [720/1299] D_loss: -0.0260, G_loss: -0.1075\n",
      "  Batch [730/1299] D_loss: -0.0954, G_loss: -0.3007\n",
      "  Batch [740/1299] D_loss: -0.0796, G_loss: -0.3972\n",
      "  Batch [750/1299] D_loss: -0.1739, G_loss: -0.4904\n",
      "  Batch [760/1299] D_loss: -0.0827, G_loss: -0.4557\n",
      "  Batch [770/1299] D_loss: -0.0782, G_loss: -0.5074\n",
      "  Batch [780/1299] D_loss: -0.0464, G_loss: -0.3311\n",
      "  Batch [790/1299] D_loss: -0.0095, G_loss: -0.1120\n",
      "  Batch [800/1299] D_loss: -0.7743, G_loss: 2.5736\n",
      "  Batch [810/1299] D_loss: -0.0305, G_loss: -0.0843\n",
      "  Batch [820/1299] D_loss: -0.0383, G_loss: -0.2190\n",
      "  Batch [830/1299] D_loss: -0.1205, G_loss: -0.2676\n",
      "  Batch [840/1299] D_loss: -0.3033, G_loss: -0.3956\n",
      "  Batch [850/1299] D_loss: -0.1017, G_loss: -0.4823\n",
      "  Batch [860/1299] D_loss: -0.1074, G_loss: -0.5704\n",
      "  Batch [870/1299] D_loss: -0.0293, G_loss: -0.4338\n",
      "  Batch [880/1299] D_loss: 0.0227, G_loss: -0.1626\n",
      "  Batch [890/1299] D_loss: -1.7087, G_loss: 4.4414\n",
      "  Batch [900/1299] D_loss: -0.0125, G_loss: -0.0710\n",
      "  Batch [910/1299] D_loss: -0.0389, G_loss: -0.1157\n",
      "  Batch [920/1299] D_loss: -0.0596, G_loss: -0.2054\n",
      "  Batch [930/1299] D_loss: -0.0909, G_loss: -0.3666\n",
      "  Batch [940/1299] D_loss: -0.1197, G_loss: -0.4718\n",
      "  Batch [950/1299] D_loss: -0.1296, G_loss: -0.4825\n",
      "  Batch [960/1299] D_loss: -0.1114, G_loss: -0.4730\n",
      "  Batch [970/1299] D_loss: -0.0299, G_loss: -0.1718\n",
      "  Batch [980/1299] D_loss: -0.0315, G_loss: -0.1821\n",
      "  Batch [990/1299] D_loss: -3.3114, G_loss: 9.9505\n",
      "  Batch [1000/1299] D_loss: -0.0140, G_loss: -0.0433\n",
      "  Batch [1010/1299] D_loss: -0.4733, G_loss: 0.9699\n",
      "  Batch [1020/1299] D_loss: -0.0119, G_loss: -0.1355\n",
      "  Batch [1030/1299] D_loss: -0.0580, G_loss: -0.2324\n",
      "  Batch [1040/1299] D_loss: -0.1497, G_loss: -0.4249\n",
      "  Batch [1050/1299] D_loss: -0.1060, G_loss: -0.4135\n",
      "  Batch [1060/1299] D_loss: -0.0920, G_loss: -0.4750\n",
      "  Batch [1070/1299] D_loss: -0.0062, G_loss: -0.4035\n",
      "  Batch [1080/1299] D_loss: -0.0015, G_loss: -0.3555\n",
      "  Batch [1090/1299] D_loss: -0.0658, G_loss: -0.1920\n",
      "  Batch [1100/1299] D_loss: -1.4874, G_loss: 4.5749\n",
      "  Batch [1110/1299] D_loss: -0.3413, G_loss: 0.7756\n",
      "  Batch [1120/1299] D_loss: -0.0766, G_loss: -0.1455\n",
      "  Batch [1130/1299] D_loss: -0.2204, G_loss: -0.3054\n",
      "  Batch [1140/1299] D_loss: -0.1974, G_loss: -0.4528\n",
      "  Batch [1150/1299] D_loss: -0.1497, G_loss: -0.6361\n",
      "  Batch [1160/1299] D_loss: -0.1521, G_loss: -0.7815\n",
      "  Batch [1170/1299] D_loss: -0.0539, G_loss: -0.5711\n",
      "  Batch [1180/1299] D_loss: -0.0725, G_loss: -0.3834\n",
      "  Batch [1190/1299] D_loss: 0.0258, G_loss: -0.2864\n",
      "  Batch [1200/1299] D_loss: -1.7748, G_loss: 5.0415\n",
      "  Batch [1210/1299] D_loss: -0.4531, G_loss: 1.1604\n",
      "  Batch [1220/1299] D_loss: -0.0263, G_loss: 0.0839\n",
      "  Batch [1230/1299] D_loss: -0.0286, G_loss: -0.0860\n",
      "  Batch [1240/1299] D_loss: -0.1112, G_loss: -0.2760\n",
      "  Batch [1250/1299] D_loss: -0.2495, G_loss: -0.5512\n",
      "  Batch [1260/1299] D_loss: -0.2382, G_loss: -0.6799\n",
      "  Batch [1270/1299] D_loss: -0.1748, G_loss: -0.8283\n",
      "  Batch [1280/1299] D_loss: -0.1692, G_loss: -0.8046\n",
      "  Batch [1290/1299] D_loss: 0.0772, G_loss: -0.4328\n",
      "\n",
      "Epoch 15 Summary:\n",
      "  Average D_loss: -0.0873\n",
      "  Average G_loss: 0.0476\n",
      "\n",
      "Epoch [16/100]\n",
      "  Batch [0/1299] D_loss: 0.0467, G_loss: -0.2709\n",
      "  Batch [10/1299] D_loss: -0.0175, G_loss: -0.1049\n",
      "  Batch [20/1299] D_loss: -0.6434, G_loss: 2.2660\n",
      "  Batch [30/1299] D_loss: -0.2642, G_loss: 0.6210\n",
      "  Batch [40/1299] D_loss: -0.0400, G_loss: -0.0686\n",
      "  Batch [50/1299] D_loss: -0.0767, G_loss: -0.1930\n",
      "  Batch [60/1299] D_loss: -0.2261, G_loss: -0.3584\n",
      "  Batch [70/1299] D_loss: -0.1409, G_loss: -0.6545\n",
      "  Batch [80/1299] D_loss: -0.2463, G_loss: -0.7621\n",
      "  Batch [90/1299] D_loss: -0.1145, G_loss: -0.9760\n",
      "  Batch [100/1299] D_loss: 0.0401, G_loss: -0.6013\n",
      "  Batch [110/1299] D_loss: 0.0146, G_loss: -0.2350\n",
      "  Batch [120/1299] D_loss: -0.0191, G_loss: -0.0779\n",
      "  Batch [130/1299] D_loss: 0.2793, G_loss: 0.3114\n",
      "  Batch [140/1299] D_loss: -0.0144, G_loss: -0.0723\n",
      "  Batch [150/1299] D_loss: -0.0111, G_loss: -0.0980\n",
      "  Batch [160/1299] D_loss: -0.0541, G_loss: -0.1657\n",
      "  Batch [170/1299] D_loss: -0.1296, G_loss: -0.2261\n",
      "  Batch [180/1299] D_loss: -0.3159, G_loss: 1.3199\n",
      "  Batch [190/1299] D_loss: -0.3932, G_loss: 1.9543\n",
      "  Batch [200/1299] D_loss: -0.2334, G_loss: 0.2835\n",
      "  Batch [210/1299] D_loss: -0.1494, G_loss: 0.3574\n",
      "  Batch [220/1299] D_loss: -0.0541, G_loss: -0.1790\n",
      "  Batch [230/1299] D_loss: -0.1290, G_loss: -0.3897\n",
      "  Batch [240/1299] D_loss: -0.1995, G_loss: -0.4998\n",
      "  Batch [250/1299] D_loss: -0.1665, G_loss: -0.7119\n",
      "  Batch [260/1299] D_loss: -0.0518, G_loss: -0.6662\n",
      "  Batch [270/1299] D_loss: -0.1136, G_loss: -0.6360\n",
      "  Batch [280/1299] D_loss: 0.0839, G_loss: -0.3832\n",
      "  Batch [290/1299] D_loss: -0.0167, G_loss: -0.2624\n",
      "  Batch [300/1299] D_loss: -0.0216, G_loss: -0.1192\n",
      "  Batch [310/1299] D_loss: -0.0177, G_loss: -0.0627\n",
      "  Batch [320/1299] D_loss: -0.0286, G_loss: -0.1335\n",
      "  Batch [330/1299] D_loss: -0.0697, G_loss: -0.1996\n",
      "  Batch [340/1299] D_loss: -0.1568, G_loss: -0.3700\n",
      "  Batch [350/1299] D_loss: -0.0670, G_loss: -0.3505\n",
      "  Batch [360/1299] D_loss: -0.0401, G_loss: -0.3107\n",
      "  Batch [370/1299] D_loss: -1.0880, G_loss: 2.3786\n",
      "  Batch [380/1299] D_loss: -0.0434, G_loss: -0.0972\n",
      "  Batch [390/1299] D_loss: -0.0346, G_loss: -0.2037\n",
      "  Batch [400/1299] D_loss: -0.0834, G_loss: -0.3472\n",
      "  Batch [410/1299] D_loss: -0.0100, G_loss: -0.2916\n",
      "  Batch [420/1299] D_loss: -0.0762, G_loss: -0.4322\n",
      "  Batch [430/1299] D_loss: 0.0193, G_loss: -0.3877\n",
      "  Batch [440/1299] D_loss: -0.0197, G_loss: -0.2353\n",
      "  Batch [450/1299] D_loss: -0.0349, G_loss: -0.1336\n",
      "  Batch [460/1299] D_loss: -2.0611, G_loss: 5.5553\n",
      "  Batch [470/1299] D_loss: -0.3383, G_loss: 0.5192\n",
      "  Batch [480/1299] D_loss: -0.0761, G_loss: -0.2602\n",
      "  Batch [490/1299] D_loss: -0.1818, G_loss: -0.4743\n",
      "  Batch [500/1299] D_loss: -0.2518, G_loss: -0.5044\n",
      "  Batch [510/1299] D_loss: -0.0513, G_loss: -0.6102\n",
      "  Batch [520/1299] D_loss: -0.0931, G_loss: -0.6221\n",
      "  Batch [530/1299] D_loss: -0.0261, G_loss: -0.3048\n",
      "  Batch [540/1299] D_loss: 0.0134, G_loss: -0.2317\n",
      "  Batch [550/1299] D_loss: -0.0251, G_loss: -0.0671\n",
      "  Batch [560/1299] D_loss: 0.0198, G_loss: -0.0724\n",
      "  Batch [570/1299] D_loss: 0.0014, G_loss: -0.0281\n",
      "  Batch [580/1299] D_loss: -1.4437, G_loss: 5.3225\n",
      "  Batch [590/1299] D_loss: -0.8277, G_loss: 3.1185\n",
      "  Batch [600/1299] D_loss: -0.0251, G_loss: -0.1207\n",
      "  Batch [610/1299] D_loss: -0.0636, G_loss: -0.2816\n",
      "  Batch [620/1299] D_loss: -0.1971, G_loss: -0.4753\n",
      "  Batch [630/1299] D_loss: 0.0175, G_loss: -0.4693\n",
      "  Batch [640/1299] D_loss: -0.0216, G_loss: -0.5839\n",
      "  Batch [650/1299] D_loss: -0.0185, G_loss: -0.4677\n",
      "  Batch [660/1299] D_loss: -0.0300, G_loss: -0.2283\n",
      "  Batch [670/1299] D_loss: -0.0031, G_loss: -0.0937\n",
      "  Batch [680/1299] D_loss: -0.6669, G_loss: 1.7509\n",
      "  Batch [690/1299] D_loss: -0.0488, G_loss: -0.1667\n",
      "  Batch [700/1299] D_loss: -0.1222, G_loss: -0.2562\n",
      "  Batch [710/1299] D_loss: -0.0815, G_loss: -0.3468\n",
      "  Batch [720/1299] D_loss: -0.0261, G_loss: -0.4239\n",
      "  Batch [730/1299] D_loss: -0.0269, G_loss: -0.4330\n",
      "  Batch [740/1299] D_loss: -0.1023, G_loss: -0.3637\n",
      "  Batch [750/1299] D_loss: -0.0601, G_loss: -0.2923\n",
      "  Batch [760/1299] D_loss: -0.0284, G_loss: -0.0849\n",
      "  Batch [770/1299] D_loss: -0.0244, G_loss: -0.3180\n",
      "  Batch [780/1299] D_loss: 0.0035, G_loss: -0.0957\n",
      "  Batch [790/1299] D_loss: -0.0222, G_loss: -0.0939\n",
      "  Batch [800/1299] D_loss: -0.0182, G_loss: -0.0319\n",
      "  Batch [810/1299] D_loss: -0.6401, G_loss: 1.1973\n",
      "  Batch [820/1299] D_loss: -0.0613, G_loss: -0.2041\n",
      "  Batch [830/1299] D_loss: -0.1157, G_loss: -0.3484\n",
      "  Batch [840/1299] D_loss: -0.0990, G_loss: -0.3435\n",
      "  Batch [850/1299] D_loss: -0.3059, G_loss: -0.4341\n",
      "  Batch [860/1299] D_loss: -0.0811, G_loss: -0.5276\n",
      "  Batch [870/1299] D_loss: -0.1002, G_loss: -0.4888\n",
      "  Batch [880/1299] D_loss: -0.0441, G_loss: -0.2907\n",
      "  Batch [890/1299] D_loss: -0.0374, G_loss: -0.1251\n",
      "  Batch [900/1299] D_loss: -0.0534, G_loss: -0.1232\n",
      "  Batch [910/1299] D_loss: -0.1519, G_loss: 0.2583\n",
      "  Batch [920/1299] D_loss: -0.1775, G_loss: 0.5813\n",
      "  Batch [930/1299] D_loss: 0.0206, G_loss: -0.1291\n",
      "  Batch [940/1299] D_loss: -0.1298, G_loss: -0.2728\n",
      "  Batch [950/1299] D_loss: -0.1673, G_loss: -0.4694\n",
      "  Batch [960/1299] D_loss: -0.1752, G_loss: -0.6115\n",
      "  Batch [970/1299] D_loss: -0.0778, G_loss: -0.7302\n",
      "  Batch [980/1299] D_loss: 0.0174, G_loss: -0.6111\n",
      "  Batch [990/1299] D_loss: -0.0108, G_loss: -0.5035\n",
      "  Batch [1000/1299] D_loss: -0.0661, G_loss: -0.4311\n",
      "  Batch [1010/1299] D_loss: 0.0276, G_loss: -0.1060\n",
      "  Batch [1020/1299] D_loss: 0.0035, G_loss: -0.0553\n",
      "  Batch [1030/1299] D_loss: -0.3848, G_loss: 1.2189\n",
      "  Batch [1040/1299] D_loss: -0.0227, G_loss: -0.0422\n",
      "  Batch [1050/1299] D_loss: -0.0491, G_loss: -0.1271\n",
      "  Batch [1060/1299] D_loss: -0.0855, G_loss: -0.2720\n",
      "  Batch [1070/1299] D_loss: -0.0917, G_loss: -0.3520\n",
      "  Batch [1080/1299] D_loss: -0.1700, G_loss: -0.5324\n",
      "  Batch [1090/1299] D_loss: -0.0961, G_loss: -0.5945\n",
      "  Batch [1100/1299] D_loss: -0.1139, G_loss: -0.5891\n",
      "  Batch [1110/1299] D_loss: 0.0378, G_loss: -0.3860\n",
      "  Batch [1120/1299] D_loss: 0.0122, G_loss: -0.2177\n",
      "  Batch [1130/1299] D_loss: -0.0333, G_loss: -0.1679\n",
      "  Batch [1140/1299] D_loss: -0.0132, G_loss: -0.1208\n",
      "  Batch [1150/1299] D_loss: -0.3095, G_loss: 0.8411\n",
      "  Batch [1160/1299] D_loss: -0.8986, G_loss: 1.8637\n",
      "  Batch [1170/1299] D_loss: -0.0256, G_loss: -0.1837\n",
      "  Batch [1180/1299] D_loss: -0.0475, G_loss: -0.3257\n",
      "  Batch [1190/1299] D_loss: -0.2243, G_loss: -0.4618\n",
      "  Batch [1200/1299] D_loss: -0.1075, G_loss: -0.5779\n",
      "  Batch [1210/1299] D_loss: -0.0800, G_loss: -0.5145\n",
      "  Batch [1220/1299] D_loss: -0.0690, G_loss: -0.5705\n",
      "  Batch [1230/1299] D_loss: -0.0061, G_loss: -0.3251\n",
      "  Batch [1240/1299] D_loss: 0.0068, G_loss: -0.1059\n",
      "  Batch [1250/1299] D_loss: -0.0421, G_loss: -0.0529\n",
      "  Batch [1260/1299] D_loss: -0.0477, G_loss: -0.1456\n",
      "  Batch [1270/1299] D_loss: -0.0533, G_loss: -0.1787\n",
      "  Batch [1280/1299] D_loss: -0.1206, G_loss: -0.2265\n",
      "  Batch [1290/1299] D_loss: -0.1033, G_loss: -0.2613\n",
      "\n",
      "Epoch 16 Summary:\n",
      "  Average D_loss: -0.0837\n",
      "  Average G_loss: 0.0671\n",
      "\n",
      "Epoch [17/100]\n",
      "  Batch [0/1299] D_loss: -0.0500, G_loss: -0.2545\n",
      "  Batch [10/1299] D_loss: -0.0146, G_loss: -0.2091\n",
      "  Batch [20/1299] D_loss: -1.0351, G_loss: 4.2158\n",
      "  Batch [30/1299] D_loss: -0.1317, G_loss: 0.2699\n",
      "  Batch [40/1299] D_loss: -0.0578, G_loss: -0.1585\n",
      "  Batch [50/1299] D_loss: -0.0241, G_loss: -0.2229\n",
      "  Batch [60/1299] D_loss: -0.0725, G_loss: -0.3357\n",
      "  Batch [70/1299] D_loss: -0.1466, G_loss: -0.3552\n",
      "  Batch [80/1299] D_loss: -0.1160, G_loss: -0.3559\n",
      "  Batch [90/1299] D_loss: -0.0813, G_loss: -0.3361\n",
      "  Batch [100/1299] D_loss: 0.0085, G_loss: -0.2504\n",
      "  Batch [110/1299] D_loss: -0.4731, G_loss: 1.5494\n",
      "  Batch [120/1299] D_loss: -0.2695, G_loss: 0.7588\n",
      "  Batch [130/1299] D_loss: -0.0370, G_loss: -0.0857\n",
      "  Batch [140/1299] D_loss: -0.1736, G_loss: -0.3145\n",
      "  Batch [150/1299] D_loss: -0.0403, G_loss: -0.4860\n",
      "  Batch [160/1299] D_loss: -0.0577, G_loss: -0.5632\n",
      "  Batch [170/1299] D_loss: -0.0973, G_loss: -0.8187\n",
      "  Batch [180/1299] D_loss: -0.0417, G_loss: -0.6646\n",
      "  Batch [190/1299] D_loss: -0.0354, G_loss: -0.5444\n",
      "  Batch [200/1299] D_loss: -0.0594, G_loss: -0.3469\n",
      "  Batch [210/1299] D_loss: -0.0193, G_loss: -0.1935\n",
      "  Batch [220/1299] D_loss: -0.4632, G_loss: 2.4279\n",
      "  Batch [230/1299] D_loss: -0.1506, G_loss: 0.3211\n",
      "  Batch [240/1299] D_loss: -0.0239, G_loss: -0.1076\n",
      "  Batch [250/1299] D_loss: -0.0826, G_loss: -0.2089\n",
      "  Batch [260/1299] D_loss: -0.1650, G_loss: -0.3301\n",
      "  Batch [270/1299] D_loss: -0.0772, G_loss: -0.3964\n",
      "  Batch [280/1299] D_loss: -0.1641, G_loss: -0.4577\n",
      "  Batch [290/1299] D_loss: 0.0533, G_loss: -0.3369\n",
      "  Batch [300/1299] D_loss: -0.0270, G_loss: -0.3837\n",
      "  Batch [310/1299] D_loss: -0.0221, G_loss: -0.1754\n",
      "  Batch [320/1299] D_loss: -0.9748, G_loss: 2.9177\n",
      "  Batch [330/1299] D_loss: -0.0055, G_loss: -0.0559\n",
      "  Batch [340/1299] D_loss: -0.0440, G_loss: -0.1500\n",
      "  Batch [350/1299] D_loss: -0.0825, G_loss: -0.2610\n",
      "  Batch [360/1299] D_loss: -0.0714, G_loss: -0.3136\n",
      "  Batch [370/1299] D_loss: -0.0282, G_loss: -0.3926\n",
      "  Batch [380/1299] D_loss: -0.0777, G_loss: -0.4347\n",
      "  Batch [390/1299] D_loss: 0.0549, G_loss: -0.2948\n",
      "  Batch [400/1299] D_loss: -0.0785, G_loss: -0.2820\n",
      "  Batch [410/1299] D_loss: -0.0225, G_loss: -0.0927\n",
      "  Batch [420/1299] D_loss: -0.9184, G_loss: 2.4041\n",
      "  Batch [430/1299] D_loss: -0.7676, G_loss: 1.3318\n",
      "  Batch [440/1299] D_loss: -0.0437, G_loss: -0.1575\n",
      "  Batch [450/1299] D_loss: -0.1444, G_loss: -0.3001\n",
      "  Batch [460/1299] D_loss: -0.0234, G_loss: -0.4342\n",
      "  Batch [470/1299] D_loss: -0.0971, G_loss: -0.5724\n",
      "  Batch [480/1299] D_loss: -0.2615, G_loss: -0.7009\n",
      "  Batch [490/1299] D_loss: -0.0220, G_loss: -0.4697\n",
      "  Batch [500/1299] D_loss: 0.0280, G_loss: -0.2805\n",
      "  Batch [510/1299] D_loss: -0.0423, G_loss: -0.1192\n",
      "  Batch [520/1299] D_loss: -0.0281, G_loss: -0.1120\n",
      "  Batch [530/1299] D_loss: 0.0020, G_loss: -0.0571\n",
      "  Batch [540/1299] D_loss: -0.0047, G_loss: -0.0288\n",
      "  Batch [550/1299] D_loss: -0.7712, G_loss: 2.5103\n",
      "  Batch [560/1299] D_loss: -0.0672, G_loss: 0.0781\n",
      "  Batch [570/1299] D_loss: -0.2763, G_loss: 0.1658\n",
      "  Batch [580/1299] D_loss: -0.0518, G_loss: -0.1103\n",
      "  Batch [590/1299] D_loss: -0.1444, G_loss: -0.3412\n",
      "  Batch [600/1299] D_loss: -0.2851, G_loss: -0.5148\n",
      "  Batch [610/1299] D_loss: -0.0639, G_loss: -0.6881\n",
      "  Batch [620/1299] D_loss: -0.1354, G_loss: -0.7883\n",
      "  Batch [630/1299] D_loss: -0.0505, G_loss: -0.8124\n",
      "  Batch [640/1299] D_loss: -0.0348, G_loss: -0.5139\n",
      "  Batch [650/1299] D_loss: 0.0231, G_loss: -0.3905\n",
      "  Batch [660/1299] D_loss: 0.0091, G_loss: -0.1887\n",
      "  Batch [670/1299] D_loss: 0.0096, G_loss: -0.0779\n",
      "  Batch [680/1299] D_loss: -0.6806, G_loss: 1.6406\n",
      "  Batch [690/1299] D_loss: 0.0066, G_loss: -0.0994\n",
      "  Batch [700/1299] D_loss: -0.0361, G_loss: -0.2205\n",
      "  Batch [710/1299] D_loss: -0.1459, G_loss: -0.3414\n",
      "  Batch [720/1299] D_loss: -0.1064, G_loss: -0.3694\n",
      "  Batch [730/1299] D_loss: -0.1383, G_loss: -0.4597\n",
      "  Batch [740/1299] D_loss: -0.0495, G_loss: -0.5291\n",
      "  Batch [750/1299] D_loss: -0.1656, G_loss: -0.5143\n",
      "  Batch [760/1299] D_loss: -0.0345, G_loss: -0.3275\n",
      "  Batch [770/1299] D_loss: -0.0491, G_loss: -0.3398\n",
      "  Batch [780/1299] D_loss: -0.0376, G_loss: -0.1659\n",
      "  Batch [790/1299] D_loss: -0.0342, G_loss: -0.0464\n",
      "  Batch [800/1299] D_loss: 0.0045, G_loss: -0.0273\n",
      "  Batch [810/1299] D_loss: -0.9233, G_loss: 5.6456\n",
      "  Batch [820/1299] D_loss: 0.0030, G_loss: -0.1059\n",
      "  Batch [830/1299] D_loss: -0.0746, G_loss: -0.2048\n",
      "  Batch [840/1299] D_loss: -0.0624, G_loss: -0.2533\n",
      "  Batch [850/1299] D_loss: -0.0868, G_loss: -0.3190\n",
      "  Batch [860/1299] D_loss: -0.0631, G_loss: -0.3865\n",
      "  Batch [870/1299] D_loss: -0.0085, G_loss: -0.2426\n",
      "  Batch [880/1299] D_loss: -0.0062, G_loss: -0.0800\n",
      "  Batch [890/1299] D_loss: -0.0856, G_loss: 0.3081\n",
      "  Batch [900/1299] D_loss: -0.6160, G_loss: 1.9151\n",
      "  Batch [910/1299] D_loss: -0.5276, G_loss: 1.2693\n",
      "  Batch [920/1299] D_loss: -0.0365, G_loss: -0.0442\n",
      "  Batch [930/1299] D_loss: -0.0674, G_loss: -0.1410\n",
      "  Batch [940/1299] D_loss: -0.1190, G_loss: -0.2671\n",
      "  Batch [950/1299] D_loss: -0.2945, G_loss: -0.4859\n",
      "  Batch [960/1299] D_loss: -0.1234, G_loss: -0.6128\n",
      "  Batch [970/1299] D_loss: -0.2937, G_loss: -0.7965\n",
      "  Batch [980/1299] D_loss: -0.1484, G_loss: -0.6110\n",
      "  Batch [990/1299] D_loss: -0.0842, G_loss: -0.3942\n",
      "  Batch [1000/1299] D_loss: -0.0573, G_loss: -0.2453\n",
      "  Batch [1010/1299] D_loss: -0.0676, G_loss: -0.2760\n",
      "  Batch [1020/1299] D_loss: -0.0555, G_loss: -0.1598\n",
      "  Batch [1030/1299] D_loss: -1.2703, G_loss: 4.3494\n",
      "  Batch [1040/1299] D_loss: -0.0328, G_loss: -0.0978\n",
      "  Batch [1050/1299] D_loss: -0.0821, G_loss: -0.1860\n",
      "  Batch [1060/1299] D_loss: -0.1460, G_loss: -0.3415\n",
      "  Batch [1070/1299] D_loss: -0.0282, G_loss: -0.3006\n",
      "  Batch [1080/1299] D_loss: -0.0177, G_loss: -0.3435\n",
      "  Batch [1090/1299] D_loss: -0.0339, G_loss: -0.3036\n",
      "  Batch [1100/1299] D_loss: -0.0240, G_loss: -0.3639\n",
      "  Batch [1110/1299] D_loss: -0.0205, G_loss: -0.1454\n",
      "  Batch [1120/1299] D_loss: -1.2115, G_loss: 3.2731\n",
      "  Batch [1130/1299] D_loss: -0.4275, G_loss: 0.6512\n",
      "  Batch [1140/1299] D_loss: -0.0068, G_loss: -0.0692\n",
      "  Batch [1150/1299] D_loss: -0.0839, G_loss: -0.2752\n",
      "  Batch [1160/1299] D_loss: -0.0558, G_loss: -0.4475\n",
      "  Batch [1170/1299] D_loss: -0.0434, G_loss: -0.7377\n",
      "  Batch [1180/1299] D_loss: -0.1371, G_loss: -0.8061\n",
      "  Batch [1190/1299] D_loss: -0.0049, G_loss: -0.6409\n",
      "  Batch [1200/1299] D_loss: 0.0316, G_loss: -0.3687\n",
      "  Batch [1210/1299] D_loss: -0.0096, G_loss: -0.2069\n",
      "  Batch [1220/1299] D_loss: 0.0084, G_loss: -0.0517\n",
      "  Batch [1230/1299] D_loss: -0.0039, G_loss: -0.0536\n",
      "  Batch [1240/1299] D_loss: -0.0082, G_loss: -0.0663\n",
      "  Batch [1250/1299] D_loss: -0.0051, G_loss: -0.0954\n",
      "  Batch [1260/1299] D_loss: -3.1892, G_loss: 6.9392\n",
      "  Batch [1270/1299] D_loss: -0.0298, G_loss: -0.1278\n",
      "  Batch [1280/1299] D_loss: 0.0009, G_loss: -0.1491\n",
      "  Batch [1290/1299] D_loss: -0.0757, G_loss: -0.2506\n",
      "\n",
      "Epoch 17 Summary:\n",
      "  Average D_loss: -0.0818\n",
      "  Average G_loss: 0.0691\n",
      "\n",
      "Epoch [18/100]\n",
      "  Batch [0/1299] D_loss: -0.0471, G_loss: -0.2860\n",
      "  Batch [10/1299] D_loss: -0.0134, G_loss: -0.2996\n",
      "  Batch [20/1299] D_loss: -0.0423, G_loss: -0.2666\n",
      "  Batch [30/1299] D_loss: -0.0427, G_loss: -0.1768\n",
      "  Batch [40/1299] D_loss: -1.1147, G_loss: 3.5552\n",
      "  Batch [50/1299] D_loss: -0.0377, G_loss: -0.1533\n",
      "  Batch [60/1299] D_loss: -0.1166, G_loss: -0.2317\n",
      "  Batch [70/1299] D_loss: -0.0779, G_loss: -0.4331\n",
      "  Batch [80/1299] D_loss: -0.0977, G_loss: -0.4069\n",
      "  Batch [90/1299] D_loss: -0.1889, G_loss: -0.5531\n",
      "  Batch [100/1299] D_loss: -0.0597, G_loss: -0.4994\n",
      "  Batch [110/1299] D_loss: -0.0358, G_loss: -0.3695\n",
      "  Batch [120/1299] D_loss: -2.2546, G_loss: 6.1522\n",
      "  Batch [130/1299] D_loss: -0.6623, G_loss: 1.4021\n",
      "  Batch [140/1299] D_loss: -0.0412, G_loss: -0.0723\n",
      "  Batch [150/1299] D_loss: -0.0512, G_loss: -0.2522\n",
      "  Batch [160/1299] D_loss: -0.0529, G_loss: -0.3159\n",
      "  Batch [170/1299] D_loss: -0.0899, G_loss: -0.4573\n",
      "  Batch [180/1299] D_loss: -0.0435, G_loss: -0.4846\n",
      "  Batch [190/1299] D_loss: -0.0546, G_loss: -0.4843\n",
      "  Batch [200/1299] D_loss: -0.0385, G_loss: -0.5102\n",
      "  Batch [210/1299] D_loss: 0.0175, G_loss: -0.2392\n",
      "  Batch [220/1299] D_loss: -0.4095, G_loss: 1.1680\n",
      "  Batch [230/1299] D_loss: -0.7676, G_loss: 1.7928\n",
      "  Batch [240/1299] D_loss: -0.0414, G_loss: -0.1376\n",
      "  Batch [250/1299] D_loss: -0.0480, G_loss: -0.2785\n",
      "  Batch [260/1299] D_loss: -0.0802, G_loss: -0.4090\n",
      "  Batch [270/1299] D_loss: -0.0808, G_loss: -0.5133\n",
      "  Batch [280/1299] D_loss: -0.0774, G_loss: -0.5458\n",
      "  Batch [290/1299] D_loss: -0.0029, G_loss: -0.4428\n",
      "  Batch [300/1299] D_loss: 0.0066, G_loss: -0.1292\n",
      "  Batch [310/1299] D_loss: -1.8021, G_loss: 4.7669\n",
      "  Batch [320/1299] D_loss: -0.6628, G_loss: 1.8830\n",
      "  Batch [330/1299] D_loss: -0.3281, G_loss: 1.2981\n",
      "  Batch [340/1299] D_loss: -0.0090, G_loss: -0.1264\n",
      "  Batch [350/1299] D_loss: -0.0689, G_loss: -0.2545\n",
      "  Batch [360/1299] D_loss: -0.1190, G_loss: -0.4112\n",
      "  Batch [370/1299] D_loss: -0.1589, G_loss: -0.6823\n",
      "  Batch [380/1299] D_loss: -0.1338, G_loss: -0.6617\n",
      "  Batch [390/1299] D_loss: -0.0720, G_loss: -0.4474\n",
      "  Batch [400/1299] D_loss: -0.0610, G_loss: -0.3234\n",
      "  Batch [410/1299] D_loss: -0.0457, G_loss: -0.2664\n",
      "  Batch [420/1299] D_loss: -0.0439, G_loss: -0.1313\n",
      "  Batch [430/1299] D_loss: -1.4361, G_loss: 2.5280\n",
      "  Batch [440/1299] D_loss: -0.0301, G_loss: -0.0667\n",
      "  Batch [450/1299] D_loss: -0.0298, G_loss: -0.1489\n",
      "  Batch [460/1299] D_loss: -0.1454, G_loss: -0.2764\n",
      "  Batch [470/1299] D_loss: -0.1342, G_loss: -0.5105\n",
      "  Batch [480/1299] D_loss: -0.0910, G_loss: -0.4593\n",
      "  Batch [490/1299] D_loss: -0.0576, G_loss: -0.4383\n",
      "  Batch [500/1299] D_loss: -0.0776, G_loss: -0.4305\n",
      "  Batch [510/1299] D_loss: 0.0046, G_loss: -0.2425\n",
      "  Batch [520/1299] D_loss: -0.0087, G_loss: -0.1821\n",
      "  Batch [530/1299] D_loss: -0.6516, G_loss: 1.9769\n",
      "  Batch [540/1299] D_loss: 0.0025, G_loss: -0.1213\n",
      "  Batch [550/1299] D_loss: -0.0568, G_loss: -0.2011\n",
      "  Batch [560/1299] D_loss: -0.0569, G_loss: -0.2067\n",
      "  Batch [570/1299] D_loss: -0.0468, G_loss: -0.2288\n",
      "  Batch [580/1299] D_loss: -0.0698, G_loss: -0.3379\n",
      "  Batch [590/1299] D_loss: -0.0262, G_loss: -0.1363\n",
      "  Batch [600/1299] D_loss: -0.0233, G_loss: -0.1672\n",
      "  Batch [610/1299] D_loss: -1.1734, G_loss: 3.2913\n",
      "  Batch [620/1299] D_loss: -0.0995, G_loss: -0.1936\n",
      "  Batch [630/1299] D_loss: -0.1129, G_loss: -0.2975\n",
      "  Batch [640/1299] D_loss: -0.0453, G_loss: -0.3807\n",
      "  Batch [650/1299] D_loss: -0.1065, G_loss: -0.4173\n",
      "  Batch [660/1299] D_loss: -0.0509, G_loss: -0.4008\n",
      "  Batch [670/1299] D_loss: -0.0471, G_loss: -0.2464\n",
      "  Batch [680/1299] D_loss: -0.0438, G_loss: -0.2185\n",
      "  Batch [690/1299] D_loss: -0.0305, G_loss: -0.2103\n",
      "  Batch [700/1299] D_loss: -0.7410, G_loss: 2.6483\n",
      "  Batch [710/1299] D_loss: -0.6778, G_loss: 2.3327\n",
      "  Batch [720/1299] D_loss: -0.0436, G_loss: -0.1190\n",
      "  Batch [730/1299] D_loss: -0.0428, G_loss: -0.2168\n",
      "  Batch [740/1299] D_loss: -0.1241, G_loss: -0.3602\n",
      "  Batch [750/1299] D_loss: -0.1270, G_loss: -0.4565\n",
      "  Batch [760/1299] D_loss: -0.0690, G_loss: -0.4799\n",
      "  Batch [770/1299] D_loss: -0.0284, G_loss: -0.5807\n",
      "  Batch [780/1299] D_loss: -0.0477, G_loss: -0.4597\n",
      "  Batch [790/1299] D_loss: -0.0103, G_loss: -0.4318\n",
      "  Batch [800/1299] D_loss: 0.0071, G_loss: -0.1420\n",
      "  Batch [810/1299] D_loss: -0.3290, G_loss: 2.1902\n",
      "  Batch [820/1299] D_loss: -0.0643, G_loss: 0.3393\n",
      "  Batch [830/1299] D_loss: -0.5057, G_loss: 1.2380\n",
      "  Batch [840/1299] D_loss: -0.0661, G_loss: -0.1283\n",
      "  Batch [850/1299] D_loss: -0.0872, G_loss: -0.2689\n",
      "  Batch [860/1299] D_loss: -0.1650, G_loss: -0.5211\n",
      "  Batch [870/1299] D_loss: -0.3079, G_loss: -0.7304\n",
      "  Batch [880/1299] D_loss: -0.0213, G_loss: -0.7773\n",
      "  Batch [890/1299] D_loss: 0.0338, G_loss: -0.6234\n",
      "  Batch [900/1299] D_loss: -0.0095, G_loss: -0.3703\n",
      "  Batch [910/1299] D_loss: 0.0003, G_loss: -0.3485\n",
      "  Batch [920/1299] D_loss: -1.0779, G_loss: 3.0742\n",
      "  Batch [930/1299] D_loss: -0.1490, G_loss: 0.8306\n",
      "  Batch [940/1299] D_loss: -0.0124, G_loss: -0.0511\n",
      "  Batch [950/1299] D_loss: -0.0742, G_loss: -0.1406\n",
      "  Batch [960/1299] D_loss: -0.1450, G_loss: -0.3172\n",
      "  Batch [970/1299] D_loss: -0.2055, G_loss: -0.5073\n",
      "  Batch [980/1299] D_loss: -0.1024, G_loss: -0.6317\n",
      "  Batch [990/1299] D_loss: -0.1481, G_loss: -0.7426\n",
      "  Batch [1000/1299] D_loss: -0.0490, G_loss: -0.5550\n",
      "  Batch [1010/1299] D_loss: -0.0121, G_loss: -0.2918\n",
      "  Batch [1020/1299] D_loss: -0.0385, G_loss: -0.1906\n",
      "  Batch [1030/1299] D_loss: -0.0179, G_loss: -0.0840\n",
      "  Batch [1040/1299] D_loss: -0.6340, G_loss: 0.8462\n",
      "  Batch [1050/1299] D_loss: -0.0529, G_loss: -0.1772\n",
      "  Batch [1060/1299] D_loss: -0.0016, G_loss: -0.1821\n",
      "  Batch [1070/1299] D_loss: -0.0358, G_loss: -0.2247\n",
      "  Batch [1080/1299] D_loss: -0.0255, G_loss: -0.2294\n",
      "  Batch [1090/1299] D_loss: -0.0082, G_loss: -0.2061\n",
      "  Batch [1100/1299] D_loss: -0.0513, G_loss: -0.2156\n",
      "  Batch [1110/1299] D_loss: -1.2001, G_loss: 4.3760\n",
      "  Batch [1120/1299] D_loss: -0.3888, G_loss: 1.7373\n",
      "  Batch [1130/1299] D_loss: -0.0395, G_loss: -0.1038\n",
      "  Batch [1140/1299] D_loss: -0.0231, G_loss: -0.2602\n",
      "  Batch [1150/1299] D_loss: -0.1484, G_loss: -0.5488\n",
      "  Batch [1160/1299] D_loss: -0.1708, G_loss: -0.7239\n",
      "  Batch [1170/1299] D_loss: -0.1481, G_loss: -0.7844\n",
      "  Batch [1180/1299] D_loss: -0.0122, G_loss: -0.6950\n",
      "  Batch [1190/1299] D_loss: 0.0444, G_loss: -0.3944\n",
      "  Batch [1200/1299] D_loss: -0.0291, G_loss: -0.2348\n",
      "  Batch [1210/1299] D_loss: -0.0393, G_loss: -0.2100\n",
      "  Batch [1220/1299] D_loss: -1.4239, G_loss: 3.8270\n",
      "  Batch [1230/1299] D_loss: -0.8958, G_loss: 2.0667\n",
      "  Batch [1240/1299] D_loss: -0.1182, G_loss: -0.1774\n",
      "  Batch [1250/1299] D_loss: -0.1443, G_loss: -0.3167\n",
      "  Batch [1260/1299] D_loss: -0.1331, G_loss: -0.4764\n",
      "  Batch [1270/1299] D_loss: -0.0801, G_loss: -0.5090\n",
      "  Batch [1280/1299] D_loss: -0.0070, G_loss: -0.5592\n",
      "  Batch [1290/1299] D_loss: -0.0702, G_loss: -0.5571\n",
      "\n",
      "Epoch 18 Summary:\n",
      "  Average D_loss: -0.0804\n",
      "  Average G_loss: 0.0466\n",
      "\n",
      "Epoch [19/100]\n",
      "  Batch [0/1299] D_loss: -0.0088, G_loss: -0.2956\n",
      "  Batch [10/1299] D_loss: 0.0160, G_loss: 1.7831\n",
      "  Batch [20/1299] D_loss: -0.8989, G_loss: 1.5534\n",
      "  Batch [30/1299] D_loss: -0.0406, G_loss: -0.1468\n",
      "  Batch [40/1299] D_loss: -0.1053, G_loss: -0.2122\n",
      "  Batch [50/1299] D_loss: -0.0660, G_loss: -0.3862\n",
      "  Batch [60/1299] D_loss: -0.2083, G_loss: -0.5466\n",
      "  Batch [70/1299] D_loss: -0.0823, G_loss: -0.5866\n",
      "  Batch [80/1299] D_loss: -0.0961, G_loss: -0.4968\n",
      "  Batch [90/1299] D_loss: 0.0288, G_loss: -0.2664\n",
      "  Batch [100/1299] D_loss: 0.0009, G_loss: -0.1564\n",
      "  Batch [110/1299] D_loss: -1.1912, G_loss: 4.2252\n",
      "  Batch [120/1299] D_loss: -0.3447, G_loss: 1.0671\n",
      "  Batch [130/1299] D_loss: -1.0524, G_loss: 2.1578\n",
      "  Batch [140/1299] D_loss: -0.0716, G_loss: -0.1296\n",
      "  Batch [150/1299] D_loss: -0.0839, G_loss: -0.3306\n",
      "  Batch [160/1299] D_loss: -0.1668, G_loss: -0.4757\n",
      "  Batch [170/1299] D_loss: -0.1408, G_loss: -0.6508\n",
      "  Batch [180/1299] D_loss: -0.0361, G_loss: -0.5947\n",
      "  Batch [190/1299] D_loss: -0.0728, G_loss: -0.7380\n",
      "  Batch [200/1299] D_loss: -0.1348, G_loss: -0.6819\n",
      "  Batch [210/1299] D_loss: -0.0663, G_loss: -0.3135\n",
      "  Batch [220/1299] D_loss: 0.0080, G_loss: -0.1061\n",
      "  Batch [230/1299] D_loss: -0.0270, G_loss: 1.8161\n",
      "  Batch [240/1299] D_loss: -0.2954, G_loss: 0.4392\n",
      "  Batch [250/1299] D_loss: -0.0478, G_loss: -0.1249\n",
      "  Batch [260/1299] D_loss: -0.0357, G_loss: -0.2242\n",
      "  Batch [270/1299] D_loss: -0.0914, G_loss: -0.3863\n",
      "  Batch [280/1299] D_loss: -0.0983, G_loss: -0.4386\n",
      "  Batch [290/1299] D_loss: -0.0474, G_loss: -0.5066\n",
      "  Batch [300/1299] D_loss: -0.0123, G_loss: -0.5710\n",
      "  Batch [310/1299] D_loss: -0.0651, G_loss: -0.4261\n",
      "  Batch [320/1299] D_loss: 0.0356, G_loss: -0.1361\n",
      "  Batch [330/1299] D_loss: -0.0276, G_loss: 0.1176\n",
      "  Batch [340/1299] D_loss: -0.7023, G_loss: 1.9656\n",
      "  Batch [350/1299] D_loss: -0.0328, G_loss: -0.0700\n",
      "  Batch [360/1299] D_loss: -0.0620, G_loss: -0.1700\n",
      "  Batch [370/1299] D_loss: -0.0936, G_loss: -0.2611\n",
      "  Batch [380/1299] D_loss: -0.1165, G_loss: -0.4457\n",
      "  Batch [390/1299] D_loss: -0.0535, G_loss: -0.5198\n",
      "  Batch [400/1299] D_loss: -0.1622, G_loss: -0.6413\n",
      "  Batch [410/1299] D_loss: -0.0020, G_loss: -0.5095\n",
      "  Batch [420/1299] D_loss: 0.0407, G_loss: -0.3408\n",
      "  Batch [430/1299] D_loss: -0.0500, G_loss: -0.4365\n",
      "  Batch [440/1299] D_loss: -0.0080, G_loss: -0.1297\n",
      "  Batch [450/1299] D_loss: -0.0245, G_loss: -0.0607\n",
      "  Batch [460/1299] D_loss: -0.1023, G_loss: 2.2216\n",
      "  Batch [470/1299] D_loss: -0.2257, G_loss: 0.9554\n",
      "  Batch [480/1299] D_loss: -0.0931, G_loss: 0.3316\n",
      "  Batch [490/1299] D_loss: -0.1557, G_loss: -0.2050\n",
      "  Batch [500/1299] D_loss: -0.1993, G_loss: -0.3802\n",
      "  Batch [510/1299] D_loss: -0.0067, G_loss: -0.5438\n",
      "  Batch [520/1299] D_loss: -0.0467, G_loss: -0.6155\n",
      "  Batch [530/1299] D_loss: -0.2051, G_loss: -0.7948\n",
      "  Batch [540/1299] D_loss: 0.0265, G_loss: -0.4414\n",
      "  Batch [550/1299] D_loss: 0.0134, G_loss: -0.2407\n",
      "  Batch [560/1299] D_loss: -0.0168, G_loss: -0.2020\n",
      "  Batch [570/1299] D_loss: -0.7042, G_loss: 3.1712\n",
      "  Batch [580/1299] D_loss: -0.3200, G_loss: 1.0144\n",
      "  Batch [590/1299] D_loss: -0.0324, G_loss: -0.0727\n",
      "  Batch [600/1299] D_loss: -0.0476, G_loss: -0.1691\n",
      "  Batch [610/1299] D_loss: -0.1231, G_loss: -0.2305\n",
      "  Batch [620/1299] D_loss: -0.0385, G_loss: -0.4925\n",
      "  Batch [630/1299] D_loss: -0.1424, G_loss: -0.6023\n",
      "  Batch [640/1299] D_loss: -0.2128, G_loss: -0.7159\n",
      "  Batch [650/1299] D_loss: -0.0764, G_loss: -0.6493\n",
      "  Batch [660/1299] D_loss: -0.0621, G_loss: -0.3324\n",
      "  Batch [670/1299] D_loss: -0.0922, G_loss: -0.3622\n",
      "  Batch [680/1299] D_loss: -0.0847, G_loss: -0.1606\n",
      "  Batch [690/1299] D_loss: -0.0134, G_loss: -0.0664\n",
      "  Batch [700/1299] D_loss: -0.2238, G_loss: 3.8287\n",
      "  Batch [710/1299] D_loss: -1.5499, G_loss: 3.4703\n",
      "  Batch [720/1299] D_loss: -0.4481, G_loss: 0.3126\n",
      "  Batch [730/1299] D_loss: -0.0371, G_loss: -0.1274\n",
      "  Batch [740/1299] D_loss: -0.0661, G_loss: -0.3408\n",
      "  Batch [750/1299] D_loss: -0.2293, G_loss: -0.4400\n",
      "  Batch [760/1299] D_loss: -0.1350, G_loss: -0.5450\n",
      "  Batch [770/1299] D_loss: -0.1611, G_loss: -0.5604\n",
      "  Batch [780/1299] D_loss: -0.0206, G_loss: -0.4443\n",
      "  Batch [790/1299] D_loss: -0.0422, G_loss: -0.4454\n",
      "  Batch [800/1299] D_loss: -0.0747, G_loss: -0.5405\n",
      "  Batch [810/1299] D_loss: -0.0421, G_loss: -0.3405\n",
      "  Batch [820/1299] D_loss: -0.0453, G_loss: -0.1293\n",
      "  Batch [830/1299] D_loss: -0.4946, G_loss: 2.5948\n",
      "  Batch [840/1299] D_loss: -0.2557, G_loss: 0.6305\n",
      "  Batch [850/1299] D_loss: -0.0559, G_loss: -0.1371\n",
      "  Batch [860/1299] D_loss: -0.0622, G_loss: -0.2390\n",
      "  Batch [870/1299] D_loss: -0.0692, G_loss: -0.3047\n",
      "  Batch [880/1299] D_loss: -0.0821, G_loss: -0.4439\n",
      "  Batch [890/1299] D_loss: -0.0417, G_loss: -0.4739\n",
      "  Batch [900/1299] D_loss: -0.0786, G_loss: -0.4689\n",
      "  Batch [910/1299] D_loss: 0.0235, G_loss: -0.3409\n",
      "  Batch [920/1299] D_loss: -0.0159, G_loss: -0.1476\n",
      "  Batch [930/1299] D_loss: -0.6698, G_loss: 1.4705\n",
      "  Batch [940/1299] D_loss: -0.5925, G_loss: 1.3515\n",
      "  Batch [950/1299] D_loss: -0.0628, G_loss: -0.1167\n",
      "  Batch [960/1299] D_loss: -0.0219, G_loss: -0.2702\n",
      "  Batch [970/1299] D_loss: -0.1455, G_loss: -0.3947\n",
      "  Batch [980/1299] D_loss: -0.1307, G_loss: -0.4552\n",
      "  Batch [990/1299] D_loss: -0.1608, G_loss: -0.4885\n",
      "  Batch [1000/1299] D_loss: -0.0020, G_loss: -0.3733\n",
      "  Batch [1010/1299] D_loss: -0.0583, G_loss: -0.3507\n",
      "  Batch [1020/1299] D_loss: -0.0598, G_loss: -0.4252\n",
      "  Batch [1030/1299] D_loss: -0.0049, G_loss: -0.1538\n",
      "  Batch [1040/1299] D_loss: -0.3147, G_loss: 1.9140\n",
      "  Batch [1050/1299] D_loss: -0.3786, G_loss: 0.2590\n",
      "  Batch [1060/1299] D_loss: -0.0228, G_loss: -0.1540\n",
      "  Batch [1070/1299] D_loss: -0.0669, G_loss: -0.2307\n",
      "  Batch [1080/1299] D_loss: -0.1049, G_loss: -0.4128\n",
      "  Batch [1090/1299] D_loss: -0.1436, G_loss: -0.4182\n",
      "  Batch [1100/1299] D_loss: -0.1249, G_loss: -0.6709\n",
      "  Batch [1110/1299] D_loss: -0.0909, G_loss: -0.4170\n",
      "  Batch [1120/1299] D_loss: 0.0329, G_loss: -0.3887\n",
      "  Batch [1130/1299] D_loss: -0.0610, G_loss: -0.2877\n",
      "  Batch [1140/1299] D_loss: -1.4084, G_loss: 4.1645\n",
      "  Batch [1150/1299] D_loss: -0.3100, G_loss: 0.3097\n",
      "  Batch [1160/1299] D_loss: -0.0167, G_loss: -0.0305\n",
      "  Batch [1170/1299] D_loss: -0.1069, G_loss: -0.1816\n",
      "  Batch [1180/1299] D_loss: -0.0653, G_loss: -0.3514\n",
      "  Batch [1190/1299] D_loss: -0.2366, G_loss: -0.5256\n",
      "  Batch [1200/1299] D_loss: -0.2477, G_loss: -0.6908\n",
      "  Batch [1210/1299] D_loss: -0.0740, G_loss: -0.7195\n",
      "  Batch [1220/1299] D_loss: -0.0785, G_loss: -0.6935\n",
      "  Batch [1230/1299] D_loss: 0.0321, G_loss: -0.4436\n",
      "  Batch [1240/1299] D_loss: 0.0274, G_loss: -0.3313\n",
      "  Batch [1250/1299] D_loss: -0.0143, G_loss: -0.1609\n",
      "  Batch [1260/1299] D_loss: -0.5721, G_loss: 1.9287\n",
      "  Batch [1270/1299] D_loss: -0.0005, G_loss: -0.0934\n",
      "  Batch [1280/1299] D_loss: -0.0724, G_loss: -0.1717\n",
      "  Batch [1290/1299] D_loss: -0.0537, G_loss: -0.2654\n",
      "\n",
      "Epoch 19 Summary:\n",
      "  Average D_loss: -0.0945\n",
      "  Average G_loss: 0.1004\n",
      "\n",
      "Epoch [20/100]\n",
      "  Batch [0/1299] D_loss: -0.1012, G_loss: -0.3011\n",
      "  Batch [10/1299] D_loss: -0.0807, G_loss: -0.3379\n",
      "  Batch [20/1299] D_loss: -0.0989, G_loss: -0.3300\n",
      "  Batch [30/1299] D_loss: -0.2133, G_loss: -0.3552\n",
      "  Batch [40/1299] D_loss: 0.0066, G_loss: -0.0074\n",
      "  Batch [50/1299] D_loss: -1.5566, G_loss: 3.7991\n",
      "  Batch [60/1299] D_loss: -1.0865, G_loss: 1.5691\n",
      "  Batch [70/1299] D_loss: -0.0319, G_loss: -0.1549\n",
      "  Batch [80/1299] D_loss: -0.0856, G_loss: -0.2447\n",
      "  Batch [90/1299] D_loss: -0.1961, G_loss: -0.4550\n",
      "  Batch [100/1299] D_loss: -0.0209, G_loss: -0.4626\n",
      "  Batch [110/1299] D_loss: -0.0769, G_loss: -0.4929\n",
      "  Batch [120/1299] D_loss: -0.0322, G_loss: -0.4586\n",
      "  Batch [130/1299] D_loss: -0.0496, G_loss: -0.3711\n",
      "  Batch [140/1299] D_loss: -0.0073, G_loss: -0.2534\n",
      "  Batch [150/1299] D_loss: 0.0173, G_loss: -0.0989\n",
      "  Batch [160/1299] D_loss: -0.0504, G_loss: -0.2643\n",
      "  Batch [170/1299] D_loss: -0.5097, G_loss: 1.7748\n",
      "  Batch [180/1299] D_loss: -0.0688, G_loss: 0.2145\n",
      "  Batch [190/1299] D_loss: -0.2810, G_loss: 0.7672\n",
      "  Batch [200/1299] D_loss: -0.0324, G_loss: -0.0607\n",
      "  Batch [210/1299] D_loss: -0.1968, G_loss: 0.3528\n",
      "  Batch [220/1299] D_loss: -0.0583, G_loss: -0.1937\n",
      "  Batch [230/1299] D_loss: -0.1435, G_loss: -0.4156\n",
      "  Batch [240/1299] D_loss: -0.2210, G_loss: -0.6475\n",
      "  Batch [250/1299] D_loss: -0.1536, G_loss: -0.8376\n",
      "  Batch [260/1299] D_loss: -0.0166, G_loss: -0.7391\n",
      "  Batch [270/1299] D_loss: -0.0518, G_loss: -0.5753\n",
      "  Batch [280/1299] D_loss: -0.0099, G_loss: -0.2236\n",
      "  Batch [290/1299] D_loss: -0.0454, G_loss: -0.0913\n",
      "  Batch [300/1299] D_loss: -1.1378, G_loss: 4.0904\n",
      "  Batch [310/1299] D_loss: -0.0080, G_loss: -0.0661\n",
      "  Batch [320/1299] D_loss: -0.1169, G_loss: -0.2283\n",
      "  Batch [330/1299] D_loss: -0.1103, G_loss: -0.3163\n",
      "  Batch [340/1299] D_loss: -0.1398, G_loss: -0.4219\n",
      "  Batch [350/1299] D_loss: -0.1331, G_loss: -0.6613\n",
      "  Batch [360/1299] D_loss: -0.1192, G_loss: -0.6339\n",
      "  Batch [370/1299] D_loss: 0.0709, G_loss: -0.4834\n",
      "  Batch [380/1299] D_loss: -0.0405, G_loss: -0.2276\n",
      "  Batch [390/1299] D_loss: -1.0507, G_loss: 5.4864\n",
      "  Batch [400/1299] D_loss: -0.0179, G_loss: -0.0477\n",
      "  Batch [410/1299] D_loss: -0.0229, G_loss: -0.1752\n",
      "  Batch [420/1299] D_loss: -0.0344, G_loss: -0.2822\n",
      "  Batch [430/1299] D_loss: -0.0631, G_loss: -0.3803\n",
      "  Batch [440/1299] D_loss: -0.0104, G_loss: -0.3492\n",
      "  Batch [450/1299] D_loss: -0.0565, G_loss: -0.3322\n",
      "  Batch [460/1299] D_loss: -0.0504, G_loss: -0.2736\n",
      "  Batch [470/1299] D_loss: -0.0155, G_loss: -0.1751\n",
      "  Batch [480/1299] D_loss: -0.0926, G_loss: -0.2308\n",
      "  Batch [490/1299] D_loss: -0.3204, G_loss: 0.9052\n",
      "  Batch [500/1299] D_loss: -0.0240, G_loss: -0.0860\n",
      "  Batch [510/1299] D_loss: -0.0589, G_loss: -0.1806\n",
      "  Batch [520/1299] D_loss: -0.0313, G_loss: -0.2389\n",
      "  Batch [530/1299] D_loss: -0.1249, G_loss: -0.3670\n",
      "  Batch [540/1299] D_loss: -0.0122, G_loss: -0.3348\n",
      "  Batch [550/1299] D_loss: -0.0747, G_loss: -0.2730\n",
      "  Batch [560/1299] D_loss: -0.0553, G_loss: -0.2461\n",
      "  Batch [570/1299] D_loss: -0.0393, G_loss: -0.1537\n",
      "  Batch [580/1299] D_loss: -0.0175, G_loss: -0.1922\n",
      "  Batch [590/1299] D_loss: -0.2016, G_loss: 2.8434\n",
      "  Batch [600/1299] D_loss: -0.0303, G_loss: -0.0766\n",
      "  Batch [610/1299] D_loss: -0.0564, G_loss: -0.1428\n",
      "  Batch [620/1299] D_loss: -0.0753, G_loss: -0.2396\n",
      "  Batch [630/1299] D_loss: -0.1740, G_loss: -0.3736\n",
      "  Batch [640/1299] D_loss: -0.1513, G_loss: -0.3995\n",
      "  Batch [650/1299] D_loss: -0.0790, G_loss: -0.3937\n",
      "  Batch [660/1299] D_loss: -0.0578, G_loss: -0.3459\n",
      "  Batch [670/1299] D_loss: -0.0767, G_loss: -0.2415\n",
      "  Batch [680/1299] D_loss: -0.0208, G_loss: -0.3071\n",
      "  Batch [690/1299] D_loss: -0.0570, G_loss: -0.2927\n",
      "  Batch [700/1299] D_loss: -0.0036, G_loss: -0.1083\n",
      "  Batch [710/1299] D_loss: -1.0116, G_loss: 2.1313\n",
      "  Batch [720/1299] D_loss: -0.1789, G_loss: 0.4636\n",
      "  Batch [730/1299] D_loss: -0.0588, G_loss: -0.0996\n",
      "  Batch [740/1299] D_loss: -0.1075, G_loss: -0.2606\n",
      "  Batch [750/1299] D_loss: -0.0522, G_loss: -0.4054\n",
      "  Batch [760/1299] D_loss: -0.1262, G_loss: -0.5503\n",
      "  Batch [770/1299] D_loss: -0.1484, G_loss: -0.6119\n",
      "  Batch [780/1299] D_loss: -0.1565, G_loss: -0.6772\n",
      "  Batch [790/1299] D_loss: -0.0534, G_loss: -0.4048\n",
      "  Batch [800/1299] D_loss: 0.0126, G_loss: -0.3298\n",
      "  Batch [810/1299] D_loss: 0.0269, G_loss: -0.1336\n",
      "  Batch [820/1299] D_loss: -0.0580, G_loss: -0.1521\n",
      "  Batch [830/1299] D_loss: -2.0907, G_loss: 4.8484\n",
      "  Batch [840/1299] D_loss: -0.0250, G_loss: 0.0207\n",
      "  Batch [850/1299] D_loss: -0.0495, G_loss: -0.1749\n",
      "  Batch [860/1299] D_loss: -0.0641, G_loss: -0.2442\n",
      "  Batch [870/1299] D_loss: -0.0520, G_loss: -0.3393\n",
      "  Batch [880/1299] D_loss: -0.0170, G_loss: -0.4008\n",
      "  Batch [890/1299] D_loss: -0.0277, G_loss: -0.3758\n",
      "  Batch [900/1299] D_loss: -0.0425, G_loss: -0.3075\n",
      "  Batch [910/1299] D_loss: -0.0408, G_loss: -0.1711\n",
      "  Batch [920/1299] D_loss: 0.0023, G_loss: -0.0903\n",
      "  Batch [930/1299] D_loss: -0.5219, G_loss: 1.9854\n",
      "  Batch [940/1299] D_loss: -0.0653, G_loss: 0.9411\n",
      "  Batch [950/1299] D_loss: -0.0854, G_loss: -0.1586\n",
      "  Batch [960/1299] D_loss: -0.0274, G_loss: -0.3106\n",
      "  Batch [970/1299] D_loss: -0.0454, G_loss: -0.4559\n",
      "  Batch [980/1299] D_loss: -0.0871, G_loss: -0.5348\n",
      "  Batch [990/1299] D_loss: -0.0475, G_loss: -0.4381\n",
      "  Batch [1000/1299] D_loss: -0.0917, G_loss: -0.5690\n",
      "  Batch [1010/1299] D_loss: 0.0003, G_loss: -0.2112\n",
      "  Batch [1020/1299] D_loss: -0.6584, G_loss: 1.6091\n",
      "  Batch [1030/1299] D_loss: -0.5742, G_loss: 0.6877\n",
      "  Batch [1040/1299] D_loss: -0.2958, G_loss: 0.7438\n",
      "  Batch [1050/1299] D_loss: -0.0566, G_loss: 0.3246\n",
      "  Batch [1060/1299] D_loss: -0.0709, G_loss: -0.2330\n",
      "  Batch [1070/1299] D_loss: -0.1418, G_loss: -0.4170\n",
      "  Batch [1080/1299] D_loss: -0.2662, G_loss: -0.6243\n",
      "  Batch [1090/1299] D_loss: -0.1113, G_loss: -0.6993\n",
      "  Batch [1100/1299] D_loss: -0.1254, G_loss: -0.7066\n",
      "  Batch [1110/1299] D_loss: -0.0939, G_loss: -0.5383\n",
      "  Batch [1120/1299] D_loss: -0.0180, G_loss: -0.3196\n",
      "  Batch [1130/1299] D_loss: -0.0029, G_loss: -0.1148\n",
      "  Batch [1140/1299] D_loss: -0.0344, G_loss: -0.1196\n",
      "  Batch [1150/1299] D_loss: -0.9165, G_loss: 2.7641\n",
      "  Batch [1160/1299] D_loss: -0.2760, G_loss: 0.5497\n",
      "  Batch [1170/1299] D_loss: -0.0449, G_loss: -0.1665\n",
      "  Batch [1180/1299] D_loss: -0.1624, G_loss: -0.3010\n",
      "  Batch [1190/1299] D_loss: -0.2196, G_loss: -0.4024\n",
      "  Batch [1200/1299] D_loss: -0.1561, G_loss: -0.5610\n",
      "  Batch [1210/1299] D_loss: -0.0826, G_loss: -0.4623\n",
      "  Batch [1220/1299] D_loss: -0.0664, G_loss: -0.5462\n",
      "  Batch [1230/1299] D_loss: -0.0750, G_loss: -0.5029\n",
      "  Batch [1240/1299] D_loss: -0.0074, G_loss: -0.3051\n",
      "  Batch [1250/1299] D_loss: -0.1069, G_loss: 0.7995\n",
      "  Batch [1260/1299] D_loss: -0.0299, G_loss: -0.0940\n",
      "  Batch [1270/1299] D_loss: -0.0272, G_loss: -0.1101\n",
      "  Batch [1280/1299] D_loss: -0.0151, G_loss: -0.1464\n",
      "  Batch [1290/1299] D_loss: -0.3297, G_loss: 0.7478\n",
      "\n",
      "Epoch 20 Summary:\n",
      "  Average D_loss: -0.0783\n",
      "  Average G_loss: 0.0780\n",
      "\n",
      "Epoch [21/100]\n",
      "  Batch [0/1299] D_loss: -0.0134, G_loss: -0.0983\n",
      "  Batch [10/1299] D_loss: -0.0900, G_loss: -0.2256\n",
      "  Batch [20/1299] D_loss: -0.0545, G_loss: -0.3315\n",
      "  Batch [30/1299] D_loss: -0.0483, G_loss: -0.4000\n",
      "  Batch [40/1299] D_loss: -0.0777, G_loss: -0.3506\n",
      "  Batch [50/1299] D_loss: -0.0416, G_loss: -0.4631\n",
      "  Batch [60/1299] D_loss: -0.0135, G_loss: -0.5380\n",
      "  Batch [70/1299] D_loss: -0.0047, G_loss: -0.1722\n",
      "  Batch [80/1299] D_loss: -0.6355, G_loss: 2.6627\n",
      "  Batch [90/1299] D_loss: 0.0002, G_loss: 0.3030\n",
      "  Batch [100/1299] D_loss: -0.0231, G_loss: -0.1876\n",
      "  Batch [110/1299] D_loss: -0.1155, G_loss: -0.2681\n",
      "  Batch [120/1299] D_loss: -0.0367, G_loss: -0.3905\n",
      "  Batch [130/1299] D_loss: -0.1223, G_loss: -0.3859\n",
      "  Batch [140/1299] D_loss: -0.0860, G_loss: -0.4939\n",
      "  Batch [150/1299] D_loss: -0.1316, G_loss: -0.4838\n",
      "  Batch [160/1299] D_loss: -0.0412, G_loss: -0.2291\n",
      "  Batch [170/1299] D_loss: -0.0511, G_loss: -0.1711\n",
      "  Batch [180/1299] D_loss: -0.6946, G_loss: 3.3952\n",
      "  Batch [190/1299] D_loss: -0.0556, G_loss: 0.0897\n",
      "  Batch [200/1299] D_loss: -0.3609, G_loss: 0.3582\n",
      "  Batch [210/1299] D_loss: -0.0351, G_loss: -0.1408\n",
      "  Batch [220/1299] D_loss: -0.1508, G_loss: -0.2989\n",
      "  Batch [230/1299] D_loss: -0.0672, G_loss: -0.4212\n",
      "  Batch [240/1299] D_loss: -0.1452, G_loss: -0.5309\n",
      "  Batch [250/1299] D_loss: -0.1250, G_loss: -0.5589\n",
      "  Batch [260/1299] D_loss: -0.0038, G_loss: -0.4829\n",
      "  Batch [270/1299] D_loss: 0.0094, G_loss: -0.3022\n",
      "  Batch [280/1299] D_loss: -0.0120, G_loss: -0.2510\n",
      "  Batch [290/1299] D_loss: -0.0667, G_loss: -0.1994\n",
      "  Batch [300/1299] D_loss: -0.0333, G_loss: -0.2215\n",
      "  Batch [310/1299] D_loss: -0.0302, G_loss: -0.0491\n",
      "  Batch [320/1299] D_loss: -0.1033, G_loss: 0.1388\n",
      "  Batch [330/1299] D_loss: -0.0143, G_loss: -0.0635\n",
      "  Batch [340/1299] D_loss: -0.0605, G_loss: -0.1804\n",
      "  Batch [350/1299] D_loss: -0.1519, G_loss: -0.3343\n",
      "  Batch [360/1299] D_loss: -0.1949, G_loss: -0.3564\n",
      "  Batch [370/1299] D_loss: -0.0465, G_loss: -0.4001\n",
      "  Batch [380/1299] D_loss: -0.2187, G_loss: -0.6380\n",
      "  Batch [390/1299] D_loss: 0.0061, G_loss: -0.5499\n",
      "  Batch [400/1299] D_loss: 0.0104, G_loss: -0.2764\n",
      "  Batch [410/1299] D_loss: -0.0047, G_loss: -0.0846\n",
      "  Batch [420/1299] D_loss: -0.2862, G_loss: 0.6617\n",
      "  Batch [430/1299] D_loss: 0.0523, G_loss: 0.0460\n",
      "  Batch [440/1299] D_loss: -0.0184, G_loss: -0.0819\n",
      "  Batch [450/1299] D_loss: -0.0241, G_loss: -0.1725\n",
      "  Batch [460/1299] D_loss: -0.0841, G_loss: -0.3169\n",
      "  Batch [470/1299] D_loss: -0.0901, G_loss: -0.3924\n",
      "  Batch [480/1299] D_loss: -0.0700, G_loss: -0.4388\n",
      "  Batch [490/1299] D_loss: -0.0282, G_loss: -0.4775\n",
      "  Batch [500/1299] D_loss: -0.0593, G_loss: -0.3122\n",
      "  Batch [510/1299] D_loss: -0.0077, G_loss: -0.0801\n",
      "  Batch [520/1299] D_loss: -0.7302, G_loss: 1.9704\n",
      "  Batch [530/1299] D_loss: -0.0471, G_loss: -0.0573\n",
      "  Batch [540/1299] D_loss: -0.0892, G_loss: -0.1860\n",
      "  Batch [550/1299] D_loss: -0.1034, G_loss: -0.3258\n",
      "  Batch [560/1299] D_loss: -0.1263, G_loss: -0.4506\n",
      "  Batch [570/1299] D_loss: -0.0200, G_loss: -0.4223\n",
      "  Batch [580/1299] D_loss: -0.0189, G_loss: -0.4733\n",
      "  Batch [590/1299] D_loss: -0.0499, G_loss: -0.3728\n",
      "  Batch [600/1299] D_loss: 0.0001, G_loss: -0.1930\n",
      "  Batch [610/1299] D_loss: -0.0030, G_loss: -0.0565\n",
      "  Batch [620/1299] D_loss: -0.7055, G_loss: 1.8126\n",
      "  Batch [630/1299] D_loss: -0.2803, G_loss: 0.7015\n",
      "  Batch [640/1299] D_loss: -0.0148, G_loss: -0.1111\n",
      "  Batch [650/1299] D_loss: -0.0362, G_loss: -0.2104\n",
      "  Batch [660/1299] D_loss: -0.1360, G_loss: -0.3861\n",
      "  Batch [670/1299] D_loss: -0.1757, G_loss: -0.5712\n",
      "  Batch [680/1299] D_loss: -0.1153, G_loss: -0.5325\n",
      "  Batch [690/1299] D_loss: -0.0040, G_loss: -0.5190\n",
      "  Batch [700/1299] D_loss: -0.0171, G_loss: -0.3862\n",
      "  Batch [710/1299] D_loss: -0.0172, G_loss: -0.3295\n",
      "  Batch [720/1299] D_loss: -0.7235, G_loss: 2.8370\n",
      "  Batch [730/1299] D_loss: -0.1006, G_loss: 0.1278\n",
      "  Batch [740/1299] D_loss: -1.2370, G_loss: 1.9176\n",
      "  Batch [750/1299] D_loss: -0.0172, G_loss: -0.0663\n",
      "  Batch [760/1299] D_loss: -0.0726, G_loss: -0.1740\n",
      "  Batch [770/1299] D_loss: -0.1585, G_loss: -0.3394\n",
      "  Batch [780/1299] D_loss: -0.1383, G_loss: -0.4856\n",
      "  Batch [790/1299] D_loss: -0.1535, G_loss: -0.5431\n",
      "  Batch [800/1299] D_loss: -0.0349, G_loss: -0.4597\n",
      "  Batch [810/1299] D_loss: -0.0120, G_loss: -0.4731\n",
      "  Batch [820/1299] D_loss: 0.0076, G_loss: -0.2564\n",
      "  Batch [830/1299] D_loss: -0.0428, G_loss: -0.1687\n",
      "  Batch [840/1299] D_loss: -0.5371, G_loss: 1.6531\n",
      "  Batch [850/1299] D_loss: -0.5383, G_loss: 0.8411\n",
      "  Batch [860/1299] D_loss: -0.0430, G_loss: -0.0629\n",
      "  Batch [870/1299] D_loss: -0.1218, G_loss: -0.2636\n",
      "  Batch [880/1299] D_loss: -0.0513, G_loss: -0.4283\n",
      "  Batch [890/1299] D_loss: -0.3891, G_loss: -0.6480\n",
      "  Batch [900/1299] D_loss: -0.0995, G_loss: -0.7030\n",
      "  Batch [910/1299] D_loss: -0.1062, G_loss: -0.6241\n",
      "  Batch [920/1299] D_loss: 0.0368, G_loss: -0.3245\n",
      "  Batch [930/1299] D_loss: -0.1197, G_loss: -0.4457\n",
      "  Batch [940/1299] D_loss: -0.9323, G_loss: 4.6661\n",
      "  Batch [950/1299] D_loss: -0.0220, G_loss: -0.0869\n",
      "  Batch [960/1299] D_loss: -0.0394, G_loss: -0.0984\n",
      "  Batch [970/1299] D_loss: -0.0649, G_loss: -0.2021\n",
      "  Batch [980/1299] D_loss: -0.0840, G_loss: -0.2049\n",
      "  Batch [990/1299] D_loss: -0.0762, G_loss: -0.1645\n",
      "  Batch [1000/1299] D_loss: -0.0215, G_loss: -0.2946\n",
      "  Batch [1010/1299] D_loss: -0.0276, G_loss: -0.1807\n",
      "  Batch [1020/1299] D_loss: -1.2551, G_loss: 2.7240\n",
      "  Batch [1030/1299] D_loss: -0.3338, G_loss: 1.2283\n",
      "  Batch [1040/1299] D_loss: -0.0528, G_loss: -0.0945\n",
      "  Batch [1050/1299] D_loss: -0.0514, G_loss: -0.2254\n",
      "  Batch [1060/1299] D_loss: -0.1330, G_loss: -0.4769\n",
      "  Batch [1070/1299] D_loss: -0.0652, G_loss: -0.6077\n",
      "  Batch [1080/1299] D_loss: -0.1909, G_loss: -0.8724\n",
      "  Batch [1090/1299] D_loss: 0.0176, G_loss: -0.5477\n",
      "  Batch [1100/1299] D_loss: -0.0270, G_loss: -0.5826\n",
      "  Batch [1110/1299] D_loss: -0.0216, G_loss: -0.2274\n",
      "  Batch [1120/1299] D_loss: -0.0196, G_loss: -0.0645\n",
      "  Batch [1130/1299] D_loss: -0.0088, G_loss: -0.1252\n",
      "  Batch [1140/1299] D_loss: -0.0162, G_loss: -0.1566\n",
      "  Batch [1150/1299] D_loss: -0.0554, G_loss: -0.1934\n",
      "  Batch [1160/1299] D_loss: -0.1214, G_loss: -0.3280\n",
      "  Batch [1170/1299] D_loss: -0.0472, G_loss: -0.2726\n",
      "  Batch [1180/1299] D_loss: -0.0444, G_loss: -0.1308\n",
      "  Batch [1190/1299] D_loss: -0.2428, G_loss: 0.6160\n",
      "  Batch [1200/1299] D_loss: -0.4437, G_loss: 1.4697\n",
      "  Batch [1210/1299] D_loss: -0.0236, G_loss: -0.1209\n",
      "  Batch [1220/1299] D_loss: -0.1407, G_loss: -0.2191\n",
      "  Batch [1230/1299] D_loss: -0.1080, G_loss: -0.3167\n",
      "  Batch [1240/1299] D_loss: -0.0573, G_loss: -0.4838\n",
      "  Batch [1250/1299] D_loss: 0.0418, G_loss: -0.5800\n",
      "  Batch [1260/1299] D_loss: -0.0473, G_loss: -0.3821\n",
      "  Batch [1270/1299] D_loss: -0.0199, G_loss: -0.3921\n",
      "  Batch [1280/1299] D_loss: -0.0172, G_loss: -0.1204\n",
      "  Batch [1290/1299] D_loss: -0.0187, G_loss: -0.0290\n",
      "\n",
      "Epoch 21 Summary:\n",
      "  Average D_loss: -0.0797\n",
      "  Average G_loss: 0.0648\n",
      "\n",
      "Models saved at epoch 21:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250120_143948_dataset+cell_type/generator_20250120_143948_dataset+cell_type_epoch_21.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250120_143948_dataset+cell_type/discriminator_20250120_143948_dataset+cell_type_epoch_21.pt\n",
      "\n",
      "Epoch [22/100]\n",
      "  Batch [0/1299] D_loss: -0.0078, G_loss: -0.0888\n",
      "  Batch [10/1299] D_loss: -0.0010, G_loss: -0.0819\n",
      "  Batch [20/1299] D_loss: -0.0113, G_loss: -0.1379\n",
      "  Batch [30/1299] D_loss: 0.0028, G_loss: -0.0993\n",
      "  Batch [40/1299] D_loss: -0.5329, G_loss: 1.3322\n",
      "  Batch [50/1299] D_loss: -0.0272, G_loss: -0.0639\n",
      "  Batch [60/1299] D_loss: -0.0178, G_loss: -0.0893\n",
      "  Batch [70/1299] D_loss: -0.0246, G_loss: 0.0149\n",
      "  Batch [80/1299] D_loss: -0.1133, G_loss: -0.1747\n",
      "  Batch [90/1299] D_loss: -0.1074, G_loss: -0.3260\n",
      "  Batch [100/1299] D_loss: -0.2084, G_loss: -0.6727\n",
      "  Batch [110/1299] D_loss: -0.3205, G_loss: -0.7211\n",
      "  Batch [120/1299] D_loss: -0.1771, G_loss: -0.8931\n",
      "  Batch [130/1299] D_loss: -0.0277, G_loss: -0.6092\n",
      "  Batch [140/1299] D_loss: -0.1056, G_loss: -0.3591\n",
      "  Batch [150/1299] D_loss: -0.0178, G_loss: -0.2434\n",
      "  Batch [160/1299] D_loss: -0.0248, G_loss: -0.1760\n",
      "  Batch [170/1299] D_loss: -0.0124, G_loss: 1.3481\n",
      "  Batch [180/1299] D_loss: -0.4666, G_loss: 1.1046\n",
      "  Batch [190/1299] D_loss: -0.0190, G_loss: -0.0931\n",
      "  Batch [200/1299] D_loss: -0.0632, G_loss: -0.1902\n",
      "  Batch [210/1299] D_loss: -0.1331, G_loss: -0.2970\n",
      "  Batch [220/1299] D_loss: -0.1583, G_loss: -0.3917\n",
      "  Batch [230/1299] D_loss: -0.1471, G_loss: -0.4909\n",
      "  Batch [240/1299] D_loss: -0.0387, G_loss: -0.4803\n",
      "  Batch [250/1299] D_loss: -0.0326, G_loss: -0.3994\n",
      "  Batch [260/1299] D_loss: -0.0198, G_loss: -0.3188\n",
      "  Batch [270/1299] D_loss: -0.0705, G_loss: -0.1752\n",
      "  Batch [280/1299] D_loss: -0.5379, G_loss: 1.6707\n",
      "  Batch [290/1299] D_loss: -1.0621, G_loss: 3.3044\n",
      "  Batch [300/1299] D_loss: -0.0592, G_loss: -0.1071\n",
      "  Batch [310/1299] D_loss: -0.0539, G_loss: -0.2566\n",
      "  Batch [320/1299] D_loss: -0.1323, G_loss: -0.4125\n",
      "  Batch [330/1299] D_loss: -0.1638, G_loss: -0.5237\n",
      "  Batch [340/1299] D_loss: -0.0488, G_loss: -0.5214\n",
      "  Batch [350/1299] D_loss: -0.0595, G_loss: -0.5859\n",
      "  Batch [360/1299] D_loss: -0.1100, G_loss: -0.6773\n",
      "  Batch [370/1299] D_loss: -0.0461, G_loss: -0.2598\n",
      "  Batch [380/1299] D_loss: -0.0329, G_loss: -0.2432\n",
      "  Batch [390/1299] D_loss: -0.0261, G_loss: -0.1502\n",
      "  Batch [400/1299] D_loss: -0.0285, G_loss: 0.3975\n",
      "  Batch [410/1299] D_loss: -0.5244, G_loss: 1.8814\n",
      "  Batch [420/1299] D_loss: -0.0461, G_loss: 0.0516\n",
      "  Batch [430/1299] D_loss: -0.0334, G_loss: -0.1327\n",
      "  Batch [440/1299] D_loss: -0.0643, G_loss: -0.2622\n",
      "  Batch [450/1299] D_loss: -0.1542, G_loss: -0.3952\n",
      "  Batch [460/1299] D_loss: -0.0435, G_loss: -0.4241\n",
      "  Batch [470/1299] D_loss: -0.0463, G_loss: -0.4891\n",
      "  Batch [480/1299] D_loss: -0.2346, G_loss: -0.7198\n",
      "  Batch [490/1299] D_loss: 0.0080, G_loss: -0.5566\n",
      "  Batch [500/1299] D_loss: -0.0697, G_loss: -0.3385\n",
      "  Batch [510/1299] D_loss: -0.0367, G_loss: -0.1576\n",
      "  Batch [520/1299] D_loss: -1.5194, G_loss: 5.2089\n",
      "  Batch [530/1299] D_loss: -0.2346, G_loss: 0.8064\n",
      "  Batch [540/1299] D_loss: -0.0439, G_loss: -0.0852\n",
      "  Batch [550/1299] D_loss: -0.1468, G_loss: -0.3216\n",
      "  Batch [560/1299] D_loss: -0.2355, G_loss: -0.5337\n",
      "  Batch [570/1299] D_loss: -0.3637, G_loss: -0.7058\n",
      "  Batch [580/1299] D_loss: -0.1723, G_loss: -0.7300\n",
      "  Batch [590/1299] D_loss: -0.1341, G_loss: -0.7853\n",
      "  Batch [600/1299] D_loss: 0.0159, G_loss: -0.5285\n",
      "  Batch [610/1299] D_loss: -0.0435, G_loss: -0.2206\n",
      "  Batch [620/1299] D_loss: 0.0039, G_loss: -0.1045\n",
      "  Batch [630/1299] D_loss: -1.9530, G_loss: 7.1024\n",
      "  Batch [640/1299] D_loss: -0.9450, G_loss: 1.5352\n",
      "  Batch [650/1299] D_loss: -0.0068, G_loss: -0.0884\n",
      "  Batch [660/1299] D_loss: -0.0901, G_loss: -0.2317\n",
      "  Batch [670/1299] D_loss: -0.1406, G_loss: -0.3286\n",
      "  Batch [680/1299] D_loss: -0.1335, G_loss: -0.5114\n",
      "  Batch [690/1299] D_loss: -0.1685, G_loss: -0.6132\n",
      "  Batch [700/1299] D_loss: 0.0193, G_loss: -0.5795\n",
      "  Batch [710/1299] D_loss: 0.0228, G_loss: -0.3841\n",
      "  Batch [720/1299] D_loss: -0.0087, G_loss: -0.1838\n",
      "  Batch [730/1299] D_loss: -0.1796, G_loss: -0.4016\n",
      "  Batch [740/1299] D_loss: -0.0744, G_loss: 1.6516\n",
      "  Batch [750/1299] D_loss: -0.0196, G_loss: -0.0943\n",
      "  Batch [760/1299] D_loss: -0.0402, G_loss: -0.1479\n",
      "  Batch [770/1299] D_loss: -0.0552, G_loss: -0.2319\n",
      "  Batch [780/1299] D_loss: -0.1423, G_loss: -0.3445\n",
      "  Batch [790/1299] D_loss: 0.0037, G_loss: -0.3615\n",
      "  Batch [800/1299] D_loss: -0.0634, G_loss: -0.4210\n",
      "  Batch [810/1299] D_loss: -0.0992, G_loss: -0.4196\n",
      "  Batch [820/1299] D_loss: -1.0366, G_loss: 2.4346\n",
      "  Batch [830/1299] D_loss: -0.0331, G_loss: 0.0108\n",
      "  Batch [840/1299] D_loss: -0.3005, G_loss: 0.5793\n",
      "  Batch [850/1299] D_loss: -0.0649, G_loss: -0.1403\n",
      "  Batch [860/1299] D_loss: -0.1901, G_loss: -0.4451\n",
      "  Batch [870/1299] D_loss: -0.3143, G_loss: -0.5574\n",
      "  Batch [880/1299] D_loss: -0.1358, G_loss: -0.8336\n",
      "  Batch [890/1299] D_loss: 0.0547, G_loss: -0.6935\n",
      "  Batch [900/1299] D_loss: -0.0152, G_loss: -0.4198\n",
      "  Batch [910/1299] D_loss: -0.0460, G_loss: -0.3031\n",
      "  Batch [920/1299] D_loss: -0.1079, G_loss: 0.5656\n",
      "  Batch [930/1299] D_loss: -0.3386, G_loss: 0.7272\n",
      "  Batch [940/1299] D_loss: -0.0494, G_loss: -0.1156\n",
      "  Batch [950/1299] D_loss: -0.0483, G_loss: -0.2241\n",
      "  Batch [960/1299] D_loss: -0.0734, G_loss: -0.3566\n",
      "  Batch [970/1299] D_loss: 0.0015, G_loss: -0.3319\n",
      "  Batch [980/1299] D_loss: -0.0616, G_loss: -0.3855\n",
      "  Batch [990/1299] D_loss: -0.0421, G_loss: -0.2302\n",
      "  Batch [1000/1299] D_loss: -0.0199, G_loss: -0.2122\n",
      "  Batch [1010/1299] D_loss: -1.1954, G_loss: 4.0726\n",
      "  Batch [1020/1299] D_loss: -0.0395, G_loss: -0.0623\n",
      "  Batch [1030/1299] D_loss: 0.0186, G_loss: -0.1593\n",
      "  Batch [1040/1299] D_loss: -0.0478, G_loss: -0.1693\n",
      "  Batch [1050/1299] D_loss: -0.1321, G_loss: -0.2628\n",
      "  Batch [1060/1299] D_loss: -0.1099, G_loss: -0.3646\n",
      "  Batch [1070/1299] D_loss: -0.0001, G_loss: -0.3722\n",
      "  Batch [1080/1299] D_loss: -0.0454, G_loss: -0.3665\n",
      "  Batch [1090/1299] D_loss: -0.0295, G_loss: -0.2647\n",
      "  Batch [1100/1299] D_loss: -0.0176, G_loss: -0.1623\n",
      "  Batch [1110/1299] D_loss: -0.7410, G_loss: 1.6708\n",
      "  Batch [1120/1299] D_loss: -0.0839, G_loss: 0.0286\n",
      "  Batch [1130/1299] D_loss: -0.0577, G_loss: -0.0849\n",
      "  Batch [1140/1299] D_loss: -0.1964, G_loss: -0.2229\n",
      "  Batch [1150/1299] D_loss: -0.3222, G_loss: -0.4400\n",
      "  Batch [1160/1299] D_loss: -0.0969, G_loss: -0.5135\n",
      "  Batch [1170/1299] D_loss: -0.1475, G_loss: -0.6968\n",
      "  Batch [1180/1299] D_loss: -0.2245, G_loss: -0.8260\n",
      "  Batch [1190/1299] D_loss: -0.1883, G_loss: -0.6256\n",
      "  Batch [1200/1299] D_loss: -0.1137, G_loss: -0.8395\n",
      "  Batch [1210/1299] D_loss: -0.0817, G_loss: -0.3444\n",
      "  Batch [1220/1299] D_loss: -1.4120, G_loss: 4.6266\n",
      "  Batch [1230/1299] D_loss: -0.4914, G_loss: 1.3981\n",
      "  Batch [1240/1299] D_loss: -0.0412, G_loss: -0.1317\n",
      "  Batch [1250/1299] D_loss: -0.1111, G_loss: -0.2674\n",
      "  Batch [1260/1299] D_loss: -0.2197, G_loss: -0.4742\n",
      "  Batch [1270/1299] D_loss: -0.0778, G_loss: -0.5849\n",
      "  Batch [1280/1299] D_loss: -0.0569, G_loss: -0.5902\n",
      "  Batch [1290/1299] D_loss: 0.0325, G_loss: -0.5228\n",
      "\n",
      "Epoch 22 Summary:\n",
      "  Average D_loss: -0.0887\n",
      "  Average G_loss: 0.0616\n",
      "\n",
      "Epoch [23/100]\n",
      "  Batch [0/1299] D_loss: 0.0426, G_loss: -0.3552\n",
      "  Batch [10/1299] D_loss: -0.0024, G_loss: -0.0588\n",
      "  Batch [20/1299] D_loss: -1.0110, G_loss: 2.0509\n",
      "  Batch [30/1299] D_loss: -0.3469, G_loss: 0.8548\n",
      "  Batch [40/1299] D_loss: -0.1554, G_loss: 0.2887\n",
      "  Batch [50/1299] D_loss: -0.0492, G_loss: -0.2178\n",
      "  Batch [60/1299] D_loss: -0.2757, G_loss: -0.4079\n",
      "  Batch [70/1299] D_loss: -0.2488, G_loss: -0.6523\n",
      "  Batch [80/1299] D_loss: -0.2088, G_loss: -0.9628\n",
      "  Batch [90/1299] D_loss: -0.1284, G_loss: -0.8711\n",
      "  Batch [100/1299] D_loss: 0.0811, G_loss: -0.4010\n",
      "  Batch [110/1299] D_loss: 0.0890, G_loss: -0.3971\n",
      "  Batch [120/1299] D_loss: -0.0440, G_loss: -0.1961\n",
      "  Batch [130/1299] D_loss: -1.8562, G_loss: 5.4582\n",
      "  Batch [140/1299] D_loss: -0.0605, G_loss: -0.1215\n",
      "  Batch [150/1299] D_loss: -0.0857, G_loss: -0.1942\n",
      "  Batch [160/1299] D_loss: -0.1267, G_loss: -0.2662\n",
      "  Batch [170/1299] D_loss: -0.0412, G_loss: -0.3414\n",
      "  Batch [180/1299] D_loss: -0.1265, G_loss: -0.4252\n",
      "  Batch [190/1299] D_loss: -0.0402, G_loss: -0.3566\n",
      "  Batch [200/1299] D_loss: -0.0823, G_loss: -0.4504\n",
      "  Batch [210/1299] D_loss: -0.0047, G_loss: -0.1351\n",
      "  Batch [220/1299] D_loss: -4.1283, G_loss: 8.8480\n",
      "  Batch [230/1299] D_loss: -0.1996, G_loss: 0.5764\n",
      "  Batch [240/1299] D_loss: -0.0374, G_loss: -0.1554\n",
      "  Batch [250/1299] D_loss: -0.1270, G_loss: -0.3287\n",
      "  Batch [260/1299] D_loss: -0.1124, G_loss: -0.5775\n",
      "  Batch [270/1299] D_loss: -0.0927, G_loss: -0.6696\n",
      "  Batch [280/1299] D_loss: -0.0691, G_loss: -0.5512\n",
      "  Batch [290/1299] D_loss: -0.0856, G_loss: -0.3857\n",
      "  Batch [300/1299] D_loss: -0.6622, G_loss: 3.5528\n",
      "  Batch [310/1299] D_loss: 0.0038, G_loss: -0.1057\n",
      "  Batch [320/1299] D_loss: -0.0316, G_loss: -0.1517\n",
      "  Batch [330/1299] D_loss: -0.0820, G_loss: -0.2613\n",
      "  Batch [340/1299] D_loss: 0.0929, G_loss: -0.2722\n",
      "  Batch [350/1299] D_loss: -0.0544, G_loss: -0.4320\n",
      "  Batch [360/1299] D_loss: -0.1342, G_loss: -0.3892\n",
      "  Batch [370/1299] D_loss: -0.0199, G_loss: -0.1991\n",
      "  Batch [380/1299] D_loss: -0.0064, G_loss: -0.1706\n",
      "  Batch [390/1299] D_loss: -0.0085, G_loss: -0.0425\n",
      "  Batch [400/1299] D_loss: -0.0097, G_loss: -0.0518\n",
      "  Batch [410/1299] D_loss: -0.0071, G_loss: -0.0683\n",
      "  Batch [420/1299] D_loss: -0.0439, G_loss: -0.0959\n",
      "  Batch [430/1299] D_loss: -0.0905, G_loss: -0.1705\n",
      "  Batch [440/1299] D_loss: -0.0366, G_loss: -0.2918\n",
      "  Batch [450/1299] D_loss: -0.1953, G_loss: -0.3813\n",
      "  Batch [460/1299] D_loss: -0.1389, G_loss: -0.3782\n",
      "  Batch [470/1299] D_loss: -0.0897, G_loss: -0.5073\n",
      "  Batch [480/1299] D_loss: 0.0179, G_loss: -0.1343\n",
      "  Batch [490/1299] D_loss: -0.4606, G_loss: 2.7279\n",
      "  Batch [500/1299] D_loss: -0.2756, G_loss: 2.1934\n",
      "  Batch [510/1299] D_loss: -0.0118, G_loss: -0.0993\n",
      "  Batch [520/1299] D_loss: -0.1092, G_loss: -0.1801\n",
      "  Batch [530/1299] D_loss: -0.1260, G_loss: -0.2834\n",
      "  Batch [540/1299] D_loss: -0.2181, G_loss: -0.4411\n",
      "  Batch [550/1299] D_loss: -0.1057, G_loss: -0.6742\n",
      "  Batch [560/1299] D_loss: -0.1968, G_loss: -0.8661\n",
      "  Batch [570/1299] D_loss: 0.0144, G_loss: -0.4996\n",
      "  Batch [580/1299] D_loss: -0.0100, G_loss: -0.1571\n",
      "  Batch [590/1299] D_loss: 0.0005, G_loss: -0.0650\n",
      "  Batch [600/1299] D_loss: -0.4512, G_loss: 0.3829\n",
      "  Batch [610/1299] D_loss: -0.0552, G_loss: -0.1226\n",
      "  Batch [620/1299] D_loss: -0.1727, G_loss: -0.2953\n",
      "  Batch [630/1299] D_loss: -0.1049, G_loss: -0.7657\n",
      "  Batch [640/1299] D_loss: -0.0348, G_loss: -0.6148\n",
      "  Batch [650/1299] D_loss: -0.0346, G_loss: -0.3949\n",
      "  Batch [660/1299] D_loss: -0.1003, G_loss: -0.6453\n",
      "  Batch [670/1299] D_loss: 0.0401, G_loss: -0.3078\n",
      "  Batch [680/1299] D_loss: -0.0501, G_loss: -0.2376\n",
      "  Batch [690/1299] D_loss: -1.9493, G_loss: 6.0980\n",
      "  Batch [700/1299] D_loss: -0.1028, G_loss: 0.5925\n",
      "  Batch [710/1299] D_loss: -0.0581, G_loss: -0.1457\n",
      "  Batch [720/1299] D_loss: -0.0689, G_loss: -0.3475\n",
      "  Batch [730/1299] D_loss: -0.1664, G_loss: -0.4073\n",
      "  Batch [740/1299] D_loss: -0.1197, G_loss: -0.5281\n",
      "  Batch [750/1299] D_loss: -0.2301, G_loss: -0.5490\n",
      "  Batch [760/1299] D_loss: -0.0324, G_loss: -0.3994\n",
      "  Batch [770/1299] D_loss: 0.0370, G_loss: -0.1613\n",
      "  Batch [780/1299] D_loss: 0.0097, G_loss: -0.1755\n",
      "  Batch [790/1299] D_loss: -0.3815, G_loss: 1.9111\n",
      "  Batch [800/1299] D_loss: -0.0474, G_loss: -0.1248\n",
      "  Batch [810/1299] D_loss: -0.0651, G_loss: -0.2564\n",
      "  Batch [820/1299] D_loss: -0.1649, G_loss: -0.3497\n",
      "  Batch [830/1299] D_loss: -0.0511, G_loss: -0.4766\n",
      "  Batch [840/1299] D_loss: 0.0255, G_loss: -0.3196\n",
      "  Batch [850/1299] D_loss: 0.0388, G_loss: -0.3693\n",
      "  Batch [860/1299] D_loss: 0.0174, G_loss: -0.1797\n",
      "  Batch [870/1299] D_loss: -0.0959, G_loss: 0.2063\n",
      "  Batch [880/1299] D_loss: -0.0046, G_loss: -0.0867\n",
      "  Batch [890/1299] D_loss: -0.0244, G_loss: -0.1717\n",
      "  Batch [900/1299] D_loss: 0.0017, G_loss: -0.1396\n",
      "  Batch [910/1299] D_loss: -1.1830, G_loss: 3.7164\n",
      "  Batch [920/1299] D_loss: -0.5564, G_loss: 1.3128\n",
      "  Batch [930/1299] D_loss: -0.2746, G_loss: 0.5840\n",
      "  Batch [940/1299] D_loss: -0.0897, G_loss: -0.2699\n",
      "  Batch [950/1299] D_loss: -0.1754, G_loss: -0.4011\n",
      "  Batch [960/1299] D_loss: -0.1883, G_loss: -0.6756\n",
      "  Batch [970/1299] D_loss: -0.0692, G_loss: -0.5545\n",
      "  Batch [980/1299] D_loss: -0.0985, G_loss: -0.4461\n",
      "  Batch [990/1299] D_loss: -0.0852, G_loss: -0.3505\n",
      "  Batch [1000/1299] D_loss: -1.1121, G_loss: 1.7167\n",
      "  Batch [1010/1299] D_loss: -0.0477, G_loss: -0.0655\n",
      "  Batch [1020/1299] D_loss: -0.0818, G_loss: -0.1837\n",
      "  Batch [1030/1299] D_loss: -0.0811, G_loss: -0.3408\n",
      "  Batch [1040/1299] D_loss: -0.0615, G_loss: -0.4560\n",
      "  Batch [1050/1299] D_loss: -0.1746, G_loss: -0.5948\n",
      "  Batch [1060/1299] D_loss: 0.2134, G_loss: -0.3393\n",
      "  Batch [1070/1299] D_loss: -0.0644, G_loss: -0.3001\n",
      "  Batch [1080/1299] D_loss: -3.0024, G_loss: 9.1735\n",
      "  Batch [1090/1299] D_loss: 0.0021, G_loss: -0.0543\n",
      "  Batch [1100/1299] D_loss: -0.0605, G_loss: -0.1642\n",
      "  Batch [1110/1299] D_loss: -0.0545, G_loss: -0.3110\n",
      "  Batch [1120/1299] D_loss: -0.1181, G_loss: -0.5315\n",
      "  Batch [1130/1299] D_loss: 0.0359, G_loss: -0.5512\n",
      "  Batch [1140/1299] D_loss: -0.1412, G_loss: -0.4778\n",
      "  Batch [1150/1299] D_loss: -0.2548, G_loss: 0.8052\n",
      "  Batch [1160/1299] D_loss: -0.5192, G_loss: 1.7139\n",
      "  Batch [1170/1299] D_loss: -0.0561, G_loss: -0.1678\n",
      "  Batch [1180/1299] D_loss: -0.0833, G_loss: -0.3042\n",
      "  Batch [1190/1299] D_loss: -0.1806, G_loss: -0.4981\n",
      "  Batch [1200/1299] D_loss: -0.0975, G_loss: -0.5447\n",
      "  Batch [1210/1299] D_loss: -0.1987, G_loss: -0.5854\n",
      "  Batch [1220/1299] D_loss: -0.2518, G_loss: 1.6245\n",
      "  Batch [1230/1299] D_loss: -0.0266, G_loss: 0.0508\n",
      "  Batch [1240/1299] D_loss: -0.0130, G_loss: -0.0556\n",
      "  Batch [1250/1299] D_loss: -0.0823, G_loss: -0.1792\n",
      "  Batch [1260/1299] D_loss: -0.0375, G_loss: -0.3312\n",
      "  Batch [1270/1299] D_loss: -0.0888, G_loss: -0.4111\n",
      "  Batch [1280/1299] D_loss: -0.0582, G_loss: -0.4123\n",
      "  Batch [1290/1299] D_loss: 0.0589, G_loss: -0.3936\n",
      "\n",
      "Epoch 23 Summary:\n",
      "  Average D_loss: -0.0886\n",
      "  Average G_loss: 0.0725\n",
      "\n",
      "Epoch [24/100]\n",
      "  Batch [0/1299] D_loss: -0.0408, G_loss: -0.3068\n",
      "  Batch [10/1299] D_loss: -0.0028, G_loss: -0.1480\n",
      "  Batch [20/1299] D_loss: -0.5445, G_loss: 0.6697\n",
      "  Batch [30/1299] D_loss: -0.1727, G_loss: 0.3811\n",
      "  Batch [40/1299] D_loss: -0.2847, G_loss: 0.9980\n",
      "  Batch [50/1299] D_loss: -0.0199, G_loss: -0.1096\n",
      "  Batch [60/1299] D_loss: -0.1781, G_loss: -0.3153\n",
      "  Batch [70/1299] D_loss: -0.1472, G_loss: -0.5980\n",
      "  Batch [80/1299] D_loss: -0.2041, G_loss: -0.6449\n",
      "  Batch [90/1299] D_loss: -0.0893, G_loss: -0.7274\n",
      "  Batch [100/1299] D_loss: -0.0027, G_loss: -0.5408\n",
      "  Batch [110/1299] D_loss: -0.0143, G_loss: -0.3245\n",
      "  Batch [120/1299] D_loss: 0.0121, G_loss: -0.0951\n",
      "  Batch [130/1299] D_loss: -0.4509, G_loss: 1.4974\n",
      "  Batch [140/1299] D_loss: -0.0786, G_loss: -0.1601\n",
      "  Batch [150/1299] D_loss: -0.0513, G_loss: -0.1938\n",
      "  Batch [160/1299] D_loss: -0.0381, G_loss: -0.2535\n",
      "  Batch [170/1299] D_loss: -0.0745, G_loss: -0.3247\n",
      "  Batch [180/1299] D_loss: -0.0677, G_loss: -0.3591\n",
      "  Batch [190/1299] D_loss: 0.0555, G_loss: -0.3232\n",
      "  Batch [200/1299] D_loss: -0.0403, G_loss: -0.1539\n",
      "  Batch [210/1299] D_loss: -0.0174, G_loss: -0.0668\n",
      "  Batch [220/1299] D_loss: -0.0105, G_loss: -0.1099\n",
      "  Batch [230/1299] D_loss: -0.0339, G_loss: -0.1633\n",
      "  Batch [240/1299] D_loss: -0.0513, G_loss: -0.2302\n",
      "  Batch [250/1299] D_loss: -0.0694, G_loss: -0.1808\n",
      "  Batch [260/1299] D_loss: -0.0067, G_loss: -0.1251\n",
      "  Batch [270/1299] D_loss: 0.0098, G_loss: -0.1440\n",
      "  Batch [280/1299] D_loss: -1.8289, G_loss: 5.8927\n",
      "  Batch [290/1299] D_loss: -0.4300, G_loss: 2.2058\n",
      "  Batch [300/1299] D_loss: -0.0920, G_loss: 0.5560\n",
      "  Batch [310/1299] D_loss: -0.0288, G_loss: -0.0994\n",
      "  Batch [320/1299] D_loss: -0.0441, G_loss: -0.3694\n",
      "  Batch [330/1299] D_loss: -0.0957, G_loss: -0.6017\n",
      "  Batch [340/1299] D_loss: 0.0514, G_loss: -0.5552\n",
      "  Batch [350/1299] D_loss: 0.0734, G_loss: -0.4452\n",
      "  Batch [360/1299] D_loss: 0.0196, G_loss: -0.1475\n",
      "  Batch [370/1299] D_loss: -0.0082, G_loss: -0.0566\n",
      "  Batch [380/1299] D_loss: -0.1865, G_loss: 1.2935\n",
      "  Batch [390/1299] D_loss: -0.0407, G_loss: -0.0702\n",
      "  Batch [400/1299] D_loss: -0.0671, G_loss: -0.1601\n",
      "  Batch [410/1299] D_loss: -0.0623, G_loss: -0.3627\n",
      "  Batch [420/1299] D_loss: -0.2731, G_loss: -0.5276\n",
      "  Batch [430/1299] D_loss: -0.1236, G_loss: -0.6623\n",
      "  Batch [440/1299] D_loss: -0.2239, G_loss: -0.6571\n",
      "  Batch [450/1299] D_loss: -0.0803, G_loss: -0.3460\n",
      "  Batch [460/1299] D_loss: -0.9296, G_loss: 3.1963\n",
      "  Batch [470/1299] D_loss: -0.1103, G_loss: 0.4452\n",
      "  Batch [480/1299] D_loss: -0.0492, G_loss: -0.1502\n",
      "  Batch [490/1299] D_loss: -0.0596, G_loss: -0.2083\n",
      "  Batch [500/1299] D_loss: -0.1857, G_loss: -0.3901\n",
      "  Batch [510/1299] D_loss: -0.0003, G_loss: -0.4981\n",
      "  Batch [520/1299] D_loss: -0.1925, G_loss: -0.5132\n",
      "  Batch [530/1299] D_loss: -0.0490, G_loss: -0.4848\n",
      "  Batch [540/1299] D_loss: -0.0332, G_loss: -0.3459\n",
      "  Batch [550/1299] D_loss: 0.0351, G_loss: -0.1970\n",
      "  Batch [560/1299] D_loss: -0.0717, G_loss: 1.1647\n",
      "  Batch [570/1299] D_loss: -1.0744, G_loss: 2.5044\n",
      "  Batch [580/1299] D_loss: -0.0488, G_loss: -0.1017\n",
      "  Batch [590/1299] D_loss: -0.0666, G_loss: -0.1883\n",
      "  Batch [600/1299] D_loss: -0.0197, G_loss: -0.2876\n",
      "  Batch [610/1299] D_loss: -0.1033, G_loss: -0.4056\n",
      "  Batch [620/1299] D_loss: -0.2080, G_loss: -0.6497\n",
      "  Batch [630/1299] D_loss: -0.1369, G_loss: -0.7106\n",
      "  Batch [640/1299] D_loss: 0.0165, G_loss: -0.6309\n",
      "  Batch [650/1299] D_loss: 0.0219, G_loss: -0.3657\n",
      "  Batch [660/1299] D_loss: -0.0538, G_loss: -0.1715\n",
      "  Batch [670/1299] D_loss: -1.2682, G_loss: 3.3199\n",
      "  Batch [680/1299] D_loss: -0.0574, G_loss: 0.1830\n",
      "  Batch [690/1299] D_loss: -0.6129, G_loss: 1.7768\n",
      "  Batch [700/1299] D_loss: -0.0697, G_loss: -0.1709\n",
      "  Batch [710/1299] D_loss: -0.1834, G_loss: -0.3556\n",
      "  Batch [720/1299] D_loss: -0.1704, G_loss: -0.4346\n",
      "  Batch [730/1299] D_loss: -0.0193, G_loss: -0.5506\n",
      "  Batch [740/1299] D_loss: -0.0345, G_loss: -0.4402\n",
      "  Batch [750/1299] D_loss: 0.0085, G_loss: -0.5046\n",
      "  Batch [760/1299] D_loss: -0.0469, G_loss: -0.4457\n",
      "  Batch [770/1299] D_loss: -0.0322, G_loss: 0.5041\n",
      "  Batch [780/1299] D_loss: -0.0016, G_loss: -0.0512\n",
      "  Batch [790/1299] D_loss: -0.0067, G_loss: -0.0486\n",
      "  Batch [800/1299] D_loss: -1.6842, G_loss: 2.4277\n",
      "  Batch [810/1299] D_loss: -0.6431, G_loss: 1.0701\n",
      "  Batch [820/1299] D_loss: -0.0368, G_loss: -0.0996\n",
      "  Batch [830/1299] D_loss: -0.0792, G_loss: -0.1907\n",
      "  Batch [840/1299] D_loss: -0.1059, G_loss: -0.3351\n",
      "  Batch [850/1299] D_loss: -0.1947, G_loss: -0.6615\n",
      "  Batch [860/1299] D_loss: -0.1057, G_loss: -0.6445\n",
      "  Batch [870/1299] D_loss: -0.0755, G_loss: -0.4020\n",
      "  Batch [880/1299] D_loss: -0.0706, G_loss: -0.3455\n",
      "  Batch [890/1299] D_loss: -0.0127, G_loss: -0.1181\n",
      "  Batch [900/1299] D_loss: -0.3777, G_loss: 1.5005\n",
      "  Batch [910/1299] D_loss: -1.5500, G_loss: 2.3322\n",
      "  Batch [920/1299] D_loss: -0.0375, G_loss: -0.0834\n",
      "  Batch [930/1299] D_loss: -0.1014, G_loss: -0.2262\n",
      "  Batch [940/1299] D_loss: -0.1930, G_loss: -0.4108\n",
      "  Batch [950/1299] D_loss: -0.1069, G_loss: -0.4409\n",
      "  Batch [960/1299] D_loss: -0.2138, G_loss: -0.5810\n",
      "  Batch [970/1299] D_loss: 0.0383, G_loss: -0.4327\n",
      "  Batch [980/1299] D_loss: 0.0035, G_loss: -0.2070\n",
      "  Batch [990/1299] D_loss: -0.0186, G_loss: -0.0902\n",
      "  Batch [1000/1299] D_loss: -0.1203, G_loss: 0.4442\n",
      "  Batch [1010/1299] D_loss: 0.0047, G_loss: -0.0398\n",
      "  Batch [1020/1299] D_loss: -0.0103, G_loss: -0.0551\n",
      "  Batch [1030/1299] D_loss: 0.0051, G_loss: -0.1141\n",
      "  Batch [1040/1299] D_loss: -0.0567, G_loss: -0.1716\n",
      "  Batch [1050/1299] D_loss: -0.0323, G_loss: -0.3056\n",
      "  Batch [1060/1299] D_loss: -0.0268, G_loss: -0.2283\n",
      "  Batch [1070/1299] D_loss: -0.0304, G_loss: -0.1226\n",
      "  Batch [1080/1299] D_loss: -0.0160, G_loss: -0.0690\n",
      "  Batch [1090/1299] D_loss: -0.0205, G_loss: -0.0682\n",
      "  Batch [1100/1299] D_loss: -0.0131, G_loss: -0.0961\n",
      "  Batch [1110/1299] D_loss: -0.0380, G_loss: -0.0946\n",
      "  Batch [1120/1299] D_loss: -0.1251, G_loss: 0.6925\n",
      "  Batch [1130/1299] D_loss: -0.0617, G_loss: -0.1074\n",
      "  Batch [1140/1299] D_loss: -0.1085, G_loss: -0.3475\n",
      "  Batch [1150/1299] D_loss: -0.2317, G_loss: -0.7581\n",
      "  Batch [1160/1299] D_loss: -0.4333, G_loss: -0.6936\n",
      "  Batch [1170/1299] D_loss: -0.0225, G_loss: -0.8600\n",
      "  Batch [1180/1299] D_loss: -0.0060, G_loss: -0.5110\n",
      "  Batch [1190/1299] D_loss: 0.0790, G_loss: -0.5229\n",
      "  Batch [1200/1299] D_loss: -0.0114, G_loss: -0.0647\n",
      "  Batch [1210/1299] D_loss: -2.2743, G_loss: 6.9388\n",
      "  Batch [1220/1299] D_loss: -0.2836, G_loss: 0.4597\n",
      "  Batch [1230/1299] D_loss: -0.0359, G_loss: -0.1063\n",
      "  Batch [1240/1299] D_loss: -0.0877, G_loss: -0.2583\n",
      "  Batch [1250/1299] D_loss: -0.1224, G_loss: -0.3750\n",
      "  Batch [1260/1299] D_loss: -0.2222, G_loss: -0.6884\n",
      "  Batch [1270/1299] D_loss: -0.3026, G_loss: -0.7430\n",
      "  Batch [1280/1299] D_loss: -0.1203, G_loss: -0.7566\n",
      "  Batch [1290/1299] D_loss: 0.0559, G_loss: -0.5981\n",
      "\n",
      "Epoch 24 Summary:\n",
      "  Average D_loss: -0.0835\n",
      "  Average G_loss: 0.0701\n",
      "\n",
      "Epoch [25/100]\n",
      "  Batch [0/1299] D_loss: -0.0447, G_loss: -0.3147\n",
      "  Batch [10/1299] D_loss: 0.0114, G_loss: -0.0901\n",
      "  Batch [20/1299] D_loss: -0.2565, G_loss: 1.3537\n",
      "  Batch [30/1299] D_loss: -0.1760, G_loss: 0.7062\n",
      "  Batch [40/1299] D_loss: 0.0025, G_loss: -0.0814\n",
      "  Batch [50/1299] D_loss: -0.0296, G_loss: -0.2765\n",
      "  Batch [60/1299] D_loss: -0.2492, G_loss: -0.4528\n",
      "  Batch [70/1299] D_loss: -0.1409, G_loss: -0.7308\n",
      "  Batch [80/1299] D_loss: -0.1794, G_loss: -0.6553\n",
      "  Batch [90/1299] D_loss: -0.0307, G_loss: -0.6900\n",
      "  Batch [100/1299] D_loss: -0.1023, G_loss: -0.8127\n",
      "  Batch [110/1299] D_loss: 0.0123, G_loss: -0.3413\n",
      "  Batch [120/1299] D_loss: -0.0198, G_loss: -0.0807\n",
      "  Batch [130/1299] D_loss: -0.0104, G_loss: -0.0333\n",
      "  Batch [140/1299] D_loss: -0.0411, G_loss: -0.0806\n",
      "  Batch [150/1299] D_loss: -0.1218, G_loss: -0.1749\n",
      "  Batch [160/1299] D_loss: -0.1695, G_loss: -0.3433\n",
      "  Batch [170/1299] D_loss: -0.0840, G_loss: -0.4107\n",
      "  Batch [180/1299] D_loss: -0.0782, G_loss: -0.5185\n",
      "  Batch [190/1299] D_loss: -0.0133, G_loss: -0.3325\n",
      "  Batch [200/1299] D_loss: -0.2534, G_loss: 1.2356\n",
      "  Batch [210/1299] D_loss: -0.0015, G_loss: -0.0340\n",
      "  Batch [220/1299] D_loss: -1.2076, G_loss: 4.7162\n",
      "  Batch [230/1299] D_loss: -0.0119, G_loss: -0.0841\n",
      "  Batch [240/1299] D_loss: -0.0272, G_loss: -0.1198\n",
      "  Batch [250/1299] D_loss: -0.0541, G_loss: -0.2749\n",
      "  Batch [260/1299] D_loss: -0.0981, G_loss: -0.4432\n",
      "  Batch [270/1299] D_loss: -0.0893, G_loss: -0.2666\n",
      "  Batch [280/1299] D_loss: -0.0412, G_loss: -0.1899\n",
      "  Batch [290/1299] D_loss: -0.0317, G_loss: -0.2185\n",
      "  Batch [300/1299] D_loss: -0.6700, G_loss: 2.2333\n",
      "  Batch [310/1299] D_loss: -0.5243, G_loss: 2.4256\n",
      "  Batch [320/1299] D_loss: -0.0236, G_loss: -0.1440\n",
      "  Batch [330/1299] D_loss: -0.1292, G_loss: -0.4305\n",
      "  Batch [340/1299] D_loss: -0.0183, G_loss: -0.4790\n",
      "  Batch [350/1299] D_loss: -0.2163, G_loss: -0.7410\n",
      "  Batch [360/1299] D_loss: -0.0030, G_loss: -0.4329\n",
      "  Batch [370/1299] D_loss: 0.0039, G_loss: -0.4894\n",
      "  Batch [380/1299] D_loss: -0.0513, G_loss: -0.2816\n",
      "  Batch [390/1299] D_loss: -0.0119, G_loss: -0.0425\n",
      "  Batch [400/1299] D_loss: -0.0309, G_loss: -0.0575\n",
      "  Batch [410/1299] D_loss: -0.0063, G_loss: -0.1031\n",
      "  Batch [420/1299] D_loss: -0.0710, G_loss: -0.1585\n",
      "  Batch [430/1299] D_loss: -0.0187, G_loss: -0.1956\n",
      "  Batch [440/1299] D_loss: -0.1971, G_loss: -0.2865\n",
      "  Batch [450/1299] D_loss: -0.1120, G_loss: -0.2668\n",
      "  Batch [460/1299] D_loss: -0.0233, G_loss: -0.2887\n",
      "  Batch [470/1299] D_loss: -0.0109, G_loss: -0.0082\n",
      "  Batch [480/1299] D_loss: -0.4691, G_loss: 1.3309\n",
      "  Batch [490/1299] D_loss: -0.0314, G_loss: -0.0748\n",
      "  Batch [500/1299] D_loss: -0.2086, G_loss: -0.2475\n",
      "  Batch [510/1299] D_loss: -0.2413, G_loss: -0.4463\n",
      "  Batch [520/1299] D_loss: -0.3449, G_loss: -0.7029\n",
      "  Batch [530/1299] D_loss: -0.0246, G_loss: -0.8190\n",
      "  Batch [540/1299] D_loss: 0.2231, G_loss: -0.6109\n",
      "  Batch [550/1299] D_loss: -0.0240, G_loss: -0.2558\n",
      "  Batch [560/1299] D_loss: 0.0155, G_loss: -0.1743\n",
      "  Batch [570/1299] D_loss: -0.0053, G_loss: -0.0548\n",
      "  Batch [580/1299] D_loss: -0.0534, G_loss: 0.1937\n",
      "  Batch [590/1299] D_loss: -0.2120, G_loss: 0.7002\n",
      "  Batch [600/1299] D_loss: -0.0688, G_loss: -0.1778\n",
      "  Batch [610/1299] D_loss: -0.1373, G_loss: -0.2766\n",
      "  Batch [620/1299] D_loss: -0.1533, G_loss: -0.6193\n",
      "  Batch [630/1299] D_loss: -0.0612, G_loss: -0.5740\n",
      "  Batch [640/1299] D_loss: -0.1053, G_loss: -0.7346\n",
      "  Batch [650/1299] D_loss: -0.0022, G_loss: -0.5410\n",
      "  Batch [660/1299] D_loss: -0.0023, G_loss: -0.2066\n",
      "  Batch [670/1299] D_loss: -0.0080, G_loss: -0.2314\n",
      "  Batch [680/1299] D_loss: 0.0228, G_loss: -0.0726\n",
      "  Batch [690/1299] D_loss: -0.0193, G_loss: 0.0521\n",
      "  Batch [700/1299] D_loss: -1.0521, G_loss: 2.9800\n",
      "  Batch [710/1299] D_loss: -0.0348, G_loss: -0.1045\n",
      "  Batch [720/1299] D_loss: -0.1078, G_loss: -0.1808\n",
      "  Batch [730/1299] D_loss: -0.0798, G_loss: -0.2341\n",
      "  Batch [740/1299] D_loss: -0.0757, G_loss: -0.3396\n",
      "  Batch [750/1299] D_loss: -0.1883, G_loss: -0.3753\n",
      "  Batch [760/1299] D_loss: 0.0781, G_loss: -0.3436\n",
      "  Batch [770/1299] D_loss: -0.1193, G_loss: -0.4654\n",
      "  Batch [780/1299] D_loss: -0.1971, G_loss: -0.4376\n",
      "  Batch [790/1299] D_loss: 0.0076, G_loss: -0.0744\n",
      "  Batch [800/1299] D_loss: -2.0052, G_loss: 8.8537\n",
      "  Batch [810/1299] D_loss: -0.3502, G_loss: 0.5792\n",
      "  Batch [820/1299] D_loss: -0.0531, G_loss: -0.2050\n",
      "  Batch [830/1299] D_loss: -0.1751, G_loss: -0.3484\n",
      "  Batch [840/1299] D_loss: -0.0066, G_loss: -0.3584\n",
      "  Batch [850/1299] D_loss: -0.1854, G_loss: -0.5187\n",
      "  Batch [860/1299] D_loss: 0.0300, G_loss: -0.4680\n",
      "  Batch [870/1299] D_loss: -0.0120, G_loss: -0.3436\n",
      "  Batch [880/1299] D_loss: -0.0073, G_loss: -0.0501\n",
      "  Batch [890/1299] D_loss: -0.4594, G_loss: 1.2908\n",
      "  Batch [900/1299] D_loss: 0.0169, G_loss: 0.0063\n",
      "  Batch [910/1299] D_loss: -0.1277, G_loss: -0.1639\n",
      "  Batch [920/1299] D_loss: -0.1374, G_loss: -0.4646\n",
      "  Batch [930/1299] D_loss: -0.0237, G_loss: -0.5929\n",
      "  Batch [940/1299] D_loss: -0.1325, G_loss: -0.6070\n",
      "  Batch [950/1299] D_loss: -0.0512, G_loss: -0.5965\n",
      "  Batch [960/1299] D_loss: 0.0279, G_loss: -0.1740\n",
      "  Batch [970/1299] D_loss: -0.0002, G_loss: -0.1832\n",
      "  Batch [980/1299] D_loss: -0.0647, G_loss: 0.1105\n",
      "  Batch [990/1299] D_loss: 0.0070, G_loss: -0.1127\n",
      "  Batch [1000/1299] D_loss: -0.0452, G_loss: -0.1626\n",
      "  Batch [1010/1299] D_loss: -0.1146, G_loss: -0.4477\n",
      "  Batch [1020/1299] D_loss: -0.0820, G_loss: -0.5613\n",
      "  Batch [1030/1299] D_loss: -0.0147, G_loss: -0.1465\n",
      "  Batch [1040/1299] D_loss: -0.0003, G_loss: -0.0979\n",
      "  Batch [1050/1299] D_loss: -0.0310, G_loss: -0.0927\n",
      "  Batch [1060/1299] D_loss: -0.5087, G_loss: 1.2829\n",
      "  Batch [1070/1299] D_loss: -0.0926, G_loss: -0.1371\n",
      "  Batch [1080/1299] D_loss: -0.0705, G_loss: -0.4851\n",
      "  Batch [1090/1299] D_loss: -0.2313, G_loss: -0.4106\n",
      "  Batch [1100/1299] D_loss: -0.1658, G_loss: -0.5319\n",
      "  Batch [1110/1299] D_loss: 0.0485, G_loss: -0.5011\n",
      "  Batch [1120/1299] D_loss: -0.0100, G_loss: -0.3068\n",
      "  Batch [1130/1299] D_loss: -0.0464, G_loss: -0.0779\n",
      "  Batch [1140/1299] D_loss: -0.2824, G_loss: 1.6656\n",
      "  Batch [1150/1299] D_loss: -0.0191, G_loss: -0.1074\n",
      "  Batch [1160/1299] D_loss: -0.1031, G_loss: -0.1909\n",
      "  Batch [1170/1299] D_loss: -0.0921, G_loss: -0.2587\n",
      "  Batch [1180/1299] D_loss: -0.2370, G_loss: -0.4555\n",
      "  Batch [1190/1299] D_loss: 0.1178, G_loss: -0.5197\n",
      "  Batch [1200/1299] D_loss: 0.0724, G_loss: -0.2107\n",
      "  Batch [1210/1299] D_loss: 0.0023, G_loss: -0.2608\n",
      "  Batch [1220/1299] D_loss: -0.0760, G_loss: -0.2626\n",
      "  Batch [1230/1299] D_loss: -0.0322, G_loss: -0.0821\n",
      "  Batch [1240/1299] D_loss: -0.6218, G_loss: 1.7829\n",
      "  Batch [1250/1299] D_loss: -0.0609, G_loss: -0.1487\n",
      "  Batch [1260/1299] D_loss: -0.0858, G_loss: -0.3060\n",
      "  Batch [1270/1299] D_loss: -0.0683, G_loss: -0.3603\n",
      "  Batch [1280/1299] D_loss: -0.0961, G_loss: -0.3368\n",
      "  Batch [1290/1299] D_loss: -0.0396, G_loss: -0.4488\n",
      "\n",
      "Epoch 25 Summary:\n",
      "  Average D_loss: -0.0715\n",
      "  Average G_loss: 0.0700\n",
      "\n",
      "Epoch [26/100]\n",
      "  Batch [0/1299] D_loss: 0.0351, G_loss: -0.3168\n",
      "  Batch [10/1299] D_loss: 0.0012, G_loss: -0.0974\n",
      "  Batch [20/1299] D_loss: -1.5927, G_loss: 4.6487\n",
      "  Batch [30/1299] D_loss: -0.3254, G_loss: 1.5674\n",
      "  Batch [40/1299] D_loss: -0.2952, G_loss: 0.1011\n",
      "  Batch [50/1299] D_loss: -0.0888, G_loss: -0.1987\n",
      "  Batch [60/1299] D_loss: -0.1640, G_loss: -0.4193\n",
      "  Batch [70/1299] D_loss: -0.0625, G_loss: -0.6731\n",
      "  Batch [80/1299] D_loss: -0.1122, G_loss: -1.0681\n",
      "  Batch [90/1299] D_loss: 0.1238, G_loss: -0.3439\n",
      "  Batch [100/1299] D_loss: 0.0069, G_loss: -0.2904\n",
      "  Batch [110/1299] D_loss: -0.0203, G_loss: -0.1311\n",
      "  Batch [120/1299] D_loss: -0.6523, G_loss: 4.5589\n",
      "  Batch [130/1299] D_loss: -0.7180, G_loss: 1.4308\n",
      "  Batch [140/1299] D_loss: 0.0185, G_loss: -0.1866\n",
      "  Batch [150/1299] D_loss: -0.1014, G_loss: -0.1798\n",
      "  Batch [160/1299] D_loss: 0.0296, G_loss: -0.3866\n",
      "  Batch [170/1299] D_loss: -0.0935, G_loss: -0.4580\n",
      "  Batch [180/1299] D_loss: -0.0597, G_loss: -0.2918\n",
      "  Batch [190/1299] D_loss: 0.0013, G_loss: -0.1995\n",
      "  Batch [200/1299] D_loss: -0.3211, G_loss: 0.8433\n",
      "  Batch [210/1299] D_loss: -0.0132, G_loss: -0.0991\n",
      "  Batch [220/1299] D_loss: -0.0460, G_loss: -0.2336\n",
      "  Batch [230/1299] D_loss: -0.0070, G_loss: -0.2538\n",
      "  Batch [240/1299] D_loss: -0.1884, G_loss: -0.4511\n",
      "  Batch [250/1299] D_loss: -0.0727, G_loss: -0.3830\n",
      "  Batch [260/1299] D_loss: -0.0582, G_loss: -0.2459\n",
      "  Batch [270/1299] D_loss: -1.0755, G_loss: 5.5769\n",
      "  Batch [280/1299] D_loss: -0.0822, G_loss: 0.2026\n",
      "  Batch [290/1299] D_loss: -0.0828, G_loss: 0.1421\n",
      "  Batch [300/1299] D_loss: -0.1463, G_loss: 0.0629\n",
      "  Batch [310/1299] D_loss: -0.0129, G_loss: -0.1292\n",
      "  Batch [320/1299] D_loss: -0.1214, G_loss: -0.4397\n",
      "  Batch [330/1299] D_loss: -0.1080, G_loss: -0.4777\n",
      "  Batch [340/1299] D_loss: -0.1444, G_loss: -0.4272\n",
      "  Batch [350/1299] D_loss: -0.0650, G_loss: -0.3504\n",
      "  Batch [360/1299] D_loss: -0.3137, G_loss: 0.7667\n",
      "  Batch [370/1299] D_loss: -0.0027, G_loss: -0.1297\n",
      "  Batch [380/1299] D_loss: -0.0332, G_loss: -0.2173\n",
      "  Batch [390/1299] D_loss: -0.0382, G_loss: -0.1890\n",
      "  Batch [400/1299] D_loss: -0.0205, G_loss: -0.2865\n",
      "  Batch [410/1299] D_loss: -0.0436, G_loss: -0.2467\n",
      "  Batch [420/1299] D_loss: -0.0288, G_loss: -0.1390\n",
      "  Batch [430/1299] D_loss: -0.7393, G_loss: 3.2447\n",
      "  Batch [440/1299] D_loss: -0.0200, G_loss: -0.0639\n",
      "  Batch [450/1299] D_loss: -0.1424, G_loss: -0.3428\n",
      "  Batch [460/1299] D_loss: 0.0013, G_loss: -0.2588\n",
      "  Batch [470/1299] D_loss: -0.1119, G_loss: -0.3887\n",
      "  Batch [480/1299] D_loss: -0.0625, G_loss: -0.4325\n",
      "  Batch [490/1299] D_loss: 0.0703, G_loss: -0.2332\n",
      "  Batch [500/1299] D_loss: -1.8246, G_loss: 3.5464\n",
      "  Batch [510/1299] D_loss: -0.0584, G_loss: -0.0181\n",
      "  Batch [520/1299] D_loss: -0.1063, G_loss: 0.0764\n",
      "  Batch [530/1299] D_loss: -0.0564, G_loss: -0.1252\n",
      "  Batch [540/1299] D_loss: -0.0352, G_loss: -0.3391\n",
      "  Batch [550/1299] D_loss: -0.1407, G_loss: -0.5526\n",
      "  Batch [560/1299] D_loss: -0.0626, G_loss: -0.2712\n",
      "  Batch [570/1299] D_loss: -0.0592, G_loss: -0.6345\n",
      "  Batch [580/1299] D_loss: 0.0369, G_loss: -0.2502\n",
      "  Batch [590/1299] D_loss: -0.2700, G_loss: 0.2950\n",
      "  Batch [600/1299] D_loss: -0.3169, G_loss: 1.1968\n",
      "  Batch [610/1299] D_loss: -0.0170, G_loss: -0.1467\n",
      "  Batch [620/1299] D_loss: -0.0193, G_loss: -0.3107\n",
      "  Batch [630/1299] D_loss: -0.1369, G_loss: -0.4278\n",
      "  Batch [640/1299] D_loss: -0.0659, G_loss: -0.5508\n",
      "  Batch [650/1299] D_loss: 0.0382, G_loss: -0.3971\n",
      "  Batch [660/1299] D_loss: 0.0845, G_loss: -0.1103\n",
      "  Batch [670/1299] D_loss: -0.6977, G_loss: 2.1560\n",
      "  Batch [680/1299] D_loss: -0.0154, G_loss: -0.1596\n",
      "  Batch [690/1299] D_loss: -0.0007, G_loss: -0.3162\n",
      "  Batch [700/1299] D_loss: -0.1824, G_loss: -0.4574\n",
      "  Batch [710/1299] D_loss: -0.0426, G_loss: -0.5296\n",
      "  Batch [720/1299] D_loss: 0.0672, G_loss: -0.6802\n",
      "  Batch [730/1299] D_loss: -0.0234, G_loss: -0.1653\n",
      "  Batch [740/1299] D_loss: -0.1291, G_loss: 0.5615\n",
      "  Batch [750/1299] D_loss: -0.0213, G_loss: -0.0793\n",
      "  Batch [760/1299] D_loss: -0.0922, G_loss: -0.1725\n",
      "  Batch [770/1299] D_loss: -0.0893, G_loss: -0.3067\n",
      "  Batch [780/1299] D_loss: -0.1050, G_loss: -0.6200\n",
      "  Batch [790/1299] D_loss: 0.0922, G_loss: -0.5460\n",
      "  Batch [800/1299] D_loss: -0.0377, G_loss: -0.2981\n",
      "  Batch [810/1299] D_loss: -0.0092, G_loss: -0.3242\n",
      "  Batch [820/1299] D_loss: -1.3721, G_loss: 4.6246\n",
      "  Batch [830/1299] D_loss: -0.1288, G_loss: 0.3254\n",
      "  Batch [840/1299] D_loss: -0.0861, G_loss: -0.1905\n",
      "  Batch [850/1299] D_loss: -0.1647, G_loss: -0.3658\n",
      "  Batch [860/1299] D_loss: -0.1831, G_loss: -0.4124\n",
      "  Batch [870/1299] D_loss: 0.0372, G_loss: -0.4115\n",
      "  Batch [880/1299] D_loss: 0.0066, G_loss: -0.4860\n",
      "  Batch [890/1299] D_loss: -0.0579, G_loss: -0.3190\n",
      "  Batch [900/1299] D_loss: -0.0743, G_loss: -0.3189\n",
      "  Batch [910/1299] D_loss: -0.0883, G_loss: -0.2683\n",
      "  Batch [920/1299] D_loss: 0.0025, G_loss: -0.0935\n",
      "  Batch [930/1299] D_loss: -0.0346, G_loss: -0.0762\n",
      "  Batch [940/1299] D_loss: -0.0331, G_loss: -0.1235\n",
      "  Batch [950/1299] D_loss: -0.0130, G_loss: -0.2216\n",
      "  Batch [960/1299] D_loss: -0.2735, G_loss: -0.3802\n",
      "  Batch [970/1299] D_loss: -0.1667, G_loss: -0.4191\n",
      "  Batch [980/1299] D_loss: -0.1095, G_loss: -0.3799\n",
      "  Batch [990/1299] D_loss: 0.0126, G_loss: -0.3526\n",
      "  Batch [1000/1299] D_loss: -0.0417, G_loss: -0.2869\n",
      "  Batch [1010/1299] D_loss: -0.0131, G_loss: -0.1963\n",
      "  Batch [1020/1299] D_loss: -0.0616, G_loss: -0.2799\n",
      "  Batch [1030/1299] D_loss: -0.0304, G_loss: -0.1987\n",
      "  Batch [1040/1299] D_loss: 0.0175, G_loss: -0.0939\n",
      "  Batch [1050/1299] D_loss: -0.4132, G_loss: 1.3763\n",
      "  Batch [1060/1299] D_loss: -0.0248, G_loss: -0.0701\n",
      "  Batch [1070/1299] D_loss: -0.0301, G_loss: -0.1583\n",
      "  Batch [1080/1299] D_loss: -0.0195, G_loss: -0.2283\n",
      "  Batch [1090/1299] D_loss: -0.1172, G_loss: -0.2999\n",
      "  Batch [1100/1299] D_loss: 0.0834, G_loss: -0.2877\n",
      "  Batch [1110/1299] D_loss: 0.1943, G_loss: -0.3819\n",
      "  Batch [1120/1299] D_loss: -0.0362, G_loss: -0.1873\n",
      "  Batch [1130/1299] D_loss: 0.0486, G_loss: -0.1879\n",
      "  Batch [1140/1299] D_loss: -0.0349, G_loss: 3.7552\n",
      "  Batch [1150/1299] D_loss: -0.0433, G_loss: -0.1007\n",
      "  Batch [1160/1299] D_loss: -0.1291, G_loss: -0.2499\n",
      "  Batch [1170/1299] D_loss: -0.1364, G_loss: -0.3173\n",
      "  Batch [1180/1299] D_loss: -0.1315, G_loss: -0.3168\n",
      "  Batch [1190/1299] D_loss: -0.0463, G_loss: -0.2250\n",
      "  Batch [1200/1299] D_loss: -0.2650, G_loss: 0.8013\n",
      "  Batch [1210/1299] D_loss: -0.0518, G_loss: -0.1138\n",
      "  Batch [1220/1299] D_loss: -0.0617, G_loss: -0.2926\n",
      "  Batch [1230/1299] D_loss: -0.0447, G_loss: -0.3050\n",
      "  Batch [1240/1299] D_loss: 0.0213, G_loss: -0.3836\n",
      "  Batch [1250/1299] D_loss: -0.0651, G_loss: -0.2618\n",
      "  Batch [1260/1299] D_loss: -0.0187, G_loss: -0.2528\n",
      "  Batch [1270/1299] D_loss: 0.0132, G_loss: -0.2334\n",
      "  Batch [1280/1299] D_loss: -0.0540, G_loss: 0.0449\n",
      "  Batch [1290/1299] D_loss: -0.0002, G_loss: 0.4558\n",
      "\n",
      "Epoch 26 Summary:\n",
      "  Average D_loss: -0.0616\n",
      "  Average G_loss: 0.0734\n",
      "\n",
      "Epoch [27/100]\n",
      "  Batch [0/1299] D_loss: -0.0223, G_loss: -0.0806\n",
      "  Batch [10/1299] D_loss: -0.0838, G_loss: -0.1581\n",
      "  Batch [20/1299] D_loss: -0.1057, G_loss: -0.2906\n",
      "  Batch [30/1299] D_loss: -0.0579, G_loss: -0.3482\n",
      "  Batch [40/1299] D_loss: -0.0400, G_loss: -0.2969\n",
      "  Batch [50/1299] D_loss: -0.2257, G_loss: -0.3679\n",
      "  Batch [60/1299] D_loss: -0.0575, G_loss: -0.2334\n",
      "  Batch [70/1299] D_loss: -0.2229, G_loss: 1.0186\n",
      "  Batch [80/1299] D_loss: 0.0203, G_loss: -0.0566\n",
      "  Batch [90/1299] D_loss: -0.0090, G_loss: -0.1550\n",
      "  Batch [100/1299] D_loss: -0.0478, G_loss: -0.2199\n",
      "  Batch [110/1299] D_loss: -0.0588, G_loss: -0.4566\n",
      "  Batch [120/1299] D_loss: -0.0702, G_loss: -0.4228\n",
      "  Batch [130/1299] D_loss: -0.0268, G_loss: -0.2214\n",
      "  Batch [140/1299] D_loss: -0.5541, G_loss: 1.9498\n",
      "  Batch [150/1299] D_loss: -0.1604, G_loss: 0.2878\n",
      "  Batch [160/1299] D_loss: -0.0479, G_loss: -0.0250\n",
      "  Batch [170/1299] D_loss: -0.0855, G_loss: -0.1897\n",
      "  Batch [180/1299] D_loss: -0.1686, G_loss: -0.4845\n",
      "  Batch [190/1299] D_loss: -0.1099, G_loss: -0.7376\n",
      "  Batch [200/1299] D_loss: -0.1968, G_loss: -0.6483\n",
      "  Batch [210/1299] D_loss: -0.1494, G_loss: -0.6058\n",
      "  Batch [220/1299] D_loss: -0.0029, G_loss: -0.1963\n",
      "  Batch [230/1299] D_loss: -0.0180, G_loss: -0.1145\n",
      "  Batch [240/1299] D_loss: -2.8011, G_loss: 8.0388\n",
      "  Batch [250/1299] D_loss: -0.0151, G_loss: -0.0911\n",
      "  Batch [260/1299] D_loss: -0.0315, G_loss: -0.1400\n",
      "  Batch [270/1299] D_loss: -0.0451, G_loss: -0.2794\n",
      "  Batch [280/1299] D_loss: -0.0569, G_loss: -0.3369\n",
      "  Batch [290/1299] D_loss: -0.0575, G_loss: -0.4062\n",
      "  Batch [300/1299] D_loss: -0.0294, G_loss: -0.4047\n",
      "  Batch [310/1299] D_loss: 0.0918, G_loss: -0.2886\n",
      "  Batch [320/1299] D_loss: 0.0293, G_loss: -0.0940\n",
      "  Batch [330/1299] D_loss: -0.1050, G_loss: 0.7449\n",
      "  Batch [340/1299] D_loss: -0.0460, G_loss: -0.0792\n",
      "  Batch [350/1299] D_loss: -0.0862, G_loss: -0.2602\n",
      "  Batch [360/1299] D_loss: -0.2032, G_loss: -0.4247\n",
      "  Batch [370/1299] D_loss: -0.1949, G_loss: -0.3960\n",
      "  Batch [380/1299] D_loss: 0.0108, G_loss: -0.5266\n",
      "  Batch [390/1299] D_loss: -0.0948, G_loss: -0.3879\n",
      "  Batch [400/1299] D_loss: -0.0120, G_loss: -0.1892\n",
      "  Batch [410/1299] D_loss: -2.5663, G_loss: 6.0591\n",
      "  Batch [420/1299] D_loss: 0.0025, G_loss: -0.0451\n",
      "  Batch [430/1299] D_loss: 0.0081, G_loss: -0.0521\n",
      "  Batch [440/1299] D_loss: -0.0205, G_loss: -0.0701\n",
      "  Batch [450/1299] D_loss: -0.0342, G_loss: -0.1841\n",
      "  Batch [460/1299] D_loss: -0.0385, G_loss: -0.1765\n",
      "  Batch [470/1299] D_loss: -0.4529, G_loss: 1.4850\n",
      "  Batch [480/1299] D_loss: -0.0072, G_loss: -0.0763\n",
      "  Batch [490/1299] D_loss: -0.1066, G_loss: -0.2758\n",
      "  Batch [500/1299] D_loss: -0.1712, G_loss: -0.3655\n",
      "  Batch [510/1299] D_loss: -0.1408, G_loss: -0.5338\n",
      "  Batch [520/1299] D_loss: -0.1473, G_loss: -0.4934\n",
      "  Batch [530/1299] D_loss: -0.0786, G_loss: -0.4260\n",
      "  Batch [540/1299] D_loss: -0.0360, G_loss: -0.1598\n",
      "  Batch [550/1299] D_loss: -0.1036, G_loss: 0.2436\n",
      "  Batch [560/1299] D_loss: -0.4243, G_loss: 0.4175\n",
      "  Batch [570/1299] D_loss: -0.0353, G_loss: -0.2137\n",
      "  Batch [580/1299] D_loss: -0.1185, G_loss: -0.3110\n",
      "  Batch [590/1299] D_loss: -0.0808, G_loss: -0.5519\n",
      "  Batch [600/1299] D_loss: -0.1077, G_loss: -0.4234\n",
      "  Batch [610/1299] D_loss: -0.0396, G_loss: -0.3624\n",
      "  Batch [620/1299] D_loss: -0.0740, G_loss: -0.2309\n",
      "  Batch [630/1299] D_loss: -1.3083, G_loss: 8.5075\n",
      "  Batch [640/1299] D_loss: -0.0194, G_loss: -0.0675\n",
      "  Batch [650/1299] D_loss: -0.0114, G_loss: -0.0963\n",
      "  Batch [660/1299] D_loss: -0.0336, G_loss: -0.1300\n",
      "  Batch [670/1299] D_loss: -0.0867, G_loss: -0.3062\n",
      "  Batch [680/1299] D_loss: 0.0715, G_loss: -0.3509\n",
      "  Batch [690/1299] D_loss: -0.0771, G_loss: -0.4540\n",
      "  Batch [700/1299] D_loss: 0.0113, G_loss: -0.2719\n",
      "  Batch [710/1299] D_loss: -0.0739, G_loss: -0.2558\n",
      "  Batch [720/1299] D_loss: -0.1227, G_loss: -0.3306\n",
      "  Batch [730/1299] D_loss: -0.0302, G_loss: -0.2112\n",
      "  Batch [740/1299] D_loss: -1.1612, G_loss: 4.8934\n",
      "  Batch [750/1299] D_loss: -0.4140, G_loss: 1.0895\n",
      "  Batch [760/1299] D_loss: -0.0491, G_loss: -0.1213\n",
      "  Batch [770/1299] D_loss: -0.0608, G_loss: -0.2187\n",
      "  Batch [780/1299] D_loss: -0.1208, G_loss: -0.4013\n",
      "  Batch [790/1299] D_loss: -0.0569, G_loss: -0.4952\n",
      "  Batch [800/1299] D_loss: -0.3308, G_loss: -0.8134\n",
      "  Batch [810/1299] D_loss: -0.1707, G_loss: -0.6718\n",
      "  Batch [820/1299] D_loss: -0.0062, G_loss: -0.4365\n",
      "  Batch [830/1299] D_loss: -0.0117, G_loss: -0.1248\n",
      "  Batch [840/1299] D_loss: -0.0003, G_loss: -0.0249\n",
      "  Batch [850/1299] D_loss: -0.0064, G_loss: -0.0544\n",
      "  Batch [860/1299] D_loss: 0.0009, G_loss: -0.0627\n",
      "  Batch [870/1299] D_loss: -0.0219, G_loss: -0.0696\n",
      "  Batch [880/1299] D_loss: -0.1048, G_loss: -0.2356\n",
      "  Batch [890/1299] D_loss: -0.1542, G_loss: 0.2563\n",
      "  Batch [900/1299] D_loss: -0.0020, G_loss: -0.0327\n",
      "  Batch [910/1299] D_loss: -0.0634, G_loss: -0.1349\n",
      "  Batch [920/1299] D_loss: -0.1622, G_loss: -0.2858\n",
      "  Batch [930/1299] D_loss: -0.4329, G_loss: -0.6544\n",
      "  Batch [940/1299] D_loss: -0.0622, G_loss: -0.9384\n",
      "  Batch [950/1299] D_loss: -0.2319, G_loss: -0.9446\n",
      "  Batch [960/1299] D_loss: -0.1289, G_loss: -0.6370\n",
      "  Batch [970/1299] D_loss: -0.0096, G_loss: -0.2277\n",
      "  Batch [980/1299] D_loss: 0.0098, G_loss: -0.0445\n",
      "  Batch [990/1299] D_loss: -0.0854, G_loss: 1.0393\n",
      "  Batch [1000/1299] D_loss: -0.0311, G_loss: -0.1012\n",
      "  Batch [1010/1299] D_loss: -0.0659, G_loss: -0.1786\n",
      "  Batch [1020/1299] D_loss: -0.0821, G_loss: -0.4007\n",
      "  Batch [1030/1299] D_loss: -0.1257, G_loss: -0.3950\n",
      "  Batch [1040/1299] D_loss: -0.3373, G_loss: -0.5461\n",
      "  Batch [1050/1299] D_loss: -0.2232, G_loss: -0.4864\n",
      "  Batch [1060/1299] D_loss: -0.0632, G_loss: -0.4591\n",
      "  Batch [1070/1299] D_loss: -0.0284, G_loss: -0.1836\n",
      "  Batch [1080/1299] D_loss: -0.0174, G_loss: -0.0760\n",
      "  Batch [1090/1299] D_loss: -0.8695, G_loss: 2.5382\n",
      "  Batch [1100/1299] D_loss: -0.1267, G_loss: 0.4709\n",
      "  Batch [1110/1299] D_loss: -0.0123, G_loss: -0.0766\n",
      "  Batch [1120/1299] D_loss: -0.0205, G_loss: -0.1685\n",
      "  Batch [1130/1299] D_loss: -0.2377, G_loss: -0.3504\n",
      "  Batch [1140/1299] D_loss: -0.2393, G_loss: -0.6018\n",
      "  Batch [1150/1299] D_loss: -0.1735, G_loss: -0.4705\n",
      "  Batch [1160/1299] D_loss: 0.1593, G_loss: -0.6377\n",
      "  Batch [1170/1299] D_loss: -0.0377, G_loss: -0.2783\n",
      "  Batch [1180/1299] D_loss: -0.0543, G_loss: -0.2514\n",
      "  Batch [1190/1299] D_loss: -1.6415, G_loss: 5.8512\n",
      "  Batch [1200/1299] D_loss: -1.0851, G_loss: 1.2403\n",
      "  Batch [1210/1299] D_loss: -0.1065, G_loss: -0.1650\n",
      "  Batch [1220/1299] D_loss: -0.1496, G_loss: -0.3339\n",
      "  Batch [1230/1299] D_loss: -0.1284, G_loss: -0.4091\n",
      "  Batch [1240/1299] D_loss: -0.1210, G_loss: -0.5659\n",
      "  Batch [1250/1299] D_loss: 0.0645, G_loss: -0.3938\n",
      "  Batch [1260/1299] D_loss: -0.0355, G_loss: -0.4011\n",
      "  Batch [1270/1299] D_loss: -0.0166, G_loss: -0.2087\n",
      "  Batch [1280/1299] D_loss: -0.5358, G_loss: 2.1822\n",
      "  Batch [1290/1299] D_loss: -0.0602, G_loss: -0.0529\n",
      "\n",
      "Epoch 27 Summary:\n",
      "  Average D_loss: -0.0826\n",
      "  Average G_loss: 0.0756\n",
      "\n",
      "Epoch [28/100]\n",
      "  Batch [0/1299] D_loss: -0.0760, G_loss: -0.2478\n",
      "  Batch [10/1299] D_loss: -0.1952, G_loss: -0.3962\n",
      "  Batch [20/1299] D_loss: -0.1212, G_loss: -0.4427\n",
      "  Batch [30/1299] D_loss: 0.0034, G_loss: -0.3904\n",
      "  Batch [40/1299] D_loss: -0.1134, G_loss: -0.5463\n",
      "  Batch [50/1299] D_loss: -0.0107, G_loss: -0.1303\n",
      "  Batch [60/1299] D_loss: -0.0280, G_loss: 0.0349\n",
      "  Batch [70/1299] D_loss: -0.1845, G_loss: 0.3805\n",
      "  Batch [80/1299] D_loss: -0.0066, G_loss: 0.0061\n",
      "  Batch [90/1299] D_loss: -0.0219, G_loss: -0.1422\n",
      "  Batch [100/1299] D_loss: -0.1926, G_loss: -0.4584\n",
      "  Batch [110/1299] D_loss: -0.2324, G_loss: -0.7107\n",
      "  Batch [120/1299] D_loss: 0.0209, G_loss: -0.5325\n",
      "  Batch [130/1299] D_loss: -0.2123, G_loss: -0.5536\n",
      "  Batch [140/1299] D_loss: -0.0386, G_loss: -0.1955\n",
      "  Batch [150/1299] D_loss: -2.5595, G_loss: 5.3253\n",
      "  Batch [160/1299] D_loss: -0.0837, G_loss: 0.1165\n",
      "  Batch [170/1299] D_loss: -0.2129, G_loss: 0.8212\n",
      "  Batch [180/1299] D_loss: -0.0192, G_loss: -0.1083\n",
      "  Batch [190/1299] D_loss: -0.0868, G_loss: -0.3490\n",
      "  Batch [200/1299] D_loss: -0.0716, G_loss: -0.4851\n",
      "  Batch [210/1299] D_loss: -0.0706, G_loss: -0.4087\n",
      "  Batch [220/1299] D_loss: 0.0207, G_loss: -0.5421\n",
      "  Batch [230/1299] D_loss: -0.0389, G_loss: -0.1882\n",
      "  Batch [240/1299] D_loss: 0.0459, G_loss: -0.2813\n",
      "  Batch [250/1299] D_loss: -0.0095, G_loss: 1.3444\n",
      "  Batch [260/1299] D_loss: -0.0595, G_loss: 0.1282\n",
      "  Batch [270/1299] D_loss: -0.0508, G_loss: -0.0246\n",
      "  Batch [280/1299] D_loss: -0.1761, G_loss: -0.1835\n",
      "  Batch [290/1299] D_loss: -0.1209, G_loss: -0.2829\n",
      "  Batch [300/1299] D_loss: -0.0821, G_loss: -0.3990\n",
      "  Batch [310/1299] D_loss: -0.2226, G_loss: -0.7493\n",
      "  Batch [320/1299] D_loss: -0.0097, G_loss: -0.1655\n",
      "  Batch [330/1299] D_loss: -0.0635, G_loss: -0.1556\n",
      "  Batch [340/1299] D_loss: -0.0195, G_loss: 0.6808\n",
      "  Batch [350/1299] D_loss: -0.8106, G_loss: 3.4744\n",
      "  Batch [360/1299] D_loss: -0.0456, G_loss: -0.0855\n",
      "  Batch [370/1299] D_loss: -0.0829, G_loss: -0.2304\n",
      "  Batch [380/1299] D_loss: -0.1506, G_loss: -0.3880\n",
      "  Batch [390/1299] D_loss: -0.1469, G_loss: -0.4965\n",
      "  Batch [400/1299] D_loss: -0.0126, G_loss: -0.4141\n",
      "  Batch [410/1299] D_loss: -0.0223, G_loss: -0.0827\n",
      "  Batch [420/1299] D_loss: -0.0052, G_loss: -0.0552\n",
      "  Batch [430/1299] D_loss: -0.0252, G_loss: -0.0869\n",
      "  Batch [440/1299] D_loss: -0.0462, G_loss: -0.1500\n",
      "  Batch [450/1299] D_loss: -0.0142, G_loss: -0.3554\n",
      "  Batch [460/1299] D_loss: -0.1122, G_loss: -0.3050\n",
      "  Batch [470/1299] D_loss: 0.0039, G_loss: -0.2339\n",
      "  Batch [480/1299] D_loss: -1.1162, G_loss: 4.8987\n",
      "  Batch [490/1299] D_loss: -0.0346, G_loss: -0.0384\n",
      "  Batch [500/1299] D_loss: -0.0801, G_loss: -0.1249\n",
      "  Batch [510/1299] D_loss: -0.0684, G_loss: -0.2500\n",
      "  Batch [520/1299] D_loss: -0.1157, G_loss: -0.4206\n",
      "  Batch [530/1299] D_loss: 0.0137, G_loss: -0.3515\n",
      "  Batch [540/1299] D_loss: -0.1106, G_loss: -0.3638\n",
      "  Batch [550/1299] D_loss: 0.0173, G_loss: -0.2690\n",
      "  Batch [560/1299] D_loss: -0.0361, G_loss: -0.1578\n",
      "  Batch [570/1299] D_loss: -0.7995, G_loss: 2.2656\n",
      "  Batch [580/1299] D_loss: -0.0621, G_loss: 0.2946\n",
      "  Batch [590/1299] D_loss: -0.0140, G_loss: -0.0927\n",
      "  Batch [600/1299] D_loss: -0.0426, G_loss: -0.3295\n",
      "  Batch [610/1299] D_loss: -0.0319, G_loss: -0.5717\n",
      "  Batch [620/1299] D_loss: 0.0028, G_loss: -0.5684\n",
      "  Batch [630/1299] D_loss: -0.0483, G_loss: -0.2306\n",
      "  Batch [640/1299] D_loss: 0.0871, G_loss: -0.1774\n",
      "  Batch [650/1299] D_loss: -0.0252, G_loss: -0.0859\n",
      "  Batch [660/1299] D_loss: -0.4864, G_loss: 1.1364\n",
      "  Batch [670/1299] D_loss: -0.0906, G_loss: -0.1575\n",
      "  Batch [680/1299] D_loss: -0.0700, G_loss: -0.3271\n",
      "  Batch [690/1299] D_loss: -0.3118, G_loss: -0.4865\n",
      "  Batch [700/1299] D_loss: 0.0503, G_loss: -0.4562\n",
      "  Batch [710/1299] D_loss: -0.0413, G_loss: -0.2587\n",
      "  Batch [720/1299] D_loss: -0.0423, G_loss: -0.1275\n",
      "  Batch [730/1299] D_loss: -0.0031, G_loss: -0.1098\n",
      "  Batch [740/1299] D_loss: -0.0514, G_loss: 0.3712\n",
      "  Batch [750/1299] D_loss: -0.0125, G_loss: -0.0612\n",
      "  Batch [760/1299] D_loss: -0.0123, G_loss: -0.1492\n",
      "  Batch [770/1299] D_loss: -0.1308, G_loss: -0.3400\n",
      "  Batch [780/1299] D_loss: -0.2502, G_loss: -0.3892\n",
      "  Batch [790/1299] D_loss: -0.0271, G_loss: -0.2675\n",
      "  Batch [800/1299] D_loss: -0.1078, G_loss: -0.2681\n",
      "  Batch [810/1299] D_loss: -0.0230, G_loss: -0.1875\n",
      "  Batch [820/1299] D_loss: 0.0189, G_loss: -0.0366\n",
      "  Batch [830/1299] D_loss: -0.0125, G_loss: -0.0759\n",
      "  Batch [840/1299] D_loss: -0.1336, G_loss: 0.5179\n",
      "  Batch [850/1299] D_loss: -0.0152, G_loss: -0.0503\n",
      "  Batch [860/1299] D_loss: -0.1468, G_loss: -0.2359\n",
      "  Batch [870/1299] D_loss: -0.2100, G_loss: -0.4393\n",
      "  Batch [880/1299] D_loss: -0.1370, G_loss: -0.6573\n",
      "  Batch [890/1299] D_loss: -0.2338, G_loss: -1.1112\n",
      "  Batch [900/1299] D_loss: 0.0854, G_loss: -0.2907\n",
      "  Batch [910/1299] D_loss: -0.0201, G_loss: -0.1295\n",
      "  Batch [920/1299] D_loss: 0.0462, G_loss: -0.0617\n",
      "  Batch [930/1299] D_loss: 0.0218, G_loss: -0.0804\n",
      "  Batch [940/1299] D_loss: 0.0151, G_loss: -0.1021\n",
      "  Batch [950/1299] D_loss: -0.0182, G_loss: -0.0854\n",
      "  Batch [960/1299] D_loss: 0.0108, G_loss: -0.2099\n",
      "  Batch [970/1299] D_loss: -0.0182, G_loss: -0.0425\n",
      "  Batch [980/1299] D_loss: -0.0359, G_loss: -0.1008\n",
      "  Batch [990/1299] D_loss: -0.1449, G_loss: -0.2492\n",
      "  Batch [1000/1299] D_loss: -0.1278, G_loss: -0.3792\n",
      "  Batch [1010/1299] D_loss: -0.0454, G_loss: -0.7753\n",
      "  Batch [1020/1299] D_loss: -0.2143, G_loss: -0.7617\n",
      "  Batch [1030/1299] D_loss: 0.0492, G_loss: -0.3051\n",
      "  Batch [1040/1299] D_loss: -0.0706, G_loss: -0.2378\n",
      "  Batch [1050/1299] D_loss: -0.0283, G_loss: 0.0084\n",
      "  Batch [1060/1299] D_loss: -0.0108, G_loss: -0.0418\n",
      "  Batch [1070/1299] D_loss: -0.0215, G_loss: -0.0959\n",
      "  Batch [1080/1299] D_loss: -0.0791, G_loss: -0.1621\n",
      "  Batch [1090/1299] D_loss: -0.1737, G_loss: -0.3971\n",
      "  Batch [1100/1299] D_loss: -0.0246, G_loss: -0.3131\n",
      "  Batch [1110/1299] D_loss: -0.0684, G_loss: -0.2600\n",
      "  Batch [1120/1299] D_loss: -0.0251, G_loss: -0.2056\n",
      "  Batch [1130/1299] D_loss: 0.0247, G_loss: -0.1199\n",
      "  Batch [1140/1299] D_loss: -0.0061, G_loss: -0.0748\n",
      "  Batch [1150/1299] D_loss: -0.1690, G_loss: 0.7253\n",
      "  Batch [1160/1299] D_loss: 0.0269, G_loss: -0.1348\n",
      "  Batch [1170/1299] D_loss: -0.1644, G_loss: -0.3295\n",
      "  Batch [1180/1299] D_loss: -0.1936, G_loss: -0.5399\n",
      "  Batch [1190/1299] D_loss: -0.1017, G_loss: -0.6712\n",
      "  Batch [1200/1299] D_loss: -0.0325, G_loss: -0.4378\n",
      "  Batch [1210/1299] D_loss: -0.0939, G_loss: -0.3387\n",
      "  Batch [1220/1299] D_loss: -0.2270, G_loss: 0.6240\n",
      "  Batch [1230/1299] D_loss: -0.0337, G_loss: -0.1497\n",
      "  Batch [1240/1299] D_loss: -0.0558, G_loss: -0.2229\n",
      "  Batch [1250/1299] D_loss: -0.1461, G_loss: -0.3834\n",
      "  Batch [1260/1299] D_loss: -0.1021, G_loss: -0.3666\n",
      "  Batch [1270/1299] D_loss: -0.0849, G_loss: -0.3975\n",
      "  Batch [1280/1299] D_loss: -0.0531, G_loss: -0.1933\n",
      "  Batch [1290/1299] D_loss: -0.0612, G_loss: -0.1218\n",
      "\n",
      "Epoch 28 Summary:\n",
      "  Average D_loss: -0.0698\n",
      "  Average G_loss: 0.0529\n",
      "\n",
      "Epoch [29/100]\n",
      "  Batch [0/1299] D_loss: -0.0107, G_loss: -0.0584\n",
      "  Batch [10/1299] D_loss: -0.0201, G_loss: -0.1021\n",
      "  Batch [20/1299] D_loss: -0.1011, G_loss: -0.1970\n",
      "  Batch [30/1299] D_loss: 0.0352, G_loss: -0.2765\n",
      "  Batch [40/1299] D_loss: -0.0541, G_loss: -0.3462\n",
      "  Batch [50/1299] D_loss: -0.0126, G_loss: -0.2853\n",
      "  Batch [60/1299] D_loss: -0.0821, G_loss: 0.0801\n",
      "  Batch [70/1299] D_loss: -0.0420, G_loss: -0.1300\n",
      "  Batch [80/1299] D_loss: -0.0421, G_loss: -0.1378\n",
      "  Batch [90/1299] D_loss: -0.1786, G_loss: 0.6094\n",
      "  Batch [100/1299] D_loss: -0.0311, G_loss: -0.0628\n",
      "  Batch [110/1299] D_loss: -0.0228, G_loss: -0.1246\n",
      "  Batch [120/1299] D_loss: -0.0140, G_loss: -0.2821\n",
      "  Batch [130/1299] D_loss: -0.1202, G_loss: -0.4873\n",
      "  Batch [140/1299] D_loss: -0.1539, G_loss: -0.3864\n",
      "  Batch [150/1299] D_loss: 0.0308, G_loss: -0.1760\n",
      "  Batch [160/1299] D_loss: -0.0326, G_loss: 0.5562\n",
      "  Batch [170/1299] D_loss: -0.3397, G_loss: 0.7090\n",
      "  Batch [180/1299] D_loss: -0.0673, G_loss: 0.0751\n",
      "  Batch [190/1299] D_loss: -0.0977, G_loss: 0.7926\n",
      "  Batch [200/1299] D_loss: -0.0654, G_loss: -0.1436\n",
      "  Batch [210/1299] D_loss: -0.1989, G_loss: -0.3850\n",
      "  Batch [220/1299] D_loss: -0.0962, G_loss: -0.7498\n",
      "  Batch [230/1299] D_loss: -0.3169, G_loss: -0.9733\n",
      "  Batch [240/1299] D_loss: -0.2185, G_loss: -0.9895\n",
      "  Batch [250/1299] D_loss: -0.0328, G_loss: -0.3801\n",
      "  Batch [260/1299] D_loss: -0.0201, G_loss: -0.1155\n",
      "  Batch [270/1299] D_loss: -0.7205, G_loss: 5.1646\n",
      "  Batch [280/1299] D_loss: -1.0190, G_loss: 1.8471\n",
      "  Batch [290/1299] D_loss: -0.0143, G_loss: -0.0745\n",
      "  Batch [300/1299] D_loss: -0.0237, G_loss: -0.1377\n",
      "  Batch [310/1299] D_loss: -0.0748, G_loss: -0.2955\n",
      "  Batch [320/1299] D_loss: -0.0857, G_loss: -0.4248\n",
      "  Batch [330/1299] D_loss: 0.1396, G_loss: -0.4401\n",
      "  Batch [340/1299] D_loss: 0.1081, G_loss: -0.4054\n",
      "  Batch [350/1299] D_loss: 0.0492, G_loss: -0.2905\n",
      "  Batch [360/1299] D_loss: -0.0118, G_loss: -0.0163\n",
      "  Batch [370/1299] D_loss: -0.8358, G_loss: 3.2529\n",
      "  Batch [380/1299] D_loss: -0.0321, G_loss: -0.0730\n",
      "  Batch [390/1299] D_loss: -0.0305, G_loss: -0.2477\n",
      "  Batch [400/1299] D_loss: 0.0210, G_loss: -0.2809\n",
      "  Batch [410/1299] D_loss: -0.1279, G_loss: -0.5925\n",
      "  Batch [420/1299] D_loss: -0.1390, G_loss: -0.6733\n",
      "  Batch [430/1299] D_loss: -0.1121, G_loss: -0.3483\n",
      "  Batch [440/1299] D_loss: -0.0110, G_loss: -0.1971\n",
      "  Batch [450/1299] D_loss: -0.1245, G_loss: 1.5068\n",
      "  Batch [460/1299] D_loss: -0.7944, G_loss: 2.1412\n",
      "  Batch [470/1299] D_loss: -0.0054, G_loss: -0.1094\n",
      "  Batch [480/1299] D_loss: -0.0561, G_loss: -0.2417\n",
      "  Batch [490/1299] D_loss: -0.0356, G_loss: -0.3536\n",
      "  Batch [500/1299] D_loss: -0.1266, G_loss: -0.4739\n",
      "  Batch [510/1299] D_loss: -0.0974, G_loss: -0.5435\n",
      "  Batch [520/1299] D_loss: -0.1837, G_loss: -0.4574\n",
      "  Batch [530/1299] D_loss: -0.0959, G_loss: -0.5415\n",
      "  Batch [540/1299] D_loss: -0.0322, G_loss: -0.1662\n",
      "  Batch [550/1299] D_loss: 0.0086, G_loss: -0.0787\n",
      "  Batch [560/1299] D_loss: 0.0019, G_loss: -0.0540\n",
      "  Batch [570/1299] D_loss: -0.0367, G_loss: -0.0898\n",
      "  Batch [580/1299] D_loss: -0.0682, G_loss: -0.2199\n",
      "  Batch [590/1299] D_loss: -0.0562, G_loss: -0.2943\n",
      "  Batch [600/1299] D_loss: -0.0814, G_loss: -0.1881\n",
      "  Batch [610/1299] D_loss: 0.0128, G_loss: -0.1928\n",
      "  Batch [620/1299] D_loss: -0.0868, G_loss: -0.1840\n",
      "  Batch [630/1299] D_loss: -0.0755, G_loss: -0.1456\n",
      "  Batch [640/1299] D_loss: 0.0022, G_loss: -0.0971\n",
      "  Batch [650/1299] D_loss: -0.3213, G_loss: 0.5685\n",
      "  Batch [660/1299] D_loss: -0.0327, G_loss: -0.0330\n",
      "  Batch [670/1299] D_loss: -0.0831, G_loss: -0.2409\n",
      "  Batch [680/1299] D_loss: -0.1314, G_loss: -0.4185\n",
      "  Batch [690/1299] D_loss: 0.1639, G_loss: -0.4704\n",
      "  Batch [700/1299] D_loss: -0.1546, G_loss: -1.0736\n",
      "  Batch [710/1299] D_loss: -0.0034, G_loss: -0.2883\n",
      "  Batch [720/1299] D_loss: -0.0737, G_loss: -0.2014\n",
      "  Batch [730/1299] D_loss: -0.2716, G_loss: 0.9589\n",
      "  Batch [740/1299] D_loss: -0.2996, G_loss: 0.3331\n",
      "  Batch [750/1299] D_loss: -0.0793, G_loss: -0.1715\n",
      "  Batch [760/1299] D_loss: -0.0718, G_loss: -0.3654\n",
      "  Batch [770/1299] D_loss: -0.1748, G_loss: -0.5219\n",
      "  Batch [780/1299] D_loss: -0.1764, G_loss: -0.5041\n",
      "  Batch [790/1299] D_loss: -0.0773, G_loss: -0.6765\n",
      "  Batch [800/1299] D_loss: -0.0401, G_loss: -0.3371\n",
      "  Batch [810/1299] D_loss: -0.0270, G_loss: -0.1579\n",
      "  Batch [820/1299] D_loss: -0.0879, G_loss: 0.2735\n",
      "  Batch [830/1299] D_loss: -0.0784, G_loss: 0.4505\n",
      "  Batch [840/1299] D_loss: -0.1085, G_loss: -0.1832\n",
      "  Batch [850/1299] D_loss: 0.1029, G_loss: -0.2390\n",
      "  Batch [860/1299] D_loss: -0.0418, G_loss: -0.4436\n",
      "  Batch [870/1299] D_loss: -0.1023, G_loss: -0.4199\n",
      "  Batch [880/1299] D_loss: -0.0430, G_loss: -0.2348\n",
      "  Batch [890/1299] D_loss: -0.0984, G_loss: -0.2369\n",
      "  Batch [900/1299] D_loss: -0.3766, G_loss: 1.0393\n",
      "  Batch [910/1299] D_loss: -0.0402, G_loss: -0.0910\n",
      "  Batch [920/1299] D_loss: -0.0899, G_loss: -0.1988\n",
      "  Batch [930/1299] D_loss: -0.1086, G_loss: -0.2872\n",
      "  Batch [940/1299] D_loss: -0.2501, G_loss: -0.4588\n",
      "  Batch [950/1299] D_loss: -0.0495, G_loss: -0.5308\n",
      "  Batch [960/1299] D_loss: -0.0995, G_loss: -0.4166\n",
      "  Batch [970/1299] D_loss: -0.0259, G_loss: -0.2681\n",
      "  Batch [980/1299] D_loss: -0.0051, G_loss: -0.1435\n",
      "  Batch [990/1299] D_loss: -0.0981, G_loss: 2.7769\n",
      "  Batch [1000/1299] D_loss: -0.2493, G_loss: 0.7933\n",
      "  Batch [1010/1299] D_loss: -0.0153, G_loss: -0.0267\n",
      "  Batch [1020/1299] D_loss: -0.0452, G_loss: -0.1176\n",
      "  Batch [1030/1299] D_loss: -0.0988, G_loss: -0.2875\n",
      "  Batch [1040/1299] D_loss: -0.0555, G_loss: -0.3594\n",
      "  Batch [1050/1299] D_loss: -0.0089, G_loss: -0.3772\n",
      "  Batch [1060/1299] D_loss: 0.0902, G_loss: -0.4063\n",
      "  Batch [1070/1299] D_loss: -0.1273, G_loss: -0.4345\n",
      "  Batch [1080/1299] D_loss: -0.0226, G_loss: -0.2458\n",
      "  Batch [1090/1299] D_loss: -0.2621, G_loss: 0.3108\n",
      "  Batch [1100/1299] D_loss: -0.1186, G_loss: 0.4281\n",
      "  Batch [1110/1299] D_loss: -0.1804, G_loss: 0.6101\n",
      "  Batch [1120/1299] D_loss: -0.0617, G_loss: -0.2049\n",
      "  Batch [1130/1299] D_loss: -0.1406, G_loss: -0.4087\n",
      "  Batch [1140/1299] D_loss: -0.3727, G_loss: -0.5891\n",
      "  Batch [1150/1299] D_loss: -0.2043, G_loss: -0.7472\n",
      "  Batch [1160/1299] D_loss: 0.2107, G_loss: -0.6820\n",
      "  Batch [1170/1299] D_loss: 0.0414, G_loss: -0.5127\n",
      "  Batch [1180/1299] D_loss: 0.0988, G_loss: -0.3378\n",
      "  Batch [1190/1299] D_loss: -0.0140, G_loss: -0.1735\n",
      "  Batch [1200/1299] D_loss: -0.3157, G_loss: 1.7917\n",
      "  Batch [1210/1299] D_loss: -0.0365, G_loss: -0.0289\n",
      "  Batch [1220/1299] D_loss: -0.0284, G_loss: 0.0173\n",
      "  Batch [1230/1299] D_loss: -0.0695, G_loss: -0.1502\n",
      "  Batch [1240/1299] D_loss: -0.1529, G_loss: -0.4147\n",
      "  Batch [1250/1299] D_loss: -0.0200, G_loss: -0.4932\n",
      "  Batch [1260/1299] D_loss: -0.1221, G_loss: -0.7039\n",
      "  Batch [1270/1299] D_loss: -0.0989, G_loss: -0.7870\n",
      "  Batch [1280/1299] D_loss: 0.0111, G_loss: -0.1824\n",
      "  Batch [1290/1299] D_loss: 0.0024, G_loss: -0.0963\n",
      "\n",
      "Epoch 29 Summary:\n",
      "  Average D_loss: -0.0695\n",
      "  Average G_loss: 0.0520\n",
      "\n",
      "Epoch [30/100]\n",
      "  Batch [0/1299] D_loss: -0.0162, G_loss: -0.0570\n",
      "  Batch [10/1299] D_loss: -0.3954, G_loss: 0.7718\n",
      "  Batch [20/1299] D_loss: -0.0241, G_loss: -0.1241\n",
      "  Batch [30/1299] D_loss: -0.0412, G_loss: -0.2273\n",
      "  Batch [40/1299] D_loss: -0.0352, G_loss: -0.3035\n",
      "  Batch [50/1299] D_loss: -0.0514, G_loss: -0.4347\n",
      "  Batch [60/1299] D_loss: -0.0670, G_loss: -0.3768\n",
      "  Batch [70/1299] D_loss: -0.0344, G_loss: -0.4507\n",
      "  Batch [80/1299] D_loss: 0.1292, G_loss: -0.2105\n",
      "  Batch [90/1299] D_loss: -0.0171, G_loss: -0.1510\n",
      "  Batch [100/1299] D_loss: -1.7417, G_loss: 5.1557\n",
      "  Batch [110/1299] D_loss: -0.0424, G_loss: -0.1122\n",
      "  Batch [120/1299] D_loss: -0.0158, G_loss: -0.2044\n",
      "  Batch [130/1299] D_loss: -0.1069, G_loss: -0.5080\n",
      "  Batch [140/1299] D_loss: -0.1213, G_loss: -0.4311\n",
      "  Batch [150/1299] D_loss: -0.0465, G_loss: -0.5485\n",
      "  Batch [160/1299] D_loss: -0.1669, G_loss: -0.6787\n",
      "  Batch [170/1299] D_loss: 0.0034, G_loss: -0.2507\n",
      "  Batch [180/1299] D_loss: -0.1245, G_loss: -0.4288\n",
      "  Batch [190/1299] D_loss: -0.2167, G_loss: 0.7746\n",
      "  Batch [200/1299] D_loss: -0.0455, G_loss: -0.1021\n",
      "  Batch [210/1299] D_loss: -0.0328, G_loss: -0.2000\n",
      "  Batch [220/1299] D_loss: -0.1282, G_loss: -0.3296\n",
      "  Batch [230/1299] D_loss: -0.1976, G_loss: -0.5070\n",
      "  Batch [240/1299] D_loss: -0.1040, G_loss: -0.4584\n",
      "  Batch [250/1299] D_loss: -0.0712, G_loss: -0.5001\n",
      "  Batch [260/1299] D_loss: 0.0133, G_loss: -0.2610\n",
      "  Batch [270/1299] D_loss: 0.0071, G_loss: -0.1164\n",
      "  Batch [280/1299] D_loss: -0.5061, G_loss: 1.0918\n",
      "  Batch [290/1299] D_loss: -0.0093, G_loss: -0.0891\n",
      "  Batch [300/1299] D_loss: -0.0392, G_loss: -0.1908\n",
      "  Batch [310/1299] D_loss: -0.0848, G_loss: -0.2829\n",
      "  Batch [320/1299] D_loss: -0.0837, G_loss: -0.3036\n",
      "  Batch [330/1299] D_loss: -0.0891, G_loss: -0.5376\n",
      "  Batch [340/1299] D_loss: -0.0455, G_loss: -0.5214\n",
      "  Batch [350/1299] D_loss: -0.0033, G_loss: -0.3012\n",
      "  Batch [360/1299] D_loss: -2.0007, G_loss: 8.1790\n",
      "  Batch [370/1299] D_loss: -0.3164, G_loss: 0.6707\n",
      "  Batch [380/1299] D_loss: -0.1274, G_loss: -0.1724\n",
      "  Batch [390/1299] D_loss: -0.0793, G_loss: -0.3452\n",
      "  Batch [400/1299] D_loss: -0.1881, G_loss: -0.4257\n",
      "  Batch [410/1299] D_loss: -0.0425, G_loss: -0.5639\n",
      "  Batch [420/1299] D_loss: -0.1116, G_loss: -0.4429\n",
      "  Batch [430/1299] D_loss: -0.0696, G_loss: -0.3308\n",
      "  Batch [440/1299] D_loss: -0.0474, G_loss: -0.2461\n",
      "  Batch [450/1299] D_loss: 0.0254, G_loss: -0.2137\n",
      "  Batch [460/1299] D_loss: -0.3483, G_loss: 0.9932\n",
      "  Batch [470/1299] D_loss: -0.3944, G_loss: 1.3539\n",
      "  Batch [480/1299] D_loss: -0.0596, G_loss: -0.1322\n",
      "  Batch [490/1299] D_loss: -0.0482, G_loss: -0.1806\n",
      "  Batch [500/1299] D_loss: 0.0171, G_loss: -0.3500\n",
      "  Batch [510/1299] D_loss: -0.0378, G_loss: -0.5471\n",
      "  Batch [520/1299] D_loss: 0.0024, G_loss: -0.4451\n",
      "  Batch [530/1299] D_loss: -0.0521, G_loss: -0.5323\n",
      "  Batch [540/1299] D_loss: 0.0000, G_loss: -0.3515\n",
      "  Batch [550/1299] D_loss: 0.0573, G_loss: -0.3298\n",
      "  Batch [560/1299] D_loss: 0.0095, G_loss: -0.0514\n",
      "  Batch [570/1299] D_loss: -0.6781, G_loss: 5.9148\n",
      "  Batch [580/1299] D_loss: -0.0295, G_loss: -0.1415\n",
      "  Batch [590/1299] D_loss: -0.0323, G_loss: -0.2566\n",
      "  Batch [600/1299] D_loss: -0.0585, G_loss: -0.2764\n",
      "  Batch [610/1299] D_loss: -0.0144, G_loss: -0.2027\n",
      "  Batch [620/1299] D_loss: -0.0860, G_loss: -0.2282\n",
      "  Batch [630/1299] D_loss: -0.4209, G_loss: 1.3518\n",
      "  Batch [640/1299] D_loss: -0.0063, G_loss: -0.1167\n",
      "  Batch [650/1299] D_loss: -0.1326, G_loss: -0.3203\n",
      "  Batch [660/1299] D_loss: -0.0763, G_loss: -0.3727\n",
      "  Batch [670/1299] D_loss: -0.2235, G_loss: -0.5276\n",
      "  Batch [680/1299] D_loss: -0.0703, G_loss: -0.5571\n",
      "  Batch [690/1299] D_loss: 0.0106, G_loss: -0.2572\n",
      "  Batch [700/1299] D_loss: -3.0418, G_loss: 7.3262\n",
      "  Batch [710/1299] D_loss: -0.6589, G_loss: 2.4216\n",
      "  Batch [720/1299] D_loss: -0.0250, G_loss: -0.0553\n",
      "  Batch [730/1299] D_loss: -0.1114, G_loss: -0.2240\n",
      "  Batch [740/1299] D_loss: -0.2049, G_loss: -0.4114\n",
      "  Batch [750/1299] D_loss: -0.1510, G_loss: -0.5706\n",
      "  Batch [760/1299] D_loss: -0.1964, G_loss: -0.8894\n",
      "  Batch [770/1299] D_loss: -0.0914, G_loss: -0.5061\n",
      "  Batch [780/1299] D_loss: -0.0459, G_loss: -0.3613\n",
      "  Batch [790/1299] D_loss: -0.0430, G_loss: -0.1666\n",
      "  Batch [800/1299] D_loss: -0.7363, G_loss: 3.5418\n",
      "  Batch [810/1299] D_loss: -1.0424, G_loss: 1.7865\n",
      "  Batch [820/1299] D_loss: -0.1679, G_loss: 0.4469\n",
      "  Batch [830/1299] D_loss: -0.0131, G_loss: -0.1064\n",
      "  Batch [840/1299] D_loss: -0.2595, G_loss: -0.2755\n",
      "  Batch [850/1299] D_loss: -0.2243, G_loss: -0.5441\n",
      "  Batch [860/1299] D_loss: -0.1358, G_loss: -0.8115\n",
      "  Batch [870/1299] D_loss: -0.0626, G_loss: -0.4422\n",
      "  Batch [880/1299] D_loss: -0.1195, G_loss: -0.7895\n",
      "  Batch [890/1299] D_loss: -0.0321, G_loss: -0.2051\n",
      "  Batch [900/1299] D_loss: -0.0189, G_loss: -0.0577\n",
      "  Batch [910/1299] D_loss: -0.2191, G_loss: 1.0597\n",
      "  Batch [920/1299] D_loss: -0.5290, G_loss: 1.5872\n",
      "  Batch [930/1299] D_loss: -0.0716, G_loss: -0.0856\n",
      "  Batch [940/1299] D_loss: -0.1480, G_loss: -0.2857\n",
      "  Batch [950/1299] D_loss: -0.2345, G_loss: -0.4481\n",
      "  Batch [960/1299] D_loss: -0.2376, G_loss: -0.6913\n",
      "  Batch [970/1299] D_loss: 0.0383, G_loss: -0.3860\n",
      "  Batch [980/1299] D_loss: -0.0325, G_loss: -0.3583\n",
      "  Batch [990/1299] D_loss: -0.0162, G_loss: -0.2314\n",
      "  Batch [1000/1299] D_loss: -0.0000, G_loss: -0.0214\n",
      "  Batch [1010/1299] D_loss: -0.9126, G_loss: 1.9181\n",
      "  Batch [1020/1299] D_loss: -0.0675, G_loss: -0.2522\n",
      "  Batch [1030/1299] D_loss: -0.0509, G_loss: -0.2463\n",
      "  Batch [1040/1299] D_loss: -0.0975, G_loss: -0.3825\n",
      "  Batch [1050/1299] D_loss: -0.0504, G_loss: -0.2522\n",
      "  Batch [1060/1299] D_loss: -0.1292, G_loss: -0.4852\n",
      "  Batch [1070/1299] D_loss: -0.3383, G_loss: 1.4019\n",
      "  Batch [1080/1299] D_loss: -0.0604, G_loss: 0.2544\n",
      "  Batch [1090/1299] D_loss: -0.1270, G_loss: -0.1425\n",
      "  Batch [1100/1299] D_loss: -0.0691, G_loss: -0.3506\n",
      "  Batch [1110/1299] D_loss: -0.2056, G_loss: -0.4954\n",
      "  Batch [1120/1299] D_loss: 0.0325, G_loss: -0.6031\n",
      "  Batch [1130/1299] D_loss: 0.0371, G_loss: -0.3674\n",
      "  Batch [1140/1299] D_loss: -0.1358, G_loss: -0.4630\n",
      "  Batch [1150/1299] D_loss: -0.0085, G_loss: -0.0861\n",
      "  Batch [1160/1299] D_loss: -0.1915, G_loss: 0.4241\n",
      "  Batch [1170/1299] D_loss: -0.2177, G_loss: 0.2239\n",
      "  Batch [1180/1299] D_loss: -0.0324, G_loss: 0.0883\n",
      "  Batch [1190/1299] D_loss: -0.0873, G_loss: 0.0724\n",
      "  Batch [1200/1299] D_loss: -0.0480, G_loss: -0.1281\n",
      "  Batch [1210/1299] D_loss: -0.1874, G_loss: -0.3654\n",
      "  Batch [1220/1299] D_loss: -0.0581, G_loss: -0.8723\n",
      "  Batch [1230/1299] D_loss: -0.2499, G_loss: -0.8497\n",
      "  Batch [1240/1299] D_loss: -0.1107, G_loss: -0.6980\n",
      "  Batch [1250/1299] D_loss: 0.0167, G_loss: -0.1118\n",
      "  Batch [1260/1299] D_loss: 0.0046, G_loss: -0.0884\n",
      "  Batch [1270/1299] D_loss: 0.0124, G_loss: -0.0717\n",
      "  Batch [1280/1299] D_loss: -1.5366, G_loss: 5.1163\n",
      "  Batch [1290/1299] D_loss: -0.0304, G_loss: -0.0854\n",
      "\n",
      "Epoch 30 Summary:\n",
      "  Average D_loss: -0.0796\n",
      "  Average G_loss: 0.0910\n",
      "\n",
      "Epoch [31/100]\n",
      "  Batch [0/1299] D_loss: -0.0050, G_loss: -0.3129\n",
      "  Batch [10/1299] D_loss: -0.0518, G_loss: -0.3191\n",
      "  Batch [20/1299] D_loss: -0.1058, G_loss: -0.3910\n",
      "  Batch [30/1299] D_loss: -0.0766, G_loss: -0.2521\n",
      "  Batch [40/1299] D_loss: -0.0236, G_loss: -0.1261\n",
      "  Batch [50/1299] D_loss: -0.7932, G_loss: 1.9500\n",
      "  Batch [60/1299] D_loss: -0.2703, G_loss: 0.6815\n",
      "  Batch [70/1299] D_loss: -0.0543, G_loss: -0.1255\n",
      "  Batch [80/1299] D_loss: -0.1107, G_loss: -0.3187\n",
      "  Batch [90/1299] D_loss: -0.2722, G_loss: -0.6889\n",
      "  Batch [100/1299] D_loss: -0.1729, G_loss: -0.6431\n",
      "  Batch [110/1299] D_loss: -0.1558, G_loss: -0.8516\n",
      "  Batch [120/1299] D_loss: -0.0366, G_loss: -0.3662\n",
      "  Batch [130/1299] D_loss: -0.0674, G_loss: -0.2034\n",
      "  Batch [140/1299] D_loss: -0.3083, G_loss: 1.1231\n",
      "  Batch [150/1299] D_loss: -0.0183, G_loss: -0.0897\n",
      "  Batch [160/1299] D_loss: -0.0835, G_loss: -0.1923\n",
      "  Batch [170/1299] D_loss: -0.2263, G_loss: -0.3778\n",
      "  Batch [180/1299] D_loss: -0.2070, G_loss: -0.4811\n",
      "  Batch [190/1299] D_loss: -0.1402, G_loss: -0.5233\n",
      "  Batch [200/1299] D_loss: 0.0732, G_loss: -0.4252\n",
      "  Batch [210/1299] D_loss: -0.0054, G_loss: -0.1595\n",
      "  Batch [220/1299] D_loss: -0.0255, G_loss: -0.1521\n",
      "  Batch [230/1299] D_loss: -0.3733, G_loss: 1.2124\n",
      "  Batch [240/1299] D_loss: -0.0084, G_loss: -0.0886\n",
      "  Batch [250/1299] D_loss: -0.0398, G_loss: -0.1641\n",
      "  Batch [260/1299] D_loss: -0.1247, G_loss: -0.2518\n",
      "  Batch [270/1299] D_loss: -0.1375, G_loss: -0.5583\n",
      "  Batch [280/1299] D_loss: -0.0524, G_loss: -0.3967\n",
      "  Batch [290/1299] D_loss: -0.0776, G_loss: -0.3543\n",
      "  Batch [300/1299] D_loss: -0.0309, G_loss: -0.2541\n",
      "  Batch [310/1299] D_loss: -0.0029, G_loss: -0.0909\n",
      "  Batch [320/1299] D_loss: -0.1220, G_loss: 0.1675\n",
      "  Batch [330/1299] D_loss: -0.0287, G_loss: -0.0919\n",
      "  Batch [340/1299] D_loss: -0.1657, G_loss: -0.1958\n",
      "  Batch [350/1299] D_loss: -0.0755, G_loss: -0.3625\n",
      "  Batch [360/1299] D_loss: -0.0940, G_loss: -0.5194\n",
      "  Batch [370/1299] D_loss: -0.0045, G_loss: -0.3768\n",
      "  Batch [380/1299] D_loss: -0.0339, G_loss: -0.3287\n",
      "  Batch [390/1299] D_loss: -0.0033, G_loss: -0.1378\n",
      "  Batch [400/1299] D_loss: -1.3076, G_loss: 4.8579\n",
      "  Batch [410/1299] D_loss: -0.0199, G_loss: -0.0852\n",
      "  Batch [420/1299] D_loss: -0.1066, G_loss: -0.2120\n",
      "  Batch [430/1299] D_loss: -0.1527, G_loss: -0.4506\n",
      "  Batch [440/1299] D_loss: -0.2460, G_loss: -0.5234\n",
      "  Batch [450/1299] D_loss: -0.0699, G_loss: -0.5458\n",
      "  Batch [460/1299] D_loss: 0.0055, G_loss: -0.4756\n",
      "  Batch [470/1299] D_loss: -0.0078, G_loss: -0.1587\n",
      "  Batch [480/1299] D_loss: -0.0377, G_loss: 0.1405\n",
      "  Batch [490/1299] D_loss: -0.0272, G_loss: -0.0771\n",
      "  Batch [500/1299] D_loss: -0.0114, G_loss: -0.1701\n",
      "  Batch [510/1299] D_loss: -0.1113, G_loss: -0.2513\n",
      "  Batch [520/1299] D_loss: -0.1028, G_loss: -0.3534\n",
      "  Batch [530/1299] D_loss: -0.0720, G_loss: -0.3550\n",
      "  Batch [540/1299] D_loss: -0.0372, G_loss: -0.3281\n",
      "  Batch [550/1299] D_loss: -0.0230, G_loss: -0.2394\n",
      "  Batch [560/1299] D_loss: -0.1655, G_loss: 1.7754\n",
      "  Batch [570/1299] D_loss: -0.0095, G_loss: -0.0422\n",
      "  Batch [580/1299] D_loss: -0.0114, G_loss: -0.0708\n",
      "  Batch [590/1299] D_loss: 0.0354, G_loss: -0.1528\n",
      "  Batch [600/1299] D_loss: -0.1402, G_loss: -0.3336\n",
      "  Batch [610/1299] D_loss: -0.0060, G_loss: -0.3516\n",
      "  Batch [620/1299] D_loss: -0.0832, G_loss: -0.4091\n",
      "  Batch [630/1299] D_loss: -0.0292, G_loss: -0.1458\n",
      "  Batch [640/1299] D_loss: -0.0971, G_loss: -0.1855\n",
      "  Batch [650/1299] D_loss: -0.0071, G_loss: -0.0412\n",
      "  Batch [660/1299] D_loss: -0.0234, G_loss: -0.1144\n",
      "  Batch [670/1299] D_loss: -0.0980, G_loss: -0.2813\n",
      "  Batch [680/1299] D_loss: -0.0540, G_loss: -0.2651\n",
      "  Batch [690/1299] D_loss: -0.0216, G_loss: -0.1544\n",
      "  Batch [700/1299] D_loss: -0.0022, G_loss: -0.0763\n",
      "  Batch [710/1299] D_loss: -0.0152, G_loss: -0.0798\n",
      "  Batch [720/1299] D_loss: -0.0512, G_loss: -0.1321\n",
      "  Batch [730/1299] D_loss: -0.1068, G_loss: -0.4063\n",
      "  Batch [740/1299] D_loss: -0.0476, G_loss: -0.4109\n",
      "  Batch [750/1299] D_loss: -0.0092, G_loss: -0.1140\n",
      "  Batch [760/1299] D_loss: -0.5485, G_loss: 2.5804\n",
      "  Batch [770/1299] D_loss: -0.0044, G_loss: -0.1504\n",
      "  Batch [780/1299] D_loss: -0.0839, G_loss: -0.2761\n",
      "  Batch [790/1299] D_loss: -0.0398, G_loss: -0.3050\n",
      "  Batch [800/1299] D_loss: -0.0435, G_loss: -0.2776\n",
      "  Batch [810/1299] D_loss: -0.0333, G_loss: -0.1591\n",
      "  Batch [820/1299] D_loss: -0.6009, G_loss: 1.4206\n",
      "  Batch [830/1299] D_loss: -0.1182, G_loss: 1.0920\n",
      "  Batch [840/1299] D_loss: -0.0258, G_loss: -0.0614\n",
      "  Batch [850/1299] D_loss: -0.1117, G_loss: -0.2269\n",
      "  Batch [860/1299] D_loss: -0.4882, G_loss: -0.7477\n",
      "  Batch [870/1299] D_loss: -0.1242, G_loss: -0.9574\n",
      "  Batch [880/1299] D_loss: -0.0515, G_loss: -0.8440\n",
      "  Batch [890/1299] D_loss: 0.0070, G_loss: -0.2519\n",
      "  Batch [900/1299] D_loss: -0.0375, G_loss: -0.1279\n",
      "  Batch [910/1299] D_loss: -0.5922, G_loss: 3.2922\n",
      "  Batch [920/1299] D_loss: 0.0410, G_loss: 0.4998\n",
      "  Batch [930/1299] D_loss: -0.0698, G_loss: -0.1332\n",
      "  Batch [940/1299] D_loss: -0.1650, G_loss: -0.2441\n",
      "  Batch [950/1299] D_loss: -0.1644, G_loss: -0.6003\n",
      "  Batch [960/1299] D_loss: -0.2569, G_loss: -0.5976\n",
      "  Batch [970/1299] D_loss: -0.1225, G_loss: -0.6439\n",
      "  Batch [980/1299] D_loss: -0.1013, G_loss: -0.5557\n",
      "  Batch [990/1299] D_loss: -0.0084, G_loss: -0.1351\n",
      "  Batch [1000/1299] D_loss: 0.0133, G_loss: -0.0672\n",
      "  Batch [1010/1299] D_loss: -0.4199, G_loss: 2.4734\n",
      "  Batch [1020/1299] D_loss: -0.3682, G_loss: 1.0974\n",
      "  Batch [1030/1299] D_loss: -0.0603, G_loss: -0.1385\n",
      "  Batch [1040/1299] D_loss: -0.2459, G_loss: -0.3787\n",
      "  Batch [1050/1299] D_loss: -0.2109, G_loss: -0.6376\n",
      "  Batch [1060/1299] D_loss: 0.0574, G_loss: -0.6597\n",
      "  Batch [1070/1299] D_loss: 0.0207, G_loss: -0.1871\n",
      "  Batch [1080/1299] D_loss: -0.0082, G_loss: -0.1665\n",
      "  Batch [1090/1299] D_loss: -0.0008, G_loss: 0.0104\n",
      "  Batch [1100/1299] D_loss: 0.0289, G_loss: -0.0562\n",
      "  Batch [1110/1299] D_loss: 0.0057, G_loss: -0.1093\n",
      "  Batch [1120/1299] D_loss: 0.0040, G_loss: -0.1802\n",
      "  Batch [1130/1299] D_loss: -0.0395, G_loss: -0.2596\n",
      "  Batch [1140/1299] D_loss: -0.1513, G_loss: -0.3181\n",
      "  Batch [1150/1299] D_loss: -0.2717, G_loss: 2.1897\n",
      "  Batch [1160/1299] D_loss: -0.0572, G_loss: -0.1804\n",
      "  Batch [1170/1299] D_loss: -0.0025, G_loss: -0.2312\n",
      "  Batch [1180/1299] D_loss: -0.0649, G_loss: 0.2290\n",
      "  Batch [1190/1299] D_loss: -0.0145, G_loss: -0.1037\n",
      "  Batch [1200/1299] D_loss: -0.0466, G_loss: -0.1691\n",
      "  Batch [1210/1299] D_loss: -0.0605, G_loss: -0.2171\n",
      "  Batch [1220/1299] D_loss: -0.0877, G_loss: -0.4709\n",
      "  Batch [1230/1299] D_loss: -0.0118, G_loss: 0.5785\n",
      "  Batch [1240/1299] D_loss: -0.7542, G_loss: 3.2223\n",
      "  Batch [1250/1299] D_loss: -0.0228, G_loss: -0.0744\n",
      "  Batch [1260/1299] D_loss: -0.0622, G_loss: -0.2155\n",
      "  Batch [1270/1299] D_loss: -0.1704, G_loss: -0.5374\n",
      "  Batch [1280/1299] D_loss: -0.0367, G_loss: -0.3244\n",
      "  Batch [1290/1299] D_loss: 0.1134, G_loss: -0.3390\n",
      "\n",
      "Epoch 31 Summary:\n",
      "  Average D_loss: -0.0653\n",
      "  Average G_loss: 0.0465\n",
      "\n",
      "Epoch [32/100]\n",
      "  Batch [0/1299] D_loss: 0.0757, G_loss: -0.2603\n",
      "  Batch [10/1299] D_loss: -0.0314, G_loss: 0.0887\n",
      "  Batch [20/1299] D_loss: -0.0248, G_loss: -0.0513\n",
      "  Batch [30/1299] D_loss: -0.0150, G_loss: -0.0770\n",
      "  Batch [40/1299] D_loss: -0.0706, G_loss: -0.1649\n",
      "  Batch [50/1299] D_loss: -0.1595, G_loss: -0.2124\n",
      "  Batch [60/1299] D_loss: -0.0689, G_loss: -0.2461\n",
      "  Batch [70/1299] D_loss: -0.0842, G_loss: -0.2428\n",
      "  Batch [80/1299] D_loss: -0.1750, G_loss: 1.0116\n",
      "  Batch [90/1299] D_loss: -0.1858, G_loss: 0.2784\n",
      "  Batch [100/1299] D_loss: -0.0184, G_loss: -0.0731\n",
      "  Batch [110/1299] D_loss: 0.0047, G_loss: -0.1987\n",
      "  Batch [120/1299] D_loss: -0.2230, G_loss: -0.4885\n",
      "  Batch [130/1299] D_loss: -0.4144, G_loss: -0.5996\n",
      "  Batch [140/1299] D_loss: 0.0064, G_loss: -0.5661\n",
      "  Batch [150/1299] D_loss: 0.0694, G_loss: -0.4279\n",
      "  Batch [160/1299] D_loss: -0.0229, G_loss: -0.2798\n",
      "  Batch [170/1299] D_loss: -0.1795, G_loss: 0.7971\n",
      "  Batch [180/1299] D_loss: -0.0044, G_loss: -0.0235\n",
      "  Batch [190/1299] D_loss: -0.0047, G_loss: -0.0451\n",
      "  Batch [200/1299] D_loss: -0.0202, G_loss: -0.0811\n",
      "  Batch [210/1299] D_loss: -0.0350, G_loss: -0.1197\n",
      "  Batch [220/1299] D_loss: -0.1247, G_loss: -0.4018\n",
      "  Batch [230/1299] D_loss: -0.1812, G_loss: -0.4222\n",
      "  Batch [240/1299] D_loss: -0.1489, G_loss: -0.6430\n",
      "  Batch [250/1299] D_loss: -0.0153, G_loss: -0.4467\n",
      "  Batch [260/1299] D_loss: -0.0829, G_loss: -0.2490\n",
      "  Batch [270/1299] D_loss: -0.5167, G_loss: 1.4391\n",
      "  Batch [280/1299] D_loss: -0.0358, G_loss: -0.0597\n",
      "  Batch [290/1299] D_loss: -0.0982, G_loss: -0.1573\n",
      "  Batch [300/1299] D_loss: -0.0504, G_loss: -0.3374\n",
      "  Batch [310/1299] D_loss: -0.1386, G_loss: -0.3440\n",
      "  Batch [320/1299] D_loss: -0.1729, G_loss: -0.4388\n",
      "  Batch [330/1299] D_loss: -0.2360, G_loss: -0.5127\n",
      "  Batch [340/1299] D_loss: -0.1010, G_loss: -0.3764\n",
      "  Batch [350/1299] D_loss: -0.0324, G_loss: -0.3893\n",
      "  Batch [360/1299] D_loss: -0.1121, G_loss: 0.2433\n",
      "  Batch [370/1299] D_loss: -0.0586, G_loss: -0.1828\n",
      "  Batch [380/1299] D_loss: -0.1425, G_loss: -0.2145\n",
      "  Batch [390/1299] D_loss: -0.1011, G_loss: -0.3679\n",
      "  Batch [400/1299] D_loss: 0.0092, G_loss: -0.2949\n",
      "  Batch [410/1299] D_loss: -0.0207, G_loss: -0.2308\n",
      "  Batch [420/1299] D_loss: -0.0272, G_loss: -0.2014\n",
      "  Batch [430/1299] D_loss: -1.1846, G_loss: 5.5793\n",
      "  Batch [440/1299] D_loss: -0.0229, G_loss: -0.0777\n",
      "  Batch [450/1299] D_loss: 0.0157, G_loss: -0.1486\n",
      "  Batch [460/1299] D_loss: -0.0304, G_loss: -0.2216\n",
      "  Batch [470/1299] D_loss: -0.1609, G_loss: -0.4928\n",
      "  Batch [480/1299] D_loss: -0.1298, G_loss: -0.3439\n",
      "  Batch [490/1299] D_loss: -0.0504, G_loss: -0.3477\n",
      "  Batch [500/1299] D_loss: -0.1395, G_loss: 0.5543\n",
      "  Batch [510/1299] D_loss: -0.2011, G_loss: 0.4360\n",
      "  Batch [520/1299] D_loss: -0.0112, G_loss: -0.0166\n",
      "  Batch [530/1299] D_loss: -0.0709, G_loss: -0.1555\n",
      "  Batch [540/1299] D_loss: -0.0789, G_loss: -0.2930\n",
      "  Batch [550/1299] D_loss: -0.0279, G_loss: -0.6440\n",
      "  Batch [560/1299] D_loss: -0.0560, G_loss: -0.7972\n",
      "  Batch [570/1299] D_loss: -0.0158, G_loss: -0.4264\n",
      "  Batch [580/1299] D_loss: 0.0037, G_loss: -0.1493\n",
      "  Batch [590/1299] D_loss: -0.0253, G_loss: -0.0526\n",
      "  Batch [600/1299] D_loss: -0.0432, G_loss: 0.2443\n",
      "  Batch [610/1299] D_loss: -0.0504, G_loss: -0.0896\n",
      "  Batch [620/1299] D_loss: -0.0858, G_loss: -0.2116\n",
      "  Batch [630/1299] D_loss: -0.1490, G_loss: -0.3665\n",
      "  Batch [640/1299] D_loss: -0.0591, G_loss: -0.4226\n",
      "  Batch [650/1299] D_loss: 0.2022, G_loss: -0.4905\n",
      "  Batch [660/1299] D_loss: -0.0922, G_loss: -0.3908\n",
      "  Batch [670/1299] D_loss: 0.0017, G_loss: -0.1055\n",
      "  Batch [680/1299] D_loss: -0.0312, G_loss: 0.1942\n",
      "  Batch [690/1299] D_loss: -0.5256, G_loss: 1.3158\n",
      "  Batch [700/1299] D_loss: -0.0348, G_loss: -0.1252\n",
      "  Batch [710/1299] D_loss: -0.0805, G_loss: -0.4380\n",
      "  Batch [720/1299] D_loss: -0.1213, G_loss: -0.4858\n",
      "  Batch [730/1299] D_loss: -0.0794, G_loss: -0.6448\n",
      "  Batch [740/1299] D_loss: -0.0687, G_loss: -0.2919\n",
      "  Batch [750/1299] D_loss: 0.0477, G_loss: -0.2082\n",
      "  Batch [760/1299] D_loss: -0.0314, G_loss: 0.0296\n",
      "  Batch [770/1299] D_loss: -0.0052, G_loss: -0.0347\n",
      "  Batch [780/1299] D_loss: -0.0172, G_loss: -0.1498\n",
      "  Batch [790/1299] D_loss: -0.0315, G_loss: -0.1667\n",
      "  Batch [800/1299] D_loss: -0.0905, G_loss: 0.0293\n",
      "  Batch [810/1299] D_loss: -0.0292, G_loss: -0.1560\n",
      "  Batch [820/1299] D_loss: -0.0930, G_loss: -0.2677\n",
      "  Batch [830/1299] D_loss: 0.0110, G_loss: -0.3295\n",
      "  Batch [840/1299] D_loss: -0.0058, G_loss: -0.0981\n",
      "  Batch [850/1299] D_loss: -0.8196, G_loss: 2.9163\n",
      "  Batch [860/1299] D_loss: -0.0241, G_loss: -0.1521\n",
      "  Batch [870/1299] D_loss: -0.0288, G_loss: -0.4661\n",
      "  Batch [880/1299] D_loss: -0.0257, G_loss: -0.3264\n",
      "  Batch [890/1299] D_loss: -0.1011, G_loss: -0.4560\n",
      "  Batch [900/1299] D_loss: -0.0572, G_loss: -0.2457\n",
      "  Batch [910/1299] D_loss: -2.7129, G_loss: 6.3514\n",
      "  Batch [920/1299] D_loss: -0.0784, G_loss: 0.0769\n",
      "  Batch [930/1299] D_loss: 0.0055, G_loss: -0.1450\n",
      "  Batch [940/1299] D_loss: -0.0270, G_loss: -0.1838\n",
      "  Batch [950/1299] D_loss: -0.1261, G_loss: -0.4074\n",
      "  Batch [960/1299] D_loss: -0.1112, G_loss: -0.3906\n",
      "  Batch [970/1299] D_loss: -0.0836, G_loss: -0.2108\n",
      "  Batch [980/1299] D_loss: -0.0381, G_loss: -0.1939\n",
      "  Batch [990/1299] D_loss: -1.0009, G_loss: 4.0989\n",
      "  Batch [1000/1299] D_loss: -0.5147, G_loss: 1.0224\n",
      "  Batch [1010/1299] D_loss: -0.0148, G_loss: -0.0786\n",
      "  Batch [1020/1299] D_loss: -0.1531, G_loss: -0.2537\n",
      "  Batch [1030/1299] D_loss: -0.1469, G_loss: -0.4929\n",
      "  Batch [1040/1299] D_loss: -0.2384, G_loss: -0.6662\n",
      "  Batch [1050/1299] D_loss: -0.1860, G_loss: -0.7016\n",
      "  Batch [1060/1299] D_loss: 0.1435, G_loss: -0.7303\n",
      "  Batch [1070/1299] D_loss: -0.0787, G_loss: 0.5991\n",
      "  Batch [1080/1299] D_loss: 0.0027, G_loss: -0.0744\n",
      "  Batch [1090/1299] D_loss: -0.0251, G_loss: -0.1681\n",
      "  Batch [1100/1299] D_loss: -0.0285, G_loss: -0.1424\n",
      "  Batch [1110/1299] D_loss: -0.0769, G_loss: -0.2413\n",
      "  Batch [1120/1299] D_loss: -0.0010, G_loss: -0.2846\n",
      "  Batch [1130/1299] D_loss: -0.0624, G_loss: -0.2608\n",
      "  Batch [1140/1299] D_loss: -0.5792, G_loss: 2.8955\n",
      "  Batch [1150/1299] D_loss: -0.6127, G_loss: 1.9058\n",
      "  Batch [1160/1299] D_loss: -0.0609, G_loss: -0.1256\n",
      "  Batch [1170/1299] D_loss: -0.1347, G_loss: -0.3187\n",
      "  Batch [1180/1299] D_loss: -0.2476, G_loss: -0.6341\n",
      "  Batch [1190/1299] D_loss: 0.0233, G_loss: -0.5769\n",
      "  Batch [1200/1299] D_loss: -0.0431, G_loss: -0.4410\n",
      "  Batch [1210/1299] D_loss: 0.1038, G_loss: -0.3418\n",
      "  Batch [1220/1299] D_loss: -0.0406, G_loss: -0.1965\n",
      "  Batch [1230/1299] D_loss: 0.1193, G_loss: 1.7813\n",
      "  Batch [1240/1299] D_loss: -0.1261, G_loss: 0.3749\n",
      "  Batch [1250/1299] D_loss: -0.1487, G_loss: 0.5392\n",
      "  Batch [1260/1299] D_loss: -0.0232, G_loss: -0.0533\n",
      "  Batch [1270/1299] D_loss: -0.0818, G_loss: -0.2883\n",
      "  Batch [1280/1299] D_loss: -0.2274, G_loss: -0.5377\n",
      "  Batch [1290/1299] D_loss: -0.0686, G_loss: -1.0127\n",
      "\n",
      "Epoch 32 Summary:\n",
      "  Average D_loss: -0.0638\n",
      "  Average G_loss: 0.0711\n",
      "\n",
      "Epoch [33/100]\n",
      "  Batch [0/1299] D_loss: -0.0528, G_loss: -0.7460\n",
      "  Batch [10/1299] D_loss: 0.0274, G_loss: -0.4535\n",
      "  Batch [20/1299] D_loss: -1.3852, G_loss: 3.2696\n",
      "  Batch [30/1299] D_loss: 0.0020, G_loss: -0.0371\n",
      "  Batch [40/1299] D_loss: 0.0032, G_loss: -0.0430\n",
      "  Batch [50/1299] D_loss: -0.0158, G_loss: -0.0691\n",
      "  Batch [60/1299] D_loss: -0.0063, G_loss: -0.0875\n",
      "  Batch [70/1299] D_loss: -0.0503, G_loss: 0.1562\n",
      "  Batch [80/1299] D_loss: -0.0171, G_loss: -0.0488\n",
      "  Batch [90/1299] D_loss: -0.0651, G_loss: -0.1328\n",
      "  Batch [100/1299] D_loss: -0.1228, G_loss: -0.3924\n",
      "  Batch [110/1299] D_loss: -0.1822, G_loss: -0.4812\n",
      "  Batch [120/1299] D_loss: -0.0761, G_loss: -0.5360\n",
      "  Batch [130/1299] D_loss: -0.1017, G_loss: -0.3153\n",
      "  Batch [140/1299] D_loss: -0.1352, G_loss: 0.4917\n",
      "  Batch [150/1299] D_loss: -0.0476, G_loss: -0.0790\n",
      "  Batch [160/1299] D_loss: -0.2248, G_loss: -0.2926\n",
      "  Batch [170/1299] D_loss: -0.1661, G_loss: -0.4457\n",
      "  Batch [180/1299] D_loss: -0.0415, G_loss: -0.5830\n",
      "  Batch [190/1299] D_loss: -0.1149, G_loss: -0.5849\n",
      "  Batch [200/1299] D_loss: -0.1065, G_loss: -0.5638\n",
      "  Batch [210/1299] D_loss: 0.0120, G_loss: -0.0853\n",
      "  Batch [220/1299] D_loss: -0.0843, G_loss: 0.3489\n",
      "  Batch [230/1299] D_loss: -0.3397, G_loss: 1.5123\n",
      "  Batch [240/1299] D_loss: -0.0515, G_loss: -0.1352\n",
      "  Batch [250/1299] D_loss: -0.0874, G_loss: -0.4199\n",
      "  Batch [260/1299] D_loss: -0.2550, G_loss: -0.4982\n",
      "  Batch [270/1299] D_loss: 0.1034, G_loss: -0.7558\n",
      "  Batch [280/1299] D_loss: -0.0943, G_loss: -0.6023\n",
      "  Batch [290/1299] D_loss: 0.1204, G_loss: -0.3301\n",
      "  Batch [300/1299] D_loss: -0.0296, G_loss: -0.0558\n",
      "  Batch [310/1299] D_loss: -0.5564, G_loss: 1.9525\n",
      "  Batch [320/1299] D_loss: -0.0857, G_loss: -0.1519\n",
      "  Batch [330/1299] D_loss: -0.0695, G_loss: -0.3020\n",
      "  Batch [340/1299] D_loss: -0.0339, G_loss: -0.3923\n",
      "  Batch [350/1299] D_loss: -0.1278, G_loss: -0.7147\n",
      "  Batch [360/1299] D_loss: -0.2247, G_loss: -0.5674\n",
      "  Batch [370/1299] D_loss: -0.0017, G_loss: -0.2238\n",
      "  Batch [380/1299] D_loss: -0.0032, G_loss: -0.0554\n",
      "  Batch [390/1299] D_loss: -0.0953, G_loss: 0.3146\n",
      "  Batch [400/1299] D_loss: -0.0261, G_loss: -0.0217\n",
      "  Batch [410/1299] D_loss: -0.0389, G_loss: -0.0966\n",
      "  Batch [420/1299] D_loss: -0.1773, G_loss: -0.3137\n",
      "  Batch [430/1299] D_loss: -0.1197, G_loss: -0.5171\n",
      "  Batch [440/1299] D_loss: 0.0133, G_loss: -0.5757\n",
      "  Batch [450/1299] D_loss: -0.0103, G_loss: -0.6908\n",
      "  Batch [460/1299] D_loss: 0.0027, G_loss: -0.1353\n",
      "  Batch [470/1299] D_loss: 0.0074, G_loss: -0.0298\n",
      "  Batch [480/1299] D_loss: -0.7111, G_loss: 1.2092\n",
      "  Batch [490/1299] D_loss: -0.0248, G_loss: -0.0590\n",
      "  Batch [500/1299] D_loss: -0.0537, G_loss: -0.1582\n",
      "  Batch [510/1299] D_loss: -0.0996, G_loss: -0.4207\n",
      "  Batch [520/1299] D_loss: -0.0477, G_loss: -0.4099\n",
      "  Batch [530/1299] D_loss: -0.0447, G_loss: -0.5721\n",
      "  Batch [540/1299] D_loss: -0.1467, G_loss: -0.3682\n",
      "  Batch [550/1299] D_loss: 0.0646, G_loss: -0.2154\n",
      "  Batch [560/1299] D_loss: -0.9098, G_loss: 3.3839\n",
      "  Batch [570/1299] D_loss: -0.0413, G_loss: -0.0651\n",
      "  Batch [580/1299] D_loss: -0.0823, G_loss: -0.1173\n",
      "  Batch [590/1299] D_loss: -0.0978, G_loss: -0.3331\n",
      "  Batch [600/1299] D_loss: 0.0818, G_loss: -0.2979\n",
      "  Batch [610/1299] D_loss: -0.1133, G_loss: -0.3464\n",
      "  Batch [620/1299] D_loss: 0.0282, G_loss: -0.2097\n",
      "  Batch [630/1299] D_loss: -0.4457, G_loss: 1.1323\n",
      "  Batch [640/1299] D_loss: -0.0487, G_loss: -0.0776\n",
      "  Batch [650/1299] D_loss: -0.0864, G_loss: -0.3238\n",
      "  Batch [660/1299] D_loss: -0.0491, G_loss: -0.3503\n",
      "  Batch [670/1299] D_loss: -0.1317, G_loss: -0.4108\n",
      "  Batch [680/1299] D_loss: -0.0234, G_loss: -0.2290\n",
      "  Batch [690/1299] D_loss: -0.0139, G_loss: -0.3261\n",
      "  Batch [700/1299] D_loss: -0.4863, G_loss: 1.9120\n",
      "  Batch [710/1299] D_loss: -0.0171, G_loss: -0.1236\n",
      "  Batch [720/1299] D_loss: -0.0521, G_loss: -0.2125\n",
      "  Batch [730/1299] D_loss: -0.1122, G_loss: -0.3649\n",
      "  Batch [740/1299] D_loss: -0.1160, G_loss: -0.4695\n",
      "  Batch [750/1299] D_loss: 0.0256, G_loss: -0.2678\n",
      "  Batch [760/1299] D_loss: -0.0404, G_loss: -0.3476\n",
      "  Batch [770/1299] D_loss: -0.4068, G_loss: 3.6890\n",
      "  Batch [780/1299] D_loss: -0.0036, G_loss: -0.0570\n",
      "  Batch [790/1299] D_loss: -0.0311, G_loss: -0.0871\n",
      "  Batch [800/1299] D_loss: -0.0790, G_loss: -0.2006\n",
      "  Batch [810/1299] D_loss: 0.1094, G_loss: 0.2337\n",
      "  Batch [820/1299] D_loss: -0.0145, G_loss: -0.0697\n",
      "  Batch [830/1299] D_loss: -0.1257, G_loss: -0.2751\n",
      "  Batch [840/1299] D_loss: 0.0127, G_loss: -0.4164\n",
      "  Batch [850/1299] D_loss: 0.0722, G_loss: -0.4326\n",
      "  Batch [860/1299] D_loss: -0.0334, G_loss: -0.1963\n",
      "  Batch [870/1299] D_loss: -0.0116, G_loss: -0.2078\n",
      "  Batch [880/1299] D_loss: -0.0238, G_loss: 0.0006\n",
      "  Batch [890/1299] D_loss: -0.2342, G_loss: 0.4357\n",
      "  Batch [900/1299] D_loss: -0.0519, G_loss: -0.1254\n",
      "  Batch [910/1299] D_loss: -0.0700, G_loss: -0.2449\n",
      "  Batch [920/1299] D_loss: 0.0322, G_loss: -0.3537\n",
      "  Batch [930/1299] D_loss: -0.1037, G_loss: -0.5074\n",
      "  Batch [940/1299] D_loss: -0.0290, G_loss: -0.3124\n",
      "  Batch [950/1299] D_loss: -1.0267, G_loss: 3.0801\n",
      "  Batch [960/1299] D_loss: -0.0107, G_loss: -0.0483\n",
      "  Batch [970/1299] D_loss: -0.0241, G_loss: -0.0821\n",
      "  Batch [980/1299] D_loss: -0.0894, G_loss: -0.2575\n",
      "  Batch [990/1299] D_loss: -0.1231, G_loss: -0.3496\n",
      "  Batch [1000/1299] D_loss: -0.0647, G_loss: -0.4226\n",
      "  Batch [1010/1299] D_loss: 0.0834, G_loss: -0.2812\n",
      "  Batch [1020/1299] D_loss: -3.4450, G_loss: 5.7793\n",
      "  Batch [1030/1299] D_loss: -0.0128, G_loss: -0.0603\n",
      "  Batch [1040/1299] D_loss: -0.0669, G_loss: -0.1613\n",
      "  Batch [1050/1299] D_loss: -0.0965, G_loss: -0.2829\n",
      "  Batch [1060/1299] D_loss: -0.1437, G_loss: -0.4447\n",
      "  Batch [1070/1299] D_loss: 0.0288, G_loss: -0.2995\n",
      "  Batch [1080/1299] D_loss: -0.0260, G_loss: -0.1797\n",
      "  Batch [1090/1299] D_loss: -0.1610, G_loss: 0.4374\n",
      "  Batch [1100/1299] D_loss: -0.0387, G_loss: -0.0317\n",
      "  Batch [1110/1299] D_loss: -0.0407, G_loss: -0.0479\n",
      "  Batch [1120/1299] D_loss: -0.0859, G_loss: -0.2901\n",
      "  Batch [1130/1299] D_loss: -0.1030, G_loss: -0.5101\n",
      "  Batch [1140/1299] D_loss: -0.1978, G_loss: -0.7548\n",
      "  Batch [1150/1299] D_loss: -0.0044, G_loss: -0.3557\n",
      "  Batch [1160/1299] D_loss: -0.0145, G_loss: -0.0954\n",
      "  Batch [1170/1299] D_loss: -0.0323, G_loss: -0.0964\n",
      "  Batch [1180/1299] D_loss: -0.0797, G_loss: -0.2426\n",
      "  Batch [1190/1299] D_loss: 0.1300, G_loss: -0.3537\n",
      "  Batch [1200/1299] D_loss: 0.0422, G_loss: -0.4320\n",
      "  Batch [1210/1299] D_loss: 0.0113, G_loss: -0.3271\n",
      "  Batch [1220/1299] D_loss: 0.0038, G_loss: -0.3473\n",
      "  Batch [1230/1299] D_loss: -1.0111, G_loss: 2.9140\n",
      "  Batch [1240/1299] D_loss: -0.3117, G_loss: 0.8434\n",
      "  Batch [1250/1299] D_loss: -0.0819, G_loss: -0.1463\n",
      "  Batch [1260/1299] D_loss: -0.0531, G_loss: -0.3889\n",
      "  Batch [1270/1299] D_loss: -0.0973, G_loss: -0.4277\n",
      "  Batch [1280/1299] D_loss: 0.0553, G_loss: -0.3508\n",
      "  Batch [1290/1299] D_loss: -0.1129, G_loss: -0.2710\n",
      "\n",
      "Epoch 33 Summary:\n",
      "  Average D_loss: -0.0590\n",
      "  Average G_loss: 0.0566\n",
      "\n",
      "Epoch [34/100]\n",
      "  Batch [0/1299] D_loss: -4.4758, G_loss: 11.5729\n",
      "  Batch [10/1299] D_loss: -0.1269, G_loss: 0.3680\n",
      "  Batch [20/1299] D_loss: -0.0989, G_loss: -0.2894\n",
      "  Batch [30/1299] D_loss: -0.0932, G_loss: -0.3425\n",
      "  Batch [40/1299] D_loss: -0.1324, G_loss: -0.4894\n",
      "  Batch [50/1299] D_loss: 0.0782, G_loss: -0.3992\n",
      "  Batch [60/1299] D_loss: 0.0229, G_loss: -0.4947\n",
      "  Batch [70/1299] D_loss: -2.2155, G_loss: 6.8867\n",
      "  Batch [80/1299] D_loss: 0.0034, G_loss: -0.0477\n",
      "  Batch [90/1299] D_loss: -0.0138, G_loss: -0.0740\n",
      "  Batch [100/1299] D_loss: -0.0270, G_loss: -0.0898\n",
      "  Batch [110/1299] D_loss: 0.0004, G_loss: -0.1626\n",
      "  Batch [120/1299] D_loss: -0.0537, G_loss: -0.1861\n",
      "  Batch [130/1299] D_loss: 0.0143, G_loss: -0.2279\n",
      "  Batch [140/1299] D_loss: -0.0241, G_loss: -0.1987\n",
      "  Batch [150/1299] D_loss: -0.0514, G_loss: -0.2198\n",
      "  Batch [160/1299] D_loss: 0.0128, G_loss: -0.1102\n",
      "  Batch [170/1299] D_loss: -0.0294, G_loss: -0.0073\n",
      "  Batch [180/1299] D_loss: -0.4648, G_loss: 1.8184\n",
      "  Batch [190/1299] D_loss: -0.0847, G_loss: -0.1374\n",
      "  Batch [200/1299] D_loss: -0.1495, G_loss: -0.3261\n",
      "  Batch [210/1299] D_loss: -0.1757, G_loss: -0.5744\n",
      "  Batch [220/1299] D_loss: -0.0850, G_loss: -0.7029\n",
      "  Batch [230/1299] D_loss: -0.0591, G_loss: -0.7136\n",
      "  Batch [240/1299] D_loss: -0.1350, G_loss: -0.5389\n",
      "  Batch [250/1299] D_loss: -1.0007, G_loss: 5.2115\n",
      "  Batch [260/1299] D_loss: -0.1449, G_loss: 0.6573\n",
      "  Batch [270/1299] D_loss: -0.0636, G_loss: 0.1657\n",
      "  Batch [280/1299] D_loss: -0.1338, G_loss: 1.2650\n",
      "  Batch [290/1299] D_loss: -0.0695, G_loss: -0.1324\n",
      "  Batch [300/1299] D_loss: -0.1693, G_loss: -0.3238\n",
      "  Batch [310/1299] D_loss: -0.1875, G_loss: -0.5216\n",
      "  Batch [320/1299] D_loss: -0.0660, G_loss: -0.6165\n",
      "  Batch [330/1299] D_loss: 0.0730, G_loss: -0.5698\n",
      "  Batch [340/1299] D_loss: -0.0283, G_loss: -0.2724\n",
      "  Batch [350/1299] D_loss: -0.0443, G_loss: -0.1440\n",
      "  Batch [360/1299] D_loss: 0.0055, G_loss: -0.0363\n",
      "  Batch [370/1299] D_loss: 0.0181, G_loss: -0.0681\n",
      "  Batch [380/1299] D_loss: -0.0072, G_loss: -0.0777\n",
      "  Batch [390/1299] D_loss: -0.0776, G_loss: -0.1600\n",
      "  Batch [400/1299] D_loss: -0.0195, G_loss: -0.0857\n",
      "  Batch [410/1299] D_loss: -0.0332, G_loss: -0.1504\n",
      "  Batch [420/1299] D_loss: -0.1261, G_loss: -0.3060\n",
      "  Batch [430/1299] D_loss: -0.1018, G_loss: -0.2869\n",
      "  Batch [440/1299] D_loss: -0.0411, G_loss: -0.2681\n",
      "  Batch [450/1299] D_loss: -0.0571, G_loss: -0.2116\n",
      "  Batch [460/1299] D_loss: -0.4093, G_loss: 0.9283\n",
      "  Batch [470/1299] D_loss: -0.0305, G_loss: -0.1517\n",
      "  Batch [480/1299] D_loss: -0.0975, G_loss: -0.3359\n",
      "  Batch [490/1299] D_loss: -0.0199, G_loss: -0.3324\n",
      "  Batch [500/1299] D_loss: -0.0371, G_loss: -0.4337\n",
      "  Batch [510/1299] D_loss: -0.0874, G_loss: -0.4418\n",
      "  Batch [520/1299] D_loss: 0.0136, G_loss: -0.2148\n",
      "  Batch [530/1299] D_loss: -0.2259, G_loss: 0.6197\n",
      "  Batch [540/1299] D_loss: -0.0472, G_loss: 0.1784\n",
      "  Batch [550/1299] D_loss: -0.0167, G_loss: -0.0769\n",
      "  Batch [560/1299] D_loss: -0.0549, G_loss: -0.2213\n",
      "  Batch [570/1299] D_loss: 0.0140, G_loss: -0.4029\n",
      "  Batch [580/1299] D_loss: -0.0923, G_loss: -0.6246\n",
      "  Batch [590/1299] D_loss: -0.0336, G_loss: -0.5481\n",
      "  Batch [600/1299] D_loss: 0.1074, G_loss: -0.1840\n",
      "  Batch [610/1299] D_loss: 0.3709, G_loss: 0.5112\n",
      "  Batch [620/1299] D_loss: -0.0191, G_loss: -0.0601\n",
      "  Batch [630/1299] D_loss: -0.0252, G_loss: -0.0909\n",
      "  Batch [640/1299] D_loss: -0.0350, G_loss: -0.1071\n",
      "  Batch [650/1299] D_loss: -0.0282, G_loss: -0.2420\n",
      "  Batch [660/1299] D_loss: -0.1973, G_loss: -0.4695\n",
      "  Batch [670/1299] D_loss: -0.0418, G_loss: -0.3131\n",
      "  Batch [680/1299] D_loss: -0.0207, G_loss: -0.1879\n",
      "  Batch [690/1299] D_loss: -0.3147, G_loss: 1.8138\n",
      "  Batch [700/1299] D_loss: -0.0181, G_loss: -0.0918\n",
      "  Batch [710/1299] D_loss: -0.0772, G_loss: -0.2426\n",
      "  Batch [720/1299] D_loss: -0.1874, G_loss: -0.3642\n",
      "  Batch [730/1299] D_loss: -0.0598, G_loss: -0.4314\n",
      "  Batch [740/1299] D_loss: -0.1374, G_loss: -0.5237\n",
      "  Batch [750/1299] D_loss: -0.0182, G_loss: -0.3788\n",
      "  Batch [760/1299] D_loss: -0.0471, G_loss: -0.2743\n",
      "  Batch [770/1299] D_loss: -2.1027, G_loss: 6.2718\n",
      "  Batch [780/1299] D_loss: -0.0152, G_loss: -0.0640\n",
      "  Batch [790/1299] D_loss: -0.1173, G_loss: -0.2007\n",
      "  Batch [800/1299] D_loss: -0.0808, G_loss: -0.3412\n",
      "  Batch [810/1299] D_loss: -0.0849, G_loss: -0.3892\n",
      "  Batch [820/1299] D_loss: 0.0221, G_loss: -0.4264\n",
      "  Batch [830/1299] D_loss: -0.1685, G_loss: -0.4544\n",
      "  Batch [840/1299] D_loss: -0.0905, G_loss: -0.4473\n",
      "  Batch [850/1299] D_loss: -0.0394, G_loss: -0.1452\n",
      "  Batch [860/1299] D_loss: -0.1013, G_loss: 1.9850\n",
      "  Batch [870/1299] D_loss: -0.0940, G_loss: 0.2500\n",
      "  Batch [880/1299] D_loss: -0.1880, G_loss: 0.5258\n",
      "  Batch [890/1299] D_loss: -0.0316, G_loss: -0.0788\n",
      "  Batch [900/1299] D_loss: -0.1647, G_loss: -0.3004\n",
      "  Batch [910/1299] D_loss: -0.1909, G_loss: -0.6373\n",
      "  Batch [920/1299] D_loss: -0.2096, G_loss: -0.7837\n",
      "  Batch [930/1299] D_loss: -0.0556, G_loss: -0.7235\n",
      "  Batch [940/1299] D_loss: 0.0195, G_loss: -0.1981\n",
      "  Batch [950/1299] D_loss: -0.0166, G_loss: -0.0674\n",
      "  Batch [960/1299] D_loss: -0.9391, G_loss: 5.1491\n",
      "  Batch [970/1299] D_loss: -0.2890, G_loss: 0.0761\n",
      "  Batch [980/1299] D_loss: -0.0291, G_loss: -0.2052\n",
      "  Batch [990/1299] D_loss: -0.0291, G_loss: -0.3229\n",
      "  Batch [1000/1299] D_loss: -0.1210, G_loss: -0.4528\n",
      "  Batch [1010/1299] D_loss: -0.0181, G_loss: -0.4964\n",
      "  Batch [1020/1299] D_loss: -0.1179, G_loss: -0.6618\n",
      "  Batch [1030/1299] D_loss: -0.0401, G_loss: -0.1546\n",
      "  Batch [1040/1299] D_loss: -0.9694, G_loss: 3.0617\n",
      "  Batch [1050/1299] D_loss: -0.0397, G_loss: -0.1384\n",
      "  Batch [1060/1299] D_loss: -0.0877, G_loss: -0.1945\n",
      "  Batch [1070/1299] D_loss: -0.0990, G_loss: -0.4727\n",
      "  Batch [1080/1299] D_loss: -0.1583, G_loss: -0.4594\n",
      "  Batch [1090/1299] D_loss: -0.0897, G_loss: -0.3695\n",
      "  Batch [1100/1299] D_loss: -0.1280, G_loss: -0.3787\n",
      "  Batch [1110/1299] D_loss: -0.1801, G_loss: -0.3455\n",
      "  Batch [1120/1299] D_loss: 0.0018, G_loss: -0.0840\n",
      "  Batch [1130/1299] D_loss: -0.0012, G_loss: 0.4625\n",
      "  Batch [1140/1299] D_loss: -0.0247, G_loss: -0.0461\n",
      "  Batch [1150/1299] D_loss: -0.0194, G_loss: -0.1166\n",
      "  Batch [1160/1299] D_loss: -0.0561, G_loss: -0.3485\n",
      "  Batch [1170/1299] D_loss: -0.0325, G_loss: -0.5036\n",
      "  Batch [1180/1299] D_loss: -0.2093, G_loss: -0.3922\n",
      "  Batch [1190/1299] D_loss: -0.0219, G_loss: -0.2838\n",
      "  Batch [1200/1299] D_loss: -0.2411, G_loss: 2.2224\n",
      "  Batch [1210/1299] D_loss: -1.0981, G_loss: 2.0949\n",
      "  Batch [1220/1299] D_loss: -0.0163, G_loss: -0.1536\n",
      "  Batch [1230/1299] D_loss: -0.0603, G_loss: -0.2962\n",
      "  Batch [1240/1299] D_loss: -0.0132, G_loss: -0.4631\n",
      "  Batch [1250/1299] D_loss: -0.1619, G_loss: -0.3908\n",
      "  Batch [1260/1299] D_loss: 0.0030, G_loss: -0.3646\n",
      "  Batch [1270/1299] D_loss: 0.0396, G_loss: -0.2144\n",
      "  Batch [1280/1299] D_loss: -0.0442, G_loss: -0.0140\n",
      "  Batch [1290/1299] D_loss: -0.0067, G_loss: -0.0348\n",
      "\n",
      "Epoch 34 Summary:\n",
      "  Average D_loss: -0.0686\n",
      "  Average G_loss: 0.0804\n",
      "\n",
      "Epoch [35/100]\n",
      "  Batch [0/1299] D_loss: -0.0277, G_loss: -0.0751\n",
      "  Batch [10/1299] D_loss: 0.0269, G_loss: -0.1380\n",
      "  Batch [20/1299] D_loss: -0.0478, G_loss: -0.2194\n",
      "  Batch [30/1299] D_loss: -0.0672, G_loss: -0.3508\n",
      "  Batch [40/1299] D_loss: 0.0235, G_loss: -0.3416\n",
      "  Batch [50/1299] D_loss: -0.0245, G_loss: -0.1177\n",
      "  Batch [60/1299] D_loss: 0.0042, G_loss: -0.1058\n",
      "  Batch [70/1299] D_loss: -0.0416, G_loss: -0.1727\n",
      "  Batch [80/1299] D_loss: 0.0105, G_loss: -0.1945\n",
      "  Batch [90/1299] D_loss: -0.0252, G_loss: -0.1158\n",
      "  Batch [100/1299] D_loss: -0.0299, G_loss: -0.0677\n",
      "  Batch [110/1299] D_loss: -0.7244, G_loss: 3.8878\n",
      "  Batch [120/1299] D_loss: 0.0196, G_loss: -0.1271\n",
      "  Batch [130/1299] D_loss: -0.0377, G_loss: -0.2883\n",
      "  Batch [140/1299] D_loss: -0.0712, G_loss: -0.3192\n",
      "  Batch [150/1299] D_loss: -0.0139, G_loss: -0.2240\n",
      "  Batch [160/1299] D_loss: -0.0964, G_loss: -0.2090\n",
      "  Batch [170/1299] D_loss: -0.3301, G_loss: 0.6308\n",
      "  Batch [180/1299] D_loss: -0.0580, G_loss: 0.0435\n",
      "  Batch [190/1299] D_loss: -0.0377, G_loss: -0.0996\n",
      "  Batch [200/1299] D_loss: -0.0679, G_loss: -0.1947\n",
      "  Batch [210/1299] D_loss: -0.1207, G_loss: -0.4457\n",
      "  Batch [220/1299] D_loss: -0.1232, G_loss: -0.3250\n",
      "  Batch [230/1299] D_loss: -0.0383, G_loss: -0.4856\n",
      "  Batch [240/1299] D_loss: -1.4470, G_loss: 3.9641\n",
      "  Batch [250/1299] D_loss: -0.0032, G_loss: -0.0778\n",
      "  Batch [260/1299] D_loss: -0.0231, G_loss: -0.1132\n",
      "  Batch [270/1299] D_loss: -0.0235, G_loss: -0.1988\n",
      "  Batch [280/1299] D_loss: -0.0096, G_loss: -0.1754\n",
      "  Batch [290/1299] D_loss: -0.1021, G_loss: -0.2412\n",
      "  Batch [300/1299] D_loss: -0.0900, G_loss: -0.2896\n",
      "  Batch [310/1299] D_loss: 0.0089, G_loss: -0.0544\n",
      "  Batch [320/1299] D_loss: -0.0179, G_loss: -0.0943\n",
      "  Batch [330/1299] D_loss: -0.0647, G_loss: -0.1922\n",
      "  Batch [340/1299] D_loss: -0.0347, G_loss: -0.2089\n",
      "  Batch [350/1299] D_loss: -0.0070, G_loss: -0.1051\n",
      "  Batch [360/1299] D_loss: -0.0226, G_loss: -0.1090\n",
      "  Batch [370/1299] D_loss: -0.0266, G_loss: -0.1454\n",
      "  Batch [380/1299] D_loss: -2.4210, G_loss: 6.1389\n",
      "  Batch [390/1299] D_loss: -0.6409, G_loss: 0.8434\n",
      "  Batch [400/1299] D_loss: -0.0957, G_loss: -0.1566\n",
      "  Batch [410/1299] D_loss: -0.0643, G_loss: -0.3628\n",
      "  Batch [420/1299] D_loss: -0.0071, G_loss: -0.5357\n",
      "  Batch [430/1299] D_loss: -0.1861, G_loss: -0.6831\n",
      "  Batch [440/1299] D_loss: 0.0115, G_loss: -0.1854\n",
      "  Batch [450/1299] D_loss: 0.0176, G_loss: -0.1009\n",
      "  Batch [460/1299] D_loss: -0.6914, G_loss: 1.7244\n",
      "  Batch [470/1299] D_loss: -0.1009, G_loss: 0.2382\n",
      "  Batch [480/1299] D_loss: -0.0904, G_loss: 0.2523\n",
      "  Batch [490/1299] D_loss: -0.0975, G_loss: -0.0866\n",
      "  Batch [500/1299] D_loss: -0.2799, G_loss: -0.3137\n",
      "  Batch [510/1299] D_loss: -0.2618, G_loss: -0.6359\n",
      "  Batch [520/1299] D_loss: -0.3998, G_loss: -1.0004\n",
      "  Batch [530/1299] D_loss: 0.0995, G_loss: -0.7881\n",
      "  Batch [540/1299] D_loss: 0.0749, G_loss: -0.2277\n",
      "  Batch [550/1299] D_loss: -0.0070, G_loss: -0.0654\n",
      "  Batch [560/1299] D_loss: -0.0012, G_loss: -0.0443\n",
      "  Batch [570/1299] D_loss: -0.0012, G_loss: -0.0174\n",
      "  Batch [580/1299] D_loss: -0.0002, G_loss: -0.0063\n",
      "  Batch [590/1299] D_loss: -0.0216, G_loss: 0.1021\n",
      "  Batch [600/1299] D_loss: -0.2820, G_loss: 1.2122\n",
      "  Batch [610/1299] D_loss: 0.0019, G_loss: 0.0570\n",
      "  Batch [620/1299] D_loss: -0.8602, G_loss: -4.6912\n",
      "  Batch [630/1299] D_loss: -0.9271, G_loss: -1.3330\n",
      "  Batch [640/1299] D_loss: -0.3346, G_loss: 0.1374\n",
      "  Batch [650/1299] D_loss: 0.0272, G_loss: 0.3809\n",
      "  Batch [660/1299] D_loss: 0.0676, G_loss: 0.5779\n",
      "  Batch [670/1299] D_loss: -0.1414, G_loss: 0.4254\n",
      "  Batch [680/1299] D_loss: 0.0059, G_loss: 0.4334\n",
      "  Batch [690/1299] D_loss: -3.3991, G_loss: -5.5148\n",
      "  Batch [700/1299] D_loss: 0.0378, G_loss: 0.0697\n",
      "  Batch [710/1299] D_loss: -0.3671, G_loss: -0.4204\n",
      "  Batch [720/1299] D_loss: -0.1471, G_loss: 0.0911\n",
      "  Batch [730/1299] D_loss: -0.1526, G_loss: -0.0231\n",
      "  Batch [740/1299] D_loss: -1.4916, G_loss: -0.8030\n",
      "  Batch [750/1299] D_loss: -0.7425, G_loss: -0.4504\n",
      "  Batch [760/1299] D_loss: -0.1037, G_loss: 0.2644\n",
      "  Batch [770/1299] D_loss: -1.2208, G_loss: 0.3743\n",
      "  Batch [780/1299] D_loss: -0.0121, G_loss: 0.2467\n",
      "  Batch [790/1299] D_loss: -0.4455, G_loss: 0.1995\n",
      "  Batch [800/1299] D_loss: 0.0415, G_loss: 0.1682\n",
      "  Batch [810/1299] D_loss: -0.1947, G_loss: 0.1695\n",
      "  Batch [820/1299] D_loss: -3.5930, G_loss: -1.8993\n",
      "  Batch [830/1299] D_loss: -0.6046, G_loss: 0.1372\n",
      "  Batch [840/1299] D_loss: -0.3887, G_loss: -0.0278\n",
      "  Batch [850/1299] D_loss: -0.7854, G_loss: -1.1616\n",
      "  Batch [860/1299] D_loss: -0.3890, G_loss: -0.3150\n",
      "  Batch [870/1299] D_loss: -0.0930, G_loss: 0.2878\n",
      "  Batch [880/1299] D_loss: -0.1854, G_loss: 0.6266\n",
      "  Batch [890/1299] D_loss: -0.3067, G_loss: 0.8791\n",
      "  Batch [900/1299] D_loss: -0.1491, G_loss: 0.8070\n",
      "  Batch [910/1299] D_loss: -0.7955, G_loss: -0.0902\n",
      "  Batch [920/1299] D_loss: -0.6360, G_loss: -1.0255\n",
      "  Batch [930/1299] D_loss: -1.1693, G_loss: -0.6739\n",
      "  Batch [940/1299] D_loss: -1.0068, G_loss: -0.2827\n",
      "  Batch [950/1299] D_loss: -1.1281, G_loss: 0.0364\n",
      "  Batch [960/1299] D_loss: -0.6014, G_loss: 0.1348\n",
      "  Batch [970/1299] D_loss: 0.0741, G_loss: 0.1874\n",
      "  Batch [980/1299] D_loss: 0.0565, G_loss: 0.0993\n",
      "  Batch [990/1299] D_loss: -2.0921, G_loss: -1.4721\n",
      "  Batch [1000/1299] D_loss: -1.3294, G_loss: -0.8834\n",
      "  Batch [1010/1299] D_loss: -1.1603, G_loss: -3.1422\n",
      "  Batch [1020/1299] D_loss: -1.3226, G_loss: -0.0171\n",
      "  Batch [1030/1299] D_loss: -0.6397, G_loss: -0.1278\n",
      "  Batch [1040/1299] D_loss: -0.4744, G_loss: 0.1903\n",
      "  Batch [1050/1299] D_loss: -0.7009, G_loss: 0.2046\n",
      "  Batch [1060/1299] D_loss: -0.3025, G_loss: 0.1510\n",
      "  Batch [1070/1299] D_loss: -0.2913, G_loss: 0.5231\n",
      "  Batch [1080/1299] D_loss: -0.0482, G_loss: 0.6334\n",
      "  Batch [1090/1299] D_loss: -0.0638, G_loss: 0.5218\n",
      "  Batch [1100/1299] D_loss: 0.0322, G_loss: 0.3368\n",
      "  Batch [1110/1299] D_loss: -0.0584, G_loss: 0.4151\n",
      "  Batch [1120/1299] D_loss: -0.7011, G_loss: -0.3242\n",
      "  Batch [1130/1299] D_loss: -0.4661, G_loss: -0.4923\n",
      "  Batch [1140/1299] D_loss: -0.4118, G_loss: 0.0011\n",
      "  Batch [1150/1299] D_loss: -0.3824, G_loss: -0.2359\n",
      "  Batch [1160/1299] D_loss: -0.5046, G_loss: -0.2480\n",
      "  Batch [1170/1299] D_loss: 0.0412, G_loss: 0.4121\n",
      "  Batch [1180/1299] D_loss: -0.0803, G_loss: 0.4250\n",
      "  Batch [1190/1299] D_loss: -0.0114, G_loss: 0.6400\n",
      "  Batch [1200/1299] D_loss: 0.0445, G_loss: 0.3738\n",
      "  Batch [1210/1299] D_loss: -0.0057, G_loss: 0.4112\n",
      "  Batch [1220/1299] D_loss: -0.2159, G_loss: 0.1924\n",
      "  Batch [1230/1299] D_loss: -1.1651, G_loss: -1.4555\n",
      "  Batch [1240/1299] D_loss: -0.5773, G_loss: 0.0138\n",
      "  Batch [1250/1299] D_loss: -0.0802, G_loss: 0.1498\n",
      "  Batch [1260/1299] D_loss: -0.2817, G_loss: 0.0210\n",
      "  Batch [1270/1299] D_loss: -0.0185, G_loss: 0.1272\n",
      "  Batch [1280/1299] D_loss: -0.0149, G_loss: 0.0923\n",
      "  Batch [1290/1299] D_loss: -2.1616, G_loss: -2.7734\n",
      "\n",
      "Epoch 35 Summary:\n",
      "  Average D_loss: -0.1860\n",
      "  Average G_loss: -0.0772\n",
      "\n",
      "Epoch [36/100]\n",
      "  Batch [0/1299] D_loss: -0.8884, G_loss: 0.0365\n",
      "  Batch [10/1299] D_loss: -0.3200, G_loss: 0.0479\n",
      "  Batch [20/1299] D_loss: -0.1426, G_loss: 0.1326\n",
      "  Batch [30/1299] D_loss: -0.6174, G_loss: 0.0045\n",
      "  Batch [40/1299] D_loss: -0.1636, G_loss: 0.2701\n",
      "  Batch [50/1299] D_loss: -0.2161, G_loss: 0.5999\n",
      "  Batch [60/1299] D_loss: -0.4059, G_loss: 0.7098\n",
      "  Batch [70/1299] D_loss: -0.0393, G_loss: 0.9674\n",
      "  Batch [80/1299] D_loss: -0.0383, G_loss: 0.5207\n",
      "  Batch [90/1299] D_loss: -0.1852, G_loss: 0.4751\n",
      "  Batch [100/1299] D_loss: -7.5522, G_loss: -10.3991\n",
      "  Batch [110/1299] D_loss: -0.5384, G_loss: -0.0175\n",
      "  Batch [120/1299] D_loss: -0.0200, G_loss: 0.3464\n",
      "  Batch [130/1299] D_loss: -0.9096, G_loss: -0.3104\n",
      "  Batch [140/1299] D_loss: -1.1242, G_loss: 0.0560\n",
      "  Batch [150/1299] D_loss: -0.6638, G_loss: -0.2370\n",
      "  Batch [160/1299] D_loss: -1.3460, G_loss: 0.1005\n",
      "  Batch [170/1299] D_loss: -1.1141, G_loss: -1.6857\n",
      "  Batch [180/1299] D_loss: -0.8712, G_loss: -0.1245\n",
      "  Batch [190/1299] D_loss: -2.1361, G_loss: -2.6103\n",
      "  Batch [200/1299] D_loss: -1.2300, G_loss: -0.1151\n",
      "  Batch [210/1299] D_loss: -0.5026, G_loss: 0.1266\n",
      "  Batch [220/1299] D_loss: -0.1715, G_loss: 0.1817\n",
      "  Batch [230/1299] D_loss: -0.4357, G_loss: -0.3560\n",
      "  Batch [240/1299] D_loss: -0.0614, G_loss: 0.4436\n",
      "  Batch [250/1299] D_loss: -1.0187, G_loss: 0.1684\n",
      "  Batch [260/1299] D_loss: -0.1106, G_loss: 0.3501\n",
      "  Batch [270/1299] D_loss: -0.1578, G_loss: 0.6601\n",
      "  Batch [280/1299] D_loss: 0.0485, G_loss: 0.7911\n",
      "  Batch [290/1299] D_loss: -0.0672, G_loss: 0.6730\n",
      "  Batch [300/1299] D_loss: -0.0404, G_loss: 0.8400\n",
      "  Batch [310/1299] D_loss: -3.5262, G_loss: -5.2281\n",
      "  Batch [320/1299] D_loss: -1.5657, G_loss: -0.5015\n",
      "  Batch [330/1299] D_loss: -0.1811, G_loss: 0.2255\n",
      "  Batch [340/1299] D_loss: -1.0352, G_loss: 0.1159\n",
      "  Batch [350/1299] D_loss: -0.6685, G_loss: 0.0834\n",
      "  Batch [360/1299] D_loss: -0.0323, G_loss: 0.2085\n",
      "  Batch [370/1299] D_loss: -0.9633, G_loss: 0.1274\n",
      "  Batch [380/1299] D_loss: 0.0354, G_loss: 0.1962\n",
      "  Batch [390/1299] D_loss: -2.6495, G_loss: -1.3553\n",
      "  Batch [400/1299] D_loss: -0.2059, G_loss: 0.1965\n",
      "  Batch [410/1299] D_loss: -0.9537, G_loss: 0.1944\n",
      "  Batch [420/1299] D_loss: -0.2875, G_loss: 0.1656\n",
      "  Batch [430/1299] D_loss: -0.2980, G_loss: 0.2770\n",
      "  Batch [440/1299] D_loss: -0.1893, G_loss: 0.2627\n",
      "  Batch [450/1299] D_loss: -0.3932, G_loss: 0.5123\n",
      "  Batch [460/1299] D_loss: -0.3041, G_loss: 0.7332\n",
      "  Batch [470/1299] D_loss: 0.0478, G_loss: 1.0874\n",
      "  Batch [480/1299] D_loss: -0.2602, G_loss: 1.0203\n",
      "  Batch [490/1299] D_loss: 0.0363, G_loss: 0.7046\n",
      "  Batch [500/1299] D_loss: -0.0224, G_loss: 0.3933\n",
      "  Batch [510/1299] D_loss: -0.1181, G_loss: 0.1886\n",
      "  Batch [520/1299] D_loss: -1.2882, G_loss: -1.0685\n",
      "  Batch [530/1299] D_loss: -0.5189, G_loss: -0.1002\n",
      "  Batch [540/1299] D_loss: -0.7888, G_loss: -0.7106\n",
      "  Batch [550/1299] D_loss: -0.0754, G_loss: 0.3066\n",
      "  Batch [560/1299] D_loss: -0.1390, G_loss: 0.5008\n",
      "  Batch [570/1299] D_loss: -0.1287, G_loss: 0.7686\n",
      "  Batch [580/1299] D_loss: 0.0610, G_loss: 0.5653\n",
      "  Batch [590/1299] D_loss: -0.0152, G_loss: 0.4405\n",
      "  Batch [600/1299] D_loss: -0.0408, G_loss: 0.3958\n",
      "  Batch [610/1299] D_loss: -2.0353, G_loss: -2.6496\n",
      "  Batch [620/1299] D_loss: -0.0016, G_loss: 0.4862\n",
      "  Batch [630/1299] D_loss: -0.2682, G_loss: 0.5293\n",
      "  Batch [640/1299] D_loss: 0.0843, G_loss: 0.6089\n",
      "  Batch [650/1299] D_loss: 0.0617, G_loss: 0.4279\n",
      "  Batch [660/1299] D_loss: -0.1115, G_loss: 0.4029\n",
      "  Batch [670/1299] D_loss: 0.0004, G_loss: 0.4155\n",
      "  Batch [680/1299] D_loss: -0.4610, G_loss: -2.6517\n",
      "  Batch [690/1299] D_loss: -0.3453, G_loss: -0.2355\n",
      "  Batch [700/1299] D_loss: -0.6202, G_loss: -0.7465\n",
      "  Batch [710/1299] D_loss: -0.1541, G_loss: 0.2225\n",
      "  Batch [720/1299] D_loss: -0.2635, G_loss: 0.0554\n",
      "  Batch [730/1299] D_loss: -0.2173, G_loss: 0.3919\n",
      "  Batch [740/1299] D_loss: -0.0150, G_loss: 0.5638\n",
      "  Batch [750/1299] D_loss: -0.3689, G_loss: 0.4071\n",
      "  Batch [760/1299] D_loss: -1.0596, G_loss: 0.2234\n",
      "  Batch [770/1299] D_loss: -0.1447, G_loss: 0.1430\n",
      "  Batch [780/1299] D_loss: -0.0276, G_loss: 0.1337\n",
      "  Batch [790/1299] D_loss: -0.0648, G_loss: 0.1507\n",
      "  Batch [800/1299] D_loss: -1.8868, G_loss: -2.1000\n",
      "  Batch [810/1299] D_loss: -1.2389, G_loss: -0.9881\n",
      "  Batch [820/1299] D_loss: -0.3220, G_loss: -0.3755\n",
      "  Batch [830/1299] D_loss: -0.4205, G_loss: 0.0054\n",
      "  Batch [840/1299] D_loss: -0.1416, G_loss: 0.2818\n",
      "  Batch [850/1299] D_loss: -0.1310, G_loss: 0.6578\n",
      "  Batch [860/1299] D_loss: 0.0085, G_loss: 0.8615\n",
      "  Batch [870/1299] D_loss: 0.2018, G_loss: 0.7264\n",
      "  Batch [880/1299] D_loss: -0.1174, G_loss: 0.7359\n",
      "  Batch [890/1299] D_loss: 0.0326, G_loss: 0.4902\n",
      "  Batch [900/1299] D_loss: -0.0214, G_loss: 0.3266\n",
      "  Batch [910/1299] D_loss: -1.8273, G_loss: -4.2430\n",
      "  Batch [920/1299] D_loss: -0.4371, G_loss: -0.7489\n",
      "  Batch [930/1299] D_loss: -0.2294, G_loss: -0.2675\n",
      "  Batch [940/1299] D_loss: -0.4860, G_loss: 0.1087\n",
      "  Batch [950/1299] D_loss: -0.1482, G_loss: 0.2742\n",
      "  Batch [960/1299] D_loss: 0.1408, G_loss: 0.4617\n",
      "  Batch [970/1299] D_loss: -0.0249, G_loss: 0.5073\n",
      "  Batch [980/1299] D_loss: -3.6078, G_loss: -1.5204\n",
      "  Batch [990/1299] D_loss: -1.6077, G_loss: -3.2853\n",
      "  Batch [1000/1299] D_loss: -0.0528, G_loss: 0.2040\n",
      "  Batch [1010/1299] D_loss: -0.2939, G_loss: 0.4845\n",
      "  Batch [1020/1299] D_loss: -0.1830, G_loss: 0.7273\n",
      "  Batch [1030/1299] D_loss: 0.0107, G_loss: 0.5568\n",
      "  Batch [1040/1299] D_loss: -0.0850, G_loss: 0.3672\n",
      "  Batch [1050/1299] D_loss: -0.0693, G_loss: 0.4457\n",
      "  Batch [1060/1299] D_loss: -5.6459, G_loss: -8.0058\n",
      "  Batch [1070/1299] D_loss: -0.1692, G_loss: 0.2946\n",
      "  Batch [1080/1299] D_loss: -0.1795, G_loss: 0.4109\n",
      "  Batch [1090/1299] D_loss: -1.7856, G_loss: -2.0183\n",
      "  Batch [1100/1299] D_loss: -0.3475, G_loss: 0.0315\n",
      "  Batch [1110/1299] D_loss: -0.1859, G_loss: -0.3318\n",
      "  Batch [1120/1299] D_loss: -1.5485, G_loss: -0.9820\n",
      "  Batch [1130/1299] D_loss: -0.9389, G_loss: -1.7881\n",
      "  Batch [1140/1299] D_loss: -1.0996, G_loss: -0.8523\n",
      "  Batch [1150/1299] D_loss: -0.1515, G_loss: 0.3562\n",
      "  Batch [1160/1299] D_loss: -0.1956, G_loss: 0.7003\n",
      "  Batch [1170/1299] D_loss: -0.0349, G_loss: 0.7016\n",
      "  Batch [1180/1299] D_loss: -0.0455, G_loss: 0.6948\n",
      "  Batch [1190/1299] D_loss: -0.0491, G_loss: 0.6033\n",
      "  Batch [1200/1299] D_loss: 0.0514, G_loss: 0.2770\n",
      "  Batch [1210/1299] D_loss: -0.2117, G_loss: 0.4127\n",
      "  Batch [1220/1299] D_loss: -1.3602, G_loss: -3.1371\n",
      "  Batch [1230/1299] D_loss: -0.1789, G_loss: 0.0230\n",
      "  Batch [1240/1299] D_loss: -0.3810, G_loss: 0.0277\n",
      "  Batch [1250/1299] D_loss: -1.7732, G_loss: -4.3115\n",
      "  Batch [1260/1299] D_loss: -0.4016, G_loss: 0.1881\n",
      "  Batch [1270/1299] D_loss: 0.0104, G_loss: 0.1894\n",
      "  Batch [1280/1299] D_loss: -1.2661, G_loss: -0.0679\n",
      "  Batch [1290/1299] D_loss: -0.0797, G_loss: 0.2417\n",
      "\n",
      "Epoch 36 Summary:\n",
      "  Average D_loss: -0.2869\n",
      "  Average G_loss: -0.1023\n",
      "\n",
      "Epoch [37/100]\n",
      "  Batch [0/1299] D_loss: -0.1018, G_loss: 0.6392\n",
      "  Batch [10/1299] D_loss: -0.3222, G_loss: 0.8805\n",
      "  Batch [20/1299] D_loss: -0.0243, G_loss: 0.6568\n",
      "  Batch [30/1299] D_loss: -0.1625, G_loss: 0.7477\n",
      "  Batch [40/1299] D_loss: -0.0734, G_loss: 0.4976\n",
      "  Batch [50/1299] D_loss: -2.5916, G_loss: -1.3698\n",
      "  Batch [60/1299] D_loss: -0.9472, G_loss: -0.1248\n",
      "  Batch [70/1299] D_loss: -0.0139, G_loss: 0.3275\n",
      "  Batch [80/1299] D_loss: -0.0331, G_loss: 0.3171\n",
      "  Batch [90/1299] D_loss: -0.0937, G_loss: 0.3272\n",
      "  Batch [100/1299] D_loss: -0.1161, G_loss: 0.3663\n",
      "  Batch [110/1299] D_loss: -0.1516, G_loss: 0.3055\n",
      "  Batch [120/1299] D_loss: -0.7099, G_loss: -0.0229\n",
      "  Batch [130/1299] D_loss: -0.1612, G_loss: 0.1471\n",
      "  Batch [140/1299] D_loss: -0.3164, G_loss: 0.1265\n",
      "  Batch [150/1299] D_loss: -0.0711, G_loss: 0.3312\n",
      "  Batch [160/1299] D_loss: -0.2149, G_loss: 0.5423\n",
      "  Batch [170/1299] D_loss: -0.1738, G_loss: 0.4107\n",
      "  Batch [180/1299] D_loss: -0.1567, G_loss: 0.5789\n",
      "  Batch [190/1299] D_loss: -0.1159, G_loss: -9.0989\n",
      "  Batch [200/1299] D_loss: -0.6154, G_loss: -0.1249\n",
      "  Batch [210/1299] D_loss: -0.1182, G_loss: 0.0718\n",
      "  Batch [220/1299] D_loss: 0.0165, G_loss: 0.1554\n",
      "  Batch [230/1299] D_loss: -0.1809, G_loss: 0.4405\n",
      "  Batch [240/1299] D_loss: 0.0045, G_loss: 0.2805\n",
      "  Batch [250/1299] D_loss: -0.1994, G_loss: 0.2619\n",
      "  Batch [260/1299] D_loss: -0.1783, G_loss: 0.4473\n",
      "  Batch [270/1299] D_loss: 0.0718, G_loss: 0.6349\n",
      "  Batch [280/1299] D_loss: -1.5302, G_loss: -0.3579\n",
      "  Batch [290/1299] D_loss: -0.8011, G_loss: -0.0024\n",
      "  Batch [300/1299] D_loss: -1.2526, G_loss: -0.9119\n",
      "  Batch [310/1299] D_loss: -0.1362, G_loss: -0.0704\n",
      "  Batch [320/1299] D_loss: -0.6168, G_loss: -0.1158\n",
      "  Batch [330/1299] D_loss: -0.8976, G_loss: 0.1794\n",
      "  Batch [340/1299] D_loss: -0.8105, G_loss: -0.9929\n",
      "  Batch [350/1299] D_loss: -0.0149, G_loss: 0.2745\n",
      "  Batch [360/1299] D_loss: -0.0533, G_loss: 0.5086\n",
      "  Batch [370/1299] D_loss: -0.1507, G_loss: 0.5334\n",
      "  Batch [380/1299] D_loss: -0.1093, G_loss: 0.5344\n",
      "  Batch [390/1299] D_loss: -0.0934, G_loss: 0.4568\n",
      "  Batch [400/1299] D_loss: 0.0084, G_loss: 0.2810\n",
      "  Batch [410/1299] D_loss: -2.7519, G_loss: -8.3840\n",
      "  Batch [420/1299] D_loss: -1.4106, G_loss: -1.8847\n",
      "  Batch [430/1299] D_loss: 0.0252, G_loss: 0.3164\n",
      "  Batch [440/1299] D_loss: -0.0113, G_loss: 0.4917\n",
      "  Batch [450/1299] D_loss: -0.2343, G_loss: 0.5758\n",
      "  Batch [460/1299] D_loss: -0.0312, G_loss: 0.7180\n",
      "  Batch [470/1299] D_loss: -0.1041, G_loss: 1.0661\n",
      "  Batch [480/1299] D_loss: -0.1061, G_loss: 0.3559\n",
      "  Batch [490/1299] D_loss: -0.9794, G_loss: -2.0333\n",
      "  Batch [500/1299] D_loss: -0.4350, G_loss: 0.0209\n",
      "  Batch [510/1299] D_loss: -0.6033, G_loss: -0.0603\n",
      "  Batch [520/1299] D_loss: -0.3601, G_loss: 0.1554\n",
      "  Batch [530/1299] D_loss: -0.3248, G_loss: 0.0603\n",
      "  Batch [540/1299] D_loss: -1.2223, G_loss: -0.6437\n",
      "  Batch [550/1299] D_loss: -0.1050, G_loss: 0.1331\n",
      "  Batch [560/1299] D_loss: -1.1100, G_loss: 0.0798\n",
      "  Batch [570/1299] D_loss: -0.0864, G_loss: 0.0872\n",
      "  Batch [580/1299] D_loss: -0.4539, G_loss: -0.0500\n",
      "  Batch [590/1299] D_loss: -0.0553, G_loss: 0.2779\n",
      "  Batch [600/1299] D_loss: -0.2155, G_loss: 0.7246\n",
      "  Batch [610/1299] D_loss: -0.1748, G_loss: 0.7470\n",
      "  Batch [620/1299] D_loss: -0.0088, G_loss: 0.9136\n",
      "  Batch [630/1299] D_loss: -0.0224, G_loss: 0.5253\n",
      "  Batch [640/1299] D_loss: -0.0308, G_loss: 0.3053\n",
      "  Batch [650/1299] D_loss: 0.0045, G_loss: -0.7092\n",
      "  Batch [660/1299] D_loss: -0.6008, G_loss: -0.1374\n",
      "  Batch [670/1299] D_loss: -0.1469, G_loss: 0.4864\n",
      "  Batch [680/1299] D_loss: -0.0081, G_loss: 0.5853\n",
      "  Batch [690/1299] D_loss: -0.0805, G_loss: 0.6503\n",
      "  Batch [700/1299] D_loss: -0.1776, G_loss: 0.6026\n",
      "  Batch [710/1299] D_loss: 0.0119, G_loss: 0.3769\n",
      "  Batch [720/1299] D_loss: -5.3446, G_loss: -10.4306\n",
      "  Batch [730/1299] D_loss: 0.0039, G_loss: 0.1250\n",
      "  Batch [740/1299] D_loss: -0.1497, G_loss: 0.2824\n",
      "  Batch [750/1299] D_loss: -0.1486, G_loss: 0.4729\n",
      "  Batch [760/1299] D_loss: -0.1421, G_loss: 0.8540\n",
      "  Batch [770/1299] D_loss: 0.0506, G_loss: 0.5727\n",
      "  Batch [780/1299] D_loss: -0.0491, G_loss: 0.4453\n",
      "  Batch [790/1299] D_loss: -0.9884, G_loss: -5.8446\n",
      "  Batch [800/1299] D_loss: -0.1822, G_loss: -0.0109\n",
      "  Batch [810/1299] D_loss: 0.0382, G_loss: 0.3265\n",
      "  Batch [820/1299] D_loss: -0.3595, G_loss: 0.4733\n",
      "  Batch [830/1299] D_loss: -0.0131, G_loss: 0.4750\n",
      "  Batch [840/1299] D_loss: -0.0052, G_loss: 0.4112\n",
      "  Batch [850/1299] D_loss: -0.2279, G_loss: 0.4310\n",
      "  Batch [860/1299] D_loss: -2.7925, G_loss: -2.2252\n",
      "  Batch [870/1299] D_loss: -0.6464, G_loss: -0.2698\n",
      "  Batch [880/1299] D_loss: -0.4356, G_loss: 0.0005\n",
      "  Batch [890/1299] D_loss: -0.0603, G_loss: 0.1635\n",
      "  Batch [900/1299] D_loss: -0.4030, G_loss: 0.0913\n",
      "  Batch [910/1299] D_loss: -0.0911, G_loss: 0.4222\n",
      "  Batch [920/1299] D_loss: -0.1342, G_loss: 0.4725\n",
      "  Batch [930/1299] D_loss: 0.0432, G_loss: 0.4546\n",
      "  Batch [940/1299] D_loss: -0.0236, G_loss: 0.4849\n",
      "  Batch [950/1299] D_loss: -0.0091, G_loss: 0.4430\n",
      "  Batch [960/1299] D_loss: -4.7799, G_loss: -10.2090\n",
      "  Batch [970/1299] D_loss: -0.5664, G_loss: -0.4708\n",
      "  Batch [980/1299] D_loss: -0.6035, G_loss: -0.2162\n",
      "  Batch [990/1299] D_loss: -0.2289, G_loss: 0.1357\n",
      "  Batch [1000/1299] D_loss: -1.0516, G_loss: 0.2151\n",
      "  Batch [1010/1299] D_loss: -0.1419, G_loss: 0.0748\n",
      "  Batch [1020/1299] D_loss: 0.0319, G_loss: 0.1080\n",
      "  Batch [1030/1299] D_loss: 0.0088, G_loss: 0.2188\n",
      "  Batch [1040/1299] D_loss: -0.1829, G_loss: 0.4756\n",
      "  Batch [1050/1299] D_loss: -0.0500, G_loss: 0.7644\n",
      "  Batch [1060/1299] D_loss: -0.2501, G_loss: 0.6819\n",
      "  Batch [1070/1299] D_loss: -0.1659, G_loss: 0.3388\n",
      "  Batch [1080/1299] D_loss: -1.6139, G_loss: -0.3583\n",
      "  Batch [1090/1299] D_loss: -1.1025, G_loss: -1.0259\n",
      "  Batch [1100/1299] D_loss: -0.4634, G_loss: -0.2448\n",
      "  Batch [1110/1299] D_loss: -0.7565, G_loss: -0.6845\n",
      "  Batch [1120/1299] D_loss: -0.1865, G_loss: 0.1173\n",
      "  Batch [1130/1299] D_loss: -0.0914, G_loss: 0.2168\n",
      "  Batch [1140/1299] D_loss: -0.0899, G_loss: 0.3139\n",
      "  Batch [1150/1299] D_loss: -0.0363, G_loss: 0.4415\n",
      "  Batch [1160/1299] D_loss: -0.0250, G_loss: 0.7359\n",
      "  Batch [1170/1299] D_loss: 0.0812, G_loss: 0.5664\n",
      "  Batch [1180/1299] D_loss: -1.0280, G_loss: -0.2049\n",
      "  Batch [1190/1299] D_loss: -0.2615, G_loss: -0.7204\n",
      "  Batch [1200/1299] D_loss: -0.0520, G_loss: 0.2106\n",
      "  Batch [1210/1299] D_loss: 0.0325, G_loss: 0.3088\n",
      "  Batch [1220/1299] D_loss: -0.1080, G_loss: 0.3109\n",
      "  Batch [1230/1299] D_loss: -0.0094, G_loss: 0.3240\n",
      "  Batch [1240/1299] D_loss: -0.0361, G_loss: 0.2680\n",
      "  Batch [1250/1299] D_loss: -0.0016, G_loss: 0.2348\n",
      "  Batch [1260/1299] D_loss: -0.3877, G_loss: 0.1898\n",
      "  Batch [1270/1299] D_loss: -0.1506, G_loss: 0.0328\n",
      "  Batch [1280/1299] D_loss: -0.2386, G_loss: 0.0140\n",
      "  Batch [1290/1299] D_loss: -2.1836, G_loss: -1.9124\n",
      "\n",
      "Epoch 37 Summary:\n",
      "  Average D_loss: -0.2179\n",
      "  Average G_loss: -0.0976\n",
      "\n",
      "Epoch [38/100]\n",
      "  Batch [0/1299] D_loss: -0.1472, G_loss: -0.0112\n",
      "  Batch [10/1299] D_loss: -0.3804, G_loss: 0.0898\n",
      "  Batch [20/1299] D_loss: -2.1432, G_loss: -2.0019\n",
      "  Batch [30/1299] D_loss: -0.0158, G_loss: 0.2888\n",
      "  Batch [40/1299] D_loss: -1.0513, G_loss: 0.1447\n",
      "  Batch [50/1299] D_loss: -0.9410, G_loss: -0.0551\n",
      "  Batch [60/1299] D_loss: -0.1303, G_loss: 0.1619\n",
      "  Batch [70/1299] D_loss: -0.1564, G_loss: 0.2442\n",
      "  Batch [80/1299] D_loss: -0.0726, G_loss: 0.4416\n",
      "  Batch [90/1299] D_loss: -0.1369, G_loss: 0.8026\n",
      "  Batch [100/1299] D_loss: -0.0479, G_loss: 0.8099\n",
      "  Batch [110/1299] D_loss: -0.3923, G_loss: 0.7895\n",
      "  Batch [120/1299] D_loss: -0.0784, G_loss: 0.6159\n",
      "  Batch [130/1299] D_loss: -0.0806, G_loss: 0.3603\n",
      "  Batch [140/1299] D_loss: -1.5634, G_loss: -5.1149\n",
      "  Batch [150/1299] D_loss: -0.8287, G_loss: -0.0029\n",
      "  Batch [160/1299] D_loss: -0.7233, G_loss: 0.0680\n",
      "  Batch [170/1299] D_loss: -0.6722, G_loss: 0.0730\n",
      "  Batch [180/1299] D_loss: -0.6125, G_loss: 0.0135\n",
      "  Batch [190/1299] D_loss: -0.2083, G_loss: 0.1341\n",
      "  Batch [200/1299] D_loss: -0.7858, G_loss: -0.9661\n",
      "  Batch [210/1299] D_loss: -0.2257, G_loss: 0.1906\n",
      "  Batch [220/1299] D_loss: -1.0578, G_loss: 0.1881\n",
      "  Batch [230/1299] D_loss: -1.0557, G_loss: 0.2802\n",
      "  Batch [240/1299] D_loss: -0.1650, G_loss: 0.2378\n",
      "  Batch [250/1299] D_loss: -2.8432, G_loss: -3.9641\n",
      "  Batch [260/1299] D_loss: -1.5241, G_loss: -1.6946\n",
      "  Batch [270/1299] D_loss: 0.0085, G_loss: 0.1385\n",
      "  Batch [280/1299] D_loss: -0.2514, G_loss: 0.1498\n",
      "  Batch [290/1299] D_loss: -0.2662, G_loss: 0.1125\n",
      "  Batch [300/1299] D_loss: -0.5626, G_loss: 0.2947\n",
      "  Batch [310/1299] D_loss: -0.0383, G_loss: 0.6556\n",
      "  Batch [320/1299] D_loss: 0.0653, G_loss: 0.5708\n",
      "  Batch [330/1299] D_loss: -0.0653, G_loss: 0.5578\n",
      "  Batch [340/1299] D_loss: -0.0157, G_loss: 0.5610\n",
      "  Batch [350/1299] D_loss: -0.0045, G_loss: 0.2920\n",
      "  Batch [360/1299] D_loss: -2.3694, G_loss: -2.9247\n",
      "  Batch [370/1299] D_loss: 0.0104, G_loss: 0.0530\n",
      "  Batch [380/1299] D_loss: -0.0467, G_loss: 0.3932\n",
      "  Batch [390/1299] D_loss: -0.0959, G_loss: 0.3403\n",
      "  Batch [400/1299] D_loss: -0.1626, G_loss: 0.4320\n",
      "  Batch [410/1299] D_loss: -0.2235, G_loss: 0.5263\n",
      "  Batch [420/1299] D_loss: -0.0367, G_loss: 0.3577\n",
      "  Batch [430/1299] D_loss: -0.0920, G_loss: 0.3109\n",
      "  Batch [440/1299] D_loss: -3.2866, G_loss: -6.5072\n",
      "  Batch [450/1299] D_loss: 0.0035, G_loss: 0.2231\n",
      "  Batch [460/1299] D_loss: -0.0924, G_loss: 0.3797\n",
      "  Batch [470/1299] D_loss: 0.0002, G_loss: 0.3931\n",
      "  Batch [480/1299] D_loss: 0.0135, G_loss: 0.6246\n",
      "  Batch [490/1299] D_loss: -0.2228, G_loss: 0.5588\n",
      "  Batch [500/1299] D_loss: -0.0101, G_loss: 0.4701\n",
      "  Batch [510/1299] D_loss: -3.1886, G_loss: -8.6987\n",
      "  Batch [520/1299] D_loss: -0.2075, G_loss: -0.0605\n",
      "  Batch [530/1299] D_loss: -0.2812, G_loss: -0.0394\n",
      "  Batch [540/1299] D_loss: -0.1186, G_loss: -0.3719\n",
      "  Batch [550/1299] D_loss: -0.2358, G_loss: -0.1458\n",
      "  Batch [560/1299] D_loss: -0.1074, G_loss: 0.3066\n",
      "  Batch [570/1299] D_loss: -0.0696, G_loss: 0.4776\n",
      "  Batch [580/1299] D_loss: -0.1539, G_loss: 0.5891\n",
      "  Batch [590/1299] D_loss: -0.1787, G_loss: 0.8793\n",
      "  Batch [600/1299] D_loss: -0.1601, G_loss: 0.7000\n",
      "  Batch [610/1299] D_loss: 0.0094, G_loss: 0.3574\n",
      "  Batch [620/1299] D_loss: -0.0273, G_loss: 0.1799\n",
      "  Batch [630/1299] D_loss: -0.7046, G_loss: -2.1864\n",
      "  Batch [640/1299] D_loss: -0.6543, G_loss: 0.0969\n",
      "  Batch [650/1299] D_loss: -0.1296, G_loss: 0.1462\n",
      "  Batch [660/1299] D_loss: -0.0390, G_loss: 0.2140\n",
      "  Batch [670/1299] D_loss: -0.7989, G_loss: 0.0858\n",
      "  Batch [680/1299] D_loss: 0.0523, G_loss: 0.2037\n",
      "  Batch [690/1299] D_loss: -0.0336, G_loss: 0.1724\n",
      "  Batch [700/1299] D_loss: -0.7206, G_loss: 0.1772\n",
      "  Batch [710/1299] D_loss: -1.0574, G_loss: -0.1256\n",
      "  Batch [720/1299] D_loss: -0.7799, G_loss: -0.1688\n",
      "  Batch [730/1299] D_loss: -0.3617, G_loss: -0.4447\n",
      "  Batch [740/1299] D_loss: -0.0699, G_loss: 0.3270\n",
      "  Batch [750/1299] D_loss: -0.2496, G_loss: 0.5413\n",
      "  Batch [760/1299] D_loss: -0.1011, G_loss: 0.6200\n",
      "  Batch [770/1299] D_loss: -0.0269, G_loss: 0.4927\n",
      "  Batch [780/1299] D_loss: -0.0343, G_loss: 0.3049\n",
      "  Batch [790/1299] D_loss: -0.0277, G_loss: 0.3230\n",
      "  Batch [800/1299] D_loss: -1.3985, G_loss: -2.5444\n",
      "  Batch [810/1299] D_loss: -0.8605, G_loss: -0.8862\n",
      "  Batch [820/1299] D_loss: -0.4234, G_loss: -0.0241\n",
      "  Batch [830/1299] D_loss: -0.7568, G_loss: 0.1948\n",
      "  Batch [840/1299] D_loss: 0.0473, G_loss: 0.1231\n",
      "  Batch [850/1299] D_loss: -0.5231, G_loss: -0.1169\n",
      "  Batch [860/1299] D_loss: -0.0210, G_loss: 0.3550\n",
      "  Batch [870/1299] D_loss: -0.2529, G_loss: 0.4928\n",
      "  Batch [880/1299] D_loss: -0.2283, G_loss: 0.5335\n",
      "  Batch [890/1299] D_loss: -0.1083, G_loss: 0.5197\n",
      "  Batch [900/1299] D_loss: -0.0179, G_loss: 0.5306\n",
      "  Batch [910/1299] D_loss: -2.4519, G_loss: -5.3851\n",
      "  Batch [920/1299] D_loss: -0.6220, G_loss: -0.6064\n",
      "  Batch [930/1299] D_loss: -0.0578, G_loss: 0.2713\n",
      "  Batch [940/1299] D_loss: -0.1508, G_loss: 0.4719\n",
      "  Batch [950/1299] D_loss: 0.0037, G_loss: 0.3171\n",
      "  Batch [960/1299] D_loss: -0.0968, G_loss: 0.5181\n",
      "  Batch [970/1299] D_loss: 0.0356, G_loss: 0.3585\n",
      "  Batch [980/1299] D_loss: -0.2911, G_loss: -1.0987\n",
      "  Batch [990/1299] D_loss: -0.8051, G_loss: -1.2366\n",
      "  Batch [1000/1299] D_loss: -0.2649, G_loss: 0.0349\n",
      "  Batch [1010/1299] D_loss: -0.5344, G_loss: -0.2377\n",
      "  Batch [1020/1299] D_loss: -0.6152, G_loss: -0.4067\n",
      "  Batch [1030/1299] D_loss: -1.0257, G_loss: -1.5170\n",
      "  Batch [1040/1299] D_loss: -0.9915, G_loss: -0.6009\n",
      "  Batch [1050/1299] D_loss: -0.1190, G_loss: 0.2318\n",
      "  Batch [1060/1299] D_loss: -0.1112, G_loss: 0.1062\n",
      "  Batch [1070/1299] D_loss: -0.5309, G_loss: 0.1287\n",
      "  Batch [1080/1299] D_loss: -0.1627, G_loss: 0.1100\n",
      "  Batch [1090/1299] D_loss: -1.3960, G_loss: 0.2262\n",
      "  Batch [1100/1299] D_loss: -1.0998, G_loss: 0.0472\n",
      "  Batch [1110/1299] D_loss: -0.0074, G_loss: 0.2805\n",
      "  Batch [1120/1299] D_loss: 0.0661, G_loss: 0.1233\n",
      "  Batch [1130/1299] D_loss: -2.1185, G_loss: 0.1397\n",
      "  Batch [1140/1299] D_loss: -1.2771, G_loss: -0.0579\n",
      "  Batch [1150/1299] D_loss: -0.1268, G_loss: 0.5386\n",
      "  Batch [1160/1299] D_loss: -0.0132, G_loss: 0.4468\n",
      "  Batch [1170/1299] D_loss: -0.0064, G_loss: 0.6432\n",
      "  Batch [1180/1299] D_loss: 0.0269, G_loss: 1.0053\n",
      "  Batch [1190/1299] D_loss: 0.1173, G_loss: 0.6733\n",
      "  Batch [1200/1299] D_loss: -0.0290, G_loss: 0.3172\n",
      "  Batch [1210/1299] D_loss: -0.0298, G_loss: 0.2210\n",
      "  Batch [1220/1299] D_loss: -0.5954, G_loss: -2.5058\n",
      "  Batch [1230/1299] D_loss: -0.0636, G_loss: 0.1006\n",
      "  Batch [1240/1299] D_loss: -0.4272, G_loss: -0.0001\n",
      "  Batch [1250/1299] D_loss: -0.0377, G_loss: -0.0009\n",
      "  Batch [1260/1299] D_loss: -0.5528, G_loss: -0.2334\n",
      "  Batch [1270/1299] D_loss: -0.1276, G_loss: 0.1172\n",
      "  Batch [1280/1299] D_loss: -0.0562, G_loss: 0.1058\n",
      "  Batch [1290/1299] D_loss: 0.0040, G_loss: 0.3318\n",
      "\n",
      "Epoch 38 Summary:\n",
      "  Average D_loss: -0.2536\n",
      "  Average G_loss: -0.1070\n",
      "\n",
      "Epoch [39/100]\n",
      "  Batch [0/1299] D_loss: -0.6380, G_loss: -0.3720\n",
      "  Batch [10/1299] D_loss: -0.0147, G_loss: 0.4283\n",
      "  Batch [20/1299] D_loss: -0.1368, G_loss: 0.5324\n",
      "  Batch [30/1299] D_loss: -0.0016, G_loss: 0.4577\n",
      "  Batch [40/1299] D_loss: -0.0505, G_loss: 0.4146\n",
      "  Batch [50/1299] D_loss: -0.0265, G_loss: 0.4176\n",
      "  Batch [60/1299] D_loss: -0.0281, G_loss: 0.0673\n",
      "  Batch [70/1299] D_loss: -0.2561, G_loss: 0.0546\n",
      "  Batch [80/1299] D_loss: 0.0576, G_loss: 0.1777\n",
      "  Batch [90/1299] D_loss: -0.4819, G_loss: 0.0487\n",
      "  Batch [100/1299] D_loss: -0.1126, G_loss: 0.1656\n",
      "  Batch [110/1299] D_loss: 0.0326, G_loss: 0.3006\n",
      "  Batch [120/1299] D_loss: -0.0235, G_loss: 0.6096\n",
      "  Batch [130/1299] D_loss: -0.1038, G_loss: 0.4658\n",
      "  Batch [140/1299] D_loss: -0.0940, G_loss: 0.3874\n",
      "  Batch [150/1299] D_loss: -0.1513, G_loss: 0.3585\n",
      "  Batch [160/1299] D_loss: -0.2473, G_loss: -0.1890\n",
      "  Batch [170/1299] D_loss: -0.0196, G_loss: 0.1105\n",
      "  Batch [180/1299] D_loss: -1.8392, G_loss: -0.0323\n",
      "  Batch [190/1299] D_loss: -0.0953, G_loss: 0.1551\n",
      "  Batch [200/1299] D_loss: -0.1426, G_loss: 0.1590\n",
      "  Batch [210/1299] D_loss: -0.4081, G_loss: 0.2207\n",
      "  Batch [220/1299] D_loss: -0.6348, G_loss: 0.1573\n",
      "  Batch [230/1299] D_loss: -0.7237, G_loss: 0.0134\n",
      "  Batch [240/1299] D_loss: -0.2135, G_loss: 0.4278\n",
      "  Batch [250/1299] D_loss: -0.0166, G_loss: 0.6678\n",
      "  Batch [260/1299] D_loss: 0.0625, G_loss: 0.4894\n",
      "  Batch [270/1299] D_loss: 0.0342, G_loss: 0.5662\n",
      "  Batch [280/1299] D_loss: -0.0381, G_loss: 0.3088\n",
      "  Batch [290/1299] D_loss: -2.4342, G_loss: -2.9866\n",
      "  Batch [300/1299] D_loss: -1.7858, G_loss: -4.1981\n",
      "  Batch [310/1299] D_loss: -0.2094, G_loss: 0.1538\n",
      "  Batch [320/1299] D_loss: -0.0956, G_loss: 0.2474\n",
      "  Batch [330/1299] D_loss: -0.3908, G_loss: 0.2000\n",
      "  Batch [340/1299] D_loss: -0.6498, G_loss: 0.3791\n",
      "  Batch [350/1299] D_loss: -0.4018, G_loss: 0.2863\n",
      "  Batch [360/1299] D_loss: 0.0364, G_loss: 0.5055\n",
      "  Batch [370/1299] D_loss: -0.4605, G_loss: 0.2579\n",
      "  Batch [380/1299] D_loss: -0.7214, G_loss: -3.4622\n",
      "  Batch [390/1299] D_loss: -0.6837, G_loss: 0.0358\n",
      "  Batch [400/1299] D_loss: -0.4872, G_loss: -1.0089\n",
      "  Batch [410/1299] D_loss: -0.0852, G_loss: 0.3139\n",
      "  Batch [420/1299] D_loss: -0.0636, G_loss: 0.4766\n",
      "  Batch [430/1299] D_loss: 0.0049, G_loss: 0.4360\n",
      "  Batch [440/1299] D_loss: -0.0849, G_loss: 0.6939\n",
      "  Batch [450/1299] D_loss: 0.1395, G_loss: 0.4027\n",
      "  Batch [460/1299] D_loss: -0.1599, G_loss: 0.4622\n",
      "  Batch [470/1299] D_loss: -0.1043, G_loss: 0.3124\n",
      "  Batch [480/1299] D_loss: -0.5080, G_loss: -0.0808\n",
      "  Batch [490/1299] D_loss: -0.0583, G_loss: 0.0490\n",
      "  Batch [500/1299] D_loss: -0.0828, G_loss: 0.1745\n",
      "  Batch [510/1299] D_loss: -0.2379, G_loss: 0.0548\n",
      "  Batch [520/1299] D_loss: -0.7129, G_loss: -0.5256\n",
      "  Batch [530/1299] D_loss: -0.3837, G_loss: -0.1639\n",
      "  Batch [540/1299] D_loss: -0.5216, G_loss: -0.9159\n",
      "  Batch [550/1299] D_loss: -0.1328, G_loss: 0.2348\n",
      "  Batch [560/1299] D_loss: -0.9397, G_loss: 0.0002\n",
      "  Batch [570/1299] D_loss: -0.2093, G_loss: -0.0778\n",
      "  Batch [580/1299] D_loss: -0.8886, G_loss: -0.0264\n",
      "  Batch [590/1299] D_loss: -0.3803, G_loss: -0.2947\n",
      "  Batch [600/1299] D_loss: -0.4540, G_loss: 0.1849\n",
      "  Batch [610/1299] D_loss: -0.1474, G_loss: 0.4815\n",
      "  Batch [620/1299] D_loss: -0.3123, G_loss: 0.5868\n",
      "  Batch [630/1299] D_loss: -0.2369, G_loss: 0.8365\n",
      "  Batch [640/1299] D_loss: 0.0397, G_loss: 0.5712\n",
      "  Batch [650/1299] D_loss: -0.0629, G_loss: 0.5245\n",
      "  Batch [660/1299] D_loss: -0.0420, G_loss: 0.2794\n",
      "  Batch [670/1299] D_loss: -1.1954, G_loss: -3.4966\n",
      "  Batch [680/1299] D_loss: -1.3816, G_loss: 0.0232\n",
      "  Batch [690/1299] D_loss: -0.3432, G_loss: -0.2124\n",
      "  Batch [700/1299] D_loss: 0.0951, G_loss: 0.2374\n",
      "  Batch [710/1299] D_loss: -0.3191, G_loss: 0.0264\n",
      "  Batch [720/1299] D_loss: -0.0732, G_loss: 0.2689\n",
      "  Batch [730/1299] D_loss: -0.5955, G_loss: -2.8283\n",
      "  Batch [740/1299] D_loss: -0.2345, G_loss: -0.0339\n",
      "  Batch [750/1299] D_loss: -0.7037, G_loss: 0.0216\n",
      "  Batch [760/1299] D_loss: -1.2554, G_loss: -0.8079\n",
      "  Batch [770/1299] D_loss: -0.2175, G_loss: 0.3085\n",
      "  Batch [780/1299] D_loss: -0.0574, G_loss: 0.5603\n",
      "  Batch [790/1299] D_loss: -0.1821, G_loss: 0.5782\n",
      "  Batch [800/1299] D_loss: -0.0748, G_loss: 0.9247\n",
      "  Batch [810/1299] D_loss: -0.2082, G_loss: 0.7032\n",
      "  Batch [820/1299] D_loss: 0.0247, G_loss: 0.4519\n",
      "  Batch [830/1299] D_loss: -0.2273, G_loss: 0.2168\n",
      "  Batch [840/1299] D_loss: -0.0708, G_loss: 0.0585\n",
      "  Batch [850/1299] D_loss: -0.1035, G_loss: 0.2845\n",
      "  Batch [860/1299] D_loss: -0.2421, G_loss: 0.1536\n",
      "  Batch [870/1299] D_loss: 0.0708, G_loss: 0.3290\n",
      "  Batch [880/1299] D_loss: 0.0830, G_loss: 0.6334\n",
      "  Batch [890/1299] D_loss: -0.0242, G_loss: 0.4002\n",
      "  Batch [900/1299] D_loss: -0.0375, G_loss: 0.4724\n",
      "  Batch [910/1299] D_loss: -0.1775, G_loss: 0.1954\n",
      "  Batch [920/1299] D_loss: -0.2830, G_loss: -0.2363\n",
      "  Batch [930/1299] D_loss: -1.3466, G_loss: -5.1616\n",
      "  Batch [940/1299] D_loss: -0.5300, G_loss: -0.5861\n",
      "  Batch [950/1299] D_loss: -0.6482, G_loss: -0.4483\n",
      "  Batch [960/1299] D_loss: -0.0412, G_loss: 0.1897\n",
      "  Batch [970/1299] D_loss: -0.8945, G_loss: -0.1655\n",
      "  Batch [980/1299] D_loss: -0.0202, G_loss: 0.3016\n",
      "  Batch [990/1299] D_loss: -0.9063, G_loss: 0.1577\n",
      "  Batch [1000/1299] D_loss: -0.0069, G_loss: 0.1961\n",
      "  Batch [1010/1299] D_loss: -0.0837, G_loss: 0.2050\n",
      "  Batch [1020/1299] D_loss: -0.1220, G_loss: 0.3106\n",
      "  Batch [1030/1299] D_loss: -0.1199, G_loss: 0.3364\n",
      "  Batch [1040/1299] D_loss: -0.0145, G_loss: 0.2456\n",
      "  Batch [1050/1299] D_loss: 0.0121, G_loss: 0.1546\n",
      "  Batch [1060/1299] D_loss: -1.0517, G_loss: -5.8036\n",
      "  Batch [1070/1299] D_loss: -0.0702, G_loss: 0.2469\n",
      "  Batch [1080/1299] D_loss: 0.0936, G_loss: 0.2704\n",
      "  Batch [1090/1299] D_loss: -0.2293, G_loss: 0.2765\n",
      "  Batch [1100/1299] D_loss: -0.1147, G_loss: 0.4332\n",
      "  Batch [1110/1299] D_loss: 0.0582, G_loss: 0.1596\n",
      "  Batch [1120/1299] D_loss: 0.0081, G_loss: 0.1190\n",
      "  Batch [1130/1299] D_loss: -0.1233, G_loss: 0.1528\n",
      "  Batch [1140/1299] D_loss: -0.2081, G_loss: -0.1018\n",
      "  Batch [1150/1299] D_loss: -0.6951, G_loss: -1.7163\n",
      "  Batch [1160/1299] D_loss: -0.0820, G_loss: 0.1915\n",
      "  Batch [1170/1299] D_loss: -0.0036, G_loss: 0.4633\n",
      "  Batch [1180/1299] D_loss: 0.0178, G_loss: 0.6701\n",
      "  Batch [1190/1299] D_loss: -0.0050, G_loss: 0.3179\n",
      "  Batch [1200/1299] D_loss: -0.0299, G_loss: 0.3002\n",
      "  Batch [1210/1299] D_loss: -1.2788, G_loss: -1.9073\n",
      "  Batch [1220/1299] D_loss: -0.4006, G_loss: -0.7182\n",
      "  Batch [1230/1299] D_loss: -0.0856, G_loss: 0.2553\n",
      "  Batch [1240/1299] D_loss: -0.1062, G_loss: 0.1860\n",
      "  Batch [1250/1299] D_loss: -0.0558, G_loss: 0.4328\n",
      "  Batch [1260/1299] D_loss: -0.1702, G_loss: -0.0040\n",
      "  Batch [1270/1299] D_loss: 0.0513, G_loss: 0.0911\n",
      "  Batch [1280/1299] D_loss: -0.0683, G_loss: 0.1860\n",
      "  Batch [1290/1299] D_loss: -0.0870, G_loss: 0.4586\n",
      "\n",
      "Epoch 39 Summary:\n",
      "  Average D_loss: -0.1711\n",
      "  Average G_loss: -0.0776\n",
      "\n",
      "Epoch [40/100]\n",
      "  Batch [0/1299] D_loss: -0.0062, G_loss: 0.3086\n",
      "  Batch [10/1299] D_loss: 0.0255, G_loss: 0.4696\n",
      "  Batch [20/1299] D_loss: 0.0272, G_loss: 0.2987\n",
      "  Batch [30/1299] D_loss: 2.0313, G_loss: -1.5893\n",
      "  Batch [40/1299] D_loss: -0.6770, G_loss: -0.5930\n",
      "  Batch [50/1299] D_loss: -0.8415, G_loss: -0.4642\n",
      "  Batch [60/1299] D_loss: -0.1342, G_loss: 0.2089\n",
      "  Batch [70/1299] D_loss: -0.1732, G_loss: 0.4418\n",
      "  Batch [80/1299] D_loss: -0.1735, G_loss: 0.5198\n",
      "  Batch [90/1299] D_loss: 0.0904, G_loss: 0.4843\n",
      "  Batch [100/1299] D_loss: 0.0671, G_loss: 0.3884\n",
      "  Batch [110/1299] D_loss: -0.0931, G_loss: 0.4624\n",
      "  Batch [120/1299] D_loss: -0.0993, G_loss: 0.1993\n",
      "  Batch [130/1299] D_loss: -0.8231, G_loss: -1.7727\n",
      "  Batch [140/1299] D_loss: -0.6013, G_loss: -0.1168\n",
      "  Batch [150/1299] D_loss: -0.0096, G_loss: 0.1617\n",
      "  Batch [160/1299] D_loss: 0.0053, G_loss: 0.2992\n",
      "  Batch [170/1299] D_loss: -0.0761, G_loss: 0.1666\n",
      "  Batch [180/1299] D_loss: -0.0564, G_loss: 0.2839\n",
      "  Batch [190/1299] D_loss: -0.1102, G_loss: 0.2097\n",
      "  Batch [200/1299] D_loss: -0.8581, G_loss: -0.3605\n",
      "  Batch [210/1299] D_loss: -0.1890, G_loss: -0.0733\n",
      "  Batch [220/1299] D_loss: -0.1824, G_loss: -0.5226\n",
      "  Batch [230/1299] D_loss: -0.0776, G_loss: 0.1084\n",
      "  Batch [240/1299] D_loss: -1.4043, G_loss: -1.0989\n",
      "  Batch [250/1299] D_loss: -0.5597, G_loss: -0.1132\n",
      "  Batch [260/1299] D_loss: -0.2941, G_loss: 0.1049\n",
      "  Batch [270/1299] D_loss: -0.6962, G_loss: 0.1418\n",
      "  Batch [280/1299] D_loss: -0.9846, G_loss: 0.0913\n",
      "  Batch [290/1299] D_loss: 0.0529, G_loss: 0.3197\n",
      "  Batch [300/1299] D_loss: -0.0839, G_loss: 0.2828\n",
      "  Batch [310/1299] D_loss: -0.7119, G_loss: -1.0053\n",
      "  Batch [320/1299] D_loss: -0.2024, G_loss: 0.1318\n",
      "  Batch [330/1299] D_loss: -0.3902, G_loss: -0.1906\n",
      "  Batch [340/1299] D_loss: -0.0157, G_loss: 0.2678\n",
      "  Batch [350/1299] D_loss: -0.0692, G_loss: 0.4046\n",
      "  Batch [360/1299] D_loss: 0.0013, G_loss: 0.5157\n",
      "  Batch [370/1299] D_loss: -0.2345, G_loss: 0.4274\n",
      "  Batch [380/1299] D_loss: -1.2658, G_loss: -0.8980\n",
      "  Batch [390/1299] D_loss: -0.6938, G_loss: 0.0507\n",
      "  Batch [400/1299] D_loss: -0.0216, G_loss: 0.1184\n",
      "  Batch [410/1299] D_loss: -0.5246, G_loss: -0.0506\n",
      "  Batch [420/1299] D_loss: -0.0997, G_loss: 0.1253\n",
      "  Batch [430/1299] D_loss: -0.0928, G_loss: 0.5429\n",
      "  Batch [440/1299] D_loss: 0.0651, G_loss: 0.4039\n",
      "  Batch [450/1299] D_loss: 0.0992, G_loss: 0.7455\n",
      "  Batch [460/1299] D_loss: -0.0350, G_loss: 0.4610\n",
      "  Batch [470/1299] D_loss: 0.0814, G_loss: 0.4302\n",
      "  Batch [480/1299] D_loss: -0.0577, G_loss: 0.3812\n",
      "  Batch [490/1299] D_loss: -4.2672, G_loss: -4.9382\n",
      "  Batch [500/1299] D_loss: -0.5784, G_loss: -0.1715\n",
      "  Batch [510/1299] D_loss: -0.2710, G_loss: 0.1211\n",
      "  Batch [520/1299] D_loss: -0.0490, G_loss: 0.3052\n",
      "  Batch [530/1299] D_loss: 0.1076, G_loss: 0.4720\n",
      "  Batch [540/1299] D_loss: -0.0764, G_loss: 0.5224\n",
      "  Batch [550/1299] D_loss: -0.0015, G_loss: 0.4711\n",
      "  Batch [560/1299] D_loss: -0.0418, G_loss: 0.4222\n",
      "  Batch [570/1299] D_loss: -0.0035, G_loss: 0.1963\n",
      "  Batch [580/1299] D_loss: -2.8224, G_loss: -4.0771\n",
      "  Batch [590/1299] D_loss: -0.5199, G_loss: -0.1207\n",
      "  Batch [600/1299] D_loss: -0.5959, G_loss: -0.7915\n",
      "  Batch [610/1299] D_loss: -0.0863, G_loss: 0.0716\n",
      "  Batch [620/1299] D_loss: -0.1464, G_loss: 0.0072\n",
      "  Batch [630/1299] D_loss: -1.2432, G_loss: -2.3872\n",
      "  Batch [640/1299] D_loss: -1.5742, G_loss: -0.7372\n",
      "  Batch [650/1299] D_loss: -0.7503, G_loss: -0.6101\n",
      "  Batch [660/1299] D_loss: -0.8638, G_loss: -0.1522\n",
      "  Batch [670/1299] D_loss: -0.2300, G_loss: 0.0862\n",
      "  Batch [680/1299] D_loss: -0.1098, G_loss: 0.4240\n",
      "  Batch [690/1299] D_loss: -0.1219, G_loss: 0.6548\n",
      "  Batch [700/1299] D_loss: -0.2863, G_loss: 0.9186\n",
      "  Batch [710/1299] D_loss: -0.0121, G_loss: 0.9999\n",
      "  Batch [720/1299] D_loss: -0.0116, G_loss: 0.7154\n",
      "  Batch [730/1299] D_loss: -0.0182, G_loss: 0.3944\n",
      "  Batch [740/1299] D_loss: 0.0178, G_loss: 0.1484\n",
      "  Batch [750/1299] D_loss: 0.0143, G_loss: 0.0790\n",
      "  Batch [760/1299] D_loss: -0.0986, G_loss: -0.0733\n",
      "  Batch [770/1299] D_loss: -0.1783, G_loss: 0.0869\n",
      "  Batch [780/1299] D_loss: -0.4763, G_loss: 0.0170\n",
      "  Batch [790/1299] D_loss: -0.0933, G_loss: 0.1950\n",
      "  Batch [800/1299] D_loss: -0.2131, G_loss: 0.3144\n",
      "  Batch [810/1299] D_loss: -0.0351, G_loss: 0.3408\n",
      "  Batch [820/1299] D_loss: -0.2540, G_loss: 0.8196\n",
      "  Batch [830/1299] D_loss: -0.2122, G_loss: 0.7583\n",
      "  Batch [840/1299] D_loss: -0.1018, G_loss: 0.4571\n",
      "  Batch [850/1299] D_loss: -4.9430, G_loss: -10.9185\n",
      "  Batch [860/1299] D_loss: -0.4940, G_loss: -0.5960\n",
      "  Batch [870/1299] D_loss: 0.0102, G_loss: 0.0114\n",
      "  Batch [880/1299] D_loss: -0.0999, G_loss: 0.4518\n",
      "  Batch [890/1299] D_loss: -0.0645, G_loss: 0.4137\n",
      "  Batch [900/1299] D_loss: -0.0639, G_loss: 0.2824\n",
      "  Batch [910/1299] D_loss: -0.1840, G_loss: 0.3862\n",
      "  Batch [920/1299] D_loss: -0.0405, G_loss: 0.1422\n",
      "  Batch [930/1299] D_loss: -0.0968, G_loss: 0.2795\n",
      "  Batch [940/1299] D_loss: 1.6647, G_loss: -3.9638\n",
      "  Batch [950/1299] D_loss: -0.4427, G_loss: -0.5981\n",
      "  Batch [960/1299] D_loss: -0.2980, G_loss: -0.8086\n",
      "  Batch [970/1299] D_loss: -0.0445, G_loss: 0.1928\n",
      "  Batch [980/1299] D_loss: -0.1154, G_loss: 0.6215\n",
      "  Batch [990/1299] D_loss: -0.0417, G_loss: 0.5648\n",
      "  Batch [1000/1299] D_loss: -0.1272, G_loss: 0.3219\n",
      "  Batch [1010/1299] D_loss: -0.0194, G_loss: 0.3824\n",
      "  Batch [1020/1299] D_loss: -0.0138, G_loss: 0.3168\n",
      "  Batch [1030/1299] D_loss: -0.6610, G_loss: 0.1161\n",
      "  Batch [1040/1299] D_loss: 0.1198, G_loss: 0.1235\n",
      "  Batch [1050/1299] D_loss: -0.1893, G_loss: -0.0727\n",
      "  Batch [1060/1299] D_loss: -0.0919, G_loss: 0.1605\n",
      "  Batch [1070/1299] D_loss: -0.7260, G_loss: -0.3950\n",
      "  Batch [1080/1299] D_loss: -0.2782, G_loss: 0.1634\n",
      "  Batch [1090/1299] D_loss: -0.8321, G_loss: -0.8433\n",
      "  Batch [1100/1299] D_loss: -0.9059, G_loss: -0.9061\n",
      "  Batch [1110/1299] D_loss: 0.0484, G_loss: 0.2368\n",
      "  Batch [1120/1299] D_loss: -0.3607, G_loss: 0.1820\n",
      "  Batch [1130/1299] D_loss: -0.8033, G_loss: -0.0003\n",
      "  Batch [1140/1299] D_loss: -0.8239, G_loss: -0.5103\n",
      "  Batch [1150/1299] D_loss: -0.1197, G_loss: 0.0770\n",
      "  Batch [1160/1299] D_loss: -0.2796, G_loss: 0.1250\n",
      "  Batch [1170/1299] D_loss: -0.5576, G_loss: -0.0239\n",
      "  Batch [1180/1299] D_loss: -0.2339, G_loss: 0.0678\n",
      "  Batch [1190/1299] D_loss: -0.0514, G_loss: 0.2512\n",
      "  Batch [1200/1299] D_loss: -0.0308, G_loss: 0.4179\n",
      "  Batch [1210/1299] D_loss: -0.0817, G_loss: 0.7500\n",
      "  Batch [1220/1299] D_loss: 0.0311, G_loss: 0.6361\n",
      "  Batch [1230/1299] D_loss: -0.0910, G_loss: 0.4157\n",
      "  Batch [1240/1299] D_loss: -0.0685, G_loss: 0.2309\n",
      "  Batch [1250/1299] D_loss: -0.3212, G_loss: -0.1684\n",
      "  Batch [1260/1299] D_loss: -0.1293, G_loss: 0.0092\n",
      "  Batch [1270/1299] D_loss: -0.1237, G_loss: -0.0607\n",
      "  Batch [1280/1299] D_loss: -1.4751, G_loss: -0.3781\n",
      "  Batch [1290/1299] D_loss: -0.4023, G_loss: -0.1256\n",
      "\n",
      "Epoch 40 Summary:\n",
      "  Average D_loss: -0.1925\n",
      "  Average G_loss: -0.1075\n",
      "\n",
      "Epoch [41/100]\n",
      "  Batch [0/1299] D_loss: -0.2449, G_loss: -0.0993\n",
      "  Batch [10/1299] D_loss: 0.0323, G_loss: 0.3176\n",
      "  Batch [20/1299] D_loss: -0.1833, G_loss: 0.6629\n",
      "  Batch [30/1299] D_loss: -0.1908, G_loss: 0.6908\n",
      "  Batch [40/1299] D_loss: 0.0281, G_loss: 0.5998\n",
      "  Batch [50/1299] D_loss: -0.0880, G_loss: 0.6750\n",
      "  Batch [60/1299] D_loss: -0.1720, G_loss: 0.4701\n",
      "  Batch [70/1299] D_loss: -0.1695, G_loss: 0.2541\n",
      "  Batch [80/1299] D_loss: -0.7103, G_loss: -2.1946\n",
      "  Batch [90/1299] D_loss: -0.0570, G_loss: 0.0070\n",
      "  Batch [100/1299] D_loss: -0.1677, G_loss: -0.2549\n",
      "  Batch [110/1299] D_loss: -0.1507, G_loss: 0.4454\n",
      "  Batch [120/1299] D_loss: -0.1349, G_loss: 0.5857\n",
      "  Batch [130/1299] D_loss: -0.0537, G_loss: 0.7561\n",
      "  Batch [140/1299] D_loss: -0.0501, G_loss: 0.4571\n",
      "  Batch [150/1299] D_loss: 0.0162, G_loss: 0.4419\n",
      "  Batch [160/1299] D_loss: -0.1047, G_loss: 0.2384\n",
      "  Batch [170/1299] D_loss: -0.8620, G_loss: -2.2771\n",
      "  Batch [180/1299] D_loss: -0.1839, G_loss: 0.1700\n",
      "  Batch [190/1299] D_loss: -0.0955, G_loss: 0.2290\n",
      "  Batch [200/1299] D_loss: -0.2405, G_loss: 0.6846\n",
      "  Batch [210/1299] D_loss: -0.1107, G_loss: 0.7005\n",
      "  Batch [220/1299] D_loss: 0.0612, G_loss: 0.3806\n",
      "  Batch [230/1299] D_loss: 0.0816, G_loss: 0.5087\n",
      "  Batch [240/1299] D_loss: 0.0905, G_loss: 0.3124\n",
      "  Batch [250/1299] D_loss: -0.1568, G_loss: 0.0789\n",
      "  Batch [260/1299] D_loss: -0.0158, G_loss: 0.0450\n",
      "  Batch [270/1299] D_loss: -0.6435, G_loss: -0.0559\n",
      "  Batch [280/1299] D_loss: -0.5221, G_loss: -0.4647\n",
      "  Batch [290/1299] D_loss: -0.3185, G_loss: 0.2154\n",
      "  Batch [300/1299] D_loss: 0.0512, G_loss: 0.2254\n",
      "  Batch [310/1299] D_loss: 0.0480, G_loss: 0.1157\n",
      "  Batch [320/1299] D_loss: -0.0773, G_loss: 0.3704\n",
      "  Batch [330/1299] D_loss: -0.2806, G_loss: 0.4378\n",
      "  Batch [340/1299] D_loss: -0.1709, G_loss: 0.3901\n",
      "  Batch [350/1299] D_loss: -0.1238, G_loss: 0.3394\n",
      "  Batch [360/1299] D_loss: -6.3871, G_loss: -7.8186\n",
      "  Batch [370/1299] D_loss: -0.7275, G_loss: -1.0816\n",
      "  Batch [380/1299] D_loss: -0.0606, G_loss: 0.0924\n",
      "  Batch [390/1299] D_loss: -0.0937, G_loss: 0.4523\n",
      "  Batch [400/1299] D_loss: -0.1631, G_loss: 0.5888\n",
      "  Batch [410/1299] D_loss: -0.1304, G_loss: 0.6024\n",
      "  Batch [420/1299] D_loss: -0.0119, G_loss: 0.4089\n",
      "  Batch [430/1299] D_loss: 0.0506, G_loss: 0.2227\n",
      "  Batch [440/1299] D_loss: -0.1698, G_loss: -0.0818\n",
      "  Batch [450/1299] D_loss: -1.1781, G_loss: -2.6021\n",
      "  Batch [460/1299] D_loss: -0.3795, G_loss: 0.0196\n",
      "  Batch [470/1299] D_loss: -0.4588, G_loss: -0.2418\n",
      "  Batch [480/1299] D_loss: -0.0970, G_loss: 0.2475\n",
      "  Batch [490/1299] D_loss: -0.1198, G_loss: 0.3394\n",
      "  Batch [500/1299] D_loss: -0.0765, G_loss: 0.5377\n",
      "  Batch [510/1299] D_loss: -0.1791, G_loss: 0.4640\n",
      "  Batch [520/1299] D_loss: 0.0141, G_loss: 0.3987\n",
      "  Batch [530/1299] D_loss: -0.1073, G_loss: 0.3046\n",
      "  Batch [540/1299] D_loss: -1.1411, G_loss: -0.5451\n",
      "  Batch [550/1299] D_loss: -0.7547, G_loss: 0.0676\n",
      "  Batch [560/1299] D_loss: -0.5602, G_loss: 0.0740\n",
      "  Batch [570/1299] D_loss: -0.0681, G_loss: 0.1753\n",
      "  Batch [580/1299] D_loss: -0.4696, G_loss: -0.0247\n",
      "  Batch [590/1299] D_loss: 0.0037, G_loss: 0.1593\n",
      "  Batch [600/1299] D_loss: -0.0409, G_loss: 0.1342\n",
      "  Batch [610/1299] D_loss: -0.6910, G_loss: 0.1360\n",
      "  Batch [620/1299] D_loss: -0.5125, G_loss: -0.6506\n",
      "  Batch [630/1299] D_loss: -0.3713, G_loss: 0.1481\n",
      "  Batch [640/1299] D_loss: -1.0204, G_loss: -0.0524\n",
      "  Batch [650/1299] D_loss: -0.0145, G_loss: 0.1368\n",
      "  Batch [660/1299] D_loss: 0.0004, G_loss: 0.1563\n",
      "  Batch [670/1299] D_loss: -0.1995, G_loss: 0.4197\n",
      "  Batch [680/1299] D_loss: -0.0348, G_loss: 0.5534\n",
      "  Batch [690/1299] D_loss: -0.0972, G_loss: 0.8458\n",
      "  Batch [700/1299] D_loss: -0.1861, G_loss: 0.5096\n",
      "  Batch [710/1299] D_loss: -0.0416, G_loss: 0.5080\n",
      "  Batch [720/1299] D_loss: -0.9840, G_loss: -1.5613\n",
      "  Batch [730/1299] D_loss: -0.2157, G_loss: 0.0571\n",
      "  Batch [740/1299] D_loss: 0.0136, G_loss: 0.1944\n",
      "  Batch [750/1299] D_loss: 0.0093, G_loss: 0.1664\n",
      "  Batch [760/1299] D_loss: 0.0107, G_loss: 0.2743\n",
      "  Batch [770/1299] D_loss: -0.0294, G_loss: 0.3925\n",
      "  Batch [780/1299] D_loss: -0.0075, G_loss: 0.4330\n",
      "  Batch [790/1299] D_loss: -0.1499, G_loss: 0.5902\n",
      "  Batch [800/1299] D_loss: -0.0769, G_loss: 0.4132\n",
      "  Batch [810/1299] D_loss: -1.3078, G_loss: -0.4083\n",
      "  Batch [820/1299] D_loss: -0.1335, G_loss: 0.0573\n",
      "  Batch [830/1299] D_loss: -0.0442, G_loss: 0.1695\n",
      "  Batch [840/1299] D_loss: -0.0791, G_loss: 0.3986\n",
      "  Batch [850/1299] D_loss: 0.0139, G_loss: 0.2442\n",
      "  Batch [860/1299] D_loss: -0.0248, G_loss: 0.2513\n",
      "  Batch [870/1299] D_loss: -1.2244, G_loss: -0.3811\n",
      "  Batch [880/1299] D_loss: -0.6137, G_loss: -0.0377\n",
      "  Batch [890/1299] D_loss: -0.3519, G_loss: -0.5924\n",
      "  Batch [900/1299] D_loss: -0.8479, G_loss: -2.2189\n",
      "  Batch [910/1299] D_loss: 0.0084, G_loss: 0.2500\n",
      "  Batch [920/1299] D_loss: -0.0529, G_loss: 0.3290\n",
      "  Batch [930/1299] D_loss: 0.0272, G_loss: 0.3873\n",
      "  Batch [940/1299] D_loss: 0.0655, G_loss: 0.5548\n",
      "  Batch [950/1299] D_loss: -0.0192, G_loss: 0.4341\n",
      "  Batch [960/1299] D_loss: -0.0693, G_loss: 0.3957\n",
      "  Batch [970/1299] D_loss: -1.1396, G_loss: -0.9044\n",
      "  Batch [980/1299] D_loss: -0.5358, G_loss: -0.4343\n",
      "  Batch [990/1299] D_loss: -0.4435, G_loss: -0.0452\n",
      "  Batch [1000/1299] D_loss: -0.0122, G_loss: 0.2350\n",
      "  Batch [1010/1299] D_loss: -0.0210, G_loss: 0.4023\n",
      "  Batch [1020/1299] D_loss: -0.2178, G_loss: 0.5752\n",
      "  Batch [1030/1299] D_loss: -0.1801, G_loss: 0.7375\n",
      "  Batch [1040/1299] D_loss: 0.0564, G_loss: 0.4443\n",
      "  Batch [1050/1299] D_loss: -0.0029, G_loss: 0.2910\n",
      "  Batch [1060/1299] D_loss: -1.1718, G_loss: -1.1466\n",
      "  Batch [1070/1299] D_loss: 0.0056, G_loss: 0.2807\n",
      "  Batch [1080/1299] D_loss: -0.6526, G_loss: 0.1481\n",
      "  Batch [1090/1299] D_loss: -1.1209, G_loss: -0.0701\n",
      "  Batch [1100/1299] D_loss: 0.0059, G_loss: 0.0280\n",
      "  Batch [1110/1299] D_loss: -0.0015, G_loss: 0.0562\n",
      "  Batch [1120/1299] D_loss: 0.0100, G_loss: 0.0718\n",
      "  Batch [1130/1299] D_loss: -0.0153, G_loss: 0.1599\n",
      "  Batch [1140/1299] D_loss: -0.0237, G_loss: 0.3048\n",
      "  Batch [1150/1299] D_loss: -0.1088, G_loss: 0.3311\n",
      "  Batch [1160/1299] D_loss: -0.0531, G_loss: 0.4647\n",
      "  Batch [1170/1299] D_loss: -0.1736, G_loss: -0.1595\n",
      "  Batch [1180/1299] D_loss: -0.2703, G_loss: 0.1064\n",
      "  Batch [1190/1299] D_loss: -0.5834, G_loss: -1.0555\n",
      "  Batch [1200/1299] D_loss: -1.1579, G_loss: -1.6491\n",
      "  Batch [1210/1299] D_loss: -0.1758, G_loss: 0.2239\n",
      "  Batch [1220/1299] D_loss: -1.1476, G_loss: -1.3413\n",
      "  Batch [1230/1299] D_loss: -0.0781, G_loss: 0.3255\n",
      "  Batch [1240/1299] D_loss: -0.3004, G_loss: 0.6305\n",
      "  Batch [1250/1299] D_loss: -0.1507, G_loss: 0.5453\n",
      "  Batch [1260/1299] D_loss: -0.3770, G_loss: 0.6737\n",
      "  Batch [1270/1299] D_loss: 0.0272, G_loss: 0.7246\n",
      "  Batch [1280/1299] D_loss: 0.0321, G_loss: 0.5065\n",
      "  Batch [1290/1299] D_loss: -0.0338, G_loss: 0.2699\n",
      "\n",
      "Epoch 41 Summary:\n",
      "  Average D_loss: -0.1775\n",
      "  Average G_loss: -0.0702\n",
      "\n",
      "Models saved at epoch 41:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250120_145825_dataset+cell_type/generator_20250120_145825_dataset+cell_type_epoch_41.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250120_145825_dataset+cell_type/discriminator_20250120_145825_dataset+cell_type_epoch_41.pt\n",
      "\n",
      "Epoch [42/100]\n",
      "  Batch [0/1299] D_loss: -0.0118, G_loss: 0.0587\n",
      "  Batch [10/1299] D_loss: -1.7689, G_loss: -2.3835\n",
      "  Batch [20/1299] D_loss: -0.3732, G_loss: 0.0311\n",
      "  Batch [30/1299] D_loss: -0.8489, G_loss: -1.8306\n",
      "  Batch [40/1299] D_loss: -0.0210, G_loss: 0.1996\n",
      "  Batch [50/1299] D_loss: 0.0870, G_loss: 0.5427\n",
      "  Batch [60/1299] D_loss: -0.0932, G_loss: 0.6984\n",
      "  Batch [70/1299] D_loss: 0.0797, G_loss: 0.4889\n",
      "  Batch [80/1299] D_loss: 0.0382, G_loss: 0.4931\n",
      "  Batch [90/1299] D_loss: -0.0515, G_loss: 0.2520\n",
      "  Batch [100/1299] D_loss: -0.7225, G_loss: -0.1489\n",
      "  Batch [110/1299] D_loss: -0.0378, G_loss: 0.2534\n",
      "  Batch [120/1299] D_loss: -0.1451, G_loss: 0.2913\n",
      "  Batch [130/1299] D_loss: 0.0285, G_loss: 0.3366\n",
      "  Batch [140/1299] D_loss: -2.6632, G_loss: -2.5188\n",
      "  Batch [150/1299] D_loss: -0.6045, G_loss: -1.1632\n",
      "  Batch [160/1299] D_loss: -1.1920, G_loss: -1.9923\n",
      "  Batch [170/1299] D_loss: -0.2099, G_loss: 0.1673\n",
      "  Batch [180/1299] D_loss: -0.7669, G_loss: -0.1634\n",
      "  Batch [190/1299] D_loss: -0.4277, G_loss: -0.0733\n",
      "  Batch [200/1299] D_loss: -2.5871, G_loss: -1.2100\n",
      "  Batch [210/1299] D_loss: -0.5068, G_loss: -0.5254\n",
      "  Batch [220/1299] D_loss: -0.1714, G_loss: 0.1008\n",
      "  Batch [230/1299] D_loss: -1.9538, G_loss: -0.1401\n",
      "  Batch [240/1299] D_loss: -0.3491, G_loss: -0.1120\n",
      "  Batch [250/1299] D_loss: -0.1492, G_loss: 0.4002\n",
      "  Batch [260/1299] D_loss: -0.1025, G_loss: 0.5593\n",
      "  Batch [270/1299] D_loss: -0.2049, G_loss: 0.8762\n",
      "  Batch [280/1299] D_loss: -0.0569, G_loss: 0.9948\n",
      "  Batch [290/1299] D_loss: -0.1562, G_loss: 0.9757\n",
      "  Batch [300/1299] D_loss: 0.0147, G_loss: 0.2862\n",
      "  Batch [310/1299] D_loss: -0.0501, G_loss: 0.4359\n",
      "  Batch [320/1299] D_loss: -0.3842, G_loss: -0.5961\n",
      "  Batch [330/1299] D_loss: -0.7598, G_loss: -0.9284\n",
      "  Batch [340/1299] D_loss: -0.1037, G_loss: 0.1885\n",
      "  Batch [350/1299] D_loss: -0.4417, G_loss: 0.1758\n",
      "  Batch [360/1299] D_loss: -0.2202, G_loss: 0.2792\n",
      "  Batch [370/1299] D_loss: -0.2366, G_loss: 0.2451\n",
      "  Batch [380/1299] D_loss: -1.0407, G_loss: -0.8135\n",
      "  Batch [390/1299] D_loss: -0.3446, G_loss: 0.1830\n",
      "  Batch [400/1299] D_loss: -0.1277, G_loss: 0.4909\n",
      "  Batch [410/1299] D_loss: -0.4201, G_loss: 0.5659\n",
      "  Batch [420/1299] D_loss: 0.0154, G_loss: 0.5803\n",
      "  Batch [430/1299] D_loss: 0.0941, G_loss: 0.4077\n",
      "  Batch [440/1299] D_loss: -0.0725, G_loss: 0.4288\n",
      "  Batch [450/1299] D_loss: -2.1174, G_loss: -4.6136\n",
      "  Batch [460/1299] D_loss: 0.0093, G_loss: 0.1054\n",
      "  Batch [470/1299] D_loss: -0.0733, G_loss: 0.1951\n",
      "  Batch [480/1299] D_loss: 0.0170, G_loss: 0.2954\n",
      "  Batch [490/1299] D_loss: -0.1568, G_loss: 0.4626\n",
      "  Batch [500/1299] D_loss: 0.0416, G_loss: 0.4329\n",
      "  Batch [510/1299] D_loss: -0.2530, G_loss: 0.3345\n",
      "  Batch [520/1299] D_loss: -1.5559, G_loss: -2.6093\n",
      "  Batch [530/1299] D_loss: -0.0643, G_loss: 0.2017\n",
      "  Batch [540/1299] D_loss: -0.1521, G_loss: 0.3209\n",
      "  Batch [550/1299] D_loss: -0.2577, G_loss: 0.3608\n",
      "  Batch [560/1299] D_loss: -0.0935, G_loss: 0.3564\n",
      "  Batch [570/1299] D_loss: -5.2479, G_loss: -9.0516\n",
      "  Batch [580/1299] D_loss: -0.0387, G_loss: -0.0264\n",
      "  Batch [590/1299] D_loss: -0.1062, G_loss: -0.0893\n",
      "  Batch [600/1299] D_loss: -0.4937, G_loss: -0.2688\n",
      "  Batch [610/1299] D_loss: -0.4775, G_loss: -0.3054\n",
      "  Batch [620/1299] D_loss: -0.0510, G_loss: 0.3601\n",
      "  Batch [630/1299] D_loss: -0.0915, G_loss: 0.4094\n",
      "  Batch [640/1299] D_loss: 0.0067, G_loss: 0.4989\n",
      "  Batch [650/1299] D_loss: 0.0201, G_loss: 0.4525\n",
      "  Batch [660/1299] D_loss: -0.0179, G_loss: 0.3431\n",
      "  Batch [670/1299] D_loss: -1.6433, G_loss: -0.9356\n",
      "  Batch [680/1299] D_loss: -0.3847, G_loss: -0.0451\n",
      "  Batch [690/1299] D_loss: -0.6160, G_loss: 0.1486\n",
      "  Batch [700/1299] D_loss: -0.5265, G_loss: -0.5454\n",
      "  Batch [710/1299] D_loss: -0.5960, G_loss: -0.2095\n",
      "  Batch [720/1299] D_loss: -0.9343, G_loss: 0.0863\n",
      "  Batch [730/1299] D_loss: -0.0103, G_loss: 0.2586\n",
      "  Batch [740/1299] D_loss: -0.1459, G_loss: 0.4451\n",
      "  Batch [750/1299] D_loss: -0.0370, G_loss: 0.4509\n",
      "  Batch [760/1299] D_loss: -0.0880, G_loss: 0.5694\n",
      "  Batch [770/1299] D_loss: -0.0616, G_loss: 0.5184\n",
      "  Batch [780/1299] D_loss: -0.0841, G_loss: 0.5028\n",
      "  Batch [790/1299] D_loss: -0.8133, G_loss: -1.7673\n",
      "  Batch [800/1299] D_loss: -1.7855, G_loss: -2.0150\n",
      "  Batch [810/1299] D_loss: -0.1096, G_loss: 0.0021\n",
      "  Batch [820/1299] D_loss: -0.1048, G_loss: 0.2565\n",
      "  Batch [830/1299] D_loss: -0.1462, G_loss: 0.4606\n",
      "  Batch [840/1299] D_loss: -0.1316, G_loss: 0.6397\n",
      "  Batch [850/1299] D_loss: 0.0224, G_loss: 0.4244\n",
      "  Batch [860/1299] D_loss: -0.0633, G_loss: 0.3932\n",
      "  Batch [870/1299] D_loss: -0.0307, G_loss: 0.2672\n",
      "  Batch [880/1299] D_loss: -1.1366, G_loss: -0.9074\n",
      "  Batch [890/1299] D_loss: -1.2537, G_loss: -2.2433\n",
      "  Batch [900/1299] D_loss: -0.0668, G_loss: 0.1251\n",
      "  Batch [910/1299] D_loss: -0.1736, G_loss: 0.1649\n",
      "  Batch [920/1299] D_loss: -0.1501, G_loss: 0.3952\n",
      "  Batch [930/1299] D_loss: -0.0305, G_loss: 0.3811\n",
      "  Batch [940/1299] D_loss: -0.0581, G_loss: 0.7152\n",
      "  Batch [950/1299] D_loss: -0.0346, G_loss: 0.3966\n",
      "  Batch [960/1299] D_loss: -0.0706, G_loss: 0.5949\n",
      "  Batch [970/1299] D_loss: -0.3899, G_loss: 0.2123\n",
      "  Batch [980/1299] D_loss: -0.3695, G_loss: -0.1543\n",
      "  Batch [990/1299] D_loss: -1.0357, G_loss: -1.7324\n",
      "  Batch [1000/1299] D_loss: -0.1308, G_loss: 0.0342\n",
      "  Batch [1010/1299] D_loss: -1.8942, G_loss: -0.6117\n",
      "  Batch [1020/1299] D_loss: -0.3722, G_loss: 0.0618\n",
      "  Batch [1030/1299] D_loss: -0.1884, G_loss: 0.1354\n",
      "  Batch [1040/1299] D_loss: -0.0602, G_loss: 0.2979\n",
      "  Batch [1050/1299] D_loss: -0.1260, G_loss: 0.5620\n",
      "  Batch [1060/1299] D_loss: -0.0360, G_loss: 0.6702\n",
      "  Batch [1070/1299] D_loss: -0.0550, G_loss: 0.6225\n",
      "  Batch [1080/1299] D_loss: -0.0240, G_loss: 0.4010\n",
      "  Batch [1090/1299] D_loss: -1.5122, G_loss: -1.6488\n",
      "  Batch [1100/1299] D_loss: -0.8432, G_loss: -2.1507\n",
      "  Batch [1110/1299] D_loss: -0.9083, G_loss: -3.0864\n",
      "  Batch [1120/1299] D_loss: 0.0050, G_loss: 0.1516\n",
      "  Batch [1130/1299] D_loss: -0.0612, G_loss: 0.3186\n",
      "  Batch [1140/1299] D_loss: -0.0694, G_loss: 0.4589\n",
      "  Batch [1150/1299] D_loss: -0.2707, G_loss: 0.6402\n",
      "  Batch [1160/1299] D_loss: -0.1260, G_loss: 0.5853\n",
      "  Batch [1170/1299] D_loss: 0.0086, G_loss: 0.3539\n",
      "  Batch [1180/1299] D_loss: -3.5842, G_loss: -11.1163\n",
      "  Batch [1190/1299] D_loss: -0.7574, G_loss: -0.3806\n",
      "  Batch [1200/1299] D_loss: -1.0444, G_loss: -0.9983\n",
      "  Batch [1210/1299] D_loss: 0.0104, G_loss: 0.3434\n",
      "  Batch [1220/1299] D_loss: -0.2547, G_loss: 0.5989\n",
      "  Batch [1230/1299] D_loss: -0.0993, G_loss: 0.8187\n",
      "  Batch [1240/1299] D_loss: 0.0683, G_loss: 0.6106\n",
      "  Batch [1250/1299] D_loss: -0.0530, G_loss: 0.5873\n",
      "  Batch [1260/1299] D_loss: 0.0273, G_loss: 0.2591\n",
      "  Batch [1270/1299] D_loss: -0.0413, G_loss: 0.2874\n",
      "  Batch [1280/1299] D_loss: -0.3800, G_loss: -1.2119\n",
      "  Batch [1290/1299] D_loss: -0.0235, G_loss: 0.2918\n",
      "\n",
      "Epoch 42 Summary:\n",
      "  Average D_loss: -0.1814\n",
      "  Average G_loss: -0.1034\n",
      "\n",
      "Epoch [43/100]\n",
      "  Batch [0/1299] D_loss: -0.0583, G_loss: 0.1838\n",
      "  Batch [10/1299] D_loss: -0.0887, G_loss: 0.4238\n",
      "  Batch [20/1299] D_loss: -0.3173, G_loss: 0.5356\n",
      "  Batch [30/1299] D_loss: -0.0494, G_loss: 0.3976\n",
      "  Batch [40/1299] D_loss: -0.1913, G_loss: 0.4853\n",
      "  Batch [50/1299] D_loss: -0.0383, G_loss: 0.2029\n",
      "  Batch [60/1299] D_loss: -0.2624, G_loss: -0.3595\n",
      "  Batch [70/1299] D_loss: -0.7950, G_loss: -1.9611\n",
      "  Batch [80/1299] D_loss: -1.0927, G_loss: -0.6860\n",
      "  Batch [90/1299] D_loss: -0.0763, G_loss: 0.1611\n",
      "  Batch [100/1299] D_loss: -0.0523, G_loss: 0.3267\n",
      "  Batch [110/1299] D_loss: -0.0169, G_loss: 0.4403\n",
      "  Batch [120/1299] D_loss: -0.1582, G_loss: 0.4447\n",
      "  Batch [130/1299] D_loss: 0.0751, G_loss: 0.3188\n",
      "  Batch [140/1299] D_loss: -2.5447, G_loss: -6.9660\n",
      "  Batch [150/1299] D_loss: -0.0342, G_loss: 0.2433\n",
      "  Batch [160/1299] D_loss: -0.0346, G_loss: 0.2219\n",
      "  Batch [170/1299] D_loss: -0.0493, G_loss: 0.1180\n",
      "  Batch [180/1299] D_loss: 0.0200, G_loss: 0.2065\n",
      "  Batch [190/1299] D_loss: -2.4643, G_loss: -4.3301\n",
      "  Batch [200/1299] D_loss: -0.1273, G_loss: -0.1379\n",
      "  Batch [210/1299] D_loss: -0.4712, G_loss: -1.4749\n",
      "  Batch [220/1299] D_loss: -0.1299, G_loss: 0.1123\n",
      "  Batch [230/1299] D_loss: -0.0547, G_loss: 0.2534\n",
      "  Batch [240/1299] D_loss: -0.1374, G_loss: 0.4251\n",
      "  Batch [250/1299] D_loss: -0.2515, G_loss: 0.7230\n",
      "  Batch [260/1299] D_loss: -0.1098, G_loss: 0.5648\n",
      "  Batch [270/1299] D_loss: -0.1808, G_loss: 0.6211\n",
      "  Batch [280/1299] D_loss: -0.0530, G_loss: 0.2203\n",
      "  Batch [290/1299] D_loss: -0.6181, G_loss: -0.8745\n",
      "  Batch [300/1299] D_loss: -0.1212, G_loss: -0.0716\n",
      "  Batch [310/1299] D_loss: -0.0133, G_loss: 0.1382\n",
      "  Batch [320/1299] D_loss: -0.2742, G_loss: 0.2765\n",
      "  Batch [330/1299] D_loss: -1.5998, G_loss: -1.3727\n",
      "  Batch [340/1299] D_loss: -0.0689, G_loss: 0.2973\n",
      "  Batch [350/1299] D_loss: -0.0781, G_loss: 0.3187\n",
      "  Batch [360/1299] D_loss: 0.0080, G_loss: 0.3940\n",
      "  Batch [370/1299] D_loss: 0.1044, G_loss: 0.5458\n",
      "  Batch [380/1299] D_loss: -0.1397, G_loss: 0.7774\n",
      "  Batch [390/1299] D_loss: -0.3076, G_loss: 0.5456\n",
      "  Batch [400/1299] D_loss: -2.3388, G_loss: -2.2761\n",
      "  Batch [410/1299] D_loss: -0.5751, G_loss: -0.0642\n",
      "  Batch [420/1299] D_loss: -0.0199, G_loss: 0.1890\n",
      "  Batch [430/1299] D_loss: -0.3070, G_loss: -0.0132\n",
      "  Batch [440/1299] D_loss: 0.0569, G_loss: 0.1849\n",
      "  Batch [450/1299] D_loss: -0.8619, G_loss: -1.2387\n",
      "  Batch [460/1299] D_loss: -0.8345, G_loss: -0.2563\n",
      "  Batch [470/1299] D_loss: -0.0064, G_loss: 0.1969\n",
      "  Batch [480/1299] D_loss: -0.0741, G_loss: 0.3423\n",
      "  Batch [490/1299] D_loss: -0.5419, G_loss: 0.5970\n",
      "  Batch [500/1299] D_loss: -0.5675, G_loss: 0.5920\n",
      "  Batch [510/1299] D_loss: -0.0399, G_loss: 0.7320\n",
      "  Batch [520/1299] D_loss: -0.0992, G_loss: 0.6952\n",
      "  Batch [530/1299] D_loss: -0.0816, G_loss: 0.5120\n",
      "  Batch [540/1299] D_loss: 0.0134, G_loss: 0.5801\n",
      "  Batch [550/1299] D_loss: -1.4996, G_loss: -1.6440\n",
      "  Batch [560/1299] D_loss: -0.0900, G_loss: 0.1149\n",
      "  Batch [570/1299] D_loss: -0.1014, G_loss: 0.0768\n",
      "  Batch [580/1299] D_loss: -0.1960, G_loss: 0.1129\n",
      "  Batch [590/1299] D_loss: -0.2052, G_loss: -0.1064\n",
      "  Batch [600/1299] D_loss: -0.0504, G_loss: 0.3644\n",
      "  Batch [610/1299] D_loss: -0.2917, G_loss: 0.4738\n",
      "  Batch [620/1299] D_loss: -0.0401, G_loss: 0.5036\n",
      "  Batch [630/1299] D_loss: -0.1085, G_loss: 0.5962\n",
      "  Batch [640/1299] D_loss: -0.0121, G_loss: 0.5811\n",
      "  Batch [650/1299] D_loss: 0.0252, G_loss: 0.5709\n",
      "  Batch [660/1299] D_loss: -0.1311, G_loss: 0.2618\n",
      "  Batch [670/1299] D_loss: -0.6354, G_loss: -1.3941\n",
      "  Batch [680/1299] D_loss: -0.5763, G_loss: 0.0809\n",
      "  Batch [690/1299] D_loss: -0.3181, G_loss: 0.0721\n",
      "  Batch [700/1299] D_loss: -0.1780, G_loss: 0.1206\n",
      "  Batch [710/1299] D_loss: -0.0104, G_loss: 0.1499\n",
      "  Batch [720/1299] D_loss: -0.1689, G_loss: 0.0530\n",
      "  Batch [730/1299] D_loss: -0.9972, G_loss: -1.2347\n",
      "  Batch [740/1299] D_loss: -0.0647, G_loss: 0.1617\n",
      "  Batch [750/1299] D_loss: -1.2426, G_loss: -1.9249\n",
      "  Batch [760/1299] D_loss: -0.7552, G_loss: -0.1817\n",
      "  Batch [770/1299] D_loss: -0.3739, G_loss: 0.0702\n",
      "  Batch [780/1299] D_loss: -0.0192, G_loss: 0.5060\n",
      "  Batch [790/1299] D_loss: -0.0618, G_loss: 0.5866\n",
      "  Batch [800/1299] D_loss: -0.1197, G_loss: 0.6656\n",
      "  Batch [810/1299] D_loss: -0.0214, G_loss: 0.6251\n",
      "  Batch [820/1299] D_loss: 0.0760, G_loss: 0.7830\n",
      "  Batch [830/1299] D_loss: 0.0511, G_loss: 0.2742\n",
      "  Batch [840/1299] D_loss: -8.4302, G_loss: -8.1549\n",
      "  Batch [850/1299] D_loss: -0.0056, G_loss: 0.2021\n",
      "  Batch [860/1299] D_loss: -0.0828, G_loss: 0.2995\n",
      "  Batch [870/1299] D_loss: 0.0313, G_loss: 0.3704\n",
      "  Batch [880/1299] D_loss: -0.2704, G_loss: 0.6146\n",
      "  Batch [890/1299] D_loss: -0.0722, G_loss: 0.4605\n",
      "  Batch [900/1299] D_loss: -0.0556, G_loss: 0.2912\n",
      "  Batch [910/1299] D_loss: -0.0075, G_loss: 0.1745\n",
      "  Batch [920/1299] D_loss: -0.0238, G_loss: 0.1387\n",
      "  Batch [930/1299] D_loss: 0.0044, G_loss: 0.1996\n",
      "  Batch [940/1299] D_loss: -0.0733, G_loss: 0.3059\n",
      "  Batch [950/1299] D_loss: -0.0567, G_loss: 0.4605\n",
      "  Batch [960/1299] D_loss: -0.0438, G_loss: 0.2321\n",
      "  Batch [970/1299] D_loss: -0.3999, G_loss: -0.6461\n",
      "  Batch [980/1299] D_loss: 0.0351, G_loss: 0.0954\n",
      "  Batch [990/1299] D_loss: -0.2734, G_loss: 0.0373\n",
      "  Batch [1000/1299] D_loss: -0.4439, G_loss: -0.9119\n",
      "  Batch [1010/1299] D_loss: -0.7532, G_loss: -1.2798\n",
      "  Batch [1020/1299] D_loss: -0.4035, G_loss: -1.0359\n",
      "  Batch [1030/1299] D_loss: -0.1961, G_loss: 0.0354\n",
      "  Batch [1040/1299] D_loss: -1.0425, G_loss: -2.4433\n",
      "  Batch [1050/1299] D_loss: -0.2297, G_loss: 0.0033\n",
      "  Batch [1060/1299] D_loss: -0.3968, G_loss: -0.0212\n",
      "  Batch [1070/1299] D_loss: -0.0548, G_loss: 0.1403\n",
      "  Batch [1080/1299] D_loss: -0.2536, G_loss: 0.1746\n",
      "  Batch [1090/1299] D_loss: -1.3203, G_loss: 0.1687\n",
      "  Batch [1100/1299] D_loss: -0.7850, G_loss: -0.9630\n",
      "  Batch [1110/1299] D_loss: -0.6008, G_loss: -0.3220\n",
      "  Batch [1120/1299] D_loss: -0.3428, G_loss: 0.0700\n",
      "  Batch [1130/1299] D_loss: -0.0818, G_loss: 0.2926\n",
      "  Batch [1140/1299] D_loss: -0.2867, G_loss: 0.5030\n",
      "  Batch [1150/1299] D_loss: -0.1336, G_loss: 0.4894\n",
      "  Batch [1160/1299] D_loss: 0.0255, G_loss: 0.8139\n",
      "  Batch [1170/1299] D_loss: -0.0865, G_loss: 0.5617\n",
      "  Batch [1180/1299] D_loss: -0.0180, G_loss: 0.3673\n",
      "  Batch [1190/1299] D_loss: -0.0522, G_loss: 0.0187\n",
      "  Batch [1200/1299] D_loss: -0.3915, G_loss: -0.3662\n",
      "  Batch [1210/1299] D_loss: -0.1829, G_loss: 0.1094\n",
      "  Batch [1220/1299] D_loss: -1.1629, G_loss: -0.2978\n",
      "  Batch [1230/1299] D_loss: -0.1729, G_loss: 0.7321\n",
      "  Batch [1240/1299] D_loss: -0.0785, G_loss: 0.4573\n",
      "  Batch [1250/1299] D_loss: -0.0322, G_loss: 0.4736\n",
      "  Batch [1260/1299] D_loss: 0.0311, G_loss: 0.6570\n",
      "  Batch [1270/1299] D_loss: -0.0557, G_loss: 0.2759\n",
      "  Batch [1280/1299] D_loss: -2.4582, G_loss: -4.3315\n",
      "  Batch [1290/1299] D_loss: -0.8828, G_loss: -2.2376\n",
      "\n",
      "Epoch 43 Summary:\n",
      "  Average D_loss: -0.1872\n",
      "  Average G_loss: -0.0963\n",
      "\n",
      "Epoch [44/100]\n",
      "  Batch [0/1299] D_loss: -0.2642, G_loss: 0.1806\n",
      "  Batch [10/1299] D_loss: -0.5319, G_loss: -0.0224\n",
      "  Batch [20/1299] D_loss: 0.0032, G_loss: 0.0956\n",
      "  Batch [30/1299] D_loss: -0.0200, G_loss: 0.2675\n",
      "  Batch [40/1299] D_loss: -0.0188, G_loss: 0.3395\n",
      "  Batch [50/1299] D_loss: -0.0897, G_loss: 0.3653\n",
      "  Batch [60/1299] D_loss: -0.0688, G_loss: 0.4358\n",
      "  Batch [70/1299] D_loss: -0.0323, G_loss: 0.5561\n",
      "  Batch [80/1299] D_loss: -0.1630, G_loss: 0.2785\n",
      "  Batch [90/1299] D_loss: -2.3223, G_loss: -4.4519\n",
      "  Batch [100/1299] D_loss: -0.5245, G_loss: -0.8125\n",
      "  Batch [110/1299] D_loss: -0.0165, G_loss: 0.3402\n",
      "  Batch [120/1299] D_loss: -0.0010, G_loss: 0.5659\n",
      "  Batch [130/1299] D_loss: 0.0252, G_loss: 0.6181\n",
      "  Batch [140/1299] D_loss: 0.1391, G_loss: 0.4433\n",
      "  Batch [150/1299] D_loss: -0.0737, G_loss: 0.4337\n",
      "  Batch [160/1299] D_loss: 0.0408, G_loss: 0.5403\n",
      "  Batch [170/1299] D_loss: -0.9301, G_loss: -1.4428\n",
      "  Batch [180/1299] D_loss: -0.5583, G_loss: 0.0526\n",
      "  Batch [190/1299] D_loss: -0.0749, G_loss: 0.1724\n",
      "  Batch [200/1299] D_loss: -0.0374, G_loss: 0.2037\n",
      "  Batch [210/1299] D_loss: -0.1460, G_loss: 0.2245\n",
      "  Batch [220/1299] D_loss: -0.2020, G_loss: 0.1068\n",
      "  Batch [230/1299] D_loss: -0.0016, G_loss: 0.2962\n",
      "  Batch [240/1299] D_loss: -0.0846, G_loss: 0.3944\n",
      "  Batch [250/1299] D_loss: 0.0327, G_loss: 0.3615\n",
      "  Batch [260/1299] D_loss: -0.0171, G_loss: 0.4102\n",
      "  Batch [270/1299] D_loss: 0.0105, G_loss: 0.4054\n",
      "  Batch [280/1299] D_loss: -0.1933, G_loss: -0.1968\n",
      "  Batch [290/1299] D_loss: 0.0468, G_loss: 0.1764\n",
      "  Batch [300/1299] D_loss: 0.0041, G_loss: 0.1181\n",
      "  Batch [310/1299] D_loss: -1.7514, G_loss: -0.9522\n",
      "  Batch [320/1299] D_loss: -0.6569, G_loss: -1.6464\n",
      "  Batch [330/1299] D_loss: -0.0336, G_loss: 0.1840\n",
      "  Batch [340/1299] D_loss: -0.0080, G_loss: 0.3025\n",
      "  Batch [350/1299] D_loss: -0.1232, G_loss: 0.4769\n",
      "  Batch [360/1299] D_loss: 0.0168, G_loss: 0.5229\n",
      "  Batch [370/1299] D_loss: -0.0388, G_loss: 0.5351\n",
      "  Batch [380/1299] D_loss: -0.0261, G_loss: 0.1297\n",
      "  Batch [390/1299] D_loss: -0.9858, G_loss: -0.7306\n",
      "  Batch [400/1299] D_loss: -0.1313, G_loss: 0.0470\n",
      "  Batch [410/1299] D_loss: -0.3036, G_loss: -0.3530\n",
      "  Batch [420/1299] D_loss: -0.7814, G_loss: -0.3408\n",
      "  Batch [430/1299] D_loss: -0.0586, G_loss: 0.1792\n",
      "  Batch [440/1299] D_loss: -0.0548, G_loss: 0.2730\n",
      "  Batch [450/1299] D_loss: -0.0074, G_loss: 0.5103\n",
      "  Batch [460/1299] D_loss: -0.1924, G_loss: 0.5509\n",
      "  Batch [470/1299] D_loss: -0.2645, G_loss: 0.6736\n",
      "  Batch [480/1299] D_loss: -0.0149, G_loss: 0.2427\n",
      "  Batch [490/1299] D_loss: -1.3411, G_loss: -1.1813\n",
      "  Batch [500/1299] D_loss: 0.0068, G_loss: 0.2236\n",
      "  Batch [510/1299] D_loss: -0.0575, G_loss: 0.3737\n",
      "  Batch [520/1299] D_loss: -0.0042, G_loss: 0.5296\n",
      "  Batch [530/1299] D_loss: -0.1140, G_loss: 0.5438\n",
      "  Batch [540/1299] D_loss: -0.1313, G_loss: 0.5896\n",
      "  Batch [550/1299] D_loss: -0.1093, G_loss: 0.4641\n",
      "  Batch [560/1299] D_loss: -0.0509, G_loss: 0.1213\n",
      "  Batch [570/1299] D_loss: -0.0163, G_loss: 0.0777\n",
      "  Batch [580/1299] D_loss: -0.0094, G_loss: 0.1655\n",
      "  Batch [590/1299] D_loss: -0.0685, G_loss: 0.3026\n",
      "  Batch [600/1299] D_loss: -0.1470, G_loss: 0.3017\n",
      "  Batch [610/1299] D_loss: -0.5940, G_loss: -0.2469\n",
      "  Batch [620/1299] D_loss: -0.0720, G_loss: 0.0578\n",
      "  Batch [630/1299] D_loss: -0.0072, G_loss: 0.1355\n",
      "  Batch [640/1299] D_loss: 0.0544, G_loss: 0.4155\n",
      "  Batch [650/1299] D_loss: -0.0345, G_loss: 0.4763\n",
      "  Batch [660/1299] D_loss: 0.0786, G_loss: 0.4060\n",
      "  Batch [670/1299] D_loss: -0.0483, G_loss: 0.2389\n",
      "  Batch [680/1299] D_loss: 0.0305, G_loss: 0.1732\n",
      "  Batch [690/1299] D_loss: -0.0849, G_loss: 0.0492\n",
      "  Batch [700/1299] D_loss: -0.1369, G_loss: -0.0962\n",
      "  Batch [710/1299] D_loss: 0.0055, G_loss: 0.1178\n",
      "  Batch [720/1299] D_loss: 0.0033, G_loss: 0.3365\n",
      "  Batch [730/1299] D_loss: -0.0068, G_loss: 0.4306\n",
      "  Batch [740/1299] D_loss: -0.1567, G_loss: 0.5448\n",
      "  Batch [750/1299] D_loss: -0.0599, G_loss: 0.5050\n",
      "  Batch [760/1299] D_loss: -0.1607, G_loss: 0.3389\n",
      "  Batch [770/1299] D_loss: -0.2923, G_loss: 0.3437\n",
      "  Batch [780/1299] D_loss: -2.5990, G_loss: -9.1854\n",
      "  Batch [790/1299] D_loss: -0.0151, G_loss: 0.0406\n",
      "  Batch [800/1299] D_loss: -0.1917, G_loss: -0.0574\n",
      "  Batch [810/1299] D_loss: -0.0452, G_loss: 0.1221\n",
      "  Batch [820/1299] D_loss: -0.3742, G_loss: -0.7268\n",
      "  Batch [830/1299] D_loss: -0.5306, G_loss: -0.2073\n",
      "  Batch [840/1299] D_loss: -0.0757, G_loss: 0.2223\n",
      "  Batch [850/1299] D_loss: -0.0011, G_loss: 0.3746\n",
      "  Batch [860/1299] D_loss: -0.0530, G_loss: 0.5219\n",
      "  Batch [870/1299] D_loss: -0.1373, G_loss: 0.7321\n",
      "  Batch [880/1299] D_loss: -0.1536, G_loss: 0.6995\n",
      "  Batch [890/1299] D_loss: 0.0191, G_loss: 0.5027\n",
      "  Batch [900/1299] D_loss: -0.0053, G_loss: 0.3043\n",
      "  Batch [910/1299] D_loss: -1.0255, G_loss: -2.1722\n",
      "  Batch [920/1299] D_loss: -0.1553, G_loss: 0.0467\n",
      "  Batch [930/1299] D_loss: -0.5612, G_loss: -0.3710\n",
      "  Batch [940/1299] D_loss: -0.2623, G_loss: -0.1457\n",
      "  Batch [950/1299] D_loss: -0.0789, G_loss: 0.2614\n",
      "  Batch [960/1299] D_loss: -0.1181, G_loss: 0.4745\n",
      "  Batch [970/1299] D_loss: -0.0234, G_loss: 0.4823\n",
      "  Batch [980/1299] D_loss: -0.0111, G_loss: 0.4369\n",
      "  Batch [990/1299] D_loss: -0.0499, G_loss: 0.3673\n",
      "  Batch [1000/1299] D_loss: -0.0534, G_loss: 0.4005\n",
      "  Batch [1010/1299] D_loss: 1.4513, G_loss: 0.0204\n",
      "  Batch [1020/1299] D_loss: -0.0835, G_loss: 0.1116\n",
      "  Batch [1030/1299] D_loss: -0.7261, G_loss: -0.2309\n",
      "  Batch [1040/1299] D_loss: -0.1074, G_loss: 0.2129\n",
      "  Batch [1050/1299] D_loss: 0.0062, G_loss: 0.3495\n",
      "  Batch [1060/1299] D_loss: -0.0726, G_loss: 0.4561\n",
      "  Batch [1070/1299] D_loss: -0.1988, G_loss: 0.4800\n",
      "  Batch [1080/1299] D_loss: 0.0056, G_loss: 0.3973\n",
      "  Batch [1090/1299] D_loss: -0.0073, G_loss: 0.3965\n",
      "  Batch [1100/1299] D_loss: -3.9974, G_loss: -4.3808\n",
      "  Batch [1110/1299] D_loss: -1.1937, G_loss: -3.0198\n",
      "  Batch [1120/1299] D_loss: -0.1490, G_loss: -0.4468\n",
      "  Batch [1130/1299] D_loss: -1.5483, G_loss: -2.0336\n",
      "  Batch [1140/1299] D_loss: -0.0956, G_loss: -0.0638\n",
      "  Batch [1150/1299] D_loss: -0.0768, G_loss: 0.2825\n",
      "  Batch [1160/1299] D_loss: -0.1160, G_loss: 0.4809\n",
      "  Batch [1170/1299] D_loss: -0.0086, G_loss: 0.4663\n",
      "  Batch [1180/1299] D_loss: -0.0831, G_loss: 0.4402\n",
      "  Batch [1190/1299] D_loss: -0.0374, G_loss: 0.3348\n",
      "  Batch [1200/1299] D_loss: -0.0728, G_loss: 0.2107\n",
      "  Batch [1210/1299] D_loss: -0.8809, G_loss: -0.2315\n",
      "  Batch [1220/1299] D_loss: 0.0149, G_loss: 0.2325\n",
      "  Batch [1230/1299] D_loss: 0.0020, G_loss: 0.3389\n",
      "  Batch [1240/1299] D_loss: -0.2164, G_loss: 0.4820\n",
      "  Batch [1250/1299] D_loss: -0.0675, G_loss: 0.3929\n",
      "  Batch [1260/1299] D_loss: -0.0580, G_loss: 0.2758\n",
      "  Batch [1270/1299] D_loss: -0.0371, G_loss: 0.2092\n",
      "  Batch [1280/1299] D_loss: -2.0894, G_loss: -4.7384\n",
      "  Batch [1290/1299] D_loss: -0.0053, G_loss: 0.1361\n",
      "\n",
      "Epoch 44 Summary:\n",
      "  Average D_loss: -0.1337\n",
      "  Average G_loss: -0.0684\n",
      "\n",
      "Epoch [45/100]\n",
      "  Batch [0/1299] D_loss: -0.0451, G_loss: 0.2468\n",
      "  Batch [10/1299] D_loss: -0.1296, G_loss: 0.5814\n",
      "  Batch [20/1299] D_loss: 0.0526, G_loss: 0.5212\n",
      "  Batch [30/1299] D_loss: 0.0466, G_loss: 0.4210\n",
      "  Batch [40/1299] D_loss: -1.8399, G_loss: -2.6609\n",
      "  Batch [50/1299] D_loss: -0.2098, G_loss: 0.0393\n",
      "  Batch [60/1299] D_loss: -0.1885, G_loss: 0.1622\n",
      "  Batch [70/1299] D_loss: -0.0630, G_loss: 0.1840\n",
      "  Batch [80/1299] D_loss: -0.5210, G_loss: 0.1074\n",
      "  Batch [90/1299] D_loss: -0.1170, G_loss: 0.2565\n",
      "  Batch [100/1299] D_loss: -0.1002, G_loss: 0.3448\n",
      "  Batch [110/1299] D_loss: -0.1151, G_loss: 0.3110\n",
      "  Batch [120/1299] D_loss: -0.1386, G_loss: 0.4266\n",
      "  Batch [130/1299] D_loss: -0.0261, G_loss: 0.2833\n",
      "  Batch [140/1299] D_loss: -0.2540, G_loss: 0.0988\n",
      "  Batch [150/1299] D_loss: -0.8642, G_loss: -2.8919\n",
      "  Batch [160/1299] D_loss: -0.0782, G_loss: -0.0123\n",
      "  Batch [170/1299] D_loss: -0.9963, G_loss: -0.4191\n",
      "  Batch [180/1299] D_loss: -0.0519, G_loss: 0.1053\n",
      "  Batch [190/1299] D_loss: -0.3201, G_loss: -0.0147\n",
      "  Batch [200/1299] D_loss: -0.1214, G_loss: 0.2014\n",
      "  Batch [210/1299] D_loss: -0.0519, G_loss: 0.4857\n",
      "  Batch [220/1299] D_loss: -0.1668, G_loss: 0.6448\n",
      "  Batch [230/1299] D_loss: 0.0807, G_loss: 1.0058\n",
      "  Batch [240/1299] D_loss: -0.0847, G_loss: 0.6418\n",
      "  Batch [250/1299] D_loss: -0.1514, G_loss: 0.7304\n",
      "  Batch [260/1299] D_loss: -0.0414, G_loss: 0.3124\n",
      "  Batch [270/1299] D_loss: -0.0238, G_loss: 0.2180\n",
      "  Batch [280/1299] D_loss: -0.0328, G_loss: 0.1244\n",
      "  Batch [290/1299] D_loss: -0.3869, G_loss: -1.2829\n",
      "  Batch [300/1299] D_loss: -0.7074, G_loss: -0.2244\n",
      "  Batch [310/1299] D_loss: -0.3243, G_loss: -0.2908\n",
      "  Batch [320/1299] D_loss: -0.0608, G_loss: 0.2167\n",
      "  Batch [330/1299] D_loss: -0.0471, G_loss: 0.3515\n",
      "  Batch [340/1299] D_loss: -0.0264, G_loss: 0.4144\n",
      "  Batch [350/1299] D_loss: -0.0350, G_loss: 0.4791\n",
      "  Batch [360/1299] D_loss: -0.0026, G_loss: 0.2571\n",
      "  Batch [370/1299] D_loss: -0.0381, G_loss: 0.3276\n",
      "  Batch [380/1299] D_loss: -0.5601, G_loss: -1.1278\n",
      "  Batch [390/1299] D_loss: -0.1529, G_loss: 0.2213\n",
      "  Batch [400/1299] D_loss: -1.4088, G_loss: -0.8472\n",
      "  Batch [410/1299] D_loss: -0.0988, G_loss: 0.0471\n",
      "  Batch [420/1299] D_loss: -0.0663, G_loss: 0.1977\n",
      "  Batch [430/1299] D_loss: -0.2027, G_loss: 0.0674\n",
      "  Batch [440/1299] D_loss: -0.1332, G_loss: 0.0891\n",
      "  Batch [450/1299] D_loss: -0.0704, G_loss: 0.1834\n",
      "  Batch [460/1299] D_loss: 0.0046, G_loss: 0.2649\n",
      "  Batch [470/1299] D_loss: -0.0193, G_loss: 0.4346\n",
      "  Batch [480/1299] D_loss: -0.1701, G_loss: 0.5553\n",
      "  Batch [490/1299] D_loss: 0.0567, G_loss: 0.5357\n",
      "  Batch [500/1299] D_loss: 0.0481, G_loss: 0.5017\n",
      "  Batch [510/1299] D_loss: -0.2169, G_loss: 0.4185\n",
      "  Batch [520/1299] D_loss: -4.1748, G_loss: -6.9292\n",
      "  Batch [530/1299] D_loss: -0.0992, G_loss: 0.0357\n",
      "  Batch [540/1299] D_loss: -0.7852, G_loss: -1.1119\n",
      "  Batch [550/1299] D_loss: -0.0143, G_loss: 0.1458\n",
      "  Batch [560/1299] D_loss: -0.8813, G_loss: -0.7399\n",
      "  Batch [570/1299] D_loss: -0.0414, G_loss: 0.3262\n",
      "  Batch [580/1299] D_loss: -0.0271, G_loss: 0.4282\n",
      "  Batch [590/1299] D_loss: -0.2677, G_loss: 0.5959\n",
      "  Batch [600/1299] D_loss: -0.0442, G_loss: 0.6396\n",
      "  Batch [610/1299] D_loss: 0.0900, G_loss: 0.5605\n",
      "  Batch [620/1299] D_loss: -0.1485, G_loss: 0.5495\n",
      "  Batch [630/1299] D_loss: -0.0046, G_loss: 0.2433\n",
      "  Batch [640/1299] D_loss: -1.2829, G_loss: -3.0302\n",
      "  Batch [650/1299] D_loss: -0.0352, G_loss: 0.1030\n",
      "  Batch [660/1299] D_loss: -0.0299, G_loss: 0.2381\n",
      "  Batch [670/1299] D_loss: -0.1010, G_loss: 0.5296\n",
      "  Batch [680/1299] D_loss: -0.2584, G_loss: 0.3937\n",
      "  Batch [690/1299] D_loss: -0.2096, G_loss: 0.6155\n",
      "  Batch [700/1299] D_loss: -0.0155, G_loss: 0.3525\n",
      "  Batch [710/1299] D_loss: -0.1709, G_loss: -1.6801\n",
      "  Batch [720/1299] D_loss: -0.0013, G_loss: 0.0893\n",
      "  Batch [730/1299] D_loss: 0.0295, G_loss: 0.1280\n",
      "  Batch [740/1299] D_loss: -0.1068, G_loss: 0.3054\n",
      "  Batch [750/1299] D_loss: 0.0331, G_loss: 0.2503\n",
      "  Batch [760/1299] D_loss: 0.0182, G_loss: 0.3583\n",
      "  Batch [770/1299] D_loss: -0.0265, G_loss: 0.2748\n",
      "  Batch [780/1299] D_loss: -0.0186, G_loss: 0.1516\n",
      "  Batch [790/1299] D_loss: -0.0385, G_loss: 0.2786\n",
      "  Batch [800/1299] D_loss: -0.8847, G_loss: -1.4692\n",
      "  Batch [810/1299] D_loss: -0.7637, G_loss: -1.7935\n",
      "  Batch [820/1299] D_loss: -0.0522, G_loss: 0.0839\n",
      "  Batch [830/1299] D_loss: -0.0894, G_loss: 0.0824\n",
      "  Batch [840/1299] D_loss: -0.1148, G_loss: 0.3250\n",
      "  Batch [850/1299] D_loss: -0.1001, G_loss: 0.4433\n",
      "  Batch [860/1299] D_loss: -0.1783, G_loss: 0.5484\n",
      "  Batch [870/1299] D_loss: -0.0559, G_loss: 0.4865\n",
      "  Batch [880/1299] D_loss: 0.0174, G_loss: 0.3203\n",
      "  Batch [890/1299] D_loss: -2.0433, G_loss: -3.1925\n",
      "  Batch [900/1299] D_loss: -0.1124, G_loss: -0.0246\n",
      "  Batch [910/1299] D_loss: -0.0073, G_loss: 0.1779\n",
      "  Batch [920/1299] D_loss: 0.0082, G_loss: 0.1464\n",
      "  Batch [930/1299] D_loss: -0.3421, G_loss: -0.0806\n",
      "  Batch [940/1299] D_loss: -0.0301, G_loss: 0.2171\n",
      "  Batch [950/1299] D_loss: -0.9887, G_loss: -2.7066\n",
      "  Batch [960/1299] D_loss: -0.4217, G_loss: -0.3498\n",
      "  Batch [970/1299] D_loss: -0.1139, G_loss: 0.3322\n",
      "  Batch [980/1299] D_loss: -0.2074, G_loss: 0.6491\n",
      "  Batch [990/1299] D_loss: -0.1107, G_loss: 0.7447\n",
      "  Batch [1000/1299] D_loss: -0.1552, G_loss: 0.6452\n",
      "  Batch [1010/1299] D_loss: -0.0862, G_loss: 0.4543\n",
      "  Batch [1020/1299] D_loss: 0.0046, G_loss: 0.3450\n",
      "  Batch [1030/1299] D_loss: -0.6616, G_loss: -3.3314\n",
      "  Batch [1040/1299] D_loss: 0.0403, G_loss: 0.2644\n",
      "  Batch [1050/1299] D_loss: 0.0048, G_loss: 0.1946\n",
      "  Batch [1060/1299] D_loss: -0.1111, G_loss: 0.2476\n",
      "  Batch [1070/1299] D_loss: 0.0328, G_loss: 0.2212\n",
      "  Batch [1080/1299] D_loss: -0.4308, G_loss: -2.1777\n",
      "  Batch [1090/1299] D_loss: -1.1062, G_loss: -3.0652\n",
      "  Batch [1100/1299] D_loss: -0.5689, G_loss: -0.3605\n",
      "  Batch [1110/1299] D_loss: -0.2136, G_loss: -0.0617\n",
      "  Batch [1120/1299] D_loss: -0.9837, G_loss: -0.1008\n",
      "  Batch [1130/1299] D_loss: -0.0855, G_loss: 0.1198\n",
      "  Batch [1140/1299] D_loss: -0.1449, G_loss: 0.2335\n",
      "  Batch [1150/1299] D_loss: 0.0020, G_loss: 0.5203\n",
      "  Batch [1160/1299] D_loss: -0.3077, G_loss: 0.5719\n",
      "  Batch [1170/1299] D_loss: -0.0091, G_loss: 0.7056\n",
      "  Batch [1180/1299] D_loss: -0.0550, G_loss: 0.5562\n",
      "  Batch [1190/1299] D_loss: 0.0564, G_loss: 0.3929\n",
      "  Batch [1200/1299] D_loss: -0.0456, G_loss: 0.2156\n",
      "  Batch [1210/1299] D_loss: -0.5080, G_loss: -1.9691\n",
      "  Batch [1220/1299] D_loss: -0.4950, G_loss: -0.4613\n",
      "  Batch [1230/1299] D_loss: -0.2100, G_loss: 0.2171\n",
      "  Batch [1240/1299] D_loss: -0.0915, G_loss: 0.0439\n",
      "  Batch [1250/1299] D_loss: -0.0151, G_loss: 0.1792\n",
      "  Batch [1260/1299] D_loss: 0.0030, G_loss: 0.2612\n",
      "  Batch [1270/1299] D_loss: -0.0712, G_loss: 0.4706\n",
      "  Batch [1280/1299] D_loss: -0.1164, G_loss: 0.4167\n",
      "  Batch [1290/1299] D_loss: -0.1417, G_loss: 0.3730\n",
      "\n",
      "Epoch 45 Summary:\n",
      "  Average D_loss: -0.1459\n",
      "  Average G_loss: -0.0731\n",
      "\n",
      "Epoch [46/100]\n",
      "  Batch [0/1299] D_loss: -0.5313, G_loss: -0.0912\n",
      "  Batch [10/1299] D_loss: -0.5225, G_loss: -0.9224\n",
      "  Batch [20/1299] D_loss: -0.2303, G_loss: -0.0070\n",
      "  Batch [30/1299] D_loss: -0.1699, G_loss: 0.1316\n",
      "  Batch [40/1299] D_loss: -1.4538, G_loss: -1.8740\n",
      "  Batch [50/1299] D_loss: -0.0367, G_loss: 0.1925\n",
      "  Batch [60/1299] D_loss: -0.2935, G_loss: 0.0943\n",
      "  Batch [70/1299] D_loss: -0.0130, G_loss: 0.1725\n",
      "  Batch [80/1299] D_loss: -0.1721, G_loss: 0.5892\n",
      "  Batch [90/1299] D_loss: -0.1481, G_loss: 0.6567\n",
      "  Batch [100/1299] D_loss: 0.0959, G_loss: 0.7745\n",
      "  Batch [110/1299] D_loss: -0.0760, G_loss: 0.7323\n",
      "  Batch [120/1299] D_loss: -0.1167, G_loss: 0.7486\n",
      "  Batch [130/1299] D_loss: -0.0288, G_loss: 0.4619\n",
      "  Batch [140/1299] D_loss: -1.7510, G_loss: -2.9122\n",
      "  Batch [150/1299] D_loss: -0.2494, G_loss: 0.0614\n",
      "  Batch [160/1299] D_loss: -0.2530, G_loss: -0.0563\n",
      "  Batch [170/1299] D_loss: -0.0103, G_loss: 0.1924\n",
      "  Batch [180/1299] D_loss: 0.0178, G_loss: 0.3611\n",
      "  Batch [190/1299] D_loss: -0.0490, G_loss: 0.5568\n",
      "  Batch [200/1299] D_loss: -0.1073, G_loss: 0.6775\n",
      "  Batch [210/1299] D_loss: -0.0842, G_loss: 0.5303\n",
      "  Batch [220/1299] D_loss: -0.1884, G_loss: 0.4876\n",
      "  Batch [230/1299] D_loss: -0.0737, G_loss: 0.3496\n",
      "  Batch [240/1299] D_loss: -0.7867, G_loss: -1.1142\n",
      "  Batch [250/1299] D_loss: 0.0212, G_loss: 0.2477\n",
      "  Batch [260/1299] D_loss: 0.0108, G_loss: 0.3296\n",
      "  Batch [270/1299] D_loss: -0.0463, G_loss: 0.3809\n",
      "  Batch [280/1299] D_loss: -0.0804, G_loss: 0.2987\n",
      "  Batch [290/1299] D_loss: -1.7349, G_loss: -4.1532\n",
      "  Batch [300/1299] D_loss: -0.1149, G_loss: -0.2118\n",
      "  Batch [310/1299] D_loss: -0.0985, G_loss: 0.0996\n",
      "  Batch [320/1299] D_loss: -0.1928, G_loss: 0.1030\n",
      "  Batch [330/1299] D_loss: -0.5302, G_loss: 0.1310\n",
      "  Batch [340/1299] D_loss: -0.0609, G_loss: 0.0811\n",
      "  Batch [350/1299] D_loss: -0.3584, G_loss: 0.2222\n",
      "  Batch [360/1299] D_loss: -0.1489, G_loss: 0.3875\n",
      "  Batch [370/1299] D_loss: -0.2934, G_loss: 0.6110\n",
      "  Batch [380/1299] D_loss: -0.1442, G_loss: 0.7222\n",
      "  Batch [390/1299] D_loss: 0.0712, G_loss: 0.6263\n",
      "  Batch [400/1299] D_loss: -0.2086, G_loss: 0.7461\n",
      "  Batch [410/1299] D_loss: -0.0047, G_loss: 0.4126\n",
      "  Batch [420/1299] D_loss: -0.0121, G_loss: 0.1682\n",
      "  Batch [430/1299] D_loss: -0.7043, G_loss: -0.8038\n",
      "  Batch [440/1299] D_loss: -0.1551, G_loss: 0.0470\n",
      "  Batch [450/1299] D_loss: -0.3973, G_loss: 0.1352\n",
      "  Batch [460/1299] D_loss: -0.2608, G_loss: -0.2374\n",
      "  Batch [470/1299] D_loss: -0.9583, G_loss: -1.2483\n",
      "  Batch [480/1299] D_loss: -1.2746, G_loss: -1.5085\n",
      "  Batch [490/1299] D_loss: -0.3509, G_loss: -0.0059\n",
      "  Batch [500/1299] D_loss: -0.1786, G_loss: 0.2901\n",
      "  Batch [510/1299] D_loss: -0.0569, G_loss: 0.4462\n",
      "  Batch [520/1299] D_loss: -0.1589, G_loss: 0.5657\n",
      "  Batch [530/1299] D_loss: -0.2144, G_loss: 0.6379\n",
      "  Batch [540/1299] D_loss: -0.1997, G_loss: 0.4593\n",
      "  Batch [550/1299] D_loss: -0.0622, G_loss: 0.5592\n",
      "  Batch [560/1299] D_loss: -0.0074, G_loss: 0.4411\n",
      "  Batch [570/1299] D_loss: -0.1020, G_loss: 0.2150\n",
      "  Batch [580/1299] D_loss: -0.4398, G_loss: -0.5784\n",
      "  Batch [590/1299] D_loss: -2.2555, G_loss: -4.1083\n",
      "  Batch [600/1299] D_loss: -0.9371, G_loss: -1.0910\n",
      "  Batch [610/1299] D_loss: -0.7903, G_loss: -1.2483\n",
      "  Batch [620/1299] D_loss: -1.3350, G_loss: -0.2456\n",
      "  Batch [630/1299] D_loss: -0.0163, G_loss: 0.1288\n",
      "  Batch [640/1299] D_loss: -0.0073, G_loss: 0.2430\n",
      "  Batch [650/1299] D_loss: -0.0018, G_loss: 0.4324\n",
      "  Batch [660/1299] D_loss: -0.0837, G_loss: 0.5409\n",
      "  Batch [670/1299] D_loss: -0.0844, G_loss: 0.6295\n",
      "  Batch [680/1299] D_loss: -0.3670, G_loss: 0.5998\n",
      "  Batch [690/1299] D_loss: -0.0207, G_loss: 0.4453\n",
      "  Batch [700/1299] D_loss: -0.0262, G_loss: 0.2873\n",
      "  Batch [710/1299] D_loss: 0.0605, G_loss: 0.2819\n",
      "  Batch [720/1299] D_loss: 0.0006, G_loss: 0.2734\n",
      "  Batch [730/1299] D_loss: 0.0356, G_loss: 0.3563\n",
      "  Batch [740/1299] D_loss: 0.0090, G_loss: 0.2290\n",
      "  Batch [750/1299] D_loss: -0.0179, G_loss: 0.1525\n",
      "  Batch [760/1299] D_loss: -0.3062, G_loss: -2.3884\n",
      "  Batch [770/1299] D_loss: -0.2845, G_loss: -0.0580\n",
      "  Batch [780/1299] D_loss: -0.1971, G_loss: 0.4354\n",
      "  Batch [790/1299] D_loss: -0.2049, G_loss: 0.5388\n",
      "  Batch [800/1299] D_loss: -0.0313, G_loss: 0.4161\n",
      "  Batch [810/1299] D_loss: -0.1300, G_loss: 0.4823\n",
      "  Batch [820/1299] D_loss: -6.8171, G_loss: -7.6447\n",
      "  Batch [830/1299] D_loss: -0.0689, G_loss: 0.1164\n",
      "  Batch [840/1299] D_loss: -0.0319, G_loss: 0.1857\n",
      "  Batch [850/1299] D_loss: -0.0491, G_loss: 0.4465\n",
      "  Batch [860/1299] D_loss: -0.0645, G_loss: 0.5144\n",
      "  Batch [870/1299] D_loss: -0.1862, G_loss: 0.5289\n",
      "  Batch [880/1299] D_loss: -0.0543, G_loss: 0.3029\n",
      "  Batch [890/1299] D_loss: -0.0823, G_loss: 0.2169\n",
      "  Batch [900/1299] D_loss: -0.7458, G_loss: 0.0045\n",
      "  Batch [910/1299] D_loss: -1.1344, G_loss: -1.9300\n",
      "  Batch [920/1299] D_loss: -0.0414, G_loss: 0.2860\n",
      "  Batch [930/1299] D_loss: -0.1498, G_loss: 0.3675\n",
      "  Batch [940/1299] D_loss: -0.1297, G_loss: 0.3849\n",
      "  Batch [950/1299] D_loss: -0.0071, G_loss: 0.4304\n",
      "  Batch [960/1299] D_loss: -0.0915, G_loss: 0.4285\n",
      "  Batch [970/1299] D_loss: -0.0636, G_loss: 0.3080\n",
      "  Batch [980/1299] D_loss: -1.6712, G_loss: -1.7196\n",
      "  Batch [990/1299] D_loss: 0.0061, G_loss: 0.1396\n",
      "  Batch [1000/1299] D_loss: 0.0045, G_loss: 0.1284\n",
      "  Batch [1010/1299] D_loss: 0.0080, G_loss: 0.0968\n",
      "  Batch [1020/1299] D_loss: -0.0013, G_loss: 0.1000\n",
      "  Batch [1030/1299] D_loss: -0.0023, G_loss: 0.1144\n",
      "  Batch [1040/1299] D_loss: -0.0097, G_loss: 0.1172\n",
      "  Batch [1050/1299] D_loss: -1.0555, G_loss: -0.7691\n",
      "  Batch [1060/1299] D_loss: -0.2227, G_loss: 0.0531\n",
      "  Batch [1070/1299] D_loss: -0.0639, G_loss: 0.1044\n",
      "  Batch [1080/1299] D_loss: -2.1177, G_loss: -1.7600\n",
      "  Batch [1090/1299] D_loss: -0.0149, G_loss: 0.1461\n",
      "  Batch [1100/1299] D_loss: -0.7148, G_loss: 0.0778\n",
      "  Batch [1110/1299] D_loss: -1.0544, G_loss: -2.0257\n",
      "  Batch [1120/1299] D_loss: -0.0517, G_loss: 0.2319\n",
      "  Batch [1130/1299] D_loss: -0.1559, G_loss: 0.5508\n",
      "  Batch [1140/1299] D_loss: -0.1934, G_loss: 0.6996\n",
      "  Batch [1150/1299] D_loss: -0.2334, G_loss: 0.6161\n",
      "  Batch [1160/1299] D_loss: -0.0065, G_loss: 0.5815\n",
      "  Batch [1170/1299] D_loss: -0.0284, G_loss: 0.3008\n",
      "  Batch [1180/1299] D_loss: -3.6942, G_loss: -5.9436\n",
      "  Batch [1190/1299] D_loss: 0.0095, G_loss: 0.1675\n",
      "  Batch [1200/1299] D_loss: -0.0340, G_loss: 0.2722\n",
      "  Batch [1210/1299] D_loss: -0.0521, G_loss: 0.3769\n",
      "  Batch [1220/1299] D_loss: -0.0437, G_loss: 0.5444\n",
      "  Batch [1230/1299] D_loss: -0.1522, G_loss: 0.3974\n",
      "  Batch [1240/1299] D_loss: -0.0906, G_loss: 0.4596\n",
      "  Batch [1250/1299] D_loss: -0.5355, G_loss: -0.4786\n",
      "  Batch [1260/1299] D_loss: -1.4663, G_loss: -2.1450\n",
      "  Batch [1270/1299] D_loss: -0.1070, G_loss: 0.2864\n",
      "  Batch [1280/1299] D_loss: 0.0220, G_loss: 0.3332\n",
      "  Batch [1290/1299] D_loss: 0.0361, G_loss: 0.4058\n",
      "\n",
      "Epoch 46 Summary:\n",
      "  Average D_loss: -0.1614\n",
      "  Average G_loss: -0.0761\n",
      "\n",
      "Epoch [47/100]\n",
      "  Batch [0/1299] D_loss: 0.0591, G_loss: 0.5339\n",
      "  Batch [10/1299] D_loss: -0.0674, G_loss: 0.4026\n",
      "  Batch [20/1299] D_loss: -0.0357, G_loss: 0.2727\n",
      "  Batch [30/1299] D_loss: -1.7429, G_loss: -0.0469\n",
      "  Batch [40/1299] D_loss: -0.0483, G_loss: 0.2050\n",
      "  Batch [50/1299] D_loss: -0.0459, G_loss: 0.2700\n",
      "  Batch [60/1299] D_loss: -0.0534, G_loss: 0.2847\n",
      "  Batch [70/1299] D_loss: -0.0755, G_loss: 0.4684\n",
      "  Batch [80/1299] D_loss: 0.0097, G_loss: 0.2799\n",
      "  Batch [90/1299] D_loss: -0.0602, G_loss: 0.1654\n",
      "  Batch [100/1299] D_loss: 0.0276, G_loss: 0.2368\n",
      "  Batch [110/1299] D_loss: -0.0223, G_loss: 0.3031\n",
      "  Batch [120/1299] D_loss: -0.0237, G_loss: 0.1773\n",
      "  Batch [130/1299] D_loss: -0.0488, G_loss: 0.0771\n",
      "  Batch [140/1299] D_loss: -0.1065, G_loss: 0.1251\n",
      "  Batch [150/1299] D_loss: -0.0623, G_loss: 0.3769\n",
      "  Batch [160/1299] D_loss: 0.0223, G_loss: 0.4432\n",
      "  Batch [170/1299] D_loss: -0.1957, G_loss: 0.7881\n",
      "  Batch [180/1299] D_loss: -0.2662, G_loss: 0.7039\n",
      "  Batch [190/1299] D_loss: -0.0421, G_loss: 0.3846\n",
      "  Batch [200/1299] D_loss: -0.0049, G_loss: 0.2327\n",
      "  Batch [210/1299] D_loss: -0.8883, G_loss: -0.1734\n",
      "  Batch [220/1299] D_loss: -0.9129, G_loss: -0.9361\n",
      "  Batch [230/1299] D_loss: -0.1810, G_loss: 0.1024\n",
      "  Batch [240/1299] D_loss: -0.8418, G_loss: -1.2802\n",
      "  Batch [250/1299] D_loss: -0.9446, G_loss: -2.1602\n",
      "  Batch [260/1299] D_loss: -1.0784, G_loss: 0.0780\n",
      "  Batch [270/1299] D_loss: -0.7672, G_loss: -1.0273\n",
      "  Batch [280/1299] D_loss: -0.7011, G_loss: 0.1613\n",
      "  Batch [290/1299] D_loss: -0.2946, G_loss: 0.1347\n",
      "  Batch [300/1299] D_loss: -0.1481, G_loss: 0.5644\n",
      "  Batch [310/1299] D_loss: -0.0535, G_loss: 0.5060\n",
      "  Batch [320/1299] D_loss: -0.0402, G_loss: 0.8332\n",
      "  Batch [330/1299] D_loss: 0.0803, G_loss: 0.6131\n",
      "  Batch [340/1299] D_loss: -0.1120, G_loss: 0.6398\n",
      "  Batch [350/1299] D_loss: -0.1551, G_loss: 0.3741\n",
      "  Batch [360/1299] D_loss: -2.1245, G_loss: -1.5548\n",
      "  Batch [370/1299] D_loss: -0.5808, G_loss: 0.0124\n",
      "  Batch [380/1299] D_loss: -0.0476, G_loss: 0.0670\n",
      "  Batch [390/1299] D_loss: -0.4822, G_loss: -0.5818\n",
      "  Batch [400/1299] D_loss: -0.1305, G_loss: 0.3151\n",
      "  Batch [410/1299] D_loss: -0.1151, G_loss: 0.5497\n",
      "  Batch [420/1299] D_loss: -0.2986, G_loss: 0.5190\n",
      "  Batch [430/1299] D_loss: -0.1409, G_loss: 0.4878\n",
      "  Batch [440/1299] D_loss: -2.2326, G_loss: -0.6688\n",
      "  Batch [450/1299] D_loss: -0.3429, G_loss: -0.1658\n",
      "  Batch [460/1299] D_loss: -1.0062, G_loss: -0.5758\n",
      "  Batch [470/1299] D_loss: -0.9232, G_loss: -0.5709\n",
      "  Batch [480/1299] D_loss: -0.1028, G_loss: 0.3653\n",
      "  Batch [490/1299] D_loss: -0.0323, G_loss: 0.4670\n",
      "  Batch [500/1299] D_loss: -0.1387, G_loss: 0.5070\n",
      "  Batch [510/1299] D_loss: -0.1294, G_loss: 0.3739\n",
      "  Batch [520/1299] D_loss: -4.4459, G_loss: -7.0927\n",
      "  Batch [530/1299] D_loss: -0.0193, G_loss: 0.2775\n",
      "  Batch [540/1299] D_loss: -0.0505, G_loss: 0.3768\n",
      "  Batch [550/1299] D_loss: -0.0115, G_loss: 0.4102\n",
      "  Batch [560/1299] D_loss: -0.0979, G_loss: 0.4070\n",
      "  Batch [570/1299] D_loss: -0.0562, G_loss: 0.1889\n",
      "  Batch [580/1299] D_loss: -1.1715, G_loss: -1.1644\n",
      "  Batch [590/1299] D_loss: -0.2904, G_loss: -0.0387\n",
      "  Batch [600/1299] D_loss: 0.0051, G_loss: 0.1876\n",
      "  Batch [610/1299] D_loss: -0.0331, G_loss: 0.3502\n",
      "  Batch [620/1299] D_loss: -0.0448, G_loss: 0.4239\n",
      "  Batch [630/1299] D_loss: -0.0285, G_loss: 0.4723\n",
      "  Batch [640/1299] D_loss: 0.0088, G_loss: 0.3379\n",
      "  Batch [650/1299] D_loss: -2.3700, G_loss: -3.0949\n",
      "  Batch [660/1299] D_loss: -0.2168, G_loss: 0.1124\n",
      "  Batch [670/1299] D_loss: -0.1755, G_loss: 0.2578\n",
      "  Batch [680/1299] D_loss: -0.0355, G_loss: 0.2992\n",
      "  Batch [690/1299] D_loss: -0.0402, G_loss: 0.3538\n",
      "  Batch [700/1299] D_loss: -0.0350, G_loss: 0.3450\n",
      "  Batch [710/1299] D_loss: -0.0398, G_loss: 0.3901\n",
      "  Batch [720/1299] D_loss: -0.0649, G_loss: 0.2735\n",
      "  Batch [730/1299] D_loss: -0.6467, G_loss: -2.3671\n",
      "  Batch [740/1299] D_loss: -0.0646, G_loss: 0.0645\n",
      "  Batch [750/1299] D_loss: -0.0283, G_loss: 0.1790\n",
      "  Batch [760/1299] D_loss: -0.1021, G_loss: 0.4244\n",
      "  Batch [770/1299] D_loss: -0.1667, G_loss: 0.6081\n",
      "  Batch [780/1299] D_loss: -0.1632, G_loss: 0.6411\n",
      "  Batch [790/1299] D_loss: -0.0211, G_loss: 0.5179\n",
      "  Batch [800/1299] D_loss: -0.0179, G_loss: 0.2928\n",
      "  Batch [810/1299] D_loss: -0.8720, G_loss: -0.9888\n",
      "  Batch [820/1299] D_loss: -0.5317, G_loss: -0.3782\n",
      "  Batch [830/1299] D_loss: -0.0177, G_loss: 0.1754\n",
      "  Batch [840/1299] D_loss: -0.0707, G_loss: 0.3727\n",
      "  Batch [850/1299] D_loss: -0.1600, G_loss: 0.4803\n",
      "  Batch [860/1299] D_loss: -0.0106, G_loss: 0.4387\n",
      "  Batch [870/1299] D_loss: -0.0860, G_loss: 0.4816\n",
      "  Batch [880/1299] D_loss: -0.0384, G_loss: 0.3944\n",
      "  Batch [890/1299] D_loss: -2.0309, G_loss: -2.8422\n",
      "  Batch [900/1299] D_loss: -0.0152, G_loss: 0.1699\n",
      "  Batch [910/1299] D_loss: -0.0232, G_loss: 0.1886\n",
      "  Batch [920/1299] D_loss: 0.0285, G_loss: 0.3150\n",
      "  Batch [930/1299] D_loss: -0.0510, G_loss: 0.2779\n",
      "  Batch [940/1299] D_loss: -0.1087, G_loss: 0.4119\n",
      "  Batch [950/1299] D_loss: -0.0007, G_loss: 0.2843\n",
      "  Batch [960/1299] D_loss: -0.4986, G_loss: -0.0410\n",
      "  Batch [970/1299] D_loss: -1.0305, G_loss: -1.2720\n",
      "  Batch [980/1299] D_loss: 0.0160, G_loss: 0.1689\n",
      "  Batch [990/1299] D_loss: -0.1092, G_loss: 0.3693\n",
      "  Batch [1000/1299] D_loss: 0.0449, G_loss: 0.3023\n",
      "  Batch [1010/1299] D_loss: 0.0437, G_loss: 0.3553\n",
      "  Batch [1020/1299] D_loss: -0.0505, G_loss: 0.3210\n",
      "  Batch [1030/1299] D_loss: -0.0687, G_loss: 0.1764\n",
      "  Batch [1040/1299] D_loss: -0.1592, G_loss: -0.0520\n",
      "  Batch [1050/1299] D_loss: -0.9117, G_loss: -3.0799\n",
      "  Batch [1060/1299] D_loss: 0.0190, G_loss: 0.1711\n",
      "  Batch [1070/1299] D_loss: -0.1808, G_loss: 0.0793\n",
      "  Batch [1080/1299] D_loss: -0.0230, G_loss: 0.3514\n",
      "  Batch [1090/1299] D_loss: -0.0023, G_loss: 0.5695\n",
      "  Batch [1100/1299] D_loss: -0.0824, G_loss: 0.8079\n",
      "  Batch [1110/1299] D_loss: -0.0632, G_loss: 0.6830\n",
      "  Batch [1120/1299] D_loss: -0.0620, G_loss: 0.0996\n",
      "  Batch [1130/1299] D_loss: -1.1717, G_loss: 0.0740\n",
      "  Batch [1140/1299] D_loss: -0.0813, G_loss: 0.1554\n",
      "  Batch [1150/1299] D_loss: -0.5755, G_loss: -2.1686\n",
      "  Batch [1160/1299] D_loss: -0.5770, G_loss: -1.0782\n",
      "  Batch [1170/1299] D_loss: -2.2015, G_loss: -4.9006\n",
      "  Batch [1180/1299] D_loss: 0.0004, G_loss: 0.2580\n",
      "  Batch [1190/1299] D_loss: -0.1270, G_loss: 0.5361\n",
      "  Batch [1200/1299] D_loss: -0.0206, G_loss: 0.6189\n",
      "  Batch [1210/1299] D_loss: -0.0996, G_loss: 0.7232\n",
      "  Batch [1220/1299] D_loss: -0.0566, G_loss: 0.7317\n",
      "  Batch [1230/1299] D_loss: -0.0534, G_loss: 0.5097\n",
      "  Batch [1240/1299] D_loss: 0.0103, G_loss: 0.4095\n",
      "  Batch [1250/1299] D_loss: -3.5456, G_loss: -8.1235\n",
      "  Batch [1260/1299] D_loss: -1.7835, G_loss: -4.0313\n",
      "  Batch [1270/1299] D_loss: -0.1186, G_loss: 0.1322\n",
      "  Batch [1280/1299] D_loss: -0.1699, G_loss: 0.2840\n",
      "  Batch [1290/1299] D_loss: -0.1625, G_loss: 0.4059\n",
      "\n",
      "Epoch 47 Summary:\n",
      "  Average D_loss: -0.1593\n",
      "  Average G_loss: -0.0753\n",
      "\n",
      "Epoch [48/100]\n",
      "  Batch [0/1299] D_loss: -0.2123, G_loss: 0.4382\n",
      "  Batch [10/1299] D_loss: 0.0155, G_loss: 0.3501\n",
      "  Batch [20/1299] D_loss: -0.5808, G_loss: 0.2879\n",
      "  Batch [30/1299] D_loss: -0.2392, G_loss: 0.1123\n",
      "  Batch [40/1299] D_loss: -0.0752, G_loss: 0.3494\n",
      "  Batch [50/1299] D_loss: -0.0392, G_loss: 0.4750\n",
      "  Batch [60/1299] D_loss: -0.1430, G_loss: 0.7652\n",
      "  Batch [70/1299] D_loss: 0.0855, G_loss: 0.5540\n",
      "  Batch [80/1299] D_loss: 0.0481, G_loss: 0.4316\n",
      "  Batch [90/1299] D_loss: -0.0515, G_loss: 0.3545\n",
      "  Batch [100/1299] D_loss: -1.4948, G_loss: -4.9945\n",
      "  Batch [110/1299] D_loss: -0.6300, G_loss: -0.8322\n",
      "  Batch [120/1299] D_loss: -0.2426, G_loss: -0.1933\n",
      "  Batch [130/1299] D_loss: -0.6911, G_loss: 0.1547\n",
      "  Batch [140/1299] D_loss: -0.0139, G_loss: 0.1423\n",
      "  Batch [150/1299] D_loss: -0.0916, G_loss: 0.3635\n",
      "  Batch [160/1299] D_loss: 0.0230, G_loss: 0.6228\n",
      "  Batch [170/1299] D_loss: -0.0920, G_loss: 0.6837\n",
      "  Batch [180/1299] D_loss: -0.1720, G_loss: 0.5905\n",
      "  Batch [190/1299] D_loss: -0.1035, G_loss: 0.6185\n",
      "  Batch [200/1299] D_loss: -0.1322, G_loss: 0.3939\n",
      "  Batch [210/1299] D_loss: -3.3113, G_loss: -4.7629\n",
      "  Batch [220/1299] D_loss: -0.5083, G_loss: -1.6763\n",
      "  Batch [230/1299] D_loss: -0.0386, G_loss: 0.1608\n",
      "  Batch [240/1299] D_loss: -0.1149, G_loss: 0.3967\n",
      "  Batch [250/1299] D_loss: -0.0515, G_loss: 0.5306\n",
      "  Batch [260/1299] D_loss: -0.2793, G_loss: 0.6426\n",
      "  Batch [270/1299] D_loss: -0.1593, G_loss: 0.6942\n",
      "  Batch [280/1299] D_loss: -0.0757, G_loss: 0.4760\n",
      "  Batch [290/1299] D_loss: -0.0918, G_loss: 0.4466\n",
      "  Batch [300/1299] D_loss: -0.9267, G_loss: -0.5651\n",
      "  Batch [310/1299] D_loss: -1.5915, G_loss: -1.9402\n",
      "  Batch [320/1299] D_loss: -0.0732, G_loss: 0.1010\n",
      "  Batch [330/1299] D_loss: -1.3620, G_loss: -0.5897\n",
      "  Batch [340/1299] D_loss: -0.0967, G_loss: 0.3712\n",
      "  Batch [350/1299] D_loss: -0.1211, G_loss: 0.4893\n",
      "  Batch [360/1299] D_loss: 0.0201, G_loss: 0.5540\n",
      "  Batch [370/1299] D_loss: -0.0070, G_loss: 0.6278\n",
      "  Batch [380/1299] D_loss: -0.1003, G_loss: 0.5451\n",
      "  Batch [390/1299] D_loss: 0.0160, G_loss: 0.2150\n",
      "  Batch [400/1299] D_loss: -1.5489, G_loss: -2.0824\n",
      "  Batch [410/1299] D_loss: -0.1523, G_loss: -0.0309\n",
      "  Batch [420/1299] D_loss: -0.6258, G_loss: -2.8591\n",
      "  Batch [430/1299] D_loss: -0.2811, G_loss: 0.0861\n",
      "  Batch [440/1299] D_loss: -0.0271, G_loss: 0.2892\n",
      "  Batch [450/1299] D_loss: 0.0134, G_loss: 0.4661\n",
      "  Batch [460/1299] D_loss: -0.0098, G_loss: 0.6021\n",
      "  Batch [470/1299] D_loss: 0.0467, G_loss: 0.3688\n",
      "  Batch [480/1299] D_loss: -0.1192, G_loss: 0.3776\n",
      "  Batch [490/1299] D_loss: -0.0644, G_loss: 0.2024\n",
      "  Batch [500/1299] D_loss: -0.0401, G_loss: 0.0899\n",
      "  Batch [510/1299] D_loss: -0.0784, G_loss: 0.1085\n",
      "  Batch [520/1299] D_loss: 0.0387, G_loss: 0.2894\n",
      "  Batch [530/1299] D_loss: 0.0284, G_loss: 0.2344\n",
      "  Batch [540/1299] D_loss: -0.0926, G_loss: 0.2968\n",
      "  Batch [550/1299] D_loss: -0.0554, G_loss: 0.3056\n",
      "  Batch [560/1299] D_loss: -0.9086, G_loss: -2.4891\n",
      "  Batch [570/1299] D_loss: -0.6936, G_loss: -0.5873\n",
      "  Batch [580/1299] D_loss: -0.4051, G_loss: -0.4697\n",
      "  Batch [590/1299] D_loss: -0.0498, G_loss: 0.3428\n",
      "  Batch [600/1299] D_loss: -0.2189, G_loss: 0.6957\n",
      "  Batch [610/1299] D_loss: -0.2710, G_loss: 0.8324\n",
      "  Batch [620/1299] D_loss: -0.0480, G_loss: 0.7284\n",
      "  Batch [630/1299] D_loss: -0.0837, G_loss: 0.5585\n",
      "  Batch [640/1299] D_loss: -0.0484, G_loss: 0.4465\n",
      "  Batch [650/1299] D_loss: -0.0593, G_loss: 0.0188\n",
      "  Batch [660/1299] D_loss: -0.0962, G_loss: 0.1960\n",
      "  Batch [670/1299] D_loss: -1.4343, G_loss: -1.7320\n",
      "  Batch [680/1299] D_loss: -0.1816, G_loss: 0.0157\n",
      "  Batch [690/1299] D_loss: 0.0376, G_loss: 0.1189\n",
      "  Batch [700/1299] D_loss: -0.0483, G_loss: 0.1318\n",
      "  Batch [710/1299] D_loss: -0.0531, G_loss: 0.2300\n",
      "  Batch [720/1299] D_loss: -0.0104, G_loss: 0.2776\n",
      "  Batch [730/1299] D_loss: -0.0272, G_loss: 0.3419\n",
      "  Batch [740/1299] D_loss: -0.1842, G_loss: 0.5019\n",
      "  Batch [750/1299] D_loss: -1.8857, G_loss: -2.0324\n",
      "  Batch [760/1299] D_loss: -1.1031, G_loss: -1.0620\n",
      "  Batch [770/1299] D_loss: -1.0955, G_loss: -1.3078\n",
      "  Batch [780/1299] D_loss: -0.0564, G_loss: 0.3396\n",
      "  Batch [790/1299] D_loss: -0.1763, G_loss: 0.3716\n",
      "  Batch [800/1299] D_loss: -0.0708, G_loss: 0.5607\n",
      "  Batch [810/1299] D_loss: 0.0016, G_loss: 0.5534\n",
      "  Batch [820/1299] D_loss: -0.1192, G_loss: 0.5712\n",
      "  Batch [830/1299] D_loss: -0.0728, G_loss: 0.3696\n",
      "  Batch [840/1299] D_loss: -1.8297, G_loss: -6.7243\n",
      "  Batch [850/1299] D_loss: -0.9385, G_loss: -0.2975\n",
      "  Batch [860/1299] D_loss: -0.1412, G_loss: 0.3094\n",
      "  Batch [870/1299] D_loss: -0.0787, G_loss: 0.3218\n",
      "  Batch [880/1299] D_loss: -0.0292, G_loss: 0.4938\n",
      "  Batch [890/1299] D_loss: -0.0311, G_loss: 0.4666\n",
      "  Batch [900/1299] D_loss: -0.0873, G_loss: 0.5510\n",
      "  Batch [910/1299] D_loss: -0.0941, G_loss: 0.5438\n",
      "  Batch [920/1299] D_loss: -0.0311, G_loss: 0.2813\n",
      "  Batch [930/1299] D_loss: -1.0140, G_loss: -1.2535\n",
      "  Batch [940/1299] D_loss: 0.0089, G_loss: 0.1856\n",
      "  Batch [950/1299] D_loss: -0.1162, G_loss: 0.3193\n",
      "  Batch [960/1299] D_loss: -0.0039, G_loss: 0.4443\n",
      "  Batch [970/1299] D_loss: -0.0463, G_loss: 0.4131\n",
      "  Batch [980/1299] D_loss: -0.0626, G_loss: 0.3040\n",
      "  Batch [990/1299] D_loss: -0.1240, G_loss: 0.2268\n",
      "  Batch [1000/1299] D_loss: -0.3445, G_loss: -0.3697\n",
      "  Batch [1010/1299] D_loss: -0.4482, G_loss: -0.3536\n",
      "  Batch [1020/1299] D_loss: -0.0909, G_loss: -0.0041\n",
      "  Batch [1030/1299] D_loss: -0.5224, G_loss: -0.4986\n",
      "  Batch [1040/1299] D_loss: -0.5511, G_loss: -0.1390\n",
      "  Batch [1050/1299] D_loss: -1.0152, G_loss: -0.3405\n",
      "  Batch [1060/1299] D_loss: -1.0978, G_loss: -1.7085\n",
      "  Batch [1070/1299] D_loss: -0.0356, G_loss: 0.3172\n",
      "  Batch [1080/1299] D_loss: -0.0642, G_loss: 0.5240\n",
      "  Batch [1090/1299] D_loss: -0.1215, G_loss: 0.5052\n",
      "  Batch [1100/1299] D_loss: -0.2009, G_loss: 0.5765\n",
      "  Batch [1110/1299] D_loss: 0.0708, G_loss: 0.6064\n",
      "  Batch [1120/1299] D_loss: -0.1337, G_loss: 0.5939\n",
      "  Batch [1130/1299] D_loss: 0.0206, G_loss: 0.2428\n",
      "  Batch [1140/1299] D_loss: -0.8029, G_loss: -1.8609\n",
      "  Batch [1150/1299] D_loss: 0.0720, G_loss: 0.1328\n",
      "  Batch [1160/1299] D_loss: -0.1226, G_loss: 0.4472\n",
      "  Batch [1170/1299] D_loss: -0.1403, G_loss: 0.5514\n",
      "  Batch [1180/1299] D_loss: 0.0980, G_loss: 0.3841\n",
      "  Batch [1190/1299] D_loss: -0.0668, G_loss: 0.3631\n",
      "  Batch [1200/1299] D_loss: -0.0406, G_loss: 0.2663\n",
      "  Batch [1210/1299] D_loss: -0.1403, G_loss: -0.0008\n",
      "  Batch [1220/1299] D_loss: -0.0350, G_loss: 0.1621\n",
      "  Batch [1230/1299] D_loss: 0.0483, G_loss: 0.1394\n",
      "  Batch [1240/1299] D_loss: -0.3233, G_loss: -0.1506\n",
      "  Batch [1250/1299] D_loss: -0.3021, G_loss: 0.0995\n",
      "  Batch [1260/1299] D_loss: -0.2280, G_loss: -0.0792\n",
      "  Batch [1270/1299] D_loss: -0.0874, G_loss: 0.1451\n",
      "  Batch [1280/1299] D_loss: -0.0970, G_loss: -0.0839\n",
      "  Batch [1290/1299] D_loss: -1.1206, G_loss: -0.4032\n",
      "\n",
      "Epoch 48 Summary:\n",
      "  Average D_loss: -0.1625\n",
      "  Average G_loss: -0.1012\n",
      "\n",
      "Epoch [49/100]\n",
      "  Batch [0/1299] D_loss: -0.1944, G_loss: 0.1138\n",
      "  Batch [10/1299] D_loss: -0.4544, G_loss: 0.0326\n",
      "  Batch [20/1299] D_loss: -0.2477, G_loss: 0.0012\n",
      "  Batch [30/1299] D_loss: -0.0100, G_loss: 0.4470\n",
      "  Batch [40/1299] D_loss: -0.1570, G_loss: 0.6229\n",
      "  Batch [50/1299] D_loss: -0.3707, G_loss: 0.7252\n",
      "  Batch [60/1299] D_loss: -0.1041, G_loss: 0.8370\n",
      "  Batch [70/1299] D_loss: -0.1100, G_loss: 0.5873\n",
      "  Batch [80/1299] D_loss: -0.0134, G_loss: 0.5475\n",
      "  Batch [90/1299] D_loss: -0.0281, G_loss: 0.3273\n",
      "  Batch [100/1299] D_loss: -1.8358, G_loss: -5.1459\n",
      "  Batch [110/1299] D_loss: -0.6019, G_loss: -0.2277\n",
      "  Batch [120/1299] D_loss: -0.0843, G_loss: 0.3065\n",
      "  Batch [130/1299] D_loss: -0.1072, G_loss: 0.3738\n",
      "  Batch [140/1299] D_loss: -0.0708, G_loss: 0.7011\n",
      "  Batch [150/1299] D_loss: -0.0233, G_loss: 0.5735\n",
      "  Batch [160/1299] D_loss: -0.1348, G_loss: 0.5332\n",
      "  Batch [170/1299] D_loss: 0.0365, G_loss: 0.3612\n",
      "  Batch [180/1299] D_loss: -0.0529, G_loss: 0.2051\n",
      "  Batch [190/1299] D_loss: -1.0300, G_loss: -0.6931\n",
      "  Batch [200/1299] D_loss: -1.7905, G_loss: -0.6975\n",
      "  Batch [210/1299] D_loss: -0.2941, G_loss: -1.1501\n",
      "  Batch [220/1299] D_loss: 0.0007, G_loss: 0.1885\n",
      "  Batch [230/1299] D_loss: -0.0252, G_loss: 0.3628\n",
      "  Batch [240/1299] D_loss: -0.0940, G_loss: 0.4240\n",
      "  Batch [250/1299] D_loss: -0.0959, G_loss: 0.2951\n",
      "  Batch [260/1299] D_loss: -0.0605, G_loss: 0.2501\n",
      "  Batch [270/1299] D_loss: -0.8434, G_loss: -1.0412\n",
      "  Batch [280/1299] D_loss: -0.5807, G_loss: -0.8904\n",
      "  Batch [290/1299] D_loss: -1.4041, G_loss: -3.3231\n",
      "  Batch [300/1299] D_loss: -0.6553, G_loss: 0.0184\n",
      "  Batch [310/1299] D_loss: -0.1418, G_loss: 0.1319\n",
      "  Batch [320/1299] D_loss: -0.1255, G_loss: 0.1265\n",
      "  Batch [330/1299] D_loss: -0.0916, G_loss: 0.3859\n",
      "  Batch [340/1299] D_loss: -0.1205, G_loss: 0.5295\n",
      "  Batch [350/1299] D_loss: 0.0153, G_loss: 0.6285\n",
      "  Batch [360/1299] D_loss: -0.0192, G_loss: 0.7814\n",
      "  Batch [370/1299] D_loss: -0.1394, G_loss: 0.7278\n",
      "  Batch [380/1299] D_loss: 0.0665, G_loss: 0.4537\n",
      "  Batch [390/1299] D_loss: -0.0296, G_loss: 0.3198\n",
      "  Batch [400/1299] D_loss: -0.1856, G_loss: 0.0228\n",
      "  Batch [410/1299] D_loss: -0.8892, G_loss: -0.5722\n",
      "  Batch [420/1299] D_loss: -0.7693, G_loss: -0.0629\n",
      "  Batch [430/1299] D_loss: -0.1474, G_loss: 0.2224\n",
      "  Batch [440/1299] D_loss: -0.5181, G_loss: -0.1333\n",
      "  Batch [450/1299] D_loss: -1.5613, G_loss: -2.8710\n",
      "  Batch [460/1299] D_loss: -1.5223, G_loss: -0.4773\n",
      "  Batch [470/1299] D_loss: -0.5578, G_loss: -0.1300\n",
      "  Batch [480/1299] D_loss: -0.5034, G_loss: -0.6286\n",
      "  Batch [490/1299] D_loss: -0.7386, G_loss: -1.0417\n",
      "  Batch [500/1299] D_loss: -0.3979, G_loss: -0.1668\n",
      "  Batch [510/1299] D_loss: -0.3031, G_loss: 0.1523\n",
      "  Batch [520/1299] D_loss: -1.5061, G_loss: -4.3132\n",
      "  Batch [530/1299] D_loss: -1.7116, G_loss: -0.3295\n",
      "  Batch [540/1299] D_loss: -0.2107, G_loss: 0.1265\n",
      "  Batch [550/1299] D_loss: -0.1975, G_loss: -0.1243\n",
      "  Batch [560/1299] D_loss: -0.2177, G_loss: 0.1913\n",
      "  Batch [570/1299] D_loss: -0.2026, G_loss: 0.4531\n",
      "  Batch [580/1299] D_loss: -0.1412, G_loss: 0.7235\n",
      "  Batch [590/1299] D_loss: -0.2842, G_loss: 0.7838\n",
      "  Batch [600/1299] D_loss: -0.2719, G_loss: 0.9493\n",
      "  Batch [610/1299] D_loss: 0.0123, G_loss: 0.8949\n",
      "  Batch [620/1299] D_loss: -0.1568, G_loss: 0.7981\n",
      "  Batch [630/1299] D_loss: -0.0732, G_loss: 0.4555\n",
      "  Batch [640/1299] D_loss: -1.1249, G_loss: -0.8047\n",
      "  Batch [650/1299] D_loss: -0.2953, G_loss: 0.0903\n",
      "  Batch [660/1299] D_loss: -0.0309, G_loss: 0.2058\n",
      "  Batch [670/1299] D_loss: -0.6041, G_loss: -0.1878\n",
      "  Batch [680/1299] D_loss: -0.7051, G_loss: -0.1372\n",
      "  Batch [690/1299] D_loss: -0.4452, G_loss: -0.0172\n",
      "  Batch [700/1299] D_loss: -1.3227, G_loss: -2.7298\n",
      "  Batch [710/1299] D_loss: -0.1033, G_loss: 0.3102\n",
      "  Batch [720/1299] D_loss: -0.0544, G_loss: 0.4597\n",
      "  Batch [730/1299] D_loss: -0.2262, G_loss: 0.7638\n",
      "  Batch [740/1299] D_loss: -0.1844, G_loss: 0.7133\n",
      "  Batch [750/1299] D_loss: -0.1409, G_loss: 0.9308\n",
      "  Batch [760/1299] D_loss: 0.0451, G_loss: 0.7408\n",
      "  Batch [770/1299] D_loss: -0.0641, G_loss: 0.6363\n",
      "  Batch [780/1299] D_loss: -0.0486, G_loss: 0.3311\n",
      "  Batch [790/1299] D_loss: -1.6158, G_loss: -5.4724\n",
      "  Batch [800/1299] D_loss: -0.3629, G_loss: -0.9490\n",
      "  Batch [810/1299] D_loss: -0.0582, G_loss: 0.3629\n",
      "  Batch [820/1299] D_loss: -0.2193, G_loss: 0.5337\n",
      "  Batch [830/1299] D_loss: 0.0610, G_loss: 0.4780\n",
      "  Batch [840/1299] D_loss: -0.0697, G_loss: 0.4367\n",
      "  Batch [850/1299] D_loss: -0.1826, G_loss: 0.4419\n",
      "  Batch [860/1299] D_loss: -0.0604, G_loss: 0.2839\n",
      "  Batch [870/1299] D_loss: -0.1234, G_loss: 0.0522\n",
      "  Batch [880/1299] D_loss: -0.0068, G_loss: 0.1274\n",
      "  Batch [890/1299] D_loss: -0.0113, G_loss: 0.1218\n",
      "  Batch [900/1299] D_loss: -0.1007, G_loss: 0.1097\n",
      "  Batch [910/1299] D_loss: -0.2025, G_loss: -0.2915\n",
      "  Batch [920/1299] D_loss: -0.0457, G_loss: 0.1453\n",
      "  Batch [930/1299] D_loss: -0.1184, G_loss: 0.2627\n",
      "  Batch [940/1299] D_loss: -0.0326, G_loss: 0.3065\n",
      "  Batch [950/1299] D_loss: -0.0032, G_loss: 0.3028\n",
      "  Batch [960/1299] D_loss: -0.0297, G_loss: 0.3085\n",
      "  Batch [970/1299] D_loss: -5.3872, G_loss: -6.8272\n",
      "  Batch [980/1299] D_loss: -0.5068, G_loss: -0.5133\n",
      "  Batch [990/1299] D_loss: -0.0222, G_loss: 0.1315\n",
      "  Batch [1000/1299] D_loss: -0.0439, G_loss: 0.3061\n",
      "  Batch [1010/1299] D_loss: -0.1308, G_loss: 0.5853\n",
      "  Batch [1020/1299] D_loss: -0.1495, G_loss: 0.5099\n",
      "  Batch [1030/1299] D_loss: 0.0155, G_loss: 0.4207\n",
      "  Batch [1040/1299] D_loss: -0.1252, G_loss: 0.5343\n",
      "  Batch [1050/1299] D_loss: -0.9081, G_loss: -3.5042\n",
      "  Batch [1060/1299] D_loss: -0.0100, G_loss: 0.1688\n",
      "  Batch [1070/1299] D_loss: 0.0252, G_loss: 0.1981\n",
      "  Batch [1080/1299] D_loss: 0.0207, G_loss: 0.3394\n",
      "  Batch [1090/1299] D_loss: -0.0656, G_loss: 0.3606\n",
      "  Batch [1100/1299] D_loss: -0.0018, G_loss: 0.3612\n",
      "  Batch [1110/1299] D_loss: -3.4583, G_loss: -5.5769\n",
      "  Batch [1120/1299] D_loss: -0.9509, G_loss: -0.3645\n",
      "  Batch [1130/1299] D_loss: -0.1702, G_loss: 0.0484\n",
      "  Batch [1140/1299] D_loss: -0.5323, G_loss: -0.0792\n",
      "  Batch [1150/1299] D_loss: -0.1091, G_loss: 0.3299\n",
      "  Batch [1160/1299] D_loss: -0.1550, G_loss: 0.4554\n",
      "  Batch [1170/1299] D_loss: -0.0099, G_loss: 0.4310\n",
      "  Batch [1180/1299] D_loss: 0.0004, G_loss: 0.5135\n",
      "  Batch [1190/1299] D_loss: -0.0755, G_loss: 0.3187\n",
      "  Batch [1200/1299] D_loss: -0.5612, G_loss: -1.2801\n",
      "  Batch [1210/1299] D_loss: -0.0573, G_loss: 0.2497\n",
      "  Batch [1220/1299] D_loss: -0.1012, G_loss: 0.2667\n",
      "  Batch [1230/1299] D_loss: -0.1976, G_loss: 0.5078\n",
      "  Batch [1240/1299] D_loss: -0.0844, G_loss: 0.4760\n",
      "  Batch [1250/1299] D_loss: -0.1314, G_loss: 0.4243\n",
      "  Batch [1260/1299] D_loss: -0.1183, G_loss: 0.2636\n",
      "  Batch [1270/1299] D_loss: -0.0028, G_loss: 0.1453\n",
      "  Batch [1280/1299] D_loss: -0.1872, G_loss: -0.0499\n",
      "  Batch [1290/1299] D_loss: -0.2264, G_loss: 0.0231\n",
      "\n",
      "Epoch 49 Summary:\n",
      "  Average D_loss: -0.2035\n",
      "  Average G_loss: -0.0881\n",
      "\n",
      "Epoch [50/100]\n",
      "  Batch [0/1299] D_loss: -0.1856, G_loss: -0.1203\n",
      "  Batch [10/1299] D_loss: -0.0276, G_loss: 0.1964\n",
      "  Batch [20/1299] D_loss: -0.0650, G_loss: 0.3454\n",
      "  Batch [30/1299] D_loss: -0.1378, G_loss: 0.3744\n",
      "  Batch [40/1299] D_loss: 0.0178, G_loss: 0.3498\n",
      "  Batch [50/1299] D_loss: -0.0598, G_loss: 0.4183\n",
      "  Batch [60/1299] D_loss: -0.0782, G_loss: 0.2915\n",
      "  Batch [70/1299] D_loss: -1.2159, G_loss: -0.1226\n",
      "  Batch [80/1299] D_loss: -0.1625, G_loss: -0.7106\n",
      "  Batch [90/1299] D_loss: -0.9894, G_loss: -0.2276\n",
      "  Batch [100/1299] D_loss: -0.0084, G_loss: 0.1517\n",
      "  Batch [110/1299] D_loss: -0.2489, G_loss: -0.1187\n",
      "  Batch [120/1299] D_loss: -0.0816, G_loss: 0.2169\n",
      "  Batch [130/1299] D_loss: -0.4929, G_loss: 0.0458\n",
      "  Batch [140/1299] D_loss: -0.9167, G_loss: -1.0756\n",
      "  Batch [150/1299] D_loss: -0.8313, G_loss: -0.3500\n",
      "  Batch [160/1299] D_loss: -0.5216, G_loss: -0.0805\n",
      "  Batch [170/1299] D_loss: -0.2023, G_loss: 0.1423\n",
      "  Batch [180/1299] D_loss: -0.2934, G_loss: 0.1064\n",
      "  Batch [190/1299] D_loss: -0.3718, G_loss: -1.3584\n",
      "  Batch [200/1299] D_loss: -0.5653, G_loss: -0.4626\n",
      "  Batch [210/1299] D_loss: -1.7935, G_loss: -0.0200\n",
      "  Batch [220/1299] D_loss: -0.0749, G_loss: 0.2256\n",
      "  Batch [230/1299] D_loss: -0.5018, G_loss: -1.1097\n",
      "  Batch [240/1299] D_loss: -0.2147, G_loss: 0.4936\n",
      "  Batch [250/1299] D_loss: -0.0542, G_loss: 0.7868\n",
      "  Batch [260/1299] D_loss: -0.1375, G_loss: 0.7954\n",
      "  Batch [270/1299] D_loss: -0.1325, G_loss: 0.6435\n",
      "  Batch [280/1299] D_loss: 0.0149, G_loss: 0.2759\n",
      "  Batch [290/1299] D_loss: -0.0457, G_loss: 0.2221\n",
      "  Batch [300/1299] D_loss: -1.8164, G_loss: -2.2249\n",
      "  Batch [310/1299] D_loss: -0.0327, G_loss: 0.1217\n",
      "  Batch [320/1299] D_loss: -0.5766, G_loss: -0.9390\n",
      "  Batch [330/1299] D_loss: -0.1751, G_loss: 0.1310\n",
      "  Batch [340/1299] D_loss: -0.7830, G_loss: 0.0471\n",
      "  Batch [350/1299] D_loss: -0.1396, G_loss: 0.3744\n",
      "  Batch [360/1299] D_loss: -0.0698, G_loss: 0.5532\n",
      "  Batch [370/1299] D_loss: -0.1652, G_loss: 0.8831\n",
      "  Batch [380/1299] D_loss: -0.0569, G_loss: 0.5647\n",
      "  Batch [390/1299] D_loss: 0.0279, G_loss: 0.6537\n",
      "  Batch [400/1299] D_loss: -0.0870, G_loss: 0.3419\n",
      "  Batch [410/1299] D_loss: -0.9095, G_loss: -1.6142\n",
      "  Batch [420/1299] D_loss: -1.3481, G_loss: -2.8257\n",
      "  Batch [430/1299] D_loss: -0.0506, G_loss: 0.2254\n",
      "  Batch [440/1299] D_loss: -0.0745, G_loss: 0.3020\n",
      "  Batch [450/1299] D_loss: 0.0064, G_loss: 0.2840\n",
      "  Batch [460/1299] D_loss: -0.0737, G_loss: 0.5603\n",
      "  Batch [470/1299] D_loss: -0.0890, G_loss: 0.5102\n",
      "  Batch [480/1299] D_loss: -0.0878, G_loss: 0.3416\n",
      "  Batch [490/1299] D_loss: -1.3790, G_loss: -2.3087\n",
      "  Batch [500/1299] D_loss: -0.1028, G_loss: 0.0399\n",
      "  Batch [510/1299] D_loss: -0.3907, G_loss: -0.1715\n",
      "  Batch [520/1299] D_loss: 0.0078, G_loss: 0.2352\n",
      "  Batch [530/1299] D_loss: -0.0630, G_loss: 0.3902\n",
      "  Batch [540/1299] D_loss: -0.2318, G_loss: 0.5568\n",
      "  Batch [550/1299] D_loss: -0.2868, G_loss: 0.6832\n",
      "  Batch [560/1299] D_loss: -0.1837, G_loss: 0.5668\n",
      "  Batch [570/1299] D_loss: -0.1011, G_loss: 0.3911\n",
      "  Batch [580/1299] D_loss: -0.1180, G_loss: 0.4320\n",
      "  Batch [590/1299] D_loss: -0.5581, G_loss: -0.9635\n",
      "  Batch [600/1299] D_loss: -1.0739, G_loss: -1.6594\n",
      "  Batch [610/1299] D_loss: -0.3973, G_loss: 0.0241\n",
      "  Batch [620/1299] D_loss: -0.6622, G_loss: -0.5187\n",
      "  Batch [630/1299] D_loss: -0.7981, G_loss: -1.8872\n",
      "  Batch [640/1299] D_loss: -0.2006, G_loss: 0.0312\n",
      "  Batch [650/1299] D_loss: -0.0336, G_loss: 0.3197\n",
      "  Batch [660/1299] D_loss: -0.2077, G_loss: 0.6486\n",
      "  Batch [670/1299] D_loss: 0.0392, G_loss: 0.9379\n",
      "  Batch [680/1299] D_loss: -0.0851, G_loss: 0.5361\n",
      "  Batch [690/1299] D_loss: -0.0982, G_loss: 0.7522\n",
      "  Batch [700/1299] D_loss: -0.1402, G_loss: 0.7057\n",
      "  Batch [710/1299] D_loss: -2.5976, G_loss: -4.3749\n",
      "  Batch [720/1299] D_loss: -0.5280, G_loss: 0.1086\n",
      "  Batch [730/1299] D_loss: -0.3751, G_loss: -0.0664\n",
      "  Batch [740/1299] D_loss: 0.0037, G_loss: 0.2236\n",
      "  Batch [750/1299] D_loss: -0.0501, G_loss: 0.3443\n",
      "  Batch [760/1299] D_loss: -0.0731, G_loss: 0.4092\n",
      "  Batch [770/1299] D_loss: 0.0089, G_loss: 0.4586\n",
      "  Batch [780/1299] D_loss: -0.0868, G_loss: 0.3758\n",
      "  Batch [790/1299] D_loss: -0.0420, G_loss: 0.2126\n",
      "  Batch [800/1299] D_loss: -1.2470, G_loss: -3.8744\n",
      "  Batch [810/1299] D_loss: -0.5219, G_loss: -0.2254\n",
      "  Batch [820/1299] D_loss: -0.4122, G_loss: -0.0387\n",
      "  Batch [830/1299] D_loss: -0.0521, G_loss: 0.2403\n",
      "  Batch [840/1299] D_loss: -0.0553, G_loss: 0.3861\n",
      "  Batch [850/1299] D_loss: -0.0723, G_loss: 0.6361\n",
      "  Batch [860/1299] D_loss: -0.1887, G_loss: 0.7592\n",
      "  Batch [870/1299] D_loss: -0.2397, G_loss: 0.5750\n",
      "  Batch [880/1299] D_loss: -0.0041, G_loss: 0.4757\n",
      "  Batch [890/1299] D_loss: -1.4135, G_loss: -0.6698\n",
      "  Batch [900/1299] D_loss: -0.6425, G_loss: -0.6494\n",
      "  Batch [910/1299] D_loss: -0.7742, G_loss: -0.2684\n",
      "  Batch [920/1299] D_loss: -0.5094, G_loss: -0.0688\n",
      "  Batch [930/1299] D_loss: -1.1556, G_loss: -1.8305\n",
      "  Batch [940/1299] D_loss: -1.1873, G_loss: -0.0281\n",
      "  Batch [950/1299] D_loss: -0.2917, G_loss: -0.0897\n",
      "  Batch [960/1299] D_loss: -0.9378, G_loss: 0.0870\n",
      "  Batch [970/1299] D_loss: -1.8242, G_loss: -0.2206\n",
      "  Batch [980/1299] D_loss: -0.1642, G_loss: 0.0377\n",
      "  Batch [990/1299] D_loss: -0.6275, G_loss: 0.0770\n",
      "  Batch [1000/1299] D_loss: -0.6413, G_loss: -0.7942\n",
      "  Batch [1010/1299] D_loss: -0.0281, G_loss: 0.1393\n",
      "  Batch [1020/1299] D_loss: -1.0858, G_loss: -2.3409\n",
      "  Batch [1030/1299] D_loss: -1.4398, G_loss: -1.9030\n",
      "  Batch [1040/1299] D_loss: -0.4211, G_loss: 0.0501\n",
      "  Batch [1050/1299] D_loss: -0.4320, G_loss: -0.0107\n",
      "  Batch [1060/1299] D_loss: -0.1634, G_loss: 0.3489\n",
      "  Batch [1070/1299] D_loss: -0.1986, G_loss: 0.5114\n",
      "  Batch [1080/1299] D_loss: -0.3930, G_loss: 0.6632\n",
      "  Batch [1090/1299] D_loss: -0.1458, G_loss: 0.8719\n",
      "  Batch [1100/1299] D_loss: -0.1163, G_loss: 0.6716\n",
      "  Batch [1110/1299] D_loss: -0.2666, G_loss: 0.7177\n",
      "  Batch [1120/1299] D_loss: -0.0538, G_loss: 0.4377\n",
      "  Batch [1130/1299] D_loss: -0.2289, G_loss: -0.2530\n",
      "  Batch [1140/1299] D_loss: -0.6619, G_loss: 0.1168\n",
      "  Batch [1150/1299] D_loss: -0.6597, G_loss: -0.0049\n",
      "  Batch [1160/1299] D_loss: -0.3497, G_loss: 0.1450\n",
      "  Batch [1170/1299] D_loss: -0.2184, G_loss: 0.0915\n",
      "  Batch [1180/1299] D_loss: -0.0725, G_loss: 0.3428\n",
      "  Batch [1190/1299] D_loss: -0.0384, G_loss: 0.4443\n",
      "  Batch [1200/1299] D_loss: -0.0600, G_loss: 0.4472\n",
      "  Batch [1210/1299] D_loss: -0.1760, G_loss: 0.5395\n",
      "  Batch [1220/1299] D_loss: -0.2514, G_loss: 0.4744\n",
      "  Batch [1230/1299] D_loss: -0.0640, G_loss: 0.2865\n",
      "  Batch [1240/1299] D_loss: -1.5842, G_loss: -0.7545\n",
      "  Batch [1250/1299] D_loss: -0.5133, G_loss: -0.2789\n",
      "  Batch [1260/1299] D_loss: 0.0113, G_loss: 0.2385\n",
      "  Batch [1270/1299] D_loss: -0.1035, G_loss: 0.1796\n",
      "  Batch [1280/1299] D_loss: -0.1350, G_loss: 0.5281\n",
      "  Batch [1290/1299] D_loss: -0.1835, G_loss: 0.3999\n",
      "\n",
      "Epoch 50 Summary:\n",
      "  Average D_loss: -0.2119\n",
      "  Average G_loss: -0.0984\n",
      "\n",
      "Epoch [51/100]\n",
      "  Batch [0/1299] D_loss: -0.1101, G_loss: 0.4068\n",
      "  Batch [10/1299] D_loss: -0.0859, G_loss: 0.5048\n",
      "  Batch [20/1299] D_loss: -0.7574, G_loss: -0.3566\n",
      "  Batch [30/1299] D_loss: -0.0454, G_loss: -0.0491\n",
      "  Batch [40/1299] D_loss: -0.0302, G_loss: 0.2231\n",
      "  Batch [50/1299] D_loss: -0.3628, G_loss: -0.0798\n",
      "  Batch [60/1299] D_loss: -0.2643, G_loss: -0.1880\n",
      "  Batch [70/1299] D_loss: -0.0513, G_loss: 0.1709\n",
      "  Batch [80/1299] D_loss: -0.1098, G_loss: 0.3229\n",
      "  Batch [90/1299] D_loss: -0.1330, G_loss: 0.6129\n",
      "  Batch [100/1299] D_loss: -0.1444, G_loss: 0.5934\n",
      "  Batch [110/1299] D_loss: 0.0340, G_loss: 0.3234\n",
      "  Batch [120/1299] D_loss: -0.2742, G_loss: 0.6544\n",
      "  Batch [130/1299] D_loss: -0.0910, G_loss: 0.5068\n",
      "  Batch [140/1299] D_loss: -1.2472, G_loss: -3.1931\n",
      "  Batch [150/1299] D_loss: -0.6107, G_loss: -0.0630\n",
      "  Batch [160/1299] D_loss: -0.4670, G_loss: 0.0085\n",
      "  Batch [170/1299] D_loss: -0.1240, G_loss: 0.2403\n",
      "  Batch [180/1299] D_loss: -0.1746, G_loss: 0.1133\n",
      "  Batch [190/1299] D_loss: -0.5294, G_loss: -0.0630\n",
      "  Batch [200/1299] D_loss: -1.2708, G_loss: 0.0332\n",
      "  Batch [210/1299] D_loss: -1.1482, G_loss: -2.4380\n",
      "  Batch [220/1299] D_loss: -0.4970, G_loss: -0.9563\n",
      "  Batch [230/1299] D_loss: -0.1218, G_loss: 0.2198\n",
      "  Batch [240/1299] D_loss: 0.0015, G_loss: 0.2483\n",
      "  Batch [250/1299] D_loss: -0.0983, G_loss: 0.4836\n",
      "  Batch [260/1299] D_loss: -0.1851, G_loss: 0.7393\n",
      "  Batch [270/1299] D_loss: -0.1119, G_loss: 0.6884\n",
      "  Batch [280/1299] D_loss: -0.1604, G_loss: 0.6553\n",
      "  Batch [290/1299] D_loss: -0.0116, G_loss: 0.2460\n",
      "  Batch [300/1299] D_loss: -2.0689, G_loss: -2.5853\n",
      "  Batch [310/1299] D_loss: -0.3020, G_loss: -0.3821\n",
      "  Batch [320/1299] D_loss: -0.4898, G_loss: -1.2450\n",
      "  Batch [330/1299] D_loss: -0.3323, G_loss: -0.2583\n",
      "  Batch [340/1299] D_loss: -1.5583, G_loss: -0.0823\n",
      "  Batch [350/1299] D_loss: -1.1199, G_loss: -0.4616\n",
      "  Batch [360/1299] D_loss: -0.7272, G_loss: -0.0194\n",
      "  Batch [370/1299] D_loss: -0.3455, G_loss: 0.0667\n",
      "  Batch [380/1299] D_loss: -0.1107, G_loss: 0.3335\n",
      "  Batch [390/1299] D_loss: -0.4719, G_loss: 0.2349\n",
      "  Batch [400/1299] D_loss: -1.8106, G_loss: -3.7122\n",
      "  Batch [410/1299] D_loss: -1.1799, G_loss: -5.6848\n",
      "  Batch [420/1299] D_loss: -0.5678, G_loss: -0.6946\n",
      "  Batch [430/1299] D_loss: -2.2564, G_loss: -3.7664\n",
      "  Batch [440/1299] D_loss: -0.3116, G_loss: -0.0915\n",
      "  Batch [450/1299] D_loss: -0.4002, G_loss: -0.2658\n",
      "  Batch [460/1299] D_loss: -0.1662, G_loss: 0.0758\n",
      "  Batch [470/1299] D_loss: -0.5177, G_loss: -0.0081\n",
      "  Batch [480/1299] D_loss: -0.1898, G_loss: 0.2264\n",
      "  Batch [490/1299] D_loss: -0.5726, G_loss: 0.2691\n",
      "  Batch [500/1299] D_loss: -0.0414, G_loss: 0.3849\n",
      "  Batch [510/1299] D_loss: -0.3281, G_loss: 0.5237\n",
      "  Batch [520/1299] D_loss: -0.0617, G_loss: 0.6763\n",
      "  Batch [530/1299] D_loss: -0.0770, G_loss: 0.9496\n",
      "  Batch [540/1299] D_loss: -0.1171, G_loss: 0.7602\n",
      "  Batch [550/1299] D_loss: 0.1224, G_loss: 0.5781\n",
      "  Batch [560/1299] D_loss: -2.5513, G_loss: -4.6669\n",
      "  Batch [570/1299] D_loss: -1.6983, G_loss: -1.8737\n",
      "  Batch [580/1299] D_loss: -0.3495, G_loss: 0.0692\n",
      "  Batch [590/1299] D_loss: -0.8539, G_loss: -0.0949\n",
      "  Batch [600/1299] D_loss: -0.9175, G_loss: -0.3423\n",
      "  Batch [610/1299] D_loss: -0.0613, G_loss: 0.2412\n",
      "  Batch [620/1299] D_loss: -0.1389, G_loss: 0.3895\n",
      "  Batch [630/1299] D_loss: -0.1811, G_loss: 0.5786\n",
      "  Batch [640/1299] D_loss: -0.2180, G_loss: 0.6686\n",
      "  Batch [650/1299] D_loss: -0.1440, G_loss: 0.5754\n",
      "  Batch [660/1299] D_loss: -0.2443, G_loss: 0.5730\n",
      "  Batch [670/1299] D_loss: 0.0172, G_loss: 0.3780\n",
      "  Batch [680/1299] D_loss: -0.0271, G_loss: 0.1892\n",
      "  Batch [690/1299] D_loss: -0.3237, G_loss: -0.4147\n",
      "  Batch [700/1299] D_loss: -0.5871, G_loss: -0.9030\n",
      "  Batch [710/1299] D_loss: -0.2037, G_loss: -0.1862\n",
      "  Batch [720/1299] D_loss: -0.5385, G_loss: -0.3689\n",
      "  Batch [730/1299] D_loss: -1.2053, G_loss: -1.9421\n",
      "  Batch [740/1299] D_loss: -0.0565, G_loss: 0.3211\n",
      "  Batch [750/1299] D_loss: -0.1550, G_loss: 0.5775\n",
      "  Batch [760/1299] D_loss: -0.3126, G_loss: 0.7030\n",
      "  Batch [770/1299] D_loss: -0.2429, G_loss: 0.9164\n",
      "  Batch [780/1299] D_loss: -0.2119, G_loss: 0.7776\n",
      "  Batch [790/1299] D_loss: 0.0142, G_loss: 0.3775\n",
      "  Batch [800/1299] D_loss: 0.0526, G_loss: 0.3368\n",
      "  Batch [810/1299] D_loss: -2.5796, G_loss: -4.9731\n",
      "  Batch [820/1299] D_loss: -0.1013, G_loss: 0.0780\n",
      "  Batch [830/1299] D_loss: -0.0572, G_loss: 0.3271\n",
      "  Batch [840/1299] D_loss: -0.1169, G_loss: 0.4819\n",
      "  Batch [850/1299] D_loss: -0.0002, G_loss: 0.5768\n",
      "  Batch [860/1299] D_loss: -0.0553, G_loss: 0.5088\n",
      "  Batch [870/1299] D_loss: -0.0615, G_loss: 0.5036\n",
      "  Batch [880/1299] D_loss: -0.0184, G_loss: 0.3052\n",
      "  Batch [890/1299] D_loss: -0.4935, G_loss: -1.7521\n",
      "  Batch [900/1299] D_loss: -0.7571, G_loss: -1.2113\n",
      "  Batch [910/1299] D_loss: -0.3046, G_loss: -0.4535\n",
      "  Batch [920/1299] D_loss: -0.2734, G_loss: 0.0449\n",
      "  Batch [930/1299] D_loss: -0.0399, G_loss: 0.3605\n",
      "  Batch [940/1299] D_loss: -0.1629, G_loss: 0.4739\n",
      "  Batch [950/1299] D_loss: -0.1112, G_loss: 0.7606\n",
      "  Batch [960/1299] D_loss: -0.0805, G_loss: 0.5680\n",
      "  Batch [970/1299] D_loss: 0.0355, G_loss: 0.4636\n",
      "  Batch [980/1299] D_loss: -2.7184, G_loss: -5.5998\n",
      "  Batch [990/1299] D_loss: -0.7801, G_loss: -2.2263\n",
      "  Batch [1000/1299] D_loss: -0.6309, G_loss: -1.0627\n",
      "  Batch [1010/1299] D_loss: 0.0004, G_loss: 0.1248\n",
      "  Batch [1020/1299] D_loss: -0.0676, G_loss: 0.3092\n",
      "  Batch [1030/1299] D_loss: -0.1661, G_loss: 0.5229\n",
      "  Batch [1040/1299] D_loss: -0.1725, G_loss: 0.6599\n",
      "  Batch [1050/1299] D_loss: -0.2128, G_loss: 0.6808\n",
      "  Batch [1060/1299] D_loss: 0.0187, G_loss: 0.4601\n",
      "  Batch [1070/1299] D_loss: -0.0658, G_loss: 0.4279\n",
      "  Batch [1080/1299] D_loss: -0.6956, G_loss: -1.7283\n",
      "  Batch [1090/1299] D_loss: -1.1817, G_loss: -0.2690\n",
      "  Batch [1100/1299] D_loss: -0.4430, G_loss: -0.5270\n",
      "  Batch [1110/1299] D_loss: -0.0001, G_loss: 0.1721\n",
      "  Batch [1120/1299] D_loss: -0.2753, G_loss: -0.0813\n",
      "  Batch [1130/1299] D_loss: -1.0063, G_loss: -1.8019\n",
      "  Batch [1140/1299] D_loss: -0.0990, G_loss: 0.2196\n",
      "  Batch [1150/1299] D_loss: -0.1684, G_loss: 0.1478\n",
      "  Batch [1160/1299] D_loss: -0.2837, G_loss: 0.0952\n",
      "  Batch [1170/1299] D_loss: -0.0062, G_loss: 0.0804\n",
      "  Batch [1180/1299] D_loss: -0.0714, G_loss: 0.2088\n",
      "  Batch [1190/1299] D_loss: -0.2005, G_loss: 0.5408\n",
      "  Batch [1200/1299] D_loss: -0.0721, G_loss: 0.6364\n",
      "  Batch [1210/1299] D_loss: 0.0193, G_loss: 0.6760\n",
      "  Batch [1220/1299] D_loss: -0.1578, G_loss: 0.8097\n",
      "  Batch [1230/1299] D_loss: -0.0481, G_loss: 0.6221\n",
      "  Batch [1240/1299] D_loss: -0.0630, G_loss: 0.2729\n",
      "  Batch [1250/1299] D_loss: -1.0061, G_loss: -3.6655\n",
      "  Batch [1260/1299] D_loss: -0.9882, G_loss: -0.3370\n",
      "  Batch [1270/1299] D_loss: -0.3526, G_loss: 0.0402\n",
      "  Batch [1280/1299] D_loss: -0.0794, G_loss: 0.2069\n",
      "  Batch [1290/1299] D_loss: -0.1094, G_loss: 0.3971\n",
      "\n",
      "Epoch 51 Summary:\n",
      "  Average D_loss: -0.2321\n",
      "  Average G_loss: -0.1174\n",
      "\n",
      "Epoch [52/100]\n",
      "  Batch [0/1299] D_loss: -0.0339, G_loss: 0.6459\n",
      "  Batch [10/1299] D_loss: -0.1609, G_loss: 0.6042\n",
      "  Batch [20/1299] D_loss: 0.1243, G_loss: 0.5166\n",
      "  Batch [30/1299] D_loss: -0.0095, G_loss: 0.5391\n",
      "  Batch [40/1299] D_loss: 0.0486, G_loss: 0.4070\n",
      "  Batch [50/1299] D_loss: -2.4439, G_loss: -5.6081\n",
      "  Batch [60/1299] D_loss: -0.4661, G_loss: -0.6848\n",
      "  Batch [70/1299] D_loss: -0.0234, G_loss: 0.2557\n",
      "  Batch [80/1299] D_loss: 0.0095, G_loss: 0.3032\n",
      "  Batch [90/1299] D_loss: -0.1272, G_loss: 0.4324\n",
      "  Batch [100/1299] D_loss: -0.0210, G_loss: 0.3938\n",
      "  Batch [110/1299] D_loss: 0.0202, G_loss: 0.3505\n",
      "  Batch [120/1299] D_loss: 0.0276, G_loss: 0.3780\n",
      "  Batch [130/1299] D_loss: -0.0876, G_loss: 0.1634\n",
      "  Batch [140/1299] D_loss: -0.2469, G_loss: -0.1700\n",
      "  Batch [150/1299] D_loss: 0.0382, G_loss: 0.1277\n",
      "  Batch [160/1299] D_loss: -0.0531, G_loss: 0.3554\n",
      "  Batch [170/1299] D_loss: -0.0125, G_loss: 0.3021\n",
      "  Batch [180/1299] D_loss: -0.1009, G_loss: 0.5805\n",
      "  Batch [190/1299] D_loss: -0.0679, G_loss: 0.3159\n",
      "  Batch [200/1299] D_loss: -0.9541, G_loss: -2.2860\n",
      "  Batch [210/1299] D_loss: -0.7523, G_loss: -1.2910\n",
      "  Batch [220/1299] D_loss: -0.0214, G_loss: 0.1228\n",
      "  Batch [230/1299] D_loss: -0.0465, G_loss: 0.2859\n",
      "  Batch [240/1299] D_loss: 0.0025, G_loss: 0.2240\n",
      "  Batch [250/1299] D_loss: -0.0487, G_loss: 0.2672\n",
      "  Batch [260/1299] D_loss: -0.0003, G_loss: 0.3715\n",
      "  Batch [270/1299] D_loss: -1.1380, G_loss: -3.3126\n",
      "  Batch [280/1299] D_loss: -1.0381, G_loss: -0.8169\n",
      "  Batch [290/1299] D_loss: -0.6855, G_loss: -0.1424\n",
      "  Batch [300/1299] D_loss: -0.4528, G_loss: -0.0808\n",
      "  Batch [310/1299] D_loss: -0.0276, G_loss: 0.0819\n",
      "  Batch [320/1299] D_loss: -0.0743, G_loss: 0.1931\n",
      "  Batch [330/1299] D_loss: -0.0016, G_loss: 0.3143\n",
      "  Batch [340/1299] D_loss: -0.2400, G_loss: 0.6126\n",
      "  Batch [350/1299] D_loss: -0.1357, G_loss: 0.5591\n",
      "  Batch [360/1299] D_loss: -0.1402, G_loss: 0.4851\n",
      "  Batch [370/1299] D_loss: -0.1428, G_loss: 0.2853\n",
      "  Batch [380/1299] D_loss: -1.5603, G_loss: -2.6577\n",
      "  Batch [390/1299] D_loss: -0.1637, G_loss: -0.1504\n",
      "  Batch [400/1299] D_loss: -0.0388, G_loss: 0.2729\n",
      "  Batch [410/1299] D_loss: -0.1406, G_loss: 0.4508\n",
      "  Batch [420/1299] D_loss: -0.0599, G_loss: 0.3427\n",
      "  Batch [430/1299] D_loss: -0.0191, G_loss: 0.4164\n",
      "  Batch [440/1299] D_loss: 0.0167, G_loss: 0.2884\n",
      "  Batch [450/1299] D_loss: -0.8389, G_loss: -1.4862\n",
      "  Batch [460/1299] D_loss: -0.2510, G_loss: 0.0200\n",
      "  Batch [470/1299] D_loss: -0.2617, G_loss: 0.0249\n",
      "  Batch [480/1299] D_loss: -0.4881, G_loss: 0.1149\n",
      "  Batch [490/1299] D_loss: -0.0387, G_loss: 0.1182\n",
      "  Batch [500/1299] D_loss: -0.0415, G_loss: 0.3151\n",
      "  Batch [510/1299] D_loss: -0.1930, G_loss: 0.6542\n",
      "  Batch [520/1299] D_loss: 0.0082, G_loss: 0.4958\n",
      "  Batch [530/1299] D_loss: 0.0207, G_loss: 0.3786\n",
      "  Batch [540/1299] D_loss: -0.0444, G_loss: 0.2126\n",
      "  Batch [550/1299] D_loss: -0.2117, G_loss: 0.0009\n",
      "  Batch [560/1299] D_loss: -0.3014, G_loss: -0.0676\n",
      "  Batch [570/1299] D_loss: -0.0239, G_loss: 0.2003\n",
      "  Batch [580/1299] D_loss: -0.0372, G_loss: 0.2438\n",
      "  Batch [590/1299] D_loss: -0.0647, G_loss: 0.2595\n",
      "  Batch [600/1299] D_loss: -0.0818, G_loss: 0.3202\n",
      "  Batch [610/1299] D_loss: -0.0258, G_loss: 0.2330\n",
      "  Batch [620/1299] D_loss: -0.8846, G_loss: -2.1316\n",
      "  Batch [630/1299] D_loss: -0.5768, G_loss: -0.5050\n",
      "  Batch [640/1299] D_loss: -0.7765, G_loss: -0.8027\n",
      "  Batch [650/1299] D_loss: -0.0261, G_loss: 0.2313\n",
      "  Batch [660/1299] D_loss: -0.2464, G_loss: 0.1041\n",
      "  Batch [670/1299] D_loss: 0.0112, G_loss: 0.3459\n",
      "  Batch [680/1299] D_loss: -0.1047, G_loss: 0.4442\n",
      "  Batch [690/1299] D_loss: -0.0541, G_loss: 0.5341\n",
      "  Batch [700/1299] D_loss: -0.0972, G_loss: 0.5251\n",
      "  Batch [710/1299] D_loss: -0.2444, G_loss: 0.5706\n",
      "  Batch [720/1299] D_loss: -0.5452, G_loss: 0.0231\n",
      "  Batch [730/1299] D_loss: -0.9944, G_loss: -1.3812\n",
      "  Batch [740/1299] D_loss: -0.2241, G_loss: -0.2720\n",
      "  Batch [750/1299] D_loss: -1.2940, G_loss: -2.2784\n",
      "  Batch [760/1299] D_loss: -0.2721, G_loss: -0.0731\n",
      "  Batch [770/1299] D_loss: -0.0609, G_loss: 0.0810\n",
      "  Batch [780/1299] D_loss: -0.2604, G_loss: 0.1124\n",
      "  Batch [790/1299] D_loss: -1.6223, G_loss: -2.6806\n",
      "  Batch [800/1299] D_loss: -0.1646, G_loss: 0.1133\n",
      "  Batch [810/1299] D_loss: -0.4183, G_loss: 0.1757\n",
      "  Batch [820/1299] D_loss: -1.1299, G_loss: -0.0118\n",
      "  Batch [830/1299] D_loss: -0.0308, G_loss: 0.1542\n",
      "  Batch [840/1299] D_loss: 0.0110, G_loss: 0.3503\n",
      "  Batch [850/1299] D_loss: -0.0998, G_loss: 0.6086\n",
      "  Batch [860/1299] D_loss: -0.1936, G_loss: 0.6870\n",
      "  Batch [870/1299] D_loss: -0.2219, G_loss: 0.8173\n",
      "  Batch [880/1299] D_loss: -0.4598, G_loss: 0.9316\n",
      "  Batch [890/1299] D_loss: -0.0302, G_loss: 0.5190\n",
      "  Batch [900/1299] D_loss: -0.0152, G_loss: 0.2229\n",
      "  Batch [910/1299] D_loss: -1.2935, G_loss: -0.6579\n",
      "  Batch [920/1299] D_loss: -0.4487, G_loss: -0.4903\n",
      "  Batch [930/1299] D_loss: -0.0002, G_loss: 0.2969\n",
      "  Batch [940/1299] D_loss: -0.0644, G_loss: 0.3966\n",
      "  Batch [950/1299] D_loss: -0.1059, G_loss: 0.4438\n",
      "  Batch [960/1299] D_loss: -0.1160, G_loss: 0.6492\n",
      "  Batch [970/1299] D_loss: -0.0042, G_loss: 0.3805\n",
      "  Batch [980/1299] D_loss: -0.0037, G_loss: 0.1995\n",
      "  Batch [990/1299] D_loss: -0.3664, G_loss: -1.1236\n",
      "  Batch [1000/1299] D_loss: -0.0424, G_loss: 0.1477\n",
      "  Batch [1010/1299] D_loss: -0.5735, G_loss: -0.5575\n",
      "  Batch [1020/1299] D_loss: -0.2234, G_loss: -0.0843\n",
      "  Batch [1030/1299] D_loss: -0.2767, G_loss: 0.0911\n",
      "  Batch [1040/1299] D_loss: -0.1244, G_loss: 0.2533\n",
      "  Batch [1050/1299] D_loss: -1.0061, G_loss: -0.8868\n",
      "  Batch [1060/1299] D_loss: -0.0385, G_loss: 0.3921\n",
      "  Batch [1070/1299] D_loss: -0.1143, G_loss: 0.4733\n",
      "  Batch [1080/1299] D_loss: -0.2105, G_loss: 0.7700\n",
      "  Batch [1090/1299] D_loss: 0.0101, G_loss: 0.5397\n",
      "  Batch [1100/1299] D_loss: -0.0589, G_loss: 0.4555\n",
      "  Batch [1110/1299] D_loss: -0.0029, G_loss: 0.2148\n",
      "  Batch [1120/1299] D_loss: -2.1702, G_loss: -1.6884\n",
      "  Batch [1130/1299] D_loss: -0.4586, G_loss: -0.2588\n",
      "  Batch [1140/1299] D_loss: -0.0229, G_loss: 0.0996\n",
      "  Batch [1150/1299] D_loss: -1.3081, G_loss: -2.7762\n",
      "  Batch [1160/1299] D_loss: -0.1297, G_loss: 0.1452\n",
      "  Batch [1170/1299] D_loss: -0.7227, G_loss: 0.0355\n",
      "  Batch [1180/1299] D_loss: -0.3893, G_loss: 0.1241\n",
      "  Batch [1190/1299] D_loss: -0.3422, G_loss: -0.2497\n",
      "  Batch [1200/1299] D_loss: -0.0547, G_loss: 0.3176\n",
      "  Batch [1210/1299] D_loss: -0.2556, G_loss: 0.5196\n",
      "  Batch [1220/1299] D_loss: -0.0058, G_loss: 0.6225\n",
      "  Batch [1230/1299] D_loss: -0.1897, G_loss: 0.6579\n",
      "  Batch [1240/1299] D_loss: -0.0760, G_loss: 0.7195\n",
      "  Batch [1250/1299] D_loss: -0.1311, G_loss: 0.5951\n",
      "  Batch [1260/1299] D_loss: -0.0715, G_loss: 0.3353\n",
      "  Batch [1270/1299] D_loss: -3.1179, G_loss: -9.2048\n",
      "  Batch [1280/1299] D_loss: -0.0629, G_loss: 0.2598\n",
      "  Batch [1290/1299] D_loss: -0.0363, G_loss: 0.3365\n",
      "\n",
      "Epoch 52 Summary:\n",
      "  Average D_loss: -0.1620\n",
      "  Average G_loss: -0.0646\n",
      "\n",
      "Epoch [53/100]\n",
      "  Batch [0/1299] D_loss: -0.0331, G_loss: 0.2643\n",
      "  Batch [10/1299] D_loss: -0.0107, G_loss: 0.4499\n",
      "  Batch [20/1299] D_loss: 0.0030, G_loss: 0.2227\n",
      "  Batch [30/1299] D_loss: -1.1912, G_loss: -0.4403\n",
      "  Batch [40/1299] D_loss: -1.8553, G_loss: -3.6745\n",
      "  Batch [50/1299] D_loss: -0.0261, G_loss: 0.1657\n",
      "  Batch [60/1299] D_loss: -0.1027, G_loss: 0.3101\n",
      "  Batch [70/1299] D_loss: -0.0954, G_loss: 0.4908\n",
      "  Batch [80/1299] D_loss: -0.2484, G_loss: 0.6123\n",
      "  Batch [90/1299] D_loss: 0.0083, G_loss: 0.6086\n",
      "  Batch [100/1299] D_loss: -0.0502, G_loss: 0.5128\n",
      "  Batch [110/1299] D_loss: -0.0810, G_loss: 0.3548\n",
      "  Batch [120/1299] D_loss: -0.5212, G_loss: -0.6898\n",
      "  Batch [130/1299] D_loss: -0.0035, G_loss: 0.1584\n",
      "  Batch [140/1299] D_loss: -1.0640, G_loss: -1.0908\n",
      "  Batch [150/1299] D_loss: -0.2547, G_loss: -0.7953\n",
      "  Batch [160/1299] D_loss: -0.0659, G_loss: 0.2600\n",
      "  Batch [170/1299] D_loss: -0.0529, G_loss: 0.3958\n",
      "  Batch [180/1299] D_loss: -0.0187, G_loss: 0.4343\n",
      "  Batch [190/1299] D_loss: -0.0531, G_loss: 0.4950\n",
      "  Batch [200/1299] D_loss: -0.0479, G_loss: 0.2938\n",
      "  Batch [210/1299] D_loss: -0.0546, G_loss: 0.2261\n",
      "  Batch [220/1299] D_loss: -0.2276, G_loss: -0.4535\n",
      "  Batch [230/1299] D_loss: -0.0523, G_loss: 0.1833\n",
      "  Batch [240/1299] D_loss: -0.5286, G_loss: -0.1690\n",
      "  Batch [250/1299] D_loss: -1.6073, G_loss: -3.4215\n",
      "  Batch [260/1299] D_loss: -0.5912, G_loss: -0.1452\n",
      "  Batch [270/1299] D_loss: -0.1292, G_loss: 0.1902\n",
      "  Batch [280/1299] D_loss: -0.7480, G_loss: -0.0332\n",
      "  Batch [290/1299] D_loss: -0.0322, G_loss: 0.2015\n",
      "  Batch [300/1299] D_loss: -0.0988, G_loss: 0.4304\n",
      "  Batch [310/1299] D_loss: 0.0315, G_loss: 0.4779\n",
      "  Batch [320/1299] D_loss: -0.1740, G_loss: 0.5605\n",
      "  Batch [330/1299] D_loss: -0.0915, G_loss: 0.5039\n",
      "  Batch [340/1299] D_loss: -0.1566, G_loss: 0.2768\n",
      "  Batch [350/1299] D_loss: 0.0216, G_loss: 0.0590\n",
      "  Batch [360/1299] D_loss: -1.0929, G_loss: -1.5839\n",
      "  Batch [370/1299] D_loss: -1.5848, G_loss: -3.1008\n",
      "  Batch [380/1299] D_loss: -0.0664, G_loss: 0.2918\n",
      "  Batch [390/1299] D_loss: -0.1289, G_loss: 0.4667\n",
      "  Batch [400/1299] D_loss: -0.0010, G_loss: 0.4747\n",
      "  Batch [410/1299] D_loss: -0.0535, G_loss: 0.5026\n",
      "  Batch [420/1299] D_loss: -0.0369, G_loss: 0.3705\n",
      "  Batch [430/1299] D_loss: -0.0160, G_loss: 0.2826\n",
      "  Batch [440/1299] D_loss: -1.0414, G_loss: -0.9335\n",
      "  Batch [450/1299] D_loss: -0.4942, G_loss: -1.1528\n",
      "  Batch [460/1299] D_loss: -0.1361, G_loss: 0.0707\n",
      "  Batch [470/1299] D_loss: -0.1030, G_loss: 0.1351\n",
      "  Batch [480/1299] D_loss: -0.2406, G_loss: -0.0303\n",
      "  Batch [490/1299] D_loss: -0.0513, G_loss: 0.2886\n",
      "  Batch [500/1299] D_loss: -0.2109, G_loss: 0.5324\n",
      "  Batch [510/1299] D_loss: -0.1594, G_loss: 0.6525\n",
      "  Batch [520/1299] D_loss: -0.0088, G_loss: 0.5848\n",
      "  Batch [530/1299] D_loss: -0.0112, G_loss: 0.6376\n",
      "  Batch [540/1299] D_loss: -0.2160, G_loss: -0.0758\n",
      "  Batch [550/1299] D_loss: -1.0357, G_loss: -1.3459\n",
      "  Batch [560/1299] D_loss: -0.0994, G_loss: 0.0913\n",
      "  Batch [570/1299] D_loss: -0.7308, G_loss: -1.2761\n",
      "  Batch [580/1299] D_loss: -0.6018, G_loss: -0.2077\n",
      "  Batch [590/1299] D_loss: -0.4082, G_loss: 0.0775\n",
      "  Batch [600/1299] D_loss: -1.6088, G_loss: -0.5636\n",
      "  Batch [610/1299] D_loss: -0.0492, G_loss: 0.0917\n",
      "  Batch [620/1299] D_loss: -0.0935, G_loss: 0.4111\n",
      "  Batch [630/1299] D_loss: -0.0139, G_loss: 0.4149\n",
      "  Batch [640/1299] D_loss: -0.0289, G_loss: 0.3777\n",
      "  Batch [650/1299] D_loss: -0.1154, G_loss: 0.4365\n",
      "  Batch [660/1299] D_loss: -2.0578, G_loss: -4.2455\n",
      "  Batch [670/1299] D_loss: -0.2200, G_loss: 0.0822\n",
      "  Batch [680/1299] D_loss: -0.5213, G_loss: -0.5144\n",
      "  Batch [690/1299] D_loss: -0.0643, G_loss: 0.2608\n",
      "  Batch [700/1299] D_loss: -0.1335, G_loss: 0.4560\n",
      "  Batch [710/1299] D_loss: -0.0148, G_loss: 0.4651\n",
      "  Batch [720/1299] D_loss: -0.0612, G_loss: 0.6009\n",
      "  Batch [730/1299] D_loss: -0.0815, G_loss: 0.4461\n",
      "  Batch [740/1299] D_loss: -1.8333, G_loss: -0.8090\n",
      "  Batch [750/1299] D_loss: -0.1341, G_loss: 0.1172\n",
      "  Batch [760/1299] D_loss: -0.4890, G_loss: -0.1037\n",
      "  Batch [770/1299] D_loss: -0.8093, G_loss: -1.4075\n",
      "  Batch [780/1299] D_loss: -0.1003, G_loss: 0.3735\n",
      "  Batch [790/1299] D_loss: -0.0115, G_loss: 0.4789\n",
      "  Batch [800/1299] D_loss: -0.0018, G_loss: 0.6584\n",
      "  Batch [810/1299] D_loss: -0.0793, G_loss: 0.4943\n",
      "  Batch [820/1299] D_loss: -0.0450, G_loss: 0.5153\n",
      "  Batch [830/1299] D_loss: -1.7375, G_loss: -3.6226\n",
      "  Batch [840/1299] D_loss: -0.3613, G_loss: -0.1815\n",
      "  Batch [850/1299] D_loss: -1.3238, G_loss: -0.7602\n",
      "  Batch [860/1299] D_loss: -0.3181, G_loss: 0.0465\n",
      "  Batch [870/1299] D_loss: -0.7254, G_loss: -0.3277\n",
      "  Batch [880/1299] D_loss: -0.9928, G_loss: -1.9210\n",
      "  Batch [890/1299] D_loss: -0.0051, G_loss: 0.2338\n",
      "  Batch [900/1299] D_loss: -0.0421, G_loss: 0.3190\n",
      "  Batch [910/1299] D_loss: -0.1647, G_loss: 0.6536\n",
      "  Batch [920/1299] D_loss: -0.0616, G_loss: 0.6198\n",
      "  Batch [930/1299] D_loss: -0.0359, G_loss: 0.6356\n",
      "  Batch [940/1299] D_loss: -0.1059, G_loss: 0.5810\n",
      "  Batch [950/1299] D_loss: -0.0146, G_loss: -0.1540\n",
      "  Batch [960/1299] D_loss: -0.0240, G_loss: 0.1144\n",
      "  Batch [970/1299] D_loss: -0.0141, G_loss: 0.1622\n",
      "  Batch [980/1299] D_loss: -0.1381, G_loss: 0.3973\n",
      "  Batch [990/1299] D_loss: -0.1250, G_loss: 0.4334\n",
      "  Batch [1000/1299] D_loss: -0.1817, G_loss: 0.5477\n",
      "  Batch [1010/1299] D_loss: -0.0697, G_loss: 0.2871\n",
      "  Batch [1020/1299] D_loss: -0.1281, G_loss: 0.0733\n",
      "  Batch [1030/1299] D_loss: -1.0202, G_loss: -0.2195\n",
      "  Batch [1040/1299] D_loss: -0.0350, G_loss: 0.1364\n",
      "  Batch [1050/1299] D_loss: -0.0460, G_loss: 0.2072\n",
      "  Batch [1060/1299] D_loss: -0.0796, G_loss: 0.3163\n",
      "  Batch [1070/1299] D_loss: -0.2072, G_loss: 0.1494\n",
      "  Batch [1080/1299] D_loss: -0.0518, G_loss: 0.1529\n",
      "  Batch [1090/1299] D_loss: -0.0635, G_loss: 0.2722\n",
      "  Batch [1100/1299] D_loss: -0.1709, G_loss: 0.4087\n",
      "  Batch [1110/1299] D_loss: -0.0673, G_loss: 0.5408\n",
      "  Batch [1120/1299] D_loss: -0.2489, G_loss: 0.7773\n",
      "  Batch [1130/1299] D_loss: -0.0674, G_loss: 0.3276\n",
      "  Batch [1140/1299] D_loss: 0.0584, G_loss: 0.1919\n",
      "  Batch [1150/1299] D_loss: -0.9497, G_loss: -1.9812\n",
      "  Batch [1160/1299] D_loss: -0.0145, G_loss: 0.1760\n",
      "  Batch [1170/1299] D_loss: -0.0702, G_loss: 0.2750\n",
      "  Batch [1180/1299] D_loss: -0.1081, G_loss: 0.3049\n",
      "  Batch [1190/1299] D_loss: -0.1338, G_loss: 0.4817\n",
      "  Batch [1200/1299] D_loss: -0.0475, G_loss: 0.3579\n",
      "  Batch [1210/1299] D_loss: -0.0665, G_loss: 0.3440\n",
      "  Batch [1220/1299] D_loss: -2.8401, G_loss: -4.1655\n",
      "  Batch [1230/1299] D_loss: -0.0217, G_loss: 0.1817\n",
      "  Batch [1240/1299] D_loss: -0.0651, G_loss: 0.2721\n",
      "  Batch [1250/1299] D_loss: -0.0828, G_loss: 0.3002\n",
      "  Batch [1260/1299] D_loss: -0.0830, G_loss: 0.4380\n",
      "  Batch [1270/1299] D_loss: -0.0552, G_loss: 0.2932\n",
      "  Batch [1280/1299] D_loss: -0.1829, G_loss: 0.1433\n",
      "  Batch [1290/1299] D_loss: -0.0336, G_loss: 0.0598\n",
      "\n",
      "Epoch 53 Summary:\n",
      "  Average D_loss: -0.1433\n",
      "  Average G_loss: -0.0731\n",
      "\n",
      "Epoch [54/100]\n",
      "  Batch [0/1299] D_loss: -0.0336, G_loss: 0.0839\n",
      "  Batch [10/1299] D_loss: -0.0641, G_loss: 0.3728\n",
      "  Batch [20/1299] D_loss: -0.0383, G_loss: 0.4051\n",
      "  Batch [30/1299] D_loss: 0.0300, G_loss: 0.5882\n",
      "  Batch [40/1299] D_loss: 0.0168, G_loss: 0.2833\n",
      "  Batch [50/1299] D_loss: -1.5838, G_loss: -1.9907\n",
      "  Batch [60/1299] D_loss: -0.1822, G_loss: 0.0629\n",
      "  Batch [70/1299] D_loss: -0.0574, G_loss: 0.1932\n",
      "  Batch [80/1299] D_loss: -0.0708, G_loss: 0.3471\n",
      "  Batch [90/1299] D_loss: -0.1217, G_loss: 0.6547\n",
      "  Batch [100/1299] D_loss: -0.0905, G_loss: 0.5662\n",
      "  Batch [110/1299] D_loss: 0.0591, G_loss: 0.4668\n",
      "  Batch [120/1299] D_loss: -0.0825, G_loss: 0.2733\n",
      "  Batch [130/1299] D_loss: -1.3305, G_loss: -1.9091\n",
      "  Batch [140/1299] D_loss: -0.2449, G_loss: -0.0364\n",
      "  Batch [150/1299] D_loss: 0.0080, G_loss: 0.2444\n",
      "  Batch [160/1299] D_loss: -0.0464, G_loss: 0.3393\n",
      "  Batch [170/1299] D_loss: -0.0461, G_loss: 0.2723\n",
      "  Batch [180/1299] D_loss: -1.0352, G_loss: -3.7426\n",
      "  Batch [190/1299] D_loss: 0.0106, G_loss: 0.1451\n",
      "  Batch [200/1299] D_loss: -0.4685, G_loss: -0.2597\n",
      "  Batch [210/1299] D_loss: -0.2006, G_loss: -0.2034\n",
      "  Batch [220/1299] D_loss: -0.7920, G_loss: -0.0297\n",
      "  Batch [230/1299] D_loss: -0.7473, G_loss: -1.5919\n",
      "  Batch [240/1299] D_loss: -0.0922, G_loss: 0.2881\n",
      "  Batch [250/1299] D_loss: -0.2097, G_loss: 0.4623\n",
      "  Batch [260/1299] D_loss: -0.0567, G_loss: 0.4450\n",
      "  Batch [270/1299] D_loss: -0.1374, G_loss: 0.5531\n",
      "  Batch [280/1299] D_loss: -0.0532, G_loss: 0.3510\n",
      "  Batch [290/1299] D_loss: -0.1408, G_loss: 0.3455\n",
      "  Batch [300/1299] D_loss: -3.3008, G_loss: -5.2502\n",
      "  Batch [310/1299] D_loss: -0.0471, G_loss: 0.0236\n",
      "  Batch [320/1299] D_loss: -0.1469, G_loss: 0.0903\n",
      "  Batch [330/1299] D_loss: -0.6972, G_loss: -1.1308\n",
      "  Batch [340/1299] D_loss: -0.0482, G_loss: 0.3717\n",
      "  Batch [350/1299] D_loss: -0.0304, G_loss: 0.5540\n",
      "  Batch [360/1299] D_loss: 0.0221, G_loss: 0.4277\n",
      "  Batch [370/1299] D_loss: -0.0365, G_loss: 0.6742\n",
      "  Batch [380/1299] D_loss: -0.0646, G_loss: 0.5057\n",
      "  Batch [390/1299] D_loss: -0.1379, G_loss: 0.2765\n",
      "  Batch [400/1299] D_loss: -4.3376, G_loss: -7.3110\n",
      "  Batch [410/1299] D_loss: -0.0805, G_loss: 0.1306\n",
      "  Batch [420/1299] D_loss: -0.6074, G_loss: -0.7877\n",
      "  Batch [430/1299] D_loss: -0.0006, G_loss: 0.2218\n",
      "  Batch [440/1299] D_loss: -0.0558, G_loss: 0.3692\n",
      "  Batch [450/1299] D_loss: -0.1142, G_loss: 0.5107\n",
      "  Batch [460/1299] D_loss: -0.0052, G_loss: 0.4300\n",
      "  Batch [470/1299] D_loss: 0.0565, G_loss: 0.3761\n",
      "  Batch [480/1299] D_loss: -0.0196, G_loss: 0.3274\n",
      "  Batch [490/1299] D_loss: -3.9948, G_loss: -6.1920\n",
      "  Batch [500/1299] D_loss: -0.3420, G_loss: -0.3165\n",
      "  Batch [510/1299] D_loss: 0.0367, G_loss: 0.1916\n",
      "  Batch [520/1299] D_loss: -0.0362, G_loss: 0.3440\n",
      "  Batch [530/1299] D_loss: -0.0232, G_loss: 0.4137\n",
      "  Batch [540/1299] D_loss: -0.0739, G_loss: 0.4995\n",
      "  Batch [550/1299] D_loss: -0.0258, G_loss: 0.3756\n",
      "  Batch [560/1299] D_loss: -0.1202, G_loss: 0.1066\n",
      "  Batch [570/1299] D_loss: -0.0168, G_loss: 0.0409\n",
      "  Batch [580/1299] D_loss: -0.0372, G_loss: 0.0888\n",
      "  Batch [590/1299] D_loss: -0.1881, G_loss: 0.1091\n",
      "  Batch [600/1299] D_loss: -0.0557, G_loss: 0.3115\n",
      "  Batch [610/1299] D_loss: -0.1239, G_loss: 0.4270\n",
      "  Batch [620/1299] D_loss: -0.0938, G_loss: 0.5133\n",
      "  Batch [630/1299] D_loss: -0.1181, G_loss: 0.5379\n",
      "  Batch [640/1299] D_loss: -0.1905, G_loss: 0.5407\n",
      "  Batch [650/1299] D_loss: -0.0473, G_loss: 0.4749\n",
      "  Batch [660/1299] D_loss: -0.5995, G_loss: 0.1392\n",
      "  Batch [670/1299] D_loss: -1.0648, G_loss: -0.6978\n",
      "  Batch [680/1299] D_loss: -0.4396, G_loss: -0.5646\n",
      "  Batch [690/1299] D_loss: -0.7712, G_loss: -0.0201\n",
      "  Batch [700/1299] D_loss: -0.1581, G_loss: 0.1201\n",
      "  Batch [710/1299] D_loss: -0.0801, G_loss: 0.1285\n",
      "  Batch [720/1299] D_loss: -0.0212, G_loss: 0.2011\n",
      "  Batch [730/1299] D_loss: -0.0561, G_loss: 0.3627\n",
      "  Batch [740/1299] D_loss: -0.0128, G_loss: 0.4326\n",
      "  Batch [750/1299] D_loss: -0.1207, G_loss: 0.5043\n",
      "  Batch [760/1299] D_loss: -0.1175, G_loss: 0.4473\n",
      "  Batch [770/1299] D_loss: -2.7818, G_loss: -3.0091\n",
      "  Batch [780/1299] D_loss: -0.3085, G_loss: 0.1058\n",
      "  Batch [790/1299] D_loss: -0.2154, G_loss: 0.0281\n",
      "  Batch [800/1299] D_loss: -0.0503, G_loss: 0.1920\n",
      "  Batch [810/1299] D_loss: -0.0455, G_loss: 0.2259\n",
      "  Batch [820/1299] D_loss: -0.0348, G_loss: 0.2816\n",
      "  Batch [830/1299] D_loss: -1.8703, G_loss: -0.9034\n",
      "  Batch [840/1299] D_loss: -0.2976, G_loss: 0.1421\n",
      "  Batch [850/1299] D_loss: 0.0359, G_loss: 0.1356\n",
      "  Batch [860/1299] D_loss: -0.0562, G_loss: 0.0767\n",
      "  Batch [870/1299] D_loss: -0.0812, G_loss: 0.1243\n",
      "  Batch [880/1299] D_loss: -0.0777, G_loss: 0.1525\n",
      "  Batch [890/1299] D_loss: -1.1899, G_loss: -0.1632\n",
      "  Batch [900/1299] D_loss: -2.5427, G_loss: -4.2121\n",
      "  Batch [910/1299] D_loss: -1.2392, G_loss: -0.4650\n",
      "  Batch [920/1299] D_loss: -0.5604, G_loss: -0.3167\n",
      "  Batch [930/1299] D_loss: -0.1820, G_loss: 0.4623\n",
      "  Batch [940/1299] D_loss: -0.1827, G_loss: 0.7399\n",
      "  Batch [950/1299] D_loss: -0.0460, G_loss: 0.7169\n",
      "  Batch [960/1299] D_loss: -0.0488, G_loss: 0.5548\n",
      "  Batch [970/1299] D_loss: 0.0039, G_loss: 0.6143\n",
      "  Batch [980/1299] D_loss: -1.7948, G_loss: -0.5492\n",
      "  Batch [990/1299] D_loss: -1.3800, G_loss: -1.8206\n",
      "  Batch [1000/1299] D_loss: -0.4156, G_loss: -0.8935\n",
      "  Batch [1010/1299] D_loss: -0.0104, G_loss: 0.2282\n",
      "  Batch [1020/1299] D_loss: -0.0984, G_loss: 0.3928\n",
      "  Batch [1030/1299] D_loss: -0.1486, G_loss: 0.5054\n",
      "  Batch [1040/1299] D_loss: -0.0752, G_loss: 0.6581\n",
      "  Batch [1050/1299] D_loss: 0.0201, G_loss: 0.3899\n",
      "  Batch [1060/1299] D_loss: 0.0242, G_loss: 0.3601\n",
      "  Batch [1070/1299] D_loss: -3.0290, G_loss: -3.9350\n",
      "  Batch [1080/1299] D_loss: -0.0089, G_loss: 0.0786\n",
      "  Batch [1090/1299] D_loss: -0.0108, G_loss: 0.1324\n",
      "  Batch [1100/1299] D_loss: 0.0115, G_loss: 0.2444\n",
      "  Batch [1110/1299] D_loss: -0.0453, G_loss: 0.4503\n",
      "  Batch [1120/1299] D_loss: -0.0176, G_loss: 0.3690\n",
      "  Batch [1130/1299] D_loss: -0.0246, G_loss: 0.4209\n",
      "  Batch [1140/1299] D_loss: -1.9320, G_loss: -1.3794\n",
      "  Batch [1150/1299] D_loss: -0.9384, G_loss: -0.1416\n",
      "  Batch [1160/1299] D_loss: -0.0516, G_loss: -0.1353\n",
      "  Batch [1170/1299] D_loss: -0.7809, G_loss: -0.5873\n",
      "  Batch [1180/1299] D_loss: -1.8566, G_loss: -2.0002\n",
      "  Batch [1190/1299] D_loss: -0.4993, G_loss: -0.0082\n",
      "  Batch [1200/1299] D_loss: -0.0424, G_loss: 0.1155\n",
      "  Batch [1210/1299] D_loss: -0.0606, G_loss: 0.2821\n",
      "  Batch [1220/1299] D_loss: -0.0664, G_loss: 0.4113\n",
      "  Batch [1230/1299] D_loss: -0.0808, G_loss: 0.5138\n",
      "  Batch [1240/1299] D_loss: -0.0761, G_loss: 0.7017\n",
      "  Batch [1250/1299] D_loss: -0.0671, G_loss: 0.5130\n",
      "  Batch [1260/1299] D_loss: -0.0653, G_loss: 0.4515\n",
      "  Batch [1270/1299] D_loss: -0.0169, G_loss: 0.1923\n",
      "  Batch [1280/1299] D_loss: -0.0676, G_loss: 0.0655\n",
      "  Batch [1290/1299] D_loss: -0.3516, G_loss: -0.1476\n",
      "\n",
      "Epoch 54 Summary:\n",
      "  Average D_loss: -0.1582\n",
      "  Average G_loss: -0.0616\n",
      "\n",
      "Epoch [55/100]\n",
      "  Batch [0/1299] D_loss: -0.3258, G_loss: -0.3313\n",
      "  Batch [10/1299] D_loss: -0.0314, G_loss: 0.2298\n",
      "  Batch [20/1299] D_loss: -0.1402, G_loss: 0.3221\n",
      "  Batch [30/1299] D_loss: -0.2599, G_loss: 0.6382\n",
      "  Batch [40/1299] D_loss: 0.0865, G_loss: 0.5199\n",
      "  Batch [50/1299] D_loss: -0.1708, G_loss: 0.4085\n",
      "  Batch [60/1299] D_loss: -0.0467, G_loss: 0.2165\n",
      "  Batch [70/1299] D_loss: -0.7690, G_loss: -0.1762\n",
      "  Batch [80/1299] D_loss: -0.1335, G_loss: 0.0776\n",
      "  Batch [90/1299] D_loss: -0.2369, G_loss: -0.0345\n",
      "  Batch [100/1299] D_loss: -0.0109, G_loss: 0.1925\n",
      "  Batch [110/1299] D_loss: -0.1036, G_loss: 0.4301\n",
      "  Batch [120/1299] D_loss: -0.2088, G_loss: 0.4216\n",
      "  Batch [130/1299] D_loss: -0.0083, G_loss: 0.4371\n",
      "  Batch [140/1299] D_loss: -0.0880, G_loss: 0.2694\n",
      "  Batch [150/1299] D_loss: -1.0767, G_loss: -1.3609\n",
      "  Batch [160/1299] D_loss: -0.0643, G_loss: 0.1392\n",
      "  Batch [170/1299] D_loss: -0.0222, G_loss: 0.2390\n",
      "  Batch [180/1299] D_loss: -0.0775, G_loss: 0.3244\n",
      "  Batch [190/1299] D_loss: -0.1386, G_loss: 0.3909\n",
      "  Batch [200/1299] D_loss: -0.1169, G_loss: 0.3917\n",
      "  Batch [210/1299] D_loss: -0.1236, G_loss: 0.3365\n",
      "  Batch [220/1299] D_loss: -0.1044, G_loss: 0.0404\n",
      "  Batch [230/1299] D_loss: -0.0811, G_loss: 0.2350\n",
      "  Batch [240/1299] D_loss: -0.7279, G_loss: -0.2456\n",
      "  Batch [250/1299] D_loss: -0.0407, G_loss: 0.1904\n",
      "  Batch [260/1299] D_loss: -0.0360, G_loss: 0.2897\n",
      "  Batch [270/1299] D_loss: -0.0379, G_loss: 0.2727\n",
      "  Batch [280/1299] D_loss: -0.1545, G_loss: 0.4756\n",
      "  Batch [290/1299] D_loss: -0.0363, G_loss: 0.3324\n",
      "  Batch [300/1299] D_loss: -0.0534, G_loss: 0.1955\n",
      "  Batch [310/1299] D_loss: 0.0016, G_loss: 0.1736\n",
      "  Batch [320/1299] D_loss: -0.1266, G_loss: 0.3350\n",
      "  Batch [330/1299] D_loss: -0.0819, G_loss: 0.4499\n",
      "  Batch [340/1299] D_loss: -0.1419, G_loss: 0.6639\n",
      "  Batch [350/1299] D_loss: -0.1209, G_loss: 0.6829\n",
      "  Batch [360/1299] D_loss: 0.0559, G_loss: 0.4832\n",
      "  Batch [370/1299] D_loss: -0.1249, G_loss: -2.1283\n",
      "  Batch [380/1299] D_loss: -0.5178, G_loss: -0.5163\n",
      "  Batch [390/1299] D_loss: -0.0148, G_loss: 0.2361\n",
      "  Batch [400/1299] D_loss: -0.0571, G_loss: 0.4252\n",
      "  Batch [410/1299] D_loss: -0.0864, G_loss: 0.4213\n",
      "  Batch [420/1299] D_loss: -0.0123, G_loss: 0.5595\n",
      "  Batch [430/1299] D_loss: 0.0409, G_loss: 0.4866\n",
      "  Batch [440/1299] D_loss: -0.1543, G_loss: 0.5854\n",
      "  Batch [450/1299] D_loss: -0.0045, G_loss: 0.4448\n",
      "  Batch [460/1299] D_loss: -2.7150, G_loss: -6.8155\n",
      "  Batch [470/1299] D_loss: -1.2127, G_loss: -2.1179\n",
      "  Batch [480/1299] D_loss: -0.0066, G_loss: 0.2658\n",
      "  Batch [490/1299] D_loss: -0.0706, G_loss: 0.4724\n",
      "  Batch [500/1299] D_loss: -0.0800, G_loss: 0.5123\n",
      "  Batch [510/1299] D_loss: -0.0619, G_loss: 0.4283\n",
      "  Batch [520/1299] D_loss: -0.2294, G_loss: 0.3576\n",
      "  Batch [530/1299] D_loss: -0.0179, G_loss: 0.3877\n",
      "  Batch [540/1299] D_loss: -1.4154, G_loss: -3.3354\n",
      "  Batch [550/1299] D_loss: -0.1384, G_loss: 0.1163\n",
      "  Batch [560/1299] D_loss: -0.5990, G_loss: -1.2625\n",
      "  Batch [570/1299] D_loss: -0.8429, G_loss: -0.6421\n",
      "  Batch [580/1299] D_loss: -1.0113, G_loss: -0.1974\n",
      "  Batch [590/1299] D_loss: -0.0315, G_loss: 0.3341\n",
      "  Batch [600/1299] D_loss: -0.0902, G_loss: 0.4434\n",
      "  Batch [610/1299] D_loss: -0.0950, G_loss: 0.4938\n",
      "  Batch [620/1299] D_loss: 0.0123, G_loss: 0.5855\n",
      "  Batch [630/1299] D_loss: 0.0856, G_loss: 0.5897\n",
      "  Batch [640/1299] D_loss: -0.0325, G_loss: 0.4320\n",
      "  Batch [650/1299] D_loss: -0.0621, G_loss: 0.1604\n",
      "  Batch [660/1299] D_loss: -0.0265, G_loss: 0.0013\n",
      "  Batch [670/1299] D_loss: -0.5355, G_loss: -0.5630\n",
      "  Batch [680/1299] D_loss: -1.1877, G_loss: -2.5769\n",
      "  Batch [690/1299] D_loss: -0.5170, G_loss: -0.0315\n",
      "  Batch [700/1299] D_loss: -0.6254, G_loss: -0.4711\n",
      "  Batch [710/1299] D_loss: -0.1063, G_loss: 0.4485\n",
      "  Batch [720/1299] D_loss: -0.1638, G_loss: 0.7224\n",
      "  Batch [730/1299] D_loss: -0.2076, G_loss: 0.5793\n",
      "  Batch [740/1299] D_loss: -0.1801, G_loss: 0.5501\n",
      "  Batch [750/1299] D_loss: -0.1272, G_loss: 0.5697\n",
      "  Batch [760/1299] D_loss: -0.0965, G_loss: 0.3135\n",
      "  Batch [770/1299] D_loss: -0.0136, G_loss: 0.1083\n",
      "  Batch [780/1299] D_loss: -0.2113, G_loss: 0.0894\n",
      "  Batch [790/1299] D_loss: -0.3032, G_loss: -0.1121\n",
      "  Batch [800/1299] D_loss: -0.3683, G_loss: -0.0208\n",
      "  Batch [810/1299] D_loss: -0.7756, G_loss: -1.4458\n",
      "  Batch [820/1299] D_loss: -0.6531, G_loss: 0.1230\n",
      "  Batch [830/1299] D_loss: -0.9842, G_loss: -1.7996\n",
      "  Batch [840/1299] D_loss: -0.0446, G_loss: 0.1060\n",
      "  Batch [850/1299] D_loss: -0.1444, G_loss: 0.1317\n",
      "  Batch [860/1299] D_loss: -1.2548, G_loss: -0.6534\n",
      "  Batch [870/1299] D_loss: -0.0629, G_loss: 0.3467\n",
      "  Batch [880/1299] D_loss: -0.1724, G_loss: 0.4778\n",
      "  Batch [890/1299] D_loss: -0.0467, G_loss: 0.7845\n",
      "  Batch [900/1299] D_loss: -0.1098, G_loss: 0.8384\n",
      "  Batch [910/1299] D_loss: -0.2340, G_loss: 0.8326\n",
      "  Batch [920/1299] D_loss: 0.0148, G_loss: 0.6752\n",
      "  Batch [930/1299] D_loss: 0.0141, G_loss: 0.3326\n",
      "  Batch [940/1299] D_loss: -1.4440, G_loss: -4.4467\n",
      "  Batch [950/1299] D_loss: -1.6446, G_loss: -2.0958\n",
      "  Batch [960/1299] D_loss: -0.6837, G_loss: -1.3701\n",
      "  Batch [970/1299] D_loss: -0.8817, G_loss: -1.2292\n",
      "  Batch [980/1299] D_loss: -0.0452, G_loss: 0.1436\n",
      "  Batch [990/1299] D_loss: -0.5210, G_loss: -0.4531\n",
      "  Batch [1000/1299] D_loss: -0.5732, G_loss: -0.7099\n",
      "  Batch [1010/1299] D_loss: -0.4434, G_loss: -0.2622\n",
      "  Batch [1020/1299] D_loss: -0.0649, G_loss: 0.3333\n",
      "  Batch [1030/1299] D_loss: -0.1392, G_loss: 0.5763\n",
      "  Batch [1040/1299] D_loss: -0.1915, G_loss: 0.7476\n",
      "  Batch [1050/1299] D_loss: 0.0138, G_loss: 0.8177\n",
      "  Batch [1060/1299] D_loss: -0.0545, G_loss: 0.4927\n",
      "  Batch [1070/1299] D_loss: -0.0173, G_loss: 0.6779\n",
      "  Batch [1080/1299] D_loss: 0.0079, G_loss: 0.3273\n",
      "  Batch [1090/1299] D_loss: -0.4762, G_loss: -0.8796\n",
      "  Batch [1100/1299] D_loss: -1.5516, G_loss: -0.4366\n",
      "  Batch [1110/1299] D_loss: -0.4226, G_loss: -0.4483\n",
      "  Batch [1120/1299] D_loss: -0.0477, G_loss: 0.2261\n",
      "  Batch [1130/1299] D_loss: -0.0879, G_loss: 0.2929\n",
      "  Batch [1140/1299] D_loss: -0.0339, G_loss: 0.3095\n",
      "  Batch [1150/1299] D_loss: -0.0169, G_loss: 0.3065\n",
      "  Batch [1160/1299] D_loss: -0.0838, G_loss: 0.3601\n",
      "  Batch [1170/1299] D_loss: -0.0866, G_loss: 0.3310\n",
      "  Batch [1180/1299] D_loss: -0.4348, G_loss: -0.5046\n",
      "  Batch [1190/1299] D_loss: -1.2834, G_loss: -1.0331\n",
      "  Batch [1200/1299] D_loss: -0.0075, G_loss: 0.1595\n",
      "  Batch [1210/1299] D_loss: -0.0084, G_loss: 0.3003\n",
      "  Batch [1220/1299] D_loss: -0.3182, G_loss: 0.5128\n",
      "  Batch [1230/1299] D_loss: -0.1828, G_loss: 0.6334\n",
      "  Batch [1240/1299] D_loss: -0.1751, G_loss: 0.5362\n",
      "  Batch [1250/1299] D_loss: -0.0203, G_loss: 0.5697\n",
      "  Batch [1260/1299] D_loss: -0.0474, G_loss: 0.3045\n",
      "  Batch [1270/1299] D_loss: -0.6961, G_loss: -0.0769\n",
      "  Batch [1280/1299] D_loss: -0.2124, G_loss: -0.1188\n",
      "  Batch [1290/1299] D_loss: -0.9367, G_loss: -0.1431\n",
      "\n",
      "Epoch 55 Summary:\n",
      "  Average D_loss: -0.1589\n",
      "  Average G_loss: -0.0722\n",
      "\n",
      "Epoch [56/100]\n",
      "  Batch [0/1299] D_loss: -1.8812, G_loss: -1.1367\n",
      "  Batch [10/1299] D_loss: -0.7814, G_loss: -1.1671\n",
      "  Batch [20/1299] D_loss: -0.8089, G_loss: -0.3244\n",
      "  Batch [30/1299] D_loss: -0.4096, G_loss: -0.7277\n",
      "  Batch [40/1299] D_loss: -0.1133, G_loss: 0.2151\n",
      "  Batch [50/1299] D_loss: -0.0898, G_loss: 0.3866\n",
      "  Batch [60/1299] D_loss: -0.1073, G_loss: 0.6650\n",
      "  Batch [70/1299] D_loss: -0.1450, G_loss: 0.7152\n",
      "  Batch [80/1299] D_loss: -0.2269, G_loss: 0.6708\n",
      "  Batch [90/1299] D_loss: 0.0061, G_loss: 0.4151\n",
      "  Batch [100/1299] D_loss: -0.1544, G_loss: 0.3945\n",
      "  Batch [110/1299] D_loss: -0.0041, G_loss: 0.1513\n",
      "  Batch [120/1299] D_loss: -1.0235, G_loss: -2.0388\n",
      "  Batch [130/1299] D_loss: -1.1183, G_loss: -2.6212\n",
      "  Batch [140/1299] D_loss: -1.0122, G_loss: -0.7365\n",
      "  Batch [150/1299] D_loss: -0.4495, G_loss: 0.0092\n",
      "  Batch [160/1299] D_loss: -0.1997, G_loss: 0.0505\n",
      "  Batch [170/1299] D_loss: -0.0688, G_loss: 0.2833\n",
      "  Batch [180/1299] D_loss: -0.1748, G_loss: 0.5736\n",
      "  Batch [190/1299] D_loss: 0.0315, G_loss: 0.6161\n",
      "  Batch [200/1299] D_loss: -0.0667, G_loss: 0.5883\n",
      "  Batch [210/1299] D_loss: -0.0421, G_loss: 0.5750\n",
      "  Batch [220/1299] D_loss: -0.0526, G_loss: 0.4213\n",
      "  Batch [230/1299] D_loss: -1.5918, G_loss: -4.1616\n",
      "  Batch [240/1299] D_loss: -1.5019, G_loss: -1.6116\n",
      "  Batch [250/1299] D_loss: -1.0863, G_loss: -0.7602\n",
      "  Batch [260/1299] D_loss: -0.0746, G_loss: 0.3113\n",
      "  Batch [270/1299] D_loss: -0.1122, G_loss: 0.2588\n",
      "  Batch [280/1299] D_loss: -0.6314, G_loss: 0.0292\n",
      "  Batch [290/1299] D_loss: -0.4604, G_loss: 0.2357\n",
      "  Batch [300/1299] D_loss: -0.5848, G_loss: 0.0720\n",
      "  Batch [310/1299] D_loss: -0.6303, G_loss: 0.0570\n",
      "  Batch [320/1299] D_loss: -0.0092, G_loss: 0.2663\n",
      "  Batch [330/1299] D_loss: -0.4953, G_loss: -0.3139\n",
      "  Batch [340/1299] D_loss: -0.0990, G_loss: 0.0911\n",
      "  Batch [350/1299] D_loss: -0.0580, G_loss: 0.1770\n",
      "  Batch [360/1299] D_loss: -0.0575, G_loss: 0.4125\n",
      "  Batch [370/1299] D_loss: -0.1208, G_loss: 0.7037\n",
      "  Batch [380/1299] D_loss: -0.0420, G_loss: 0.5333\n",
      "  Batch [390/1299] D_loss: -0.1995, G_loss: 0.5588\n",
      "  Batch [400/1299] D_loss: 0.0117, G_loss: 0.3914\n",
      "  Batch [410/1299] D_loss: -0.0381, G_loss: 0.2161\n",
      "  Batch [420/1299] D_loss: -0.5680, G_loss: -0.5037\n",
      "  Batch [430/1299] D_loss: -0.1892, G_loss: -0.0509\n",
      "  Batch [440/1299] D_loss: -0.2376, G_loss: 0.1210\n",
      "  Batch [450/1299] D_loss: -0.0571, G_loss: 0.3052\n",
      "  Batch [460/1299] D_loss: 0.0330, G_loss: 0.3172\n",
      "  Batch [470/1299] D_loss: -0.0464, G_loss: 0.4286\n",
      "  Batch [480/1299] D_loss: 0.0398, G_loss: 0.4017\n",
      "  Batch [490/1299] D_loss: -0.0860, G_loss: 0.3925\n",
      "  Batch [500/1299] D_loss: -2.6175, G_loss: -2.4187\n",
      "  Batch [510/1299] D_loss: -0.0342, G_loss: 0.1249\n",
      "  Batch [520/1299] D_loss: -1.6332, G_loss: -1.8978\n",
      "  Batch [530/1299] D_loss: -0.1035, G_loss: 0.2311\n",
      "  Batch [540/1299] D_loss: -0.0662, G_loss: 0.4070\n",
      "  Batch [550/1299] D_loss: 0.0221, G_loss: 0.4036\n",
      "  Batch [560/1299] D_loss: -0.0929, G_loss: 0.6174\n",
      "  Batch [570/1299] D_loss: -0.0099, G_loss: 0.3922\n",
      "  Batch [580/1299] D_loss: -0.1394, G_loss: 0.4133\n",
      "  Batch [590/1299] D_loss: -0.0346, G_loss: 0.2385\n",
      "  Batch [600/1299] D_loss: -0.2744, G_loss: 0.0396\n",
      "  Batch [610/1299] D_loss: -0.2490, G_loss: -0.5838\n",
      "  Batch [620/1299] D_loss: -1.2652, G_loss: 0.0175\n",
      "  Batch [630/1299] D_loss: 0.0017, G_loss: 0.1973\n",
      "  Batch [640/1299] D_loss: -0.0612, G_loss: 0.3848\n",
      "  Batch [650/1299] D_loss: -0.0799, G_loss: 0.4521\n",
      "  Batch [660/1299] D_loss: -0.0370, G_loss: 0.4555\n",
      "  Batch [670/1299] D_loss: -0.0445, G_loss: 0.4222\n",
      "  Batch [680/1299] D_loss: -0.0142, G_loss: 0.4288\n",
      "  Batch [690/1299] D_loss: 0.0010, G_loss: 0.1703\n",
      "  Batch [700/1299] D_loss: -0.0608, G_loss: -0.0398\n",
      "  Batch [710/1299] D_loss: -0.2354, G_loss: -0.5081\n",
      "  Batch [720/1299] D_loss: -0.0372, G_loss: 0.1280\n",
      "  Batch [730/1299] D_loss: -0.0867, G_loss: 0.2266\n",
      "  Batch [740/1299] D_loss: -0.1046, G_loss: 0.4416\n",
      "  Batch [750/1299] D_loss: -0.0325, G_loss: 0.4485\n",
      "  Batch [760/1299] D_loss: 0.0414, G_loss: 0.3707\n",
      "  Batch [770/1299] D_loss: -0.0289, G_loss: 0.2896\n",
      "  Batch [780/1299] D_loss: -1.4286, G_loss: -3.8769\n",
      "  Batch [790/1299] D_loss: -0.0560, G_loss: 0.1745\n",
      "  Batch [800/1299] D_loss: -0.0821, G_loss: 0.3009\n",
      "  Batch [810/1299] D_loss: -0.0215, G_loss: 0.3523\n",
      "  Batch [820/1299] D_loss: -0.0119, G_loss: 0.2455\n",
      "  Batch [830/1299] D_loss: -1.3184, G_loss: -1.8979\n",
      "  Batch [840/1299] D_loss: 0.0023, G_loss: 0.1231\n",
      "  Batch [850/1299] D_loss: -0.1099, G_loss: 0.2090\n",
      "  Batch [860/1299] D_loss: -0.0866, G_loss: 0.4010\n",
      "  Batch [870/1299] D_loss: -0.1158, G_loss: 0.4843\n",
      "  Batch [880/1299] D_loss: -0.0772, G_loss: 0.4062\n",
      "  Batch [890/1299] D_loss: -0.0426, G_loss: 0.2851\n",
      "  Batch [900/1299] D_loss: -0.2027, G_loss: 0.0420\n",
      "  Batch [910/1299] D_loss: -0.5897, G_loss: -0.1674\n",
      "  Batch [920/1299] D_loss: -0.0938, G_loss: 0.1230\n",
      "  Batch [930/1299] D_loss: -0.0143, G_loss: 0.1830\n",
      "  Batch [940/1299] D_loss: -0.3299, G_loss: -0.4672\n",
      "  Batch [950/1299] D_loss: -1.1312, G_loss: -1.1624\n",
      "  Batch [960/1299] D_loss: -0.0775, G_loss: 0.1387\n",
      "  Batch [970/1299] D_loss: -0.6975, G_loss: -0.1732\n",
      "  Batch [980/1299] D_loss: -0.1156, G_loss: 0.2929\n",
      "  Batch [990/1299] D_loss: -0.0511, G_loss: 0.4549\n",
      "  Batch [1000/1299] D_loss: -0.0466, G_loss: 0.6091\n",
      "  Batch [1010/1299] D_loss: -0.0772, G_loss: 0.5917\n",
      "  Batch [1020/1299] D_loss: -0.0577, G_loss: 0.5256\n",
      "  Batch [1030/1299] D_loss: -0.0939, G_loss: 0.4059\n",
      "  Batch [1040/1299] D_loss: -1.2346, G_loss: -2.1371\n",
      "  Batch [1050/1299] D_loss: 0.0194, G_loss: 0.1644\n",
      "  Batch [1060/1299] D_loss: -0.0114, G_loss: 0.1503\n",
      "  Batch [1070/1299] D_loss: -0.0097, G_loss: 0.2396\n",
      "  Batch [1080/1299] D_loss: -1.1748, G_loss: -0.9413\n",
      "  Batch [1090/1299] D_loss: -0.3533, G_loss: -0.2942\n",
      "  Batch [1100/1299] D_loss: -0.5643, G_loss: -1.2695\n",
      "  Batch [1110/1299] D_loss: -0.0897, G_loss: 0.2249\n",
      "  Batch [1120/1299] D_loss: -0.0766, G_loss: 0.3286\n",
      "  Batch [1130/1299] D_loss: -0.0114, G_loss: 0.3761\n",
      "  Batch [1140/1299] D_loss: -0.0534, G_loss: 0.5431\n",
      "  Batch [1150/1299] D_loss: -0.0024, G_loss: 0.2780\n",
      "  Batch [1160/1299] D_loss: -2.3601, G_loss: -3.8756\n",
      "  Batch [1170/1299] D_loss: -0.6731, G_loss: -0.7564\n",
      "  Batch [1180/1299] D_loss: -0.0530, G_loss: 0.1946\n",
      "  Batch [1190/1299] D_loss: -0.1091, G_loss: 0.3033\n",
      "  Batch [1200/1299] D_loss: -0.0871, G_loss: 0.4170\n",
      "  Batch [1210/1299] D_loss: 0.0221, G_loss: 0.5329\n",
      "  Batch [1220/1299] D_loss: -0.0985, G_loss: 0.4980\n",
      "  Batch [1230/1299] D_loss: -1.6805, G_loss: -3.4652\n",
      "  Batch [1240/1299] D_loss: -0.8699, G_loss: -0.2782\n",
      "  Batch [1250/1299] D_loss: -1.1338, G_loss: -0.5392\n",
      "  Batch [1260/1299] D_loss: -0.3164, G_loss: -0.8920\n",
      "  Batch [1270/1299] D_loss: -0.0709, G_loss: 0.1799\n",
      "  Batch [1280/1299] D_loss: -0.8524, G_loss: -0.6899\n",
      "  Batch [1290/1299] D_loss: -0.5249, G_loss: 0.0294\n",
      "\n",
      "Epoch 56 Summary:\n",
      "  Average D_loss: -0.1595\n",
      "  Average G_loss: -0.0850\n",
      "\n",
      "Epoch [57/100]\n",
      "  Batch [0/1299] D_loss: -0.4804, G_loss: -0.0780\n",
      "  Batch [10/1299] D_loss: 0.0724, G_loss: -0.5504\n",
      "  Batch [20/1299] D_loss: -0.1174, G_loss: 0.2566\n",
      "  Batch [30/1299] D_loss: -0.0519, G_loss: 0.4491\n",
      "  Batch [40/1299] D_loss: -0.2184, G_loss: 0.6587\n",
      "  Batch [50/1299] D_loss: -0.0525, G_loss: 0.6336\n",
      "  Batch [60/1299] D_loss: -0.1381, G_loss: 0.6236\n",
      "  Batch [70/1299] D_loss: -0.1491, G_loss: 0.6571\n",
      "  Batch [80/1299] D_loss: -0.0325, G_loss: 0.1832\n",
      "  Batch [90/1299] D_loss: -0.1079, G_loss: 0.0491\n",
      "  Batch [100/1299] D_loss: -0.0303, G_loss: 0.2298\n",
      "  Batch [110/1299] D_loss: -0.0350, G_loss: 0.2817\n",
      "  Batch [120/1299] D_loss: -0.0288, G_loss: 0.3383\n",
      "  Batch [130/1299] D_loss: 0.0689, G_loss: 0.3236\n",
      "  Batch [140/1299] D_loss: 0.0028, G_loss: 0.2882\n",
      "  Batch [150/1299] D_loss: -2.0151, G_loss: -4.3020\n",
      "  Batch [160/1299] D_loss: -0.2148, G_loss: -0.3702\n",
      "  Batch [170/1299] D_loss: -0.4403, G_loss: -0.9267\n",
      "  Batch [180/1299] D_loss: -0.7123, G_loss: 0.0291\n",
      "  Batch [190/1299] D_loss: -1.1456, G_loss: -1.6596\n",
      "  Batch [200/1299] D_loss: -0.0458, G_loss: 0.2624\n",
      "  Batch [210/1299] D_loss: -0.1907, G_loss: 0.4290\n",
      "  Batch [220/1299] D_loss: -0.2510, G_loss: 0.4355\n",
      "  Batch [230/1299] D_loss: -0.0511, G_loss: 0.5359\n",
      "  Batch [240/1299] D_loss: 0.0098, G_loss: 0.3477\n",
      "  Batch [250/1299] D_loss: -0.0513, G_loss: 0.3000\n",
      "  Batch [260/1299] D_loss: -0.2940, G_loss: -0.3739\n",
      "  Batch [270/1299] D_loss: -0.1854, G_loss: 0.0534\n",
      "  Batch [280/1299] D_loss: 0.0144, G_loss: 0.1696\n",
      "  Batch [290/1299] D_loss: 0.0352, G_loss: 0.3899\n",
      "  Batch [300/1299] D_loss: 0.0182, G_loss: 0.4194\n",
      "  Batch [310/1299] D_loss: -0.0702, G_loss: 0.3585\n",
      "  Batch [320/1299] D_loss: -0.0746, G_loss: 0.3194\n",
      "  Batch [330/1299] D_loss: -1.4867, G_loss: -2.8650\n",
      "  Batch [340/1299] D_loss: -0.8361, G_loss: -1.9910\n",
      "  Batch [350/1299] D_loss: 0.0025, G_loss: 0.1637\n",
      "  Batch [360/1299] D_loss: -0.0524, G_loss: 0.2810\n",
      "  Batch [370/1299] D_loss: -0.1148, G_loss: 0.3128\n",
      "  Batch [380/1299] D_loss: -0.0108, G_loss: 0.3042\n",
      "  Batch [390/1299] D_loss: -0.0667, G_loss: 0.3231\n",
      "  Batch [400/1299] D_loss: -0.4161, G_loss: -0.9565\n",
      "  Batch [410/1299] D_loss: -0.2412, G_loss: 0.0497\n",
      "  Batch [420/1299] D_loss: -0.2332, G_loss: 0.0116\n",
      "  Batch [430/1299] D_loss: -0.3472, G_loss: -0.1961\n",
      "  Batch [440/1299] D_loss: -0.3343, G_loss: 0.0187\n",
      "  Batch [450/1299] D_loss: -0.7250, G_loss: -0.5367\n",
      "  Batch [460/1299] D_loss: -0.1364, G_loss: 0.1058\n",
      "  Batch [470/1299] D_loss: -1.0769, G_loss: -2.0393\n",
      "  Batch [480/1299] D_loss: -1.2768, G_loss: -1.5189\n",
      "  Batch [490/1299] D_loss: -0.2109, G_loss: 0.0719\n",
      "  Batch [500/1299] D_loss: -0.4930, G_loss: -0.2400\n",
      "  Batch [510/1299] D_loss: -1.1837, G_loss: -1.7207\n",
      "  Batch [520/1299] D_loss: -0.0938, G_loss: 0.3865\n",
      "  Batch [530/1299] D_loss: -0.1054, G_loss: 0.6581\n",
      "  Batch [540/1299] D_loss: -0.2004, G_loss: 0.8726\n",
      "  Batch [550/1299] D_loss: -0.1523, G_loss: 0.8242\n",
      "  Batch [560/1299] D_loss: -0.0431, G_loss: 0.6540\n",
      "  Batch [570/1299] D_loss: -0.0580, G_loss: 0.6348\n",
      "  Batch [580/1299] D_loss: -2.9248, G_loss: -1.7965\n",
      "  Batch [590/1299] D_loss: -0.2181, G_loss: -0.0218\n",
      "  Batch [600/1299] D_loss: -1.2913, G_loss: -0.8194\n",
      "  Batch [610/1299] D_loss: -0.0163, G_loss: 0.1796\n",
      "  Batch [620/1299] D_loss: -0.0637, G_loss: 0.3369\n",
      "  Batch [630/1299] D_loss: -0.0952, G_loss: 0.4550\n",
      "  Batch [640/1299] D_loss: -0.1924, G_loss: 0.5144\n",
      "  Batch [650/1299] D_loss: -0.0834, G_loss: 0.6078\n",
      "  Batch [660/1299] D_loss: -0.0861, G_loss: 0.4811\n",
      "  Batch [670/1299] D_loss: -0.8266, G_loss: -1.3009\n",
      "  Batch [680/1299] D_loss: -0.4709, G_loss: -0.4046\n",
      "  Batch [690/1299] D_loss: -0.6449, G_loss: -0.1580\n",
      "  Batch [700/1299] D_loss: -1.2621, G_loss: -0.5742\n",
      "  Batch [710/1299] D_loss: -0.0210, G_loss: 0.2615\n",
      "  Batch [720/1299] D_loss: -0.0785, G_loss: 0.4023\n",
      "  Batch [730/1299] D_loss: -0.0873, G_loss: 0.6208\n",
      "  Batch [740/1299] D_loss: -0.1031, G_loss: 0.6866\n",
      "  Batch [750/1299] D_loss: -0.0710, G_loss: 0.6823\n",
      "  Batch [760/1299] D_loss: -0.0771, G_loss: 0.4033\n",
      "  Batch [770/1299] D_loss: -0.0806, G_loss: 0.2653\n",
      "  Batch [780/1299] D_loss: -0.2521, G_loss: -1.3049\n",
      "  Batch [790/1299] D_loss: -0.3072, G_loss: -0.8051\n",
      "  Batch [800/1299] D_loss: -0.1290, G_loss: 0.3298\n",
      "  Batch [810/1299] D_loss: -0.0709, G_loss: 0.4161\n",
      "  Batch [820/1299] D_loss: -0.0325, G_loss: 0.5568\n",
      "  Batch [830/1299] D_loss: -0.1383, G_loss: 0.4770\n",
      "  Batch [840/1299] D_loss: 0.0787, G_loss: 0.4531\n",
      "  Batch [850/1299] D_loss: 0.0287, G_loss: 0.3824\n",
      "  Batch [860/1299] D_loss: -1.4615, G_loss: -1.0923\n",
      "  Batch [870/1299] D_loss: -0.0912, G_loss: 0.0239\n",
      "  Batch [880/1299] D_loss: -0.8375, G_loss: -0.5007\n",
      "  Batch [890/1299] D_loss: -0.1181, G_loss: 0.0479\n",
      "  Batch [900/1299] D_loss: -0.5579, G_loss: -0.6450\n",
      "  Batch [910/1299] D_loss: -0.1129, G_loss: 0.2970\n",
      "  Batch [920/1299] D_loss: -0.2017, G_loss: 0.5194\n",
      "  Batch [930/1299] D_loss: -0.0131, G_loss: 0.6463\n",
      "  Batch [940/1299] D_loss: 0.0095, G_loss: 0.6265\n",
      "  Batch [950/1299] D_loss: 0.0178, G_loss: 0.5859\n",
      "  Batch [960/1299] D_loss: 0.1237, G_loss: 0.5077\n",
      "  Batch [970/1299] D_loss: -0.1265, G_loss: 0.3201\n",
      "  Batch [980/1299] D_loss: -0.0222, G_loss: -0.0552\n",
      "  Batch [990/1299] D_loss: -0.9550, G_loss: -0.7135\n",
      "  Batch [1000/1299] D_loss: -0.3190, G_loss: 0.0765\n",
      "  Batch [1010/1299] D_loss: -0.0877, G_loss: 0.0735\n",
      "  Batch [1020/1299] D_loss: -0.0730, G_loss: 0.2695\n",
      "  Batch [1030/1299] D_loss: -0.1199, G_loss: 0.4715\n",
      "  Batch [1040/1299] D_loss: -0.2491, G_loss: 0.6376\n",
      "  Batch [1050/1299] D_loss: -0.0176, G_loss: 0.5902\n",
      "  Batch [1060/1299] D_loss: 0.0309, G_loss: 0.3967\n",
      "  Batch [1070/1299] D_loss: -0.0703, G_loss: 0.3962\n",
      "  Batch [1080/1299] D_loss: -2.0360, G_loss: -5.7406\n",
      "  Batch [1090/1299] D_loss: -0.0721, G_loss: 0.1263\n",
      "  Batch [1100/1299] D_loss: -0.0737, G_loss: 0.1656\n",
      "  Batch [1110/1299] D_loss: -0.0780, G_loss: 0.2964\n",
      "  Batch [1120/1299] D_loss: 0.0261, G_loss: 0.2930\n",
      "  Batch [1130/1299] D_loss: -0.0306, G_loss: 0.3493\n",
      "  Batch [1140/1299] D_loss: -0.1252, G_loss: 0.4168\n",
      "  Batch [1150/1299] D_loss: -0.2898, G_loss: -0.2168\n",
      "  Batch [1160/1299] D_loss: -0.8648, G_loss: -0.7979\n",
      "  Batch [1170/1299] D_loss: -0.0246, G_loss: 0.1822\n",
      "  Batch [1180/1299] D_loss: -0.0743, G_loss: 0.2665\n",
      "  Batch [1190/1299] D_loss: -0.0675, G_loss: 0.4813\n",
      "  Batch [1200/1299] D_loss: -0.0226, G_loss: 0.3905\n",
      "  Batch [1210/1299] D_loss: -0.3481, G_loss: 0.4621\n",
      "  Batch [1220/1299] D_loss: -2.4326, G_loss: -3.8727\n",
      "  Batch [1230/1299] D_loss: -0.0991, G_loss: 0.0225\n",
      "  Batch [1240/1299] D_loss: -0.6867, G_loss: -0.2218\n",
      "  Batch [1250/1299] D_loss: -0.6191, G_loss: -0.1619\n",
      "  Batch [1260/1299] D_loss: -0.0830, G_loss: 0.3037\n",
      "  Batch [1270/1299] D_loss: -0.0410, G_loss: 0.4491\n",
      "  Batch [1280/1299] D_loss: -0.1422, G_loss: 0.5355\n",
      "  Batch [1290/1299] D_loss: 0.0290, G_loss: 0.5398\n",
      "\n",
      "Epoch 57 Summary:\n",
      "  Average D_loss: -0.1606\n",
      "  Average G_loss: -0.0519\n",
      "\n",
      "Epoch [58/100]\n",
      "  Batch [0/1299] D_loss: -0.1061, G_loss: 0.4494\n",
      "  Batch [10/1299] D_loss: -0.0213, G_loss: 0.2849\n",
      "  Batch [20/1299] D_loss: -0.9294, G_loss: -0.1968\n",
      "  Batch [30/1299] D_loss: -0.4871, G_loss: -0.2620\n",
      "  Batch [40/1299] D_loss: -0.3755, G_loss: 0.0204\n",
      "  Batch [50/1299] D_loss: -0.3552, G_loss: -0.0093\n",
      "  Batch [60/1299] D_loss: -0.2323, G_loss: 0.1171\n",
      "  Batch [70/1299] D_loss: -0.2484, G_loss: 0.0391\n",
      "  Batch [80/1299] D_loss: -0.6364, G_loss: -0.1742\n",
      "  Batch [90/1299] D_loss: -0.0968, G_loss: 0.3560\n",
      "  Batch [100/1299] D_loss: -0.0686, G_loss: 0.5071\n",
      "  Batch [110/1299] D_loss: 0.0742, G_loss: 0.5987\n",
      "  Batch [120/1299] D_loss: -0.0364, G_loss: 0.5847\n",
      "  Batch [130/1299] D_loss: -0.0198, G_loss: 0.2751\n",
      "  Batch [140/1299] D_loss: -0.0549, G_loss: 0.3110\n",
      "  Batch [150/1299] D_loss: -0.0421, G_loss: -0.1598\n",
      "  Batch [160/1299] D_loss: -0.0127, G_loss: 0.1689\n",
      "  Batch [170/1299] D_loss: -0.0429, G_loss: 0.3378\n",
      "  Batch [180/1299] D_loss: -0.1603, G_loss: 0.3320\n",
      "  Batch [190/1299] D_loss: -0.1296, G_loss: 0.4993\n",
      "  Batch [200/1299] D_loss: 0.0279, G_loss: 0.3611\n",
      "  Batch [210/1299] D_loss: 0.0628, G_loss: 0.2701\n",
      "  Batch [220/1299] D_loss: -0.0448, G_loss: 0.1102\n",
      "  Batch [230/1299] D_loss: 0.0046, G_loss: 0.1526\n",
      "  Batch [240/1299] D_loss: -0.0500, G_loss: 0.2183\n",
      "  Batch [250/1299] D_loss: -0.1030, G_loss: 0.4034\n",
      "  Batch [260/1299] D_loss: -0.0779, G_loss: 0.4546\n",
      "  Batch [270/1299] D_loss: -0.8423, G_loss: -0.4648\n",
      "  Batch [280/1299] D_loss: -0.9748, G_loss: -0.9754\n",
      "  Batch [290/1299] D_loss: -0.1285, G_loss: -0.0420\n",
      "  Batch [300/1299] D_loss: -1.7148, G_loss: -0.0221\n",
      "  Batch [310/1299] D_loss: 0.0000, G_loss: 0.1500\n",
      "  Batch [320/1299] D_loss: -0.2002, G_loss: 0.4264\n",
      "  Batch [330/1299] D_loss: -0.0830, G_loss: 0.6151\n",
      "  Batch [340/1299] D_loss: -0.0722, G_loss: 0.7935\n",
      "  Batch [350/1299] D_loss: -0.0715, G_loss: 0.5732\n",
      "  Batch [360/1299] D_loss: 0.0668, G_loss: 0.4517\n",
      "  Batch [370/1299] D_loss: -0.0519, G_loss: 0.4294\n",
      "  Batch [380/1299] D_loss: -3.1735, G_loss: -6.0278\n",
      "  Batch [390/1299] D_loss: -0.0115, G_loss: 0.1095\n",
      "  Batch [400/1299] D_loss: -0.0219, G_loss: 0.1614\n",
      "  Batch [410/1299] D_loss: -0.0390, G_loss: 0.1420\n",
      "  Batch [420/1299] D_loss: -1.8503, G_loss: -2.0540\n",
      "  Batch [430/1299] D_loss: -0.0066, G_loss: 0.1525\n",
      "  Batch [440/1299] D_loss: -0.7502, G_loss: -0.9245\n",
      "  Batch [450/1299] D_loss: -0.0965, G_loss: 0.0949\n",
      "  Batch [460/1299] D_loss: -0.4118, G_loss: 0.1001\n",
      "  Batch [470/1299] D_loss: -0.1468, G_loss: 0.1917\n",
      "  Batch [480/1299] D_loss: -0.8793, G_loss: -0.0204\n",
      "  Batch [490/1299] D_loss: -0.6130, G_loss: -0.5655\n",
      "  Batch [500/1299] D_loss: -0.0222, G_loss: 0.1947\n",
      "  Batch [510/1299] D_loss: -0.1045, G_loss: 0.4310\n",
      "  Batch [520/1299] D_loss: -0.0573, G_loss: 0.6011\n",
      "  Batch [530/1299] D_loss: -0.1038, G_loss: 0.6436\n",
      "  Batch [540/1299] D_loss: 0.0439, G_loss: 0.6712\n",
      "  Batch [550/1299] D_loss: -0.1821, G_loss: 0.4722\n",
      "  Batch [560/1299] D_loss: -0.0747, G_loss: 0.2560\n",
      "  Batch [570/1299] D_loss: 0.0075, G_loss: 0.1257\n",
      "  Batch [580/1299] D_loss: 0.0013, G_loss: 0.1477\n",
      "  Batch [590/1299] D_loss: -0.0371, G_loss: 0.2237\n",
      "  Batch [600/1299] D_loss: -1.0609, G_loss: -0.6891\n",
      "  Batch [610/1299] D_loss: -0.0742, G_loss: 0.1552\n",
      "  Batch [620/1299] D_loss: -0.0917, G_loss: 0.3955\n",
      "  Batch [630/1299] D_loss: -0.0128, G_loss: 0.5611\n",
      "  Batch [640/1299] D_loss: 0.0344, G_loss: 0.5515\n",
      "  Batch [650/1299] D_loss: 0.0342, G_loss: 0.4672\n",
      "  Batch [660/1299] D_loss: -0.0064, G_loss: 0.4732\n",
      "  Batch [670/1299] D_loss: -1.5256, G_loss: -0.4385\n",
      "  Batch [680/1299] D_loss: -0.1409, G_loss: -0.8567\n",
      "  Batch [690/1299] D_loss: 0.0008, G_loss: 0.0974\n",
      "  Batch [700/1299] D_loss: -0.0117, G_loss: 0.1408\n",
      "  Batch [710/1299] D_loss: -1.0720, G_loss: -0.3574\n",
      "  Batch [720/1299] D_loss: -0.1079, G_loss: 0.2883\n",
      "  Batch [730/1299] D_loss: -0.0457, G_loss: 0.4526\n",
      "  Batch [740/1299] D_loss: -0.0323, G_loss: 0.3759\n",
      "  Batch [750/1299] D_loss: 0.0003, G_loss: 0.3760\n",
      "  Batch [760/1299] D_loss: -0.0634, G_loss: 0.3256\n",
      "  Batch [770/1299] D_loss: -0.0760, G_loss: 0.3109\n",
      "  Batch [780/1299] D_loss: -0.0082, G_loss: 0.2086\n",
      "  Batch [790/1299] D_loss: -0.0198, G_loss: 0.1886\n",
      "  Batch [800/1299] D_loss: -0.9398, G_loss: -1.6133\n",
      "  Batch [810/1299] D_loss: -0.0438, G_loss: 0.1971\n",
      "  Batch [820/1299] D_loss: -0.1200, G_loss: 0.2043\n",
      "  Batch [830/1299] D_loss: -0.0458, G_loss: 0.2329\n",
      "  Batch [840/1299] D_loss: -0.3679, G_loss: 0.0491\n",
      "  Batch [850/1299] D_loss: -0.0196, G_loss: 0.1623\n",
      "  Batch [860/1299] D_loss: -0.0730, G_loss: 0.3273\n",
      "  Batch [870/1299] D_loss: -0.0814, G_loss: 0.4484\n",
      "  Batch [880/1299] D_loss: -0.1978, G_loss: 0.3870\n",
      "  Batch [890/1299] D_loss: -0.0350, G_loss: 0.2791\n",
      "  Batch [900/1299] D_loss: -0.4794, G_loss: 0.0949\n",
      "  Batch [910/1299] D_loss: -0.8767, G_loss: -2.7118\n",
      "  Batch [920/1299] D_loss: -0.0084, G_loss: 0.0100\n",
      "  Batch [930/1299] D_loss: -0.0003, G_loss: 0.0052\n",
      "  Batch [940/1299] D_loss: -0.4435, G_loss: -0.2017\n",
      "  Batch [950/1299] D_loss: -0.0970, G_loss: 0.1777\n",
      "  Batch [960/1299] D_loss: -0.0749, G_loss: 0.3042\n",
      "  Batch [970/1299] D_loss: -0.0348, G_loss: 0.5590\n",
      "  Batch [980/1299] D_loss: -0.0559, G_loss: 0.5463\n",
      "  Batch [990/1299] D_loss: 0.0326, G_loss: 0.4473\n",
      "  Batch [1000/1299] D_loss: -0.0185, G_loss: 0.5024\n",
      "  Batch [1010/1299] D_loss: -0.1165, G_loss: 0.0504\n",
      "  Batch [1020/1299] D_loss: 0.0186, G_loss: 0.1960\n",
      "  Batch [1030/1299] D_loss: -0.0300, G_loss: 0.2006\n",
      "  Batch [1040/1299] D_loss: -0.1172, G_loss: 0.1809\n",
      "  Batch [1050/1299] D_loss: -0.1042, G_loss: -0.1714\n",
      "  Batch [1060/1299] D_loss: -1.0175, G_loss: -0.7056\n",
      "  Batch [1070/1299] D_loss: 0.0674, G_loss: 0.1474\n",
      "  Batch [1080/1299] D_loss: -0.0531, G_loss: 0.2639\n",
      "  Batch [1090/1299] D_loss: -0.0890, G_loss: 0.3893\n",
      "  Batch [1100/1299] D_loss: 0.0134, G_loss: 0.5110\n",
      "  Batch [1110/1299] D_loss: 0.0963, G_loss: 0.3994\n",
      "  Batch [1120/1299] D_loss: -0.0336, G_loss: 0.4017\n",
      "  Batch [1130/1299] D_loss: -0.0814, G_loss: 0.3067\n",
      "  Batch [1140/1299] D_loss: -1.6997, G_loss: -1.9026\n",
      "  Batch [1150/1299] D_loss: -1.1138, G_loss: -1.5023\n",
      "  Batch [1160/1299] D_loss: -0.8839, G_loss: -0.0104\n",
      "  Batch [1170/1299] D_loss: -0.6004, G_loss: 0.0436\n",
      "  Batch [1180/1299] D_loss: -0.0612, G_loss: 0.1146\n",
      "  Batch [1190/1299] D_loss: -0.0602, G_loss: 0.1918\n",
      "  Batch [1200/1299] D_loss: -1.4612, G_loss: -0.7347\n",
      "  Batch [1210/1299] D_loss: -0.0042, G_loss: 0.1348\n",
      "  Batch [1220/1299] D_loss: -0.0273, G_loss: 0.5387\n",
      "  Batch [1230/1299] D_loss: -0.1840, G_loss: 0.6778\n",
      "  Batch [1240/1299] D_loss: 0.0277, G_loss: 0.8235\n",
      "  Batch [1250/1299] D_loss: -0.0752, G_loss: 0.7475\n",
      "  Batch [1260/1299] D_loss: 0.0848, G_loss: 0.3795\n",
      "  Batch [1270/1299] D_loss: -0.0090, G_loss: 0.5156\n",
      "  Batch [1280/1299] D_loss: -3.3523, G_loss: -4.1385\n",
      "  Batch [1290/1299] D_loss: -2.0645, G_loss: -3.6271\n",
      "\n",
      "Epoch 58 Summary:\n",
      "  Average D_loss: -0.1477\n",
      "  Average G_loss: -0.0824\n",
      "\n",
      "Epoch [59/100]\n",
      "  Batch [0/1299] D_loss: -0.0787, G_loss: 0.1815\n",
      "  Batch [10/1299] D_loss: -0.0859, G_loss: 0.2996\n",
      "  Batch [20/1299] D_loss: -0.0170, G_loss: 0.3456\n",
      "  Batch [30/1299] D_loss: -0.1056, G_loss: 0.5457\n",
      "  Batch [40/1299] D_loss: -0.0416, G_loss: 0.5460\n",
      "  Batch [50/1299] D_loss: -0.0158, G_loss: 0.4042\n",
      "  Batch [60/1299] D_loss: -0.0908, G_loss: 0.2890\n",
      "  Batch [70/1299] D_loss: -1.1765, G_loss: -1.2986\n",
      "  Batch [80/1299] D_loss: -1.3710, G_loss: -1.0377\n",
      "  Batch [90/1299] D_loss: -0.5230, G_loss: -0.8100\n",
      "  Batch [100/1299] D_loss: -0.0369, G_loss: 0.1308\n",
      "  Batch [110/1299] D_loss: -0.0667, G_loss: 0.3095\n",
      "  Batch [120/1299] D_loss: -0.0834, G_loss: 0.3430\n",
      "  Batch [130/1299] D_loss: -0.0921, G_loss: 0.5231\n",
      "  Batch [140/1299] D_loss: -0.0326, G_loss: 0.3790\n",
      "  Batch [150/1299] D_loss: -0.0544, G_loss: 0.3300\n",
      "  Batch [160/1299] D_loss: -0.0584, G_loss: 0.2248\n",
      "  Batch [170/1299] D_loss: -0.3424, G_loss: -0.0115\n",
      "  Batch [180/1299] D_loss: -0.2941, G_loss: 0.0219\n",
      "  Batch [190/1299] D_loss: -0.0423, G_loss: 0.3650\n",
      "  Batch [200/1299] D_loss: -0.0379, G_loss: 0.4285\n",
      "  Batch [210/1299] D_loss: -0.0137, G_loss: 0.2856\n",
      "  Batch [220/1299] D_loss: -0.0175, G_loss: 0.2239\n",
      "  Batch [230/1299] D_loss: -1.3932, G_loss: -2.2021\n",
      "  Batch [240/1299] D_loss: -0.6580, G_loss: -0.5899\n",
      "  Batch [250/1299] D_loss: -0.1733, G_loss: 0.0643\n",
      "  Batch [260/1299] D_loss: -2.2070, G_loss: -4.1770\n",
      "  Batch [270/1299] D_loss: -0.0134, G_loss: 0.2128\n",
      "  Batch [280/1299] D_loss: -0.0172, G_loss: 0.3625\n",
      "  Batch [290/1299] D_loss: -0.0993, G_loss: 0.4777\n",
      "  Batch [300/1299] D_loss: -0.0268, G_loss: 0.5157\n",
      "  Batch [310/1299] D_loss: -0.0655, G_loss: 0.5553\n",
      "  Batch [320/1299] D_loss: -0.0943, G_loss: 0.4453\n",
      "  Batch [330/1299] D_loss: -0.0515, G_loss: 0.3152\n",
      "  Batch [340/1299] D_loss: -0.1533, G_loss: -0.1485\n",
      "  Batch [350/1299] D_loss: -0.4461, G_loss: -0.2352\n",
      "  Batch [360/1299] D_loss: 0.0092, G_loss: 0.1841\n",
      "  Batch [370/1299] D_loss: -0.0931, G_loss: 0.3316\n",
      "  Batch [380/1299] D_loss: -0.0661, G_loss: 0.3903\n",
      "  Batch [390/1299] D_loss: -0.0834, G_loss: 0.4674\n",
      "  Batch [400/1299] D_loss: -0.0420, G_loss: 0.4026\n",
      "  Batch [410/1299] D_loss: -0.0380, G_loss: 0.1866\n",
      "  Batch [420/1299] D_loss: 0.0057, G_loss: 0.1263\n",
      "  Batch [430/1299] D_loss: 0.0139, G_loss: 0.1920\n",
      "  Batch [440/1299] D_loss: -0.0535, G_loss: 0.2828\n",
      "  Batch [450/1299] D_loss: -0.0178, G_loss: 0.3340\n",
      "  Batch [460/1299] D_loss: -0.1931, G_loss: 0.4554\n",
      "  Batch [470/1299] D_loss: -0.0364, G_loss: 0.3162\n",
      "  Batch [480/1299] D_loss: -0.3460, G_loss: -0.4690\n",
      "  Batch [490/1299] D_loss: -0.0586, G_loss: -0.0351\n",
      "  Batch [500/1299] D_loss: -0.4323, G_loss: -0.9795\n",
      "  Batch [510/1299] D_loss: -0.0769, G_loss: 0.2362\n",
      "  Batch [520/1299] D_loss: -0.0322, G_loss: 0.3968\n",
      "  Batch [530/1299] D_loss: -0.0343, G_loss: 0.6094\n",
      "  Batch [540/1299] D_loss: -0.1607, G_loss: 0.6052\n",
      "  Batch [550/1299] D_loss: 0.0182, G_loss: 0.3183\n",
      "  Batch [560/1299] D_loss: -0.1559, G_loss: 0.3784\n",
      "  Batch [570/1299] D_loss: -0.1187, G_loss: -0.3968\n",
      "  Batch [580/1299] D_loss: -0.1057, G_loss: -0.2618\n",
      "  Batch [590/1299] D_loss: -0.7890, G_loss: -4.0421\n",
      "  Batch [600/1299] D_loss: -0.6236, G_loss: -0.6369\n",
      "  Batch [610/1299] D_loss: -0.1588, G_loss: 0.0759\n",
      "  Batch [620/1299] D_loss: -0.0732, G_loss: 0.1361\n",
      "  Batch [630/1299] D_loss: -0.0888, G_loss: 0.4334\n",
      "  Batch [640/1299] D_loss: -0.1375, G_loss: 0.6690\n",
      "  Batch [650/1299] D_loss: -0.1802, G_loss: 0.6457\n",
      "  Batch [660/1299] D_loss: -0.0521, G_loss: 0.7874\n",
      "  Batch [670/1299] D_loss: -0.0944, G_loss: 0.3925\n",
      "  Batch [680/1299] D_loss: -0.0479, G_loss: 0.2985\n",
      "  Batch [690/1299] D_loss: -0.3139, G_loss: -0.0001\n",
      "  Batch [700/1299] D_loss: -0.8646, G_loss: -1.3013\n",
      "  Batch [710/1299] D_loss: -0.3189, G_loss: 0.0070\n",
      "  Batch [720/1299] D_loss: -0.1303, G_loss: 0.0865\n",
      "  Batch [730/1299] D_loss: -0.2238, G_loss: 0.0083\n",
      "  Batch [740/1299] D_loss: -0.0599, G_loss: 0.2096\n",
      "  Batch [750/1299] D_loss: -0.1579, G_loss: 0.3661\n",
      "  Batch [760/1299] D_loss: -0.1895, G_loss: 0.5749\n",
      "  Batch [770/1299] D_loss: -0.1223, G_loss: 0.6119\n",
      "  Batch [780/1299] D_loss: -0.1092, G_loss: 0.5799\n",
      "  Batch [790/1299] D_loss: -0.1080, G_loss: 0.4009\n",
      "  Batch [800/1299] D_loss: -0.0991, G_loss: 0.2079\n",
      "  Batch [810/1299] D_loss: -0.0161, G_loss: 0.1214\n",
      "  Batch [820/1299] D_loss: 0.0237, G_loss: 0.1384\n",
      "  Batch [830/1299] D_loss: 0.0460, G_loss: 0.2161\n",
      "  Batch [840/1299] D_loss: -0.0791, G_loss: 0.3113\n",
      "  Batch [850/1299] D_loss: -0.0835, G_loss: 0.2772\n",
      "  Batch [860/1299] D_loss: -1.1592, G_loss: -2.1221\n",
      "  Batch [870/1299] D_loss: -0.0402, G_loss: 0.1145\n",
      "  Batch [880/1299] D_loss: -0.0244, G_loss: 0.1614\n",
      "  Batch [890/1299] D_loss: -0.0088, G_loss: 0.3076\n",
      "  Batch [900/1299] D_loss: -0.0522, G_loss: 0.2647\n",
      "  Batch [910/1299] D_loss: -0.0512, G_loss: 0.2018\n",
      "  Batch [920/1299] D_loss: -0.0199, G_loss: 0.1791\n",
      "  Batch [930/1299] D_loss: -0.0535, G_loss: 0.3293\n",
      "  Batch [940/1299] D_loss: -0.0724, G_loss: 0.3608\n",
      "  Batch [950/1299] D_loss: -0.0696, G_loss: 0.4112\n",
      "  Batch [960/1299] D_loss: -0.0629, G_loss: 0.3360\n",
      "  Batch [970/1299] D_loss: -0.1999, G_loss: 0.0026\n",
      "  Batch [980/1299] D_loss: -0.1353, G_loss: -0.0356\n",
      "  Batch [990/1299] D_loss: -0.7739, G_loss: -0.3365\n",
      "  Batch [1000/1299] D_loss: -0.2830, G_loss: -0.6172\n",
      "  Batch [1010/1299] D_loss: -0.0106, G_loss: 0.1765\n",
      "  Batch [1020/1299] D_loss: -0.0360, G_loss: 0.3019\n",
      "  Batch [1030/1299] D_loss: -0.0023, G_loss: 0.4839\n",
      "  Batch [1040/1299] D_loss: -0.0291, G_loss: 0.4939\n",
      "  Batch [1050/1299] D_loss: -0.1893, G_loss: 0.5134\n",
      "  Batch [1060/1299] D_loss: -1.8236, G_loss: -3.5618\n",
      "  Batch [1070/1299] D_loss: -0.8176, G_loss: -0.1359\n",
      "  Batch [1080/1299] D_loss: -0.5503, G_loss: -0.5685\n",
      "  Batch [1090/1299] D_loss: -0.2235, G_loss: 0.0906\n",
      "  Batch [1100/1299] D_loss: -0.8991, G_loss: -0.1266\n",
      "  Batch [1110/1299] D_loss: -0.9566, G_loss: -1.7175\n",
      "  Batch [1120/1299] D_loss: -0.5548, G_loss: -0.6841\n",
      "  Batch [1130/1299] D_loss: -0.1874, G_loss: 0.1089\n",
      "  Batch [1140/1299] D_loss: -0.1711, G_loss: 0.1167\n",
      "  Batch [1150/1299] D_loss: -0.4893, G_loss: 0.1123\n",
      "  Batch [1160/1299] D_loss: -0.1251, G_loss: 0.1750\n",
      "  Batch [1170/1299] D_loss: -0.5510, G_loss: -1.3165\n",
      "  Batch [1180/1299] D_loss: -0.1081, G_loss: 0.3639\n",
      "  Batch [1190/1299] D_loss: -0.0540, G_loss: 0.5031\n",
      "  Batch [1200/1299] D_loss: -0.1939, G_loss: 0.7177\n",
      "  Batch [1210/1299] D_loss: -0.2232, G_loss: 0.8184\n",
      "  Batch [1220/1299] D_loss: -0.1129, G_loss: 0.6738\n",
      "  Batch [1230/1299] D_loss: -0.1160, G_loss: 0.5115\n",
      "  Batch [1240/1299] D_loss: -0.2202, G_loss: 0.1277\n",
      "  Batch [1250/1299] D_loss: -0.0871, G_loss: 0.0878\n",
      "  Batch [1260/1299] D_loss: -0.2091, G_loss: -0.1324\n",
      "  Batch [1270/1299] D_loss: -0.2984, G_loss: 0.0611\n",
      "  Batch [1280/1299] D_loss: -0.0488, G_loss: 0.2714\n",
      "  Batch [1290/1299] D_loss: 0.0659, G_loss: 0.3270\n",
      "\n",
      "Epoch 59 Summary:\n",
      "  Average D_loss: -0.1360\n",
      "  Average G_loss: -0.0607\n",
      "\n",
      "Epoch [60/100]\n",
      "  Batch [0/1299] D_loss: -0.1392, G_loss: 0.4308\n",
      "  Batch [10/1299] D_loss: -0.0052, G_loss: 0.3384\n",
      "  Batch [20/1299] D_loss: -0.2207, G_loss: -0.1685\n",
      "  Batch [30/1299] D_loss: -0.1653, G_loss: 0.0361\n",
      "  Batch [40/1299] D_loss: -0.7259, G_loss: -0.8904\n",
      "  Batch [50/1299] D_loss: -0.6113, G_loss: 0.1963\n",
      "  Batch [60/1299] D_loss: -1.3598, G_loss: -2.4136\n",
      "  Batch [70/1299] D_loss: -0.0253, G_loss: 0.1508\n",
      "  Batch [80/1299] D_loss: -0.0198, G_loss: 0.2941\n",
      "  Batch [90/1299] D_loss: -0.0957, G_loss: 0.5481\n",
      "  Batch [100/1299] D_loss: -0.1939, G_loss: 0.6306\n",
      "  Batch [110/1299] D_loss: 0.0457, G_loss: 0.4763\n",
      "  Batch [120/1299] D_loss: -0.0783, G_loss: 0.3454\n",
      "  Batch [130/1299] D_loss: -0.5237, G_loss: -0.5099\n",
      "  Batch [140/1299] D_loss: 0.0051, G_loss: 0.1570\n",
      "  Batch [150/1299] D_loss: 0.0134, G_loss: 0.2274\n",
      "  Batch [160/1299] D_loss: -0.0584, G_loss: 0.2999\n",
      "  Batch [170/1299] D_loss: -0.0170, G_loss: 0.3887\n",
      "  Batch [180/1299] D_loss: -2.2260, G_loss: -1.9170\n",
      "  Batch [190/1299] D_loss: -0.0040, G_loss: 0.1267\n",
      "  Batch [200/1299] D_loss: -0.0495, G_loss: 0.2305\n",
      "  Batch [210/1299] D_loss: -0.0664, G_loss: 0.4044\n",
      "  Batch [220/1299] D_loss: -0.0666, G_loss: 0.4159\n",
      "  Batch [230/1299] D_loss: -0.0868, G_loss: 0.4005\n",
      "  Batch [240/1299] D_loss: -0.0126, G_loss: 0.2828\n",
      "  Batch [250/1299] D_loss: -0.9349, G_loss: -1.8494\n",
      "  Batch [260/1299] D_loss: -0.0441, G_loss: 0.1723\n",
      "  Batch [270/1299] D_loss: -0.0228, G_loss: 0.3235\n",
      "  Batch [280/1299] D_loss: 0.0165, G_loss: 0.4622\n",
      "  Batch [290/1299] D_loss: -0.0729, G_loss: 0.5115\n",
      "  Batch [300/1299] D_loss: 0.0298, G_loss: 0.3272\n",
      "  Batch [310/1299] D_loss: -0.0094, G_loss: 0.4642\n",
      "  Batch [320/1299] D_loss: -0.0602, G_loss: 0.1831\n",
      "  Batch [330/1299] D_loss: -0.0119, G_loss: 0.0658\n",
      "  Batch [340/1299] D_loss: 0.0111, G_loss: 0.1557\n",
      "  Batch [350/1299] D_loss: -1.6428, G_loss: -2.2478\n",
      "  Batch [360/1299] D_loss: -0.2016, G_loss: -0.0477\n",
      "  Batch [370/1299] D_loss: 0.0102, G_loss: 0.1250\n",
      "  Batch [380/1299] D_loss: -1.3097, G_loss: -3.0969\n",
      "  Batch [390/1299] D_loss: -0.2400, G_loss: -0.4022\n",
      "  Batch [400/1299] D_loss: -0.1280, G_loss: 0.3740\n",
      "  Batch [410/1299] D_loss: -0.2495, G_loss: 0.4702\n",
      "  Batch [420/1299] D_loss: -0.2066, G_loss: 0.5974\n",
      "  Batch [430/1299] D_loss: -0.0399, G_loss: 0.5608\n",
      "  Batch [440/1299] D_loss: -0.0729, G_loss: 0.3862\n",
      "  Batch [450/1299] D_loss: -0.0915, G_loss: 0.2304\n",
      "  Batch [460/1299] D_loss: -0.2291, G_loss: 0.0338\n",
      "  Batch [470/1299] D_loss: -0.3255, G_loss: 0.0064\n",
      "  Batch [480/1299] D_loss: -0.3633, G_loss: 0.0135\n",
      "  Batch [490/1299] D_loss: -0.1600, G_loss: 0.1689\n",
      "  Batch [500/1299] D_loss: -0.0150, G_loss: 0.1957\n",
      "  Batch [510/1299] D_loss: -0.0309, G_loss: 0.3700\n",
      "  Batch [520/1299] D_loss: -0.0844, G_loss: 0.3431\n",
      "  Batch [530/1299] D_loss: -0.1275, G_loss: 0.3874\n",
      "  Batch [540/1299] D_loss: -0.0290, G_loss: 0.4437\n",
      "  Batch [550/1299] D_loss: -0.0873, G_loss: 0.3934\n",
      "  Batch [560/1299] D_loss: -0.6375, G_loss: -4.8071\n",
      "  Batch [570/1299] D_loss: -0.4733, G_loss: -0.1796\n",
      "  Batch [580/1299] D_loss: -0.2439, G_loss: -0.0877\n",
      "  Batch [590/1299] D_loss: -0.0889, G_loss: 0.3556\n",
      "  Batch [600/1299] D_loss: -0.0649, G_loss: 0.3638\n",
      "  Batch [610/1299] D_loss: -0.0987, G_loss: 0.4915\n",
      "  Batch [620/1299] D_loss: -0.1177, G_loss: 0.5928\n",
      "  Batch [630/1299] D_loss: -0.1080, G_loss: 0.2843\n",
      "  Batch [640/1299] D_loss: -0.0500, G_loss: 0.3205\n",
      "  Batch [650/1299] D_loss: 0.0000, G_loss: 0.0649\n",
      "  Batch [660/1299] D_loss: -0.9336, G_loss: -1.1873\n",
      "  Batch [670/1299] D_loss: -0.3943, G_loss: -0.2095\n",
      "  Batch [680/1299] D_loss: -0.0656, G_loss: 0.2374\n",
      "  Batch [690/1299] D_loss: -0.1902, G_loss: 0.0628\n",
      "  Batch [700/1299] D_loss: -0.8934, G_loss: -0.6369\n",
      "  Batch [710/1299] D_loss: -0.7860, G_loss: -1.4711\n",
      "  Batch [720/1299] D_loss: -0.0874, G_loss: 0.3210\n",
      "  Batch [730/1299] D_loss: -0.0187, G_loss: 0.4028\n",
      "  Batch [740/1299] D_loss: -0.1189, G_loss: 0.6292\n",
      "  Batch [750/1299] D_loss: -0.1285, G_loss: 0.6314\n",
      "  Batch [760/1299] D_loss: 0.1145, G_loss: 0.3714\n",
      "  Batch [770/1299] D_loss: -0.0667, G_loss: 0.2067\n",
      "  Batch [780/1299] D_loss: -1.4610, G_loss: -0.7190\n",
      "  Batch [790/1299] D_loss: 0.0229, G_loss: 0.0661\n",
      "  Batch [800/1299] D_loss: -0.5188, G_loss: -0.3953\n",
      "  Batch [810/1299] D_loss: -0.5133, G_loss: -0.3070\n",
      "  Batch [820/1299] D_loss: 0.0090, G_loss: 0.1439\n",
      "  Batch [830/1299] D_loss: -0.0386, G_loss: 0.2444\n",
      "  Batch [840/1299] D_loss: -0.0283, G_loss: 0.4626\n",
      "  Batch [850/1299] D_loss: -0.1214, G_loss: 0.5129\n",
      "  Batch [860/1299] D_loss: 0.0209, G_loss: 0.4684\n",
      "  Batch [870/1299] D_loss: -0.0793, G_loss: 0.2803\n",
      "  Batch [880/1299] D_loss: -1.7238, G_loss: -2.5968\n",
      "  Batch [890/1299] D_loss: -0.1099, G_loss: 0.0760\n",
      "  Batch [900/1299] D_loss: -2.3666, G_loss: -3.6753\n",
      "  Batch [910/1299] D_loss: -0.0644, G_loss: 0.0153\n",
      "  Batch [920/1299] D_loss: -0.0397, G_loss: 0.1328\n",
      "  Batch [930/1299] D_loss: 0.0068, G_loss: 0.2989\n",
      "  Batch [940/1299] D_loss: -0.0744, G_loss: 0.4016\n",
      "  Batch [950/1299] D_loss: -0.2216, G_loss: 0.6196\n",
      "  Batch [960/1299] D_loss: -0.1225, G_loss: 0.5461\n",
      "  Batch [970/1299] D_loss: -0.0758, G_loss: 0.4026\n",
      "  Batch [980/1299] D_loss: -0.6050, G_loss: -0.0542\n",
      "  Batch [990/1299] D_loss: -0.4597, G_loss: -0.1002\n",
      "  Batch [1000/1299] D_loss: -0.1279, G_loss: 0.0612\n",
      "  Batch [1010/1299] D_loss: -0.0792, G_loss: 0.2574\n",
      "  Batch [1020/1299] D_loss: -0.1787, G_loss: 0.6165\n",
      "  Batch [1030/1299] D_loss: -0.0130, G_loss: 0.6771\n",
      "  Batch [1040/1299] D_loss: -0.0915, G_loss: 0.7428\n",
      "  Batch [1050/1299] D_loss: -0.0209, G_loss: 0.4888\n",
      "  Batch [1060/1299] D_loss: 0.0243, G_loss: 0.2730\n",
      "  Batch [1070/1299] D_loss: -0.4437, G_loss: -0.8611\n",
      "  Batch [1080/1299] D_loss: -0.0182, G_loss: 0.1560\n",
      "  Batch [1090/1299] D_loss: -0.5716, G_loss: 0.0526\n",
      "  Batch [1100/1299] D_loss: -0.0927, G_loss: 0.0896\n",
      "  Batch [1110/1299] D_loss: -0.4097, G_loss: -0.0133\n",
      "  Batch [1120/1299] D_loss: -0.0346, G_loss: 0.1881\n",
      "  Batch [1130/1299] D_loss: -0.0643, G_loss: 0.2891\n",
      "  Batch [1140/1299] D_loss: -0.1070, G_loss: 0.4256\n",
      "  Batch [1150/1299] D_loss: -0.0703, G_loss: 0.3891\n",
      "  Batch [1160/1299] D_loss: -0.0338, G_loss: 0.3015\n",
      "  Batch [1170/1299] D_loss: -0.0741, G_loss: 0.2451\n",
      "  Batch [1180/1299] D_loss: -1.2878, G_loss: -0.6906\n",
      "  Batch [1190/1299] D_loss: -0.0005, G_loss: 0.1973\n",
      "  Batch [1200/1299] D_loss: 0.0358, G_loss: 0.2862\n",
      "  Batch [1210/1299] D_loss: -0.0620, G_loss: 0.3856\n",
      "  Batch [1220/1299] D_loss: -0.0636, G_loss: 0.2392\n",
      "  Batch [1230/1299] D_loss: -0.0244, G_loss: 0.1751\n",
      "  Batch [1240/1299] D_loss: -0.8278, G_loss: -2.3316\n",
      "  Batch [1250/1299] D_loss: -0.0503, G_loss: 0.1496\n",
      "  Batch [1260/1299] D_loss: -0.0468, G_loss: 0.1803\n",
      "  Batch [1270/1299] D_loss: -0.0432, G_loss: 0.4484\n",
      "  Batch [1280/1299] D_loss: -0.1130, G_loss: 0.6712\n",
      "  Batch [1290/1299] D_loss: 0.0007, G_loss: 0.5318\n",
      "\n",
      "Epoch 60 Summary:\n",
      "  Average D_loss: -0.1399\n",
      "  Average G_loss: -0.0583\n",
      "\n",
      "Epoch [61/100]\n",
      "  Batch [0/1299] D_loss: -0.0082, G_loss: 0.4110\n",
      "  Batch [10/1299] D_loss: -0.1621, G_loss: 0.2930\n",
      "  Batch [20/1299] D_loss: -1.0009, G_loss: -0.4734\n",
      "  Batch [30/1299] D_loss: -0.6796, G_loss: -0.9748\n",
      "  Batch [40/1299] D_loss: -0.5208, G_loss: -1.3966\n",
      "  Batch [50/1299] D_loss: -0.1160, G_loss: 0.0553\n",
      "  Batch [60/1299] D_loss: -0.1512, G_loss: 0.2267\n",
      "  Batch [70/1299] D_loss: -0.0139, G_loss: 0.3480\n",
      "  Batch [80/1299] D_loss: -0.0467, G_loss: 0.4022\n",
      "  Batch [90/1299] D_loss: -0.1254, G_loss: 0.6725\n",
      "  Batch [100/1299] D_loss: -0.1118, G_loss: 0.3482\n",
      "  Batch [110/1299] D_loss: -1.4326, G_loss: -2.3917\n",
      "  Batch [120/1299] D_loss: -0.3438, G_loss: 0.0611\n",
      "  Batch [130/1299] D_loss: -0.0592, G_loss: 0.2098\n",
      "  Batch [140/1299] D_loss: -0.1369, G_loss: 0.4129\n",
      "  Batch [150/1299] D_loss: 0.0122, G_loss: 0.3907\n",
      "  Batch [160/1299] D_loss: -0.1009, G_loss: 0.5626\n",
      "  Batch [170/1299] D_loss: -0.1306, G_loss: 0.6716\n",
      "  Batch [180/1299] D_loss: -0.0463, G_loss: 0.3303\n",
      "  Batch [190/1299] D_loss: -0.0883, G_loss: 0.0820\n",
      "  Batch [200/1299] D_loss: -0.0116, G_loss: 0.0584\n",
      "  Batch [210/1299] D_loss: -0.0343, G_loss: 0.0693\n",
      "  Batch [220/1299] D_loss: -1.3769, G_loss: -5.1401\n",
      "  Batch [230/1299] D_loss: -0.9327, G_loss: -0.4673\n",
      "  Batch [240/1299] D_loss: -1.0885, G_loss: -3.0957\n",
      "  Batch [250/1299] D_loss: -0.3630, G_loss: 0.1599\n",
      "  Batch [260/1299] D_loss: -0.0328, G_loss: 0.2214\n",
      "  Batch [270/1299] D_loss: -0.2136, G_loss: 0.2898\n",
      "  Batch [280/1299] D_loss: -0.1600, G_loss: 0.6414\n",
      "  Batch [290/1299] D_loss: -0.0287, G_loss: 0.8236\n",
      "  Batch [300/1299] D_loss: 0.0029, G_loss: 0.7872\n",
      "  Batch [310/1299] D_loss: -0.0289, G_loss: 0.4495\n",
      "  Batch [320/1299] D_loss: -0.0283, G_loss: 0.2672\n",
      "  Batch [330/1299] D_loss: -0.9108, G_loss: -0.2836\n",
      "  Batch [340/1299] D_loss: -0.9083, G_loss: -2.1871\n",
      "  Batch [350/1299] D_loss: -0.3999, G_loss: -1.3344\n",
      "  Batch [360/1299] D_loss: -1.1480, G_loss: -1.1876\n",
      "  Batch [370/1299] D_loss: -0.3448, G_loss: 0.0338\n",
      "  Batch [380/1299] D_loss: -0.0751, G_loss: 0.0963\n",
      "  Batch [390/1299] D_loss: -1.2126, G_loss: -0.7717\n",
      "  Batch [400/1299] D_loss: -0.1296, G_loss: 0.0237\n",
      "  Batch [410/1299] D_loss: -0.3351, G_loss: 0.0580\n",
      "  Batch [420/1299] D_loss: -1.0703, G_loss: -2.7604\n",
      "  Batch [430/1299] D_loss: -0.3251, G_loss: 0.1981\n",
      "  Batch [440/1299] D_loss: -0.1574, G_loss: 0.4293\n",
      "  Batch [450/1299] D_loss: -0.0908, G_loss: 0.6469\n",
      "  Batch [460/1299] D_loss: -0.0902, G_loss: 0.8360\n",
      "  Batch [470/1299] D_loss: -0.2622, G_loss: 0.7355\n",
      "  Batch [480/1299] D_loss: -0.0908, G_loss: 0.7750\n",
      "  Batch [490/1299] D_loss: -0.3815, G_loss: 0.6708\n",
      "  Batch [500/1299] D_loss: -0.0298, G_loss: 0.4458\n",
      "  Batch [510/1299] D_loss: -0.0071, G_loss: -3.3391\n",
      "  Batch [520/1299] D_loss: -0.3845, G_loss: -0.4816\n",
      "  Batch [530/1299] D_loss: -0.7203, G_loss: -0.3743\n",
      "  Batch [540/1299] D_loss: 0.0069, G_loss: 0.3256\n",
      "  Batch [550/1299] D_loss: 0.0352, G_loss: 0.4125\n",
      "  Batch [560/1299] D_loss: -0.1333, G_loss: 0.6313\n",
      "  Batch [570/1299] D_loss: -0.0760, G_loss: 0.6091\n",
      "  Batch [580/1299] D_loss: -0.0903, G_loss: 0.5914\n",
      "  Batch [590/1299] D_loss: 0.0542, G_loss: 0.3635\n",
      "  Batch [600/1299] D_loss: -0.0216, G_loss: 0.2405\n",
      "  Batch [610/1299] D_loss: -0.1197, G_loss: -0.1031\n",
      "  Batch [620/1299] D_loss: -0.1318, G_loss: 0.2242\n",
      "  Batch [630/1299] D_loss: -0.0420, G_loss: 0.2785\n",
      "  Batch [640/1299] D_loss: -0.1574, G_loss: 0.4314\n",
      "  Batch [650/1299] D_loss: -0.2118, G_loss: 0.6856\n",
      "  Batch [660/1299] D_loss: 0.0268, G_loss: 0.5071\n",
      "  Batch [670/1299] D_loss: -0.1492, G_loss: 0.5645\n",
      "  Batch [680/1299] D_loss: -2.1492, G_loss: -6.1161\n",
      "  Batch [690/1299] D_loss: -0.3815, G_loss: -1.1137\n",
      "  Batch [700/1299] D_loss: -0.5372, G_loss: -1.2658\n",
      "  Batch [710/1299] D_loss: -0.0697, G_loss: 0.0385\n",
      "  Batch [720/1299] D_loss: -0.4030, G_loss: 0.0552\n",
      "  Batch [730/1299] D_loss: -0.8188, G_loss: -2.3995\n",
      "  Batch [740/1299] D_loss: -0.0624, G_loss: 0.3374\n",
      "  Batch [750/1299] D_loss: -0.2336, G_loss: 0.6858\n",
      "  Batch [760/1299] D_loss: -0.1380, G_loss: 0.7509\n",
      "  Batch [770/1299] D_loss: -0.1331, G_loss: 0.7156\n",
      "  Batch [780/1299] D_loss: -0.0576, G_loss: 0.7538\n",
      "  Batch [790/1299] D_loss: 0.0469, G_loss: 0.4968\n",
      "  Batch [800/1299] D_loss: -0.1848, G_loss: 0.3856\n",
      "  Batch [810/1299] D_loss: -0.9132, G_loss: -1.0836\n",
      "  Batch [820/1299] D_loss: -0.0230, G_loss: 0.1552\n",
      "  Batch [830/1299] D_loss: -0.0124, G_loss: 0.2493\n",
      "  Batch [840/1299] D_loss: -0.0486, G_loss: 0.3245\n",
      "  Batch [850/1299] D_loss: -0.0421, G_loss: 0.3754\n",
      "  Batch [860/1299] D_loss: -0.0544, G_loss: 0.2775\n",
      "  Batch [870/1299] D_loss: -0.0291, G_loss: 0.2628\n",
      "  Batch [880/1299] D_loss: -0.5415, G_loss: -0.6757\n",
      "  Batch [890/1299] D_loss: 0.0092, G_loss: 0.1547\n",
      "  Batch [900/1299] D_loss: -0.0814, G_loss: 0.2798\n",
      "  Batch [910/1299] D_loss: -0.2737, G_loss: 0.2852\n",
      "  Batch [920/1299] D_loss: -0.0269, G_loss: 0.2581\n",
      "  Batch [930/1299] D_loss: -0.3459, G_loss: 0.0154\n",
      "  Batch [940/1299] D_loss: -0.7015, G_loss: 0.0527\n",
      "  Batch [950/1299] D_loss: -0.0337, G_loss: 0.3056\n",
      "  Batch [960/1299] D_loss: -0.0251, G_loss: 0.4720\n",
      "  Batch [970/1299] D_loss: -0.0738, G_loss: 0.5772\n",
      "  Batch [980/1299] D_loss: -0.0233, G_loss: 0.3164\n",
      "  Batch [990/1299] D_loss: -0.0149, G_loss: 0.4530\n",
      "  Batch [1000/1299] D_loss: -0.0821, G_loss: 0.2852\n",
      "  Batch [1010/1299] D_loss: -1.0150, G_loss: -1.7734\n",
      "  Batch [1020/1299] D_loss: -0.4641, G_loss: -0.1630\n",
      "  Batch [1030/1299] D_loss: -0.0305, G_loss: 0.2121\n",
      "  Batch [1040/1299] D_loss: -0.0825, G_loss: 0.3930\n",
      "  Batch [1050/1299] D_loss: -0.0333, G_loss: 0.2724\n",
      "  Batch [1060/1299] D_loss: -0.0728, G_loss: 0.4071\n",
      "  Batch [1070/1299] D_loss: -0.0434, G_loss: 0.4154\n",
      "  Batch [1080/1299] D_loss: -0.0366, G_loss: 0.3813\n",
      "  Batch [1090/1299] D_loss: 0.0117, G_loss: 0.1649\n",
      "  Batch [1100/1299] D_loss: -0.6809, G_loss: -0.4554\n",
      "  Batch [1110/1299] D_loss: -0.2491, G_loss: -0.2636\n",
      "  Batch [1120/1299] D_loss: -1.0891, G_loss: -0.6053\n",
      "  Batch [1130/1299] D_loss: -0.0325, G_loss: 0.2994\n",
      "  Batch [1140/1299] D_loss: -0.0741, G_loss: 0.4486\n",
      "  Batch [1150/1299] D_loss: -0.0696, G_loss: 0.6452\n",
      "  Batch [1160/1299] D_loss: -0.0757, G_loss: 0.6398\n",
      "  Batch [1170/1299] D_loss: -0.0317, G_loss: 0.6974\n",
      "  Batch [1180/1299] D_loss: 0.0290, G_loss: 0.2733\n",
      "  Batch [1190/1299] D_loss: -1.3550, G_loss: -4.5556\n",
      "  Batch [1200/1299] D_loss: -0.0856, G_loss: -0.0134\n",
      "  Batch [1210/1299] D_loss: -0.0253, G_loss: 0.2278\n",
      "  Batch [1220/1299] D_loss: -0.0798, G_loss: 0.3878\n",
      "  Batch [1230/1299] D_loss: -0.0311, G_loss: 0.5438\n",
      "  Batch [1240/1299] D_loss: 0.0517, G_loss: 0.3358\n",
      "  Batch [1250/1299] D_loss: -0.0554, G_loss: 0.2515\n",
      "  Batch [1260/1299] D_loss: -2.2686, G_loss: -1.5860\n",
      "  Batch [1270/1299] D_loss: -1.3814, G_loss: -3.2666\n",
      "  Batch [1280/1299] D_loss: -0.4967, G_loss: -0.1563\n",
      "  Batch [1290/1299] D_loss: -0.1333, G_loss: 0.0773\n",
      "\n",
      "Epoch 61 Summary:\n",
      "  Average D_loss: -0.1605\n",
      "  Average G_loss: -0.0987\n",
      "\n",
      "Models saved at epoch 61:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250120_151701_dataset+cell_type/generator_20250120_151701_dataset+cell_type_epoch_61.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250120_151701_dataset+cell_type/discriminator_20250120_151701_dataset+cell_type_epoch_61.pt\n",
      "\n",
      "Epoch [62/100]\n",
      "  Batch [0/1299] D_loss: -0.7204, G_loss: -0.2060\n",
      "  Batch [10/1299] D_loss: -0.0331, G_loss: 0.1745\n",
      "  Batch [20/1299] D_loss: -0.1584, G_loss: 0.4736\n",
      "  Batch [30/1299] D_loss: -0.1154, G_loss: 0.5948\n",
      "  Batch [40/1299] D_loss: -0.1446, G_loss: 0.7232\n",
      "  Batch [50/1299] D_loss: 0.0385, G_loss: 0.6221\n",
      "  Batch [60/1299] D_loss: -0.1452, G_loss: 0.7224\n",
      "  Batch [70/1299] D_loss: -0.1105, G_loss: 0.6805\n",
      "  Batch [80/1299] D_loss: -4.0897, G_loss: -6.0936\n",
      "  Batch [90/1299] D_loss: -0.3507, G_loss: -0.5356\n",
      "  Batch [100/1299] D_loss: -0.0152, G_loss: 0.2405\n",
      "  Batch [110/1299] D_loss: -0.6781, G_loss: -1.5675\n",
      "  Batch [120/1299] D_loss: -0.0358, G_loss: 0.1262\n",
      "  Batch [130/1299] D_loss: -0.6401, G_loss: -0.0847\n",
      "  Batch [140/1299] D_loss: -0.2851, G_loss: -1.4880\n",
      "  Batch [150/1299] D_loss: -0.0779, G_loss: 0.0905\n",
      "  Batch [160/1299] D_loss: -0.0912, G_loss: 0.3865\n",
      "  Batch [170/1299] D_loss: -0.0868, G_loss: 0.5678\n",
      "  Batch [180/1299] D_loss: -0.1216, G_loss: 0.6210\n",
      "  Batch [190/1299] D_loss: 0.0090, G_loss: 0.4112\n",
      "  Batch [200/1299] D_loss: -0.0526, G_loss: 0.3678\n",
      "  Batch [210/1299] D_loss: -0.0058, G_loss: 0.3240\n",
      "  Batch [220/1299] D_loss: -0.3847, G_loss: -0.2218\n",
      "  Batch [230/1299] D_loss: -0.2265, G_loss: 0.0184\n",
      "  Batch [240/1299] D_loss: -0.0188, G_loss: 0.1610\n",
      "  Batch [250/1299] D_loss: -0.0733, G_loss: 0.2967\n",
      "  Batch [260/1299] D_loss: -0.1313, G_loss: 0.4357\n",
      "  Batch [270/1299] D_loss: -0.1229, G_loss: 0.4972\n",
      "  Batch [280/1299] D_loss: 0.1328, G_loss: 0.3707\n",
      "  Batch [290/1299] D_loss: 0.0116, G_loss: 0.2831\n",
      "  Batch [300/1299] D_loss: -1.7407, G_loss: -3.0643\n",
      "  Batch [310/1299] D_loss: -1.3471, G_loss: -1.5393\n",
      "  Batch [320/1299] D_loss: -0.0948, G_loss: -0.1995\n",
      "  Batch [330/1299] D_loss: -0.0118, G_loss: 0.2346\n",
      "  Batch [340/1299] D_loss: -0.0373, G_loss: 0.2977\n",
      "  Batch [350/1299] D_loss: -0.1027, G_loss: 0.2609\n",
      "  Batch [360/1299] D_loss: -0.0970, G_loss: 0.3417\n",
      "  Batch [370/1299] D_loss: -0.2072, G_loss: 0.4790\n",
      "  Batch [380/1299] D_loss: -0.0603, G_loss: 0.5788\n",
      "  Batch [390/1299] D_loss: -3.8244, G_loss: -3.4521\n",
      "  Batch [400/1299] D_loss: -0.5569, G_loss: -1.5838\n",
      "  Batch [410/1299] D_loss: -0.1960, G_loss: 0.1504\n",
      "  Batch [420/1299] D_loss: -0.3169, G_loss: 0.1514\n",
      "  Batch [430/1299] D_loss: -0.0573, G_loss: 0.3307\n",
      "  Batch [440/1299] D_loss: -1.9193, G_loss: -3.8969\n",
      "  Batch [450/1299] D_loss: -0.0141, G_loss: 0.1421\n",
      "  Batch [460/1299] D_loss: -0.1049, G_loss: 0.3976\n",
      "  Batch [470/1299] D_loss: -0.0617, G_loss: 0.6661\n",
      "  Batch [480/1299] D_loss: -0.1148, G_loss: 0.6716\n",
      "  Batch [490/1299] D_loss: -0.1248, G_loss: 0.6329\n",
      "  Batch [500/1299] D_loss: -0.0092, G_loss: 0.3199\n",
      "  Batch [510/1299] D_loss: -0.0826, G_loss: 0.3804\n",
      "  Batch [520/1299] D_loss: -2.1029, G_loss: -4.1488\n",
      "  Batch [530/1299] D_loss: -0.5204, G_loss: -0.2943\n",
      "  Batch [540/1299] D_loss: -0.0352, G_loss: 0.1114\n",
      "  Batch [550/1299] D_loss: -0.0481, G_loss: 0.3192\n",
      "  Batch [560/1299] D_loss: -0.1451, G_loss: 0.4759\n",
      "  Batch [570/1299] D_loss: -0.0306, G_loss: 0.5207\n",
      "  Batch [580/1299] D_loss: -0.1285, G_loss: 0.4905\n",
      "  Batch [590/1299] D_loss: -0.0170, G_loss: 0.3619\n",
      "  Batch [600/1299] D_loss: -0.0524, G_loss: 0.3893\n",
      "  Batch [610/1299] D_loss: -0.0717, G_loss: 0.3537\n",
      "  Batch [620/1299] D_loss: -1.3804, G_loss: -1.7498\n",
      "  Batch [630/1299] D_loss: -0.3648, G_loss: 0.1053\n",
      "  Batch [640/1299] D_loss: -0.4771, G_loss: 0.1493\n",
      "  Batch [650/1299] D_loss: -0.5531, G_loss: -0.0608\n",
      "  Batch [660/1299] D_loss: -0.9070, G_loss: -0.0616\n",
      "  Batch [670/1299] D_loss: -0.5727, G_loss: -1.2348\n",
      "  Batch [680/1299] D_loss: -0.5532, G_loss: -0.5719\n",
      "  Batch [690/1299] D_loss: -0.0914, G_loss: 0.1756\n",
      "  Batch [700/1299] D_loss: -0.1338, G_loss: 0.3526\n",
      "  Batch [710/1299] D_loss: -0.2531, G_loss: 0.6545\n",
      "  Batch [720/1299] D_loss: -0.1075, G_loss: 0.7390\n",
      "  Batch [730/1299] D_loss: -0.1740, G_loss: 0.6440\n",
      "  Batch [740/1299] D_loss: 0.0454, G_loss: 0.4367\n",
      "  Batch [750/1299] D_loss: -0.7187, G_loss: 0.0211\n",
      "  Batch [760/1299] D_loss: -0.0228, G_loss: 0.1267\n",
      "  Batch [770/1299] D_loss: -0.0429, G_loss: 0.2226\n",
      "  Batch [780/1299] D_loss: -0.0922, G_loss: 0.4198\n",
      "  Batch [790/1299] D_loss: 0.0354, G_loss: 0.4745\n",
      "  Batch [800/1299] D_loss: -0.1452, G_loss: 0.4735\n",
      "  Batch [810/1299] D_loss: 0.0476, G_loss: 0.4781\n",
      "  Batch [820/1299] D_loss: -0.0043, G_loss: 0.2030\n",
      "  Batch [830/1299] D_loss: -0.1483, G_loss: 0.0615\n",
      "  Batch [840/1299] D_loss: -0.9007, G_loss: -0.6014\n",
      "  Batch [850/1299] D_loss: -0.0512, G_loss: 0.2809\n",
      "  Batch [860/1299] D_loss: -0.0014, G_loss: 0.3748\n",
      "  Batch [870/1299] D_loss: 0.0105, G_loss: 0.3028\n",
      "  Batch [880/1299] D_loss: -0.1002, G_loss: 0.4212\n",
      "  Batch [890/1299] D_loss: -0.0745, G_loss: 0.3195\n",
      "  Batch [900/1299] D_loss: -3.1318, G_loss: -7.5320\n",
      "  Batch [910/1299] D_loss: -0.8667, G_loss: -1.0506\n",
      "  Batch [920/1299] D_loss: -0.0288, G_loss: 0.1628\n",
      "  Batch [930/1299] D_loss: 0.0022, G_loss: 0.2194\n",
      "  Batch [940/1299] D_loss: -0.0694, G_loss: 0.5873\n",
      "  Batch [950/1299] D_loss: -0.1421, G_loss: 0.5596\n",
      "  Batch [960/1299] D_loss: -0.2564, G_loss: 0.5790\n",
      "  Batch [970/1299] D_loss: -0.0445, G_loss: 0.5093\n",
      "  Batch [980/1299] D_loss: -0.0058, G_loss: 0.2843\n",
      "  Batch [990/1299] D_loss: -0.4443, G_loss: -0.4387\n",
      "  Batch [1000/1299] D_loss: -0.6976, G_loss: -0.6691\n",
      "  Batch [1010/1299] D_loss: -0.2310, G_loss: -0.6537\n",
      "  Batch [1020/1299] D_loss: -0.0560, G_loss: 0.3136\n",
      "  Batch [1030/1299] D_loss: -0.0756, G_loss: 0.3440\n",
      "  Batch [1040/1299] D_loss: 0.0091, G_loss: 0.3150\n",
      "  Batch [1050/1299] D_loss: -0.0560, G_loss: 0.5243\n",
      "  Batch [1060/1299] D_loss: -0.1255, G_loss: 0.4954\n",
      "  Batch [1070/1299] D_loss: -0.2317, G_loss: 0.4572\n",
      "  Batch [1080/1299] D_loss: -0.8155, G_loss: -5.5844\n",
      "  Batch [1090/1299] D_loss: 0.0090, G_loss: 0.1463\n",
      "  Batch [1100/1299] D_loss: -0.0049, G_loss: 0.1669\n",
      "  Batch [1110/1299] D_loss: -0.0894, G_loss: 0.4253\n",
      "  Batch [1120/1299] D_loss: -0.1327, G_loss: 0.4187\n",
      "  Batch [1130/1299] D_loss: 0.0102, G_loss: 0.4862\n",
      "  Batch [1140/1299] D_loss: -0.1384, G_loss: 0.3950\n",
      "  Batch [1150/1299] D_loss: -0.5428, G_loss: -1.3483\n",
      "  Batch [1160/1299] D_loss: 0.0102, G_loss: 0.1185\n",
      "  Batch [1170/1299] D_loss: -0.1139, G_loss: 0.2797\n",
      "  Batch [1180/1299] D_loss: -0.0724, G_loss: 0.4513\n",
      "  Batch [1190/1299] D_loss: -0.0421, G_loss: 0.3423\n",
      "  Batch [1200/1299] D_loss: -0.0871, G_loss: 0.4248\n",
      "  Batch [1210/1299] D_loss: -0.0588, G_loss: 0.3191\n",
      "  Batch [1220/1299] D_loss: -0.0718, G_loss: 0.3668\n",
      "  Batch [1230/1299] D_loss: -0.0180, G_loss: -0.1781\n",
      "  Batch [1240/1299] D_loss: -0.6387, G_loss: -0.7013\n",
      "  Batch [1250/1299] D_loss: -1.9271, G_loss: -2.6548\n",
      "  Batch [1260/1299] D_loss: -0.0515, G_loss: 0.2189\n",
      "  Batch [1270/1299] D_loss: -0.1440, G_loss: 0.4053\n",
      "  Batch [1280/1299] D_loss: -0.1618, G_loss: 0.6052\n",
      "  Batch [1290/1299] D_loss: -0.2251, G_loss: 0.6133\n",
      "\n",
      "Epoch 62 Summary:\n",
      "  Average D_loss: -0.1472\n",
      "  Average G_loss: -0.0496\n",
      "\n",
      "Epoch [63/100]\n",
      "  Batch [0/1299] D_loss: -0.0753, G_loss: 0.6698\n",
      "  Batch [10/1299] D_loss: -0.0680, G_loss: 0.5087\n",
      "  Batch [20/1299] D_loss: -2.7909, G_loss: -1.4524\n",
      "  Batch [30/1299] D_loss: 0.0017, G_loss: 0.1548\n",
      "  Batch [40/1299] D_loss: -0.4619, G_loss: -0.3384\n",
      "  Batch [50/1299] D_loss: -0.1288, G_loss: 0.0125\n",
      "  Batch [60/1299] D_loss: -0.0287, G_loss: 0.2222\n",
      "  Batch [70/1299] D_loss: -0.0144, G_loss: 0.4070\n",
      "  Batch [80/1299] D_loss: -0.1438, G_loss: 0.3273\n",
      "  Batch [90/1299] D_loss: -0.0862, G_loss: 0.4064\n",
      "  Batch [100/1299] D_loss: -2.2268, G_loss: -3.9964\n",
      "  Batch [110/1299] D_loss: 0.0234, G_loss: 0.1310\n",
      "  Batch [120/1299] D_loss: -0.1207, G_loss: 0.3260\n",
      "  Batch [130/1299] D_loss: -0.0389, G_loss: 0.4729\n",
      "  Batch [140/1299] D_loss: -0.0493, G_loss: 0.4125\n",
      "  Batch [150/1299] D_loss: -0.0226, G_loss: 0.4165\n",
      "  Batch [160/1299] D_loss: -0.0128, G_loss: 0.2680\n",
      "  Batch [170/1299] D_loss: -0.0505, G_loss: 0.2410\n",
      "  Batch [180/1299] D_loss: -1.0552, G_loss: -0.5016\n",
      "  Batch [190/1299] D_loss: -0.2984, G_loss: -0.2374\n",
      "  Batch [200/1299] D_loss: -0.4355, G_loss: 0.0288\n",
      "  Batch [210/1299] D_loss: -0.7902, G_loss: -0.1585\n",
      "  Batch [220/1299] D_loss: -0.1498, G_loss: 0.3330\n",
      "  Batch [230/1299] D_loss: -0.1223, G_loss: 0.4214\n",
      "  Batch [240/1299] D_loss: -0.1243, G_loss: 0.3180\n",
      "  Batch [250/1299] D_loss: -0.0427, G_loss: 0.4534\n",
      "  Batch [260/1299] D_loss: 0.0194, G_loss: 0.2312\n",
      "  Batch [270/1299] D_loss: -1.0659, G_loss: -3.2797\n",
      "  Batch [280/1299] D_loss: -0.0590, G_loss: 0.3066\n",
      "  Batch [290/1299] D_loss: 0.0263, G_loss: 0.4458\n",
      "  Batch [300/1299] D_loss: -0.0351, G_loss: 0.6598\n",
      "  Batch [310/1299] D_loss: -0.0603, G_loss: 0.6174\n",
      "  Batch [320/1299] D_loss: -0.0234, G_loss: 0.5335\n",
      "  Batch [330/1299] D_loss: -0.0628, G_loss: 0.2740\n",
      "  Batch [340/1299] D_loss: -0.0262, G_loss: 0.2817\n",
      "  Batch [350/1299] D_loss: -1.0998, G_loss: -0.8964\n",
      "  Batch [360/1299] D_loss: -0.4825, G_loss: -0.1616\n",
      "  Batch [370/1299] D_loss: -0.1391, G_loss: 0.0852\n",
      "  Batch [380/1299] D_loss: -0.0742, G_loss: 0.0559\n",
      "  Batch [390/1299] D_loss: -0.1191, G_loss: 0.1265\n",
      "  Batch [400/1299] D_loss: -1.1860, G_loss: -0.8593\n",
      "  Batch [410/1299] D_loss: -0.6024, G_loss: -0.6029\n",
      "  Batch [420/1299] D_loss: -0.0862, G_loss: 0.4100\n",
      "  Batch [430/1299] D_loss: -0.0703, G_loss: 0.5351\n",
      "  Batch [440/1299] D_loss: -0.0876, G_loss: 0.4872\n",
      "  Batch [450/1299] D_loss: 0.0261, G_loss: 0.4407\n",
      "  Batch [460/1299] D_loss: -0.0750, G_loss: 0.5833\n",
      "  Batch [470/1299] D_loss: -2.0665, G_loss: -3.0658\n",
      "  Batch [480/1299] D_loss: -0.5399, G_loss: 0.0957\n",
      "  Batch [490/1299] D_loss: -0.5685, G_loss: 0.0618\n",
      "  Batch [500/1299] D_loss: 0.0014, G_loss: 0.1976\n",
      "  Batch [510/1299] D_loss: -0.0203, G_loss: 0.2785\n",
      "  Batch [520/1299] D_loss: -0.0719, G_loss: 0.4140\n",
      "  Batch [530/1299] D_loss: -0.0805, G_loss: 0.3965\n",
      "  Batch [540/1299] D_loss: -0.0841, G_loss: 0.3470\n",
      "  Batch [550/1299] D_loss: -0.2407, G_loss: -0.2262\n",
      "  Batch [560/1299] D_loss: -0.5401, G_loss: -0.1176\n",
      "  Batch [570/1299] D_loss: 0.0150, G_loss: 0.2104\n",
      "  Batch [580/1299] D_loss: -0.0831, G_loss: 0.5283\n",
      "  Batch [590/1299] D_loss: -0.0557, G_loss: 0.4779\n",
      "  Batch [600/1299] D_loss: -0.1991, G_loss: 0.6418\n",
      "  Batch [610/1299] D_loss: -0.1148, G_loss: 0.6177\n",
      "  Batch [620/1299] D_loss: -0.0781, G_loss: 0.4197\n",
      "  Batch [630/1299] D_loss: -3.8415, G_loss: -5.8433\n",
      "  Batch [640/1299] D_loss: -0.0967, G_loss: 0.0252\n",
      "  Batch [650/1299] D_loss: -0.1501, G_loss: -0.1627\n",
      "  Batch [660/1299] D_loss: -0.0151, G_loss: 0.1585\n",
      "  Batch [670/1299] D_loss: -0.0277, G_loss: 0.1666\n",
      "  Batch [680/1299] D_loss: -0.0630, G_loss: 0.4016\n",
      "  Batch [690/1299] D_loss: 0.0008, G_loss: 0.3994\n",
      "  Batch [700/1299] D_loss: -0.1801, G_loss: 0.6213\n",
      "  Batch [710/1299] D_loss: -0.1444, G_loss: 0.5083\n",
      "  Batch [720/1299] D_loss: -0.0561, G_loss: 0.3493\n",
      "  Batch [730/1299] D_loss: -0.1907, G_loss: -0.3083\n",
      "  Batch [740/1299] D_loss: -0.1463, G_loss: -0.2263\n",
      "  Batch [750/1299] D_loss: -0.4238, G_loss: -0.7472\n",
      "  Batch [760/1299] D_loss: -0.0390, G_loss: 0.1912\n",
      "  Batch [770/1299] D_loss: -0.0355, G_loss: 0.2241\n",
      "  Batch [780/1299] D_loss: -0.0717, G_loss: 0.3252\n",
      "  Batch [790/1299] D_loss: -0.0297, G_loss: 0.2966\n",
      "  Batch [800/1299] D_loss: -0.1517, G_loss: 0.5344\n",
      "  Batch [810/1299] D_loss: -0.0717, G_loss: 0.2607\n",
      "  Batch [820/1299] D_loss: -0.8525, G_loss: -0.0567\n",
      "  Batch [830/1299] D_loss: -1.2946, G_loss: -4.5384\n",
      "  Batch [840/1299] D_loss: -0.2606, G_loss: -0.0921\n",
      "  Batch [850/1299] D_loss: -0.1095, G_loss: 0.2548\n",
      "  Batch [860/1299] D_loss: -0.1058, G_loss: 0.4493\n",
      "  Batch [870/1299] D_loss: -0.0195, G_loss: 0.4943\n",
      "  Batch [880/1299] D_loss: -0.0662, G_loss: 0.5434\n",
      "  Batch [890/1299] D_loss: 0.0137, G_loss: 0.3052\n",
      "  Batch [900/1299] D_loss: -0.0276, G_loss: 0.3500\n",
      "  Batch [910/1299] D_loss: -0.6976, G_loss: -0.5058\n",
      "  Batch [920/1299] D_loss: -0.1577, G_loss: 0.0867\n",
      "  Batch [930/1299] D_loss: -1.0521, G_loss: -1.2213\n",
      "  Batch [940/1299] D_loss: -0.7833, G_loss: -0.0892\n",
      "  Batch [950/1299] D_loss: -0.1939, G_loss: 0.4249\n",
      "  Batch [960/1299] D_loss: -0.1865, G_loss: 0.5137\n",
      "  Batch [970/1299] D_loss: -0.0780, G_loss: 0.7007\n",
      "  Batch [980/1299] D_loss: -0.1353, G_loss: 0.5744\n",
      "  Batch [990/1299] D_loss: -0.2276, G_loss: 0.6820\n",
      "  Batch [1000/1299] D_loss: -0.0506, G_loss: 0.4705\n",
      "  Batch [1010/1299] D_loss: -0.0460, G_loss: 0.2686\n",
      "  Batch [1020/1299] D_loss: -1.1282, G_loss: -1.2291\n",
      "  Batch [1030/1299] D_loss: -0.0306, G_loss: 0.1412\n",
      "  Batch [1040/1299] D_loss: -0.1600, G_loss: 0.2564\n",
      "  Batch [1050/1299] D_loss: -0.0549, G_loss: 0.3386\n",
      "  Batch [1060/1299] D_loss: -0.0615, G_loss: 0.4174\n",
      "  Batch [1070/1299] D_loss: -0.1371, G_loss: 0.3898\n",
      "  Batch [1080/1299] D_loss: -0.0024, G_loss: 0.3801\n",
      "  Batch [1090/1299] D_loss: -0.2293, G_loss: -0.2218\n",
      "  Batch [1100/1299] D_loss: 0.0202, G_loss: 0.0818\n",
      "  Batch [1110/1299] D_loss: -0.0868, G_loss: -0.1501\n",
      "  Batch [1120/1299] D_loss: -1.1110, G_loss: -2.3329\n",
      "  Batch [1130/1299] D_loss: -0.3375, G_loss: -0.0424\n",
      "  Batch [1140/1299] D_loss: -1.1562, G_loss: 0.1006\n",
      "  Batch [1150/1299] D_loss: -0.0236, G_loss: 0.1611\n",
      "  Batch [1160/1299] D_loss: -0.0934, G_loss: 0.1562\n",
      "  Batch [1170/1299] D_loss: -0.0757, G_loss: 0.3680\n",
      "  Batch [1180/1299] D_loss: -0.0720, G_loss: 0.5153\n",
      "  Batch [1190/1299] D_loss: -0.0663, G_loss: 0.7137\n",
      "  Batch [1200/1299] D_loss: -0.1504, G_loss: 0.7800\n",
      "  Batch [1210/1299] D_loss: -0.0635, G_loss: 0.6865\n",
      "  Batch [1220/1299] D_loss: -0.0676, G_loss: 0.5309\n",
      "  Batch [1230/1299] D_loss: -0.0574, G_loss: 0.2793\n",
      "  Batch [1240/1299] D_loss: -1.4921, G_loss: -2.0492\n",
      "  Batch [1250/1299] D_loss: -1.2680, G_loss: -1.3789\n",
      "  Batch [1260/1299] D_loss: -0.0107, G_loss: 0.1417\n",
      "  Batch [1270/1299] D_loss: -0.1217, G_loss: 0.1452\n",
      "  Batch [1280/1299] D_loss: -0.0020, G_loss: 0.2576\n",
      "  Batch [1290/1299] D_loss: -0.0161, G_loss: 0.3425\n",
      "\n",
      "Epoch 63 Summary:\n",
      "  Average D_loss: -0.1586\n",
      "  Average G_loss: -0.0739\n",
      "\n",
      "Epoch [64/100]\n",
      "  Batch [0/1299] D_loss: -0.1664, G_loss: 0.4232\n",
      "  Batch [10/1299] D_loss: -0.1231, G_loss: 0.4220\n",
      "  Batch [20/1299] D_loss: -0.5526, G_loss: -0.3243\n",
      "  Batch [30/1299] D_loss: -0.0487, G_loss: 0.3137\n",
      "  Batch [40/1299] D_loss: -0.0781, G_loss: 0.3632\n",
      "  Batch [50/1299] D_loss: -0.0930, G_loss: 0.4962\n",
      "  Batch [60/1299] D_loss: -0.0411, G_loss: 0.3767\n",
      "  Batch [70/1299] D_loss: -1.3214, G_loss: -1.3079\n",
      "  Batch [80/1299] D_loss: -0.0638, G_loss: 0.1279\n",
      "  Batch [90/1299] D_loss: -0.6980, G_loss: -1.1797\n",
      "  Batch [100/1299] D_loss: 0.0014, G_loss: 0.0731\n",
      "  Batch [110/1299] D_loss: -0.0875, G_loss: 0.2722\n",
      "  Batch [120/1299] D_loss: -0.2616, G_loss: 0.3062\n",
      "  Batch [130/1299] D_loss: -0.0817, G_loss: 0.6133\n",
      "  Batch [140/1299] D_loss: -0.1196, G_loss: 0.6268\n",
      "  Batch [150/1299] D_loss: -0.0661, G_loss: 0.5481\n",
      "  Batch [160/1299] D_loss: 0.0328, G_loss: 0.3429\n",
      "  Batch [170/1299] D_loss: -0.3075, G_loss: -2.8963\n",
      "  Batch [180/1299] D_loss: -0.0850, G_loss: -0.0828\n",
      "  Batch [190/1299] D_loss: -1.2245, G_loss: -2.9459\n",
      "  Batch [200/1299] D_loss: -0.4079, G_loss: -0.0222\n",
      "  Batch [210/1299] D_loss: 0.0521, G_loss: 0.2026\n",
      "  Batch [220/1299] D_loss: -0.0899, G_loss: 0.4891\n",
      "  Batch [230/1299] D_loss: -0.1993, G_loss: 0.6181\n",
      "  Batch [240/1299] D_loss: -0.0794, G_loss: 0.4434\n",
      "  Batch [250/1299] D_loss: -0.0949, G_loss: 0.4293\n",
      "  Batch [260/1299] D_loss: -1.0872, G_loss: -1.3376\n",
      "  Batch [270/1299] D_loss: -0.2123, G_loss: -0.4460\n",
      "  Batch [280/1299] D_loss: -0.6544, G_loss: -1.2775\n",
      "  Batch [290/1299] D_loss: -0.5780, G_loss: -0.5672\n",
      "  Batch [300/1299] D_loss: -0.2654, G_loss: 0.0326\n",
      "  Batch [310/1299] D_loss: -0.1237, G_loss: -1.7202\n",
      "  Batch [320/1299] D_loss: -0.0517, G_loss: 0.3152\n",
      "  Batch [330/1299] D_loss: -0.0872, G_loss: 0.6344\n",
      "  Batch [340/1299] D_loss: -0.2037, G_loss: 0.8632\n",
      "  Batch [350/1299] D_loss: -0.0588, G_loss: 0.7566\n",
      "  Batch [360/1299] D_loss: -0.0632, G_loss: 0.7118\n",
      "  Batch [370/1299] D_loss: -0.0796, G_loss: 0.6385\n",
      "  Batch [380/1299] D_loss: -0.0426, G_loss: 0.3865\n",
      "  Batch [390/1299] D_loss: -1.4901, G_loss: -2.9887\n",
      "  Batch [400/1299] D_loss: -0.5324, G_loss: -0.5671\n",
      "  Batch [410/1299] D_loss: -0.0378, G_loss: 0.2066\n",
      "  Batch [420/1299] D_loss: -0.0819, G_loss: 0.3092\n",
      "  Batch [430/1299] D_loss: 0.0213, G_loss: 0.4288\n",
      "  Batch [440/1299] D_loss: -0.1841, G_loss: 0.4819\n",
      "  Batch [450/1299] D_loss: -0.1887, G_loss: 0.5212\n",
      "  Batch [460/1299] D_loss: -0.0741, G_loss: 0.3246\n",
      "  Batch [470/1299] D_loss: -0.1342, G_loss: 0.2304\n",
      "  Batch [480/1299] D_loss: -1.0125, G_loss: -0.4947\n",
      "  Batch [490/1299] D_loss: -0.0357, G_loss: 0.1282\n",
      "  Batch [500/1299] D_loss: -0.1036, G_loss: 0.3345\n",
      "  Batch [510/1299] D_loss: -0.0430, G_loss: 0.4659\n",
      "  Batch [520/1299] D_loss: -0.0030, G_loss: 0.4122\n",
      "  Batch [530/1299] D_loss: -0.0324, G_loss: 0.4764\n",
      "  Batch [540/1299] D_loss: -0.0185, G_loss: 0.3646\n",
      "  Batch [550/1299] D_loss: 0.0217, G_loss: 0.3912\n",
      "  Batch [560/1299] D_loss: -1.6329, G_loss: -3.6879\n",
      "  Batch [570/1299] D_loss: -0.0650, G_loss: 0.0817\n",
      "  Batch [580/1299] D_loss: -0.0624, G_loss: 0.3086\n",
      "  Batch [590/1299] D_loss: -0.0730, G_loss: 0.3040\n",
      "  Batch [600/1299] D_loss: 0.0484, G_loss: 0.3386\n",
      "  Batch [610/1299] D_loss: -0.0775, G_loss: 0.4024\n",
      "  Batch [620/1299] D_loss: -0.0320, G_loss: 0.2461\n",
      "  Batch [630/1299] D_loss: -0.0427, G_loss: -0.4698\n",
      "  Batch [640/1299] D_loss: 0.0034, G_loss: 0.1348\n",
      "  Batch [650/1299] D_loss: -0.1306, G_loss: -0.2982\n",
      "  Batch [660/1299] D_loss: -0.0044, G_loss: 0.1492\n",
      "  Batch [670/1299] D_loss: -0.0511, G_loss: 0.2103\n",
      "  Batch [680/1299] D_loss: -0.0550, G_loss: 0.2153\n",
      "  Batch [690/1299] D_loss: -1.1948, G_loss: -2.6277\n",
      "  Batch [700/1299] D_loss: -0.0843, G_loss: 0.2900\n",
      "  Batch [710/1299] D_loss: -0.0677, G_loss: 0.3932\n",
      "  Batch [720/1299] D_loss: -0.1011, G_loss: 0.5244\n",
      "  Batch [730/1299] D_loss: -0.0872, G_loss: 0.3675\n",
      "  Batch [740/1299] D_loss: -2.4513, G_loss: -1.4962\n",
      "  Batch [750/1299] D_loss: -0.0115, G_loss: 0.1864\n",
      "  Batch [760/1299] D_loss: -0.0481, G_loss: 0.2389\n",
      "  Batch [770/1299] D_loss: -0.0762, G_loss: 0.4252\n",
      "  Batch [780/1299] D_loss: -0.0287, G_loss: 0.4030\n",
      "  Batch [790/1299] D_loss: -0.0131, G_loss: 0.3142\n",
      "  Batch [800/1299] D_loss: -2.4254, G_loss: -3.9836\n",
      "  Batch [810/1299] D_loss: -0.9082, G_loss: -1.5158\n",
      "  Batch [820/1299] D_loss: -0.1339, G_loss: 0.1028\n",
      "  Batch [830/1299] D_loss: -0.6612, G_loss: -0.3303\n",
      "  Batch [840/1299] D_loss: -0.1474, G_loss: 0.0990\n",
      "  Batch [850/1299] D_loss: -0.5087, G_loss: 0.0116\n",
      "  Batch [860/1299] D_loss: -0.7950, G_loss: -0.5229\n",
      "  Batch [870/1299] D_loss: -0.0579, G_loss: 0.2781\n",
      "  Batch [880/1299] D_loss: -0.1503, G_loss: 0.5454\n",
      "  Batch [890/1299] D_loss: -0.1050, G_loss: 0.6853\n",
      "  Batch [900/1299] D_loss: -0.0600, G_loss: 0.6329\n",
      "  Batch [910/1299] D_loss: -0.1126, G_loss: 0.5965\n",
      "  Batch [920/1299] D_loss: 0.0373, G_loss: 0.3359\n",
      "  Batch [930/1299] D_loss: -0.0035, G_loss: 0.3385\n",
      "  Batch [940/1299] D_loss: -1.5813, G_loss: -2.1274\n",
      "  Batch [950/1299] D_loss: -0.7484, G_loss: -0.9794\n",
      "  Batch [960/1299] D_loss: -0.0387, G_loss: 0.0388\n",
      "  Batch [970/1299] D_loss: -0.1983, G_loss: -0.0118\n",
      "  Batch [980/1299] D_loss: -0.8533, G_loss: -1.9631\n",
      "  Batch [990/1299] D_loss: -0.5342, G_loss: -0.5664\n",
      "  Batch [1000/1299] D_loss: -0.5647, G_loss: 0.0296\n",
      "  Batch [1010/1299] D_loss: -0.0432, G_loss: 0.2173\n",
      "  Batch [1020/1299] D_loss: -0.1388, G_loss: 0.3882\n",
      "  Batch [1030/1299] D_loss: -0.1047, G_loss: 0.6455\n",
      "  Batch [1040/1299] D_loss: -0.0330, G_loss: 0.6488\n",
      "  Batch [1050/1299] D_loss: -0.0765, G_loss: 0.6167\n",
      "  Batch [1060/1299] D_loss: -0.0962, G_loss: 0.5011\n",
      "  Batch [1070/1299] D_loss: -0.0148, G_loss: 0.4543\n",
      "  Batch [1080/1299] D_loss: -0.1018, G_loss: 0.3887\n",
      "  Batch [1090/1299] D_loss: -3.0233, G_loss: -7.9762\n",
      "  Batch [1100/1299] D_loss: -0.4794, G_loss: -0.4866\n",
      "  Batch [1110/1299] D_loss: -0.3610, G_loss: -0.1498\n",
      "  Batch [1120/1299] D_loss: -0.0829, G_loss: 0.3537\n",
      "  Batch [1130/1299] D_loss: -0.0903, G_loss: 0.6450\n",
      "  Batch [1140/1299] D_loss: -0.2119, G_loss: 0.6808\n",
      "  Batch [1150/1299] D_loss: -0.1986, G_loss: 0.7626\n",
      "  Batch [1160/1299] D_loss: -0.2081, G_loss: 0.6396\n",
      "  Batch [1170/1299] D_loss: -0.0847, G_loss: 0.5163\n",
      "  Batch [1180/1299] D_loss: -0.0911, G_loss: 0.3835\n",
      "  Batch [1190/1299] D_loss: -1.8175, G_loss: -3.3450\n",
      "  Batch [1200/1299] D_loss: -0.9964, G_loss: -0.3746\n",
      "  Batch [1210/1299] D_loss: -0.1643, G_loss: -0.0300\n",
      "  Batch [1220/1299] D_loss: -0.7575, G_loss: -0.3625\n",
      "  Batch [1230/1299] D_loss: -0.2570, G_loss: -0.1491\n",
      "  Batch [1240/1299] D_loss: -1.0838, G_loss: -0.4243\n",
      "  Batch [1250/1299] D_loss: -0.5988, G_loss: -0.1404\n",
      "  Batch [1260/1299] D_loss: -0.2413, G_loss: 0.1170\n",
      "  Batch [1270/1299] D_loss: -0.0851, G_loss: 0.3821\n",
      "  Batch [1280/1299] D_loss: -0.0748, G_loss: 0.4917\n",
      "  Batch [1290/1299] D_loss: -0.1187, G_loss: 0.7080\n",
      "\n",
      "Epoch 64 Summary:\n",
      "  Average D_loss: -0.1607\n",
      "  Average G_loss: -0.0903\n",
      "\n",
      "Epoch [65/100]\n",
      "  Batch [0/1299] D_loss: -0.0625, G_loss: 0.7135\n",
      "  Batch [10/1299] D_loss: 0.1852, G_loss: 0.4593\n",
      "  Batch [20/1299] D_loss: 0.0839, G_loss: 0.4651\n",
      "  Batch [30/1299] D_loss: -0.1274, G_loss: 0.2000\n",
      "  Batch [40/1299] D_loss: -0.4426, G_loss: -0.3276\n",
      "  Batch [50/1299] D_loss: -0.3209, G_loss: -0.2610\n",
      "  Batch [60/1299] D_loss: -0.0852, G_loss: 0.2847\n",
      "  Batch [70/1299] D_loss: -0.0662, G_loss: 0.4051\n",
      "  Batch [80/1299] D_loss: -0.1016, G_loss: 0.4534\n",
      "  Batch [90/1299] D_loss: -0.1048, G_loss: 0.4723\n",
      "  Batch [100/1299] D_loss: 0.0398, G_loss: 0.2848\n",
      "  Batch [110/1299] D_loss: -0.1150, G_loss: 0.4163\n",
      "  Batch [120/1299] D_loss: -0.8226, G_loss: -2.5438\n",
      "  Batch [130/1299] D_loss: -0.2783, G_loss: -0.5494\n",
      "  Batch [140/1299] D_loss: -0.0525, G_loss: 0.1498\n",
      "  Batch [150/1299] D_loss: -0.7419, G_loss: -1.7616\n",
      "  Batch [160/1299] D_loss: -0.8583, G_loss: 0.0927\n",
      "  Batch [170/1299] D_loss: -0.0661, G_loss: -0.3286\n",
      "  Batch [180/1299] D_loss: -0.0897, G_loss: 0.2420\n",
      "  Batch [190/1299] D_loss: -0.0158, G_loss: 0.5017\n",
      "  Batch [200/1299] D_loss: -0.2945, G_loss: 0.4927\n",
      "  Batch [210/1299] D_loss: -0.1842, G_loss: 0.8118\n",
      "  Batch [220/1299] D_loss: -0.0817, G_loss: 0.5672\n",
      "  Batch [230/1299] D_loss: 0.0129, G_loss: 0.3063\n",
      "  Batch [240/1299] D_loss: -3.3281, G_loss: -2.1176\n",
      "  Batch [250/1299] D_loss: -0.4005, G_loss: -0.4024\n",
      "  Batch [260/1299] D_loss: -0.0818, G_loss: 0.2108\n",
      "  Batch [270/1299] D_loss: -0.0033, G_loss: 0.2593\n",
      "  Batch [280/1299] D_loss: -0.1363, G_loss: 0.4597\n",
      "  Batch [290/1299] D_loss: -0.1272, G_loss: 0.5072\n",
      "  Batch [300/1299] D_loss: -0.1102, G_loss: 0.5775\n",
      "  Batch [310/1299] D_loss: -0.1353, G_loss: 0.4183\n",
      "  Batch [320/1299] D_loss: -0.0724, G_loss: 0.3529\n",
      "  Batch [330/1299] D_loss: -0.0266, G_loss: 0.0674\n",
      "  Batch [340/1299] D_loss: -0.0278, G_loss: 0.1540\n",
      "  Batch [350/1299] D_loss: -0.0415, G_loss: 0.1319\n",
      "  Batch [360/1299] D_loss: -0.1358, G_loss: 0.1098\n",
      "  Batch [370/1299] D_loss: -0.5625, G_loss: -0.0787\n",
      "  Batch [380/1299] D_loss: -0.0383, G_loss: 0.1872\n",
      "  Batch [390/1299] D_loss: -0.0350, G_loss: 0.2285\n",
      "  Batch [400/1299] D_loss: -0.0655, G_loss: 0.4491\n",
      "  Batch [410/1299] D_loss: -0.0112, G_loss: 0.5565\n",
      "  Batch [420/1299] D_loss: -0.0765, G_loss: 0.5900\n",
      "  Batch [430/1299] D_loss: -0.1625, G_loss: 0.5499\n",
      "  Batch [440/1299] D_loss: 0.0197, G_loss: 0.2417\n",
      "  Batch [450/1299] D_loss: -1.2798, G_loss: -2.9599\n",
      "  Batch [460/1299] D_loss: -0.1746, G_loss: 0.0153\n",
      "  Batch [470/1299] D_loss: -0.0339, G_loss: 0.1843\n",
      "  Batch [480/1299] D_loss: -0.0587, G_loss: 0.2136\n",
      "  Batch [490/1299] D_loss: -0.1807, G_loss: 0.4863\n",
      "  Batch [500/1299] D_loss: -0.0661, G_loss: 0.5247\n",
      "  Batch [510/1299] D_loss: -0.1337, G_loss: 0.3911\n",
      "  Batch [520/1299] D_loss: -0.0663, G_loss: 0.2665\n",
      "  Batch [530/1299] D_loss: -1.4843, G_loss: -0.1378\n",
      "  Batch [540/1299] D_loss: -0.0069, G_loss: 0.0696\n",
      "  Batch [550/1299] D_loss: -0.1302, G_loss: -0.0300\n",
      "  Batch [560/1299] D_loss: -1.0589, G_loss: -0.5722\n",
      "  Batch [570/1299] D_loss: -0.6322, G_loss: -0.9860\n",
      "  Batch [580/1299] D_loss: -0.9289, G_loss: -1.6836\n",
      "  Batch [590/1299] D_loss: -0.0853, G_loss: 0.1507\n",
      "  Batch [600/1299] D_loss: -0.0997, G_loss: 0.3739\n",
      "  Batch [610/1299] D_loss: -0.1210, G_loss: 0.6137\n",
      "  Batch [620/1299] D_loss: -0.1502, G_loss: 0.6843\n",
      "  Batch [630/1299] D_loss: 0.0419, G_loss: 0.6604\n",
      "  Batch [640/1299] D_loss: -0.1128, G_loss: 0.5054\n",
      "  Batch [650/1299] D_loss: -0.0964, G_loss: 0.2212\n",
      "  Batch [660/1299] D_loss: -0.9345, G_loss: -1.8069\n",
      "  Batch [670/1299] D_loss: -0.0073, G_loss: 0.1143\n",
      "  Batch [680/1299] D_loss: -0.0273, G_loss: 0.2230\n",
      "  Batch [690/1299] D_loss: -0.6090, G_loss: -0.2544\n",
      "  Batch [700/1299] D_loss: -0.5233, G_loss: -1.1706\n",
      "  Batch [710/1299] D_loss: -0.7176, G_loss: -0.1953\n",
      "  Batch [720/1299] D_loss: -0.0274, G_loss: 0.4025\n",
      "  Batch [730/1299] D_loss: -0.0985, G_loss: 0.5556\n",
      "  Batch [740/1299] D_loss: -0.0533, G_loss: 0.5941\n",
      "  Batch [750/1299] D_loss: 0.0447, G_loss: 0.6829\n",
      "  Batch [760/1299] D_loss: -0.1503, G_loss: 0.6362\n",
      "  Batch [770/1299] D_loss: 0.0529, G_loss: 0.2695\n",
      "  Batch [780/1299] D_loss: -1.8727, G_loss: -4.9183\n",
      "  Batch [790/1299] D_loss: -0.0298, G_loss: 0.0939\n",
      "  Batch [800/1299] D_loss: -0.1214, G_loss: 0.2309\n",
      "  Batch [810/1299] D_loss: -0.0613, G_loss: 0.2256\n",
      "  Batch [820/1299] D_loss: -0.0971, G_loss: 0.3920\n",
      "  Batch [830/1299] D_loss: -0.0043, G_loss: 0.4125\n",
      "  Batch [840/1299] D_loss: -0.0432, G_loss: 0.2450\n",
      "  Batch [850/1299] D_loss: -0.1586, G_loss: -0.0053\n",
      "  Batch [860/1299] D_loss: -0.0721, G_loss: 0.1607\n",
      "  Batch [870/1299] D_loss: -0.0458, G_loss: 0.3368\n",
      "  Batch [880/1299] D_loss: 0.0504, G_loss: 0.5325\n",
      "  Batch [890/1299] D_loss: -0.2063, G_loss: 0.5810\n",
      "  Batch [900/1299] D_loss: -0.0539, G_loss: 0.4644\n",
      "  Batch [910/1299] D_loss: -0.0200, G_loss: 0.2916\n",
      "  Batch [920/1299] D_loss: -0.0307, G_loss: 0.2904\n",
      "  Batch [930/1299] D_loss: -0.6340, G_loss: -2.5528\n",
      "  Batch [940/1299] D_loss: -0.3442, G_loss: -0.6173\n",
      "  Batch [950/1299] D_loss: -0.6319, G_loss: -0.3993\n",
      "  Batch [960/1299] D_loss: -0.4437, G_loss: 0.1325\n",
      "  Batch [970/1299] D_loss: -0.0566, G_loss: 0.1252\n",
      "  Batch [980/1299] D_loss: -0.4396, G_loss: -0.0345\n",
      "  Batch [990/1299] D_loss: 0.0119, G_loss: 0.2111\n",
      "  Batch [1000/1299] D_loss: -0.0167, G_loss: 0.3663\n",
      "  Batch [1010/1299] D_loss: -0.0574, G_loss: 0.3611\n",
      "  Batch [1020/1299] D_loss: 0.0877, G_loss: 0.5433\n",
      "  Batch [1030/1299] D_loss: -0.1960, G_loss: 0.4936\n",
      "  Batch [1040/1299] D_loss: -0.0673, G_loss: 0.4310\n",
      "  Batch [1050/1299] D_loss: -0.0212, G_loss: 0.1793\n",
      "  Batch [1060/1299] D_loss: -0.5057, G_loss: -0.8571\n",
      "  Batch [1070/1299] D_loss: -0.4074, G_loss: 0.0207\n",
      "  Batch [1080/1299] D_loss: -0.0880, G_loss: 0.2883\n",
      "  Batch [1090/1299] D_loss: 0.0074, G_loss: 0.3396\n",
      "  Batch [1100/1299] D_loss: -0.0190, G_loss: 0.3452\n",
      "  Batch [1110/1299] D_loss: -0.0810, G_loss: 0.4988\n",
      "  Batch [1120/1299] D_loss: -0.1032, G_loss: 0.4631\n",
      "  Batch [1130/1299] D_loss: -0.1055, G_loss: 0.3027\n",
      "  Batch [1140/1299] D_loss: -0.0286, G_loss: 0.2125\n",
      "  Batch [1150/1299] D_loss: -0.0291, G_loss: 0.2892\n",
      "  Batch [1160/1299] D_loss: -0.1783, G_loss: 0.4030\n",
      "  Batch [1170/1299] D_loss: -0.1393, G_loss: 0.3572\n",
      "  Batch [1180/1299] D_loss: -0.0600, G_loss: 0.2602\n",
      "  Batch [1190/1299] D_loss: -0.0617, G_loss: 0.2642\n",
      "  Batch [1200/1299] D_loss: -0.2488, G_loss: -0.2276\n",
      "  Batch [1210/1299] D_loss: -0.5174, G_loss: 0.2044\n",
      "  Batch [1220/1299] D_loss: -1.6387, G_loss: -1.1023\n",
      "  Batch [1230/1299] D_loss: -0.4117, G_loss: 0.2365\n",
      "  Batch [1240/1299] D_loss: -1.3738, G_loss: -1.3410\n",
      "  Batch [1250/1299] D_loss: -0.5026, G_loss: 0.1530\n",
      "  Batch [1260/1299] D_loss: -0.4713, G_loss: -0.2308\n",
      "  Batch [1270/1299] D_loss: -0.1437, G_loss: 0.3429\n",
      "  Batch [1280/1299] D_loss: 0.0021, G_loss: 0.4374\n",
      "  Batch [1290/1299] D_loss: -0.0773, G_loss: 0.4743\n",
      "\n",
      "Epoch 65 Summary:\n",
      "  Average D_loss: -0.1400\n",
      "  Average G_loss: -0.0617\n",
      "\n",
      "Epoch [66/100]\n",
      "  Batch [0/1299] D_loss: -0.0457, G_loss: 0.4350\n",
      "  Batch [10/1299] D_loss: 0.0207, G_loss: 0.3181\n",
      "  Batch [20/1299] D_loss: -1.6862, G_loss: -2.1130\n",
      "  Batch [30/1299] D_loss: 0.0123, G_loss: 0.1519\n",
      "  Batch [40/1299] D_loss: -0.0190, G_loss: 0.1395\n",
      "  Batch [50/1299] D_loss: 0.0068, G_loss: 0.1822\n",
      "  Batch [60/1299] D_loss: -1.3646, G_loss: -1.8387\n",
      "  Batch [70/1299] D_loss: -0.5583, G_loss: -0.7925\n",
      "  Batch [80/1299] D_loss: -0.0284, G_loss: 0.0833\n",
      "  Batch [90/1299] D_loss: -0.9562, G_loss: 0.1282\n",
      "  Batch [100/1299] D_loss: -0.0324, G_loss: 0.2427\n",
      "  Batch [110/1299] D_loss: -0.1160, G_loss: 0.4969\n",
      "  Batch [120/1299] D_loss: -0.2125, G_loss: 0.6090\n",
      "  Batch [130/1299] D_loss: -0.0638, G_loss: 0.7490\n",
      "  Batch [140/1299] D_loss: -0.0983, G_loss: 0.7137\n",
      "  Batch [150/1299] D_loss: 0.0140, G_loss: 0.5556\n",
      "  Batch [160/1299] D_loss: -0.0249, G_loss: 0.3130\n",
      "  Batch [170/1299] D_loss: -0.7363, G_loss: 0.0367\n",
      "  Batch [180/1299] D_loss: -0.4415, G_loss: -0.3133\n",
      "  Batch [190/1299] D_loss: -0.3397, G_loss: 0.0376\n",
      "  Batch [200/1299] D_loss: -0.5874, G_loss: -0.3191\n",
      "  Batch [210/1299] D_loss: -0.1566, G_loss: 0.1694\n",
      "  Batch [220/1299] D_loss: -0.0866, G_loss: 0.1029\n",
      "  Batch [230/1299] D_loss: -0.0985, G_loss: 0.3009\n",
      "  Batch [240/1299] D_loss: -0.0772, G_loss: 0.3678\n",
      "  Batch [250/1299] D_loss: -0.0846, G_loss: 0.6461\n",
      "  Batch [260/1299] D_loss: 0.0446, G_loss: 0.5155\n",
      "  Batch [270/1299] D_loss: -0.0107, G_loss: 0.4580\n",
      "  Batch [280/1299] D_loss: -0.0618, G_loss: 0.4866\n",
      "  Batch [290/1299] D_loss: -1.4992, G_loss: -0.5652\n",
      "  Batch [300/1299] D_loss: -0.0014, G_loss: 0.0442\n",
      "  Batch [310/1299] D_loss: -0.1830, G_loss: -0.0537\n",
      "  Batch [320/1299] D_loss: -0.0199, G_loss: 0.1611\n",
      "  Batch [330/1299] D_loss: -2.1530, G_loss: -2.4803\n",
      "  Batch [340/1299] D_loss: -0.0067, G_loss: 0.3141\n",
      "  Batch [350/1299] D_loss: 0.0011, G_loss: 0.4487\n",
      "  Batch [360/1299] D_loss: -0.0972, G_loss: 0.4411\n",
      "  Batch [370/1299] D_loss: 0.0391, G_loss: 0.4231\n",
      "  Batch [380/1299] D_loss: -0.0727, G_loss: 0.5266\n",
      "  Batch [390/1299] D_loss: -0.0521, G_loss: 0.2195\n",
      "  Batch [400/1299] D_loss: -0.6658, G_loss: -5.4929\n",
      "  Batch [410/1299] D_loss: -0.1270, G_loss: 0.0291\n",
      "  Batch [420/1299] D_loss: -1.3852, G_loss: -3.4267\n",
      "  Batch [430/1299] D_loss: -1.2081, G_loss: -1.7018\n",
      "  Batch [440/1299] D_loss: -0.1611, G_loss: 0.1859\n",
      "  Batch [450/1299] D_loss: -0.1861, G_loss: 0.0822\n",
      "  Batch [460/1299] D_loss: -0.0400, G_loss: 0.2312\n",
      "  Batch [470/1299] D_loss: -0.2099, G_loss: 0.4481\n",
      "  Batch [480/1299] D_loss: -0.1641, G_loss: 0.6330\n",
      "  Batch [490/1299] D_loss: -0.5185, G_loss: 0.7529\n",
      "  Batch [500/1299] D_loss: 0.0042, G_loss: 0.5174\n",
      "  Batch [510/1299] D_loss: -0.0620, G_loss: 0.5344\n",
      "  Batch [520/1299] D_loss: 0.0121, G_loss: 0.4865\n",
      "  Batch [530/1299] D_loss: -0.7363, G_loss: -2.0692\n",
      "  Batch [540/1299] D_loss: -0.3841, G_loss: -0.1614\n",
      "  Batch [550/1299] D_loss: -0.1832, G_loss: -0.3871\n",
      "  Batch [560/1299] D_loss: -0.2899, G_loss: 0.0045\n",
      "  Batch [570/1299] D_loss: -0.7644, G_loss: -1.4451\n",
      "  Batch [580/1299] D_loss: -0.2138, G_loss: 0.1154\n",
      "  Batch [590/1299] D_loss: -1.2712, G_loss: 0.0430\n",
      "  Batch [600/1299] D_loss: -0.6822, G_loss: 0.0484\n",
      "  Batch [610/1299] D_loss: -0.4337, G_loss: -0.9444\n",
      "  Batch [620/1299] D_loss: -0.7218, G_loss: 0.1618\n",
      "  Batch [630/1299] D_loss: -0.0858, G_loss: 0.3253\n",
      "  Batch [640/1299] D_loss: -0.0808, G_loss: 0.5075\n",
      "  Batch [650/1299] D_loss: -0.0647, G_loss: 0.5718\n",
      "  Batch [660/1299] D_loss: -0.0414, G_loss: 0.7190\n",
      "  Batch [670/1299] D_loss: -0.0764, G_loss: 0.6009\n",
      "  Batch [680/1299] D_loss: -0.0614, G_loss: 0.3747\n",
      "  Batch [690/1299] D_loss: -0.1175, G_loss: 0.3686\n",
      "  Batch [700/1299] D_loss: -0.7500, G_loss: -1.1577\n",
      "  Batch [710/1299] D_loss: 0.0113, G_loss: 0.1039\n",
      "  Batch [720/1299] D_loss: -0.0210, G_loss: 0.1372\n",
      "  Batch [730/1299] D_loss: 0.0025, G_loss: 0.1808\n",
      "  Batch [740/1299] D_loss: -0.7209, G_loss: -0.4756\n",
      "  Batch [750/1299] D_loss: -0.1274, G_loss: 0.0405\n",
      "  Batch [760/1299] D_loss: -0.0843, G_loss: 0.2492\n",
      "  Batch [770/1299] D_loss: -0.1205, G_loss: 0.4330\n",
      "  Batch [780/1299] D_loss: -0.0760, G_loss: 0.4263\n",
      "  Batch [790/1299] D_loss: -0.0456, G_loss: 0.5283\n",
      "  Batch [800/1299] D_loss: -0.0611, G_loss: 0.5211\n",
      "  Batch [810/1299] D_loss: -0.0551, G_loss: 0.3260\n",
      "  Batch [820/1299] D_loss: -3.1659, G_loss: -5.9375\n",
      "  Batch [830/1299] D_loss: -0.2111, G_loss: -0.1975\n",
      "  Batch [840/1299] D_loss: -0.7677, G_loss: -0.7935\n",
      "  Batch [850/1299] D_loss: -0.4236, G_loss: -0.4322\n",
      "  Batch [860/1299] D_loss: -1.2249, G_loss: -0.8665\n",
      "  Batch [870/1299] D_loss: -0.4102, G_loss: 0.0714\n",
      "  Batch [880/1299] D_loss: -0.1057, G_loss: 0.1260\n",
      "  Batch [890/1299] D_loss: -0.4093, G_loss: -0.3959\n",
      "  Batch [900/1299] D_loss: -0.0159, G_loss: 0.2790\n",
      "  Batch [910/1299] D_loss: -0.1971, G_loss: 0.5612\n",
      "  Batch [920/1299] D_loss: -0.0948, G_loss: 0.7012\n",
      "  Batch [930/1299] D_loss: -0.0968, G_loss: 0.5448\n",
      "  Batch [940/1299] D_loss: -0.0514, G_loss: 0.4618\n",
      "  Batch [950/1299] D_loss: -0.0710, G_loss: 0.3733\n",
      "  Batch [960/1299] D_loss: -0.9985, G_loss: -3.2506\n",
      "  Batch [970/1299] D_loss: -0.2201, G_loss: -1.0013\n",
      "  Batch [980/1299] D_loss: -1.1289, G_loss: -1.6680\n",
      "  Batch [990/1299] D_loss: -0.3994, G_loss: 0.0818\n",
      "  Batch [1000/1299] D_loss: -1.1505, G_loss: -0.9338\n",
      "  Batch [1010/1299] D_loss: -0.1739, G_loss: 0.0879\n",
      "  Batch [1020/1299] D_loss: -0.8776, G_loss: -0.9098\n",
      "  Batch [1030/1299] D_loss: -0.6495, G_loss: -2.0459\n",
      "  Batch [1040/1299] D_loss: -0.7527, G_loss: -0.2509\n",
      "  Batch [1050/1299] D_loss: -0.0921, G_loss: 0.1169\n",
      "  Batch [1060/1299] D_loss: -0.9062, G_loss: -0.0987\n",
      "  Batch [1070/1299] D_loss: -0.1220, G_loss: 0.2881\n",
      "  Batch [1080/1299] D_loss: -0.1988, G_loss: 0.8154\n",
      "  Batch [1090/1299] D_loss: -0.2908, G_loss: 0.7435\n",
      "  Batch [1100/1299] D_loss: -0.1317, G_loss: 0.7990\n",
      "  Batch [1110/1299] D_loss: -0.2854, G_loss: 0.6999\n",
      "  Batch [1120/1299] D_loss: 0.0038, G_loss: 0.5103\n",
      "  Batch [1130/1299] D_loss: -4.9092, G_loss: -7.6085\n",
      "  Batch [1140/1299] D_loss: -0.7818, G_loss: 0.0738\n",
      "  Batch [1150/1299] D_loss: -0.7419, G_loss: -0.2132\n",
      "  Batch [1160/1299] D_loss: -0.1667, G_loss: 0.0377\n",
      "  Batch [1170/1299] D_loss: -0.0444, G_loss: 0.2283\n",
      "  Batch [1180/1299] D_loss: -0.0156, G_loss: 0.4449\n",
      "  Batch [1190/1299] D_loss: -0.0708, G_loss: 0.5325\n",
      "  Batch [1200/1299] D_loss: -0.2524, G_loss: 0.5443\n",
      "  Batch [1210/1299] D_loss: 0.0204, G_loss: 0.5512\n",
      "  Batch [1220/1299] D_loss: -0.0158, G_loss: 0.2341\n",
      "  Batch [1230/1299] D_loss: -0.1071, G_loss: 0.1193\n",
      "  Batch [1240/1299] D_loss: -0.1489, G_loss: 0.0913\n",
      "  Batch [1250/1299] D_loss: 0.0367, G_loss: 0.1637\n",
      "  Batch [1260/1299] D_loss: -0.0455, G_loss: 0.3437\n",
      "  Batch [1270/1299] D_loss: -0.0700, G_loss: 0.4422\n",
      "  Batch [1280/1299] D_loss: -0.1434, G_loss: 0.4059\n",
      "  Batch [1290/1299] D_loss: -0.0178, G_loss: 0.3413\n",
      "\n",
      "Epoch 66 Summary:\n",
      "  Average D_loss: -0.1831\n",
      "  Average G_loss: -0.1041\n",
      "\n",
      "Epoch [67/100]\n",
      "  Batch [0/1299] D_loss: -1.1459, G_loss: -0.5963\n",
      "  Batch [10/1299] D_loss: -0.0692, G_loss: 0.0888\n",
      "  Batch [20/1299] D_loss: -0.1569, G_loss: -0.2829\n",
      "  Batch [30/1299] D_loss: -0.0452, G_loss: 0.2580\n",
      "  Batch [40/1299] D_loss: -0.0436, G_loss: 0.2857\n",
      "  Batch [50/1299] D_loss: -0.0871, G_loss: 0.3527\n",
      "  Batch [60/1299] D_loss: -0.0456, G_loss: 0.4042\n",
      "  Batch [70/1299] D_loss: -0.0801, G_loss: 0.3767\n",
      "  Batch [80/1299] D_loss: -0.3007, G_loss: 0.2623\n",
      "  Batch [90/1299] D_loss: -0.1216, G_loss: -0.4727\n",
      "  Batch [100/1299] D_loss: -0.1833, G_loss: 0.1237\n",
      "  Batch [110/1299] D_loss: -0.7441, G_loss: -0.6081\n",
      "  Batch [120/1299] D_loss: -0.0598, G_loss: 0.2518\n",
      "  Batch [130/1299] D_loss: -0.0415, G_loss: 0.3912\n",
      "  Batch [140/1299] D_loss: -0.0105, G_loss: 0.5271\n",
      "  Batch [150/1299] D_loss: -0.1104, G_loss: 0.6594\n",
      "  Batch [160/1299] D_loss: 0.0248, G_loss: 0.4835\n",
      "  Batch [170/1299] D_loss: 0.0037, G_loss: 0.4674\n",
      "  Batch [180/1299] D_loss: -0.1221, G_loss: 0.4430\n",
      "  Batch [190/1299] D_loss: -0.0645, G_loss: 0.2641\n",
      "  Batch [200/1299] D_loss: 0.0078, G_loss: 0.0759\n",
      "  Batch [210/1299] D_loss: -0.0211, G_loss: 0.1361\n",
      "  Batch [220/1299] D_loss: -0.0489, G_loss: 0.2260\n",
      "  Batch [230/1299] D_loss: -0.0195, G_loss: 0.1942\n",
      "  Batch [240/1299] D_loss: -0.0840, G_loss: 0.3093\n",
      "  Batch [250/1299] D_loss: -0.0278, G_loss: 0.3720\n",
      "  Batch [260/1299] D_loss: -0.2242, G_loss: 0.4031\n",
      "  Batch [270/1299] D_loss: -0.4826, G_loss: -0.4845\n",
      "  Batch [280/1299] D_loss: -0.8935, G_loss: -2.7374\n",
      "  Batch [290/1299] D_loss: -0.1507, G_loss: 0.0698\n",
      "  Batch [300/1299] D_loss: -0.3655, G_loss: -0.9066\n",
      "  Batch [310/1299] D_loss: -1.0685, G_loss: -2.9384\n",
      "  Batch [320/1299] D_loss: -0.7216, G_loss: -0.6310\n",
      "  Batch [330/1299] D_loss: -0.1835, G_loss: 0.4380\n",
      "  Batch [340/1299] D_loss: -0.1729, G_loss: 0.7445\n",
      "  Batch [350/1299] D_loss: -0.1798, G_loss: 0.7011\n",
      "  Batch [360/1299] D_loss: -0.1274, G_loss: 0.8156\n",
      "  Batch [370/1299] D_loss: -0.0099, G_loss: 0.4938\n",
      "  Batch [380/1299] D_loss: -0.0403, G_loss: 0.5697\n",
      "  Batch [390/1299] D_loss: -0.1002, G_loss: 0.3070\n",
      "  Batch [400/1299] D_loss: -0.7346, G_loss: -1.0200\n",
      "  Batch [410/1299] D_loss: -0.0535, G_loss: 0.1329\n",
      "  Batch [420/1299] D_loss: -0.8562, G_loss: -0.9696\n",
      "  Batch [430/1299] D_loss: -0.0251, G_loss: 0.1544\n",
      "  Batch [440/1299] D_loss: -0.0429, G_loss: 0.4430\n",
      "  Batch [450/1299] D_loss: -0.2363, G_loss: 0.5148\n",
      "  Batch [460/1299] D_loss: -0.0766, G_loss: 0.5994\n",
      "  Batch [470/1299] D_loss: -0.0800, G_loss: 0.7178\n",
      "  Batch [480/1299] D_loss: -0.0696, G_loss: 0.5123\n",
      "  Batch [490/1299] D_loss: -0.1282, G_loss: 0.2511\n",
      "  Batch [500/1299] D_loss: -0.6678, G_loss: -0.9380\n",
      "  Batch [510/1299] D_loss: -0.0558, G_loss: 0.1573\n",
      "  Batch [520/1299] D_loss: -0.2250, G_loss: -0.2963\n",
      "  Batch [530/1299] D_loss: -0.5594, G_loss: 0.1756\n",
      "  Batch [540/1299] D_loss: -0.1690, G_loss: 0.3899\n",
      "  Batch [550/1299] D_loss: -0.1295, G_loss: 0.4417\n",
      "  Batch [560/1299] D_loss: -0.2535, G_loss: 0.5504\n",
      "  Batch [570/1299] D_loss: 0.0716, G_loss: 0.6286\n",
      "  Batch [580/1299] D_loss: -0.0004, G_loss: 0.4713\n",
      "  Batch [590/1299] D_loss: -0.0815, G_loss: 0.3593\n",
      "  Batch [600/1299] D_loss: -4.4655, G_loss: -4.5854\n",
      "  Batch [610/1299] D_loss: -0.8152, G_loss: -1.1111\n",
      "  Batch [620/1299] D_loss: -0.3497, G_loss: -0.1020\n",
      "  Batch [630/1299] D_loss: -0.2488, G_loss: -0.2876\n",
      "  Batch [640/1299] D_loss: -0.0690, G_loss: 0.2817\n",
      "  Batch [650/1299] D_loss: -0.0599, G_loss: 0.4292\n",
      "  Batch [660/1299] D_loss: -0.2286, G_loss: 0.4905\n",
      "  Batch [670/1299] D_loss: 0.0069, G_loss: 0.5649\n",
      "  Batch [680/1299] D_loss: -0.0260, G_loss: 0.5460\n",
      "  Batch [690/1299] D_loss: -0.0107, G_loss: 0.3499\n",
      "  Batch [700/1299] D_loss: -0.0559, G_loss: 0.2821\n",
      "  Batch [710/1299] D_loss: -0.0448, G_loss: -0.0574\n",
      "  Batch [720/1299] D_loss: -0.5370, G_loss: -0.1486\n",
      "  Batch [730/1299] D_loss: -0.0605, G_loss: 0.1211\n",
      "  Batch [740/1299] D_loss: -0.2264, G_loss: -0.2920\n",
      "  Batch [750/1299] D_loss: -0.1623, G_loss: 0.1199\n",
      "  Batch [760/1299] D_loss: -1.0459, G_loss: -1.3990\n",
      "  Batch [770/1299] D_loss: 0.1555, G_loss: 0.1460\n",
      "  Batch [780/1299] D_loss: -0.0972, G_loss: 0.4332\n",
      "  Batch [790/1299] D_loss: -0.1036, G_loss: 0.7355\n",
      "  Batch [800/1299] D_loss: -0.1831, G_loss: 0.9049\n",
      "  Batch [810/1299] D_loss: -0.0032, G_loss: 0.6181\n",
      "  Batch [820/1299] D_loss: -0.0028, G_loss: 0.4359\n",
      "  Batch [830/1299] D_loss: -0.2307, G_loss: 0.5176\n",
      "  Batch [840/1299] D_loss: -1.9068, G_loss: -6.0809\n",
      "  Batch [850/1299] D_loss: -0.3050, G_loss: -0.1278\n",
      "  Batch [860/1299] D_loss: -0.9242, G_loss: -3.0094\n",
      "  Batch [870/1299] D_loss: -0.4982, G_loss: -1.1382\n",
      "  Batch [880/1299] D_loss: -0.5071, G_loss: -0.1896\n",
      "  Batch [890/1299] D_loss: -0.1375, G_loss: 0.1996\n",
      "  Batch [900/1299] D_loss: -0.1533, G_loss: 0.3812\n",
      "  Batch [910/1299] D_loss: -0.0662, G_loss: 0.5884\n",
      "  Batch [920/1299] D_loss: -0.1569, G_loss: 0.7592\n",
      "  Batch [930/1299] D_loss: -0.0325, G_loss: 0.7679\n",
      "  Batch [940/1299] D_loss: -0.1433, G_loss: 0.4866\n",
      "  Batch [950/1299] D_loss: -0.0164, G_loss: 0.5415\n",
      "  Batch [960/1299] D_loss: -0.1180, G_loss: 0.1478\n",
      "  Batch [970/1299] D_loss: -0.7176, G_loss: -1.0142\n",
      "  Batch [980/1299] D_loss: -0.1866, G_loss: 0.0869\n",
      "  Batch [990/1299] D_loss: -0.2181, G_loss: -0.0363\n",
      "  Batch [1000/1299] D_loss: -0.3340, G_loss: -0.0101\n",
      "  Batch [1010/1299] D_loss: -0.1939, G_loss: 0.0872\n",
      "  Batch [1020/1299] D_loss: -0.1246, G_loss: 0.2024\n",
      "  Batch [1030/1299] D_loss: -1.6816, G_loss: -0.5076\n",
      "  Batch [1040/1299] D_loss: -0.4371, G_loss: -0.7219\n",
      "  Batch [1050/1299] D_loss: -0.0480, G_loss: 0.2431\n",
      "  Batch [1060/1299] D_loss: -0.0511, G_loss: 0.4060\n",
      "  Batch [1070/1299] D_loss: 0.0204, G_loss: 0.5267\n",
      "  Batch [1080/1299] D_loss: 0.0424, G_loss: 0.4749\n",
      "  Batch [1090/1299] D_loss: -0.0070, G_loss: 0.5099\n",
      "  Batch [1100/1299] D_loss: -0.0822, G_loss: 0.3669\n",
      "  Batch [1110/1299] D_loss: -1.0373, G_loss: -1.3457\n",
      "  Batch [1120/1299] D_loss: -1.2188, G_loss: -0.6198\n",
      "  Batch [1130/1299] D_loss: -0.1039, G_loss: 0.2896\n",
      "  Batch [1140/1299] D_loss: -0.0490, G_loss: 0.4284\n",
      "  Batch [1150/1299] D_loss: -0.0747, G_loss: 0.4737\n",
      "  Batch [1160/1299] D_loss: -0.1345, G_loss: 0.5001\n",
      "  Batch [1170/1299] D_loss: 0.0111, G_loss: 0.4231\n",
      "  Batch [1180/1299] D_loss: -0.0288, G_loss: 0.2772\n",
      "  Batch [1190/1299] D_loss: -1.2780, G_loss: -1.9352\n",
      "  Batch [1200/1299] D_loss: -0.0285, G_loss: 0.2375\n",
      "  Batch [1210/1299] D_loss: 0.0034, G_loss: 0.3905\n",
      "  Batch [1220/1299] D_loss: -0.1279, G_loss: 0.6322\n",
      "  Batch [1230/1299] D_loss: -0.0437, G_loss: 0.5226\n",
      "  Batch [1240/1299] D_loss: 0.0153, G_loss: 0.3673\n",
      "  Batch [1250/1299] D_loss: -0.1264, G_loss: 0.3545\n",
      "  Batch [1260/1299] D_loss: -0.6470, G_loss: -1.1131\n",
      "  Batch [1270/1299] D_loss: -0.8166, G_loss: -2.2773\n",
      "  Batch [1280/1299] D_loss: -0.8774, G_loss: -0.6968\n",
      "  Batch [1290/1299] D_loss: -1.2494, G_loss: -2.3963\n",
      "\n",
      "Epoch 67 Summary:\n",
      "  Average D_loss: -0.1782\n",
      "  Average G_loss: -0.0934\n",
      "\n",
      "Epoch [68/100]\n",
      "  Batch [0/1299] D_loss: -0.1657, G_loss: 0.1487\n",
      "  Batch [10/1299] D_loss: -0.0268, G_loss: 0.2974\n",
      "  Batch [20/1299] D_loss: -0.7091, G_loss: -0.0251\n",
      "  Batch [30/1299] D_loss: -0.0761, G_loss: 0.3922\n",
      "  Batch [40/1299] D_loss: -0.1954, G_loss: 0.6285\n",
      "  Batch [50/1299] D_loss: -0.2211, G_loss: 0.7153\n",
      "  Batch [60/1299] D_loss: -0.1873, G_loss: 0.5707\n",
      "  Batch [70/1299] D_loss: -0.0151, G_loss: 0.3634\n",
      "  Batch [80/1299] D_loss: -2.4496, G_loss: -2.8078\n",
      "  Batch [90/1299] D_loss: -0.0173, G_loss: 0.1189\n",
      "  Batch [100/1299] D_loss: -0.0227, G_loss: 0.0920\n",
      "  Batch [110/1299] D_loss: -0.0284, G_loss: 0.2032\n",
      "  Batch [120/1299] D_loss: -0.8265, G_loss: -3.4195\n",
      "  Batch [130/1299] D_loss: -0.1297, G_loss: 0.2542\n",
      "  Batch [140/1299] D_loss: -0.0688, G_loss: 0.4657\n",
      "  Batch [150/1299] D_loss: -0.0619, G_loss: 0.6533\n",
      "  Batch [160/1299] D_loss: -0.1264, G_loss: 0.5692\n",
      "  Batch [170/1299] D_loss: -0.0967, G_loss: 0.5920\n",
      "  Batch [180/1299] D_loss: -0.0388, G_loss: 0.4164\n",
      "  Batch [190/1299] D_loss: -0.0382, G_loss: 0.1943\n",
      "  Batch [200/1299] D_loss: -1.0667, G_loss: -0.8228\n",
      "  Batch [210/1299] D_loss: -0.1151, G_loss: 0.1170\n",
      "  Batch [220/1299] D_loss: -0.0230, G_loss: 0.2669\n",
      "  Batch [230/1299] D_loss: -0.0204, G_loss: 0.3063\n",
      "  Batch [240/1299] D_loss: -0.0836, G_loss: 0.3865\n",
      "  Batch [250/1299] D_loss: 0.1035, G_loss: 0.4537\n",
      "  Batch [260/1299] D_loss: -0.0661, G_loss: 0.3462\n",
      "  Batch [270/1299] D_loss: -2.5818, G_loss: -5.3433\n",
      "  Batch [280/1299] D_loss: -0.4445, G_loss: -0.0849\n",
      "  Batch [290/1299] D_loss: -0.4113, G_loss: -0.5631\n",
      "  Batch [300/1299] D_loss: -0.0769, G_loss: 0.3088\n",
      "  Batch [310/1299] D_loss: -0.1425, G_loss: 0.5914\n",
      "  Batch [320/1299] D_loss: -0.1701, G_loss: 0.6965\n",
      "  Batch [330/1299] D_loss: -0.2261, G_loss: 0.6869\n",
      "  Batch [340/1299] D_loss: -0.1674, G_loss: 0.5267\n",
      "  Batch [350/1299] D_loss: -0.1014, G_loss: 0.3298\n",
      "  Batch [360/1299] D_loss: -0.0689, G_loss: 0.2081\n",
      "  Batch [370/1299] D_loss: -0.0876, G_loss: -0.1239\n",
      "  Batch [380/1299] D_loss: -0.3703, G_loss: -0.0786\n",
      "  Batch [390/1299] D_loss: -0.3919, G_loss: -0.0205\n",
      "  Batch [400/1299] D_loss: -0.0347, G_loss: 0.1977\n",
      "  Batch [410/1299] D_loss: -0.0437, G_loss: 0.3380\n",
      "  Batch [420/1299] D_loss: -0.2039, G_loss: 0.3713\n",
      "  Batch [430/1299] D_loss: -0.0966, G_loss: 0.4681\n",
      "  Batch [440/1299] D_loss: -0.0852, G_loss: 0.4812\n",
      "  Batch [450/1299] D_loss: -0.0217, G_loss: 0.2587\n",
      "  Batch [460/1299] D_loss: -2.8504, G_loss: -4.3314\n",
      "  Batch [470/1299] D_loss: 0.0233, G_loss: 0.1443\n",
      "  Batch [480/1299] D_loss: -0.0300, G_loss: 0.2363\n",
      "  Batch [490/1299] D_loss: -0.0148, G_loss: 0.3638\n",
      "  Batch [500/1299] D_loss: -0.0812, G_loss: 0.3081\n",
      "  Batch [510/1299] D_loss: -0.0318, G_loss: 0.2864\n",
      "  Batch [520/1299] D_loss: -0.1154, G_loss: 0.2406\n",
      "  Batch [530/1299] D_loss: -0.4518, G_loss: -1.6225\n",
      "  Batch [540/1299] D_loss: -1.6667, G_loss: -3.2880\n",
      "  Batch [550/1299] D_loss: -0.1100, G_loss: 0.1211\n",
      "  Batch [560/1299] D_loss: -0.0398, G_loss: 0.3880\n",
      "  Batch [570/1299] D_loss: -0.1407, G_loss: 0.6618\n",
      "  Batch [580/1299] D_loss: -0.0608, G_loss: 0.5312\n",
      "  Batch [590/1299] D_loss: -0.0948, G_loss: 0.5041\n",
      "  Batch [600/1299] D_loss: 0.0716, G_loss: 0.3406\n",
      "  Batch [610/1299] D_loss: -0.0746, G_loss: 0.1624\n",
      "  Batch [620/1299] D_loss: -0.0702, G_loss: 0.2365\n",
      "  Batch [630/1299] D_loss: -0.1352, G_loss: 0.3353\n",
      "  Batch [640/1299] D_loss: -0.0908, G_loss: 0.4219\n",
      "  Batch [650/1299] D_loss: -0.0352, G_loss: 0.2609\n",
      "  Batch [660/1299] D_loss: -0.0565, G_loss: 0.2060\n",
      "  Batch [670/1299] D_loss: -0.2620, G_loss: -0.2369\n",
      "  Batch [680/1299] D_loss: -0.5467, G_loss: -1.7170\n",
      "  Batch [690/1299] D_loss: -0.0634, G_loss: 0.2331\n",
      "  Batch [700/1299] D_loss: -0.0323, G_loss: 0.2870\n",
      "  Batch [710/1299] D_loss: -0.0416, G_loss: 0.3312\n",
      "  Batch [720/1299] D_loss: -0.0863, G_loss: 0.3407\n",
      "  Batch [730/1299] D_loss: 0.0230, G_loss: 0.1826\n",
      "  Batch [740/1299] D_loss: -1.1308, G_loss: -2.5110\n",
      "  Batch [750/1299] D_loss: -0.2594, G_loss: 0.0290\n",
      "  Batch [760/1299] D_loss: -0.8285, G_loss: -1.5383\n",
      "  Batch [770/1299] D_loss: -0.4167, G_loss: -1.0673\n",
      "  Batch [780/1299] D_loss: -0.4179, G_loss: 0.1295\n",
      "  Batch [790/1299] D_loss: -0.3770, G_loss: -0.0775\n",
      "  Batch [800/1299] D_loss: -0.0640, G_loss: 0.3516\n",
      "  Batch [810/1299] D_loss: -0.1304, G_loss: 0.4883\n",
      "  Batch [820/1299] D_loss: -0.1118, G_loss: 0.6098\n",
      "  Batch [830/1299] D_loss: 0.0832, G_loss: 0.3506\n",
      "  Batch [840/1299] D_loss: -0.0610, G_loss: 0.3613\n",
      "  Batch [850/1299] D_loss: -4.1676, G_loss: -5.7259\n",
      "  Batch [860/1299] D_loss: -0.3578, G_loss: -0.0103\n",
      "  Batch [870/1299] D_loss: -0.5073, G_loss: -0.4003\n",
      "  Batch [880/1299] D_loss: -0.8084, G_loss: -1.3051\n",
      "  Batch [890/1299] D_loss: -0.0996, G_loss: 0.2325\n",
      "  Batch [900/1299] D_loss: -0.0601, G_loss: 0.3099\n",
      "  Batch [910/1299] D_loss: -0.0559, G_loss: 0.4609\n",
      "  Batch [920/1299] D_loss: -0.0578, G_loss: 0.3798\n",
      "  Batch [930/1299] D_loss: -0.1568, G_loss: 0.4998\n",
      "  Batch [940/1299] D_loss: -0.1394, G_loss: 0.3298\n",
      "  Batch [950/1299] D_loss: -1.0276, G_loss: -1.0878\n",
      "  Batch [960/1299] D_loss: -0.0087, G_loss: 0.1841\n",
      "  Batch [970/1299] D_loss: 0.0101, G_loss: 0.3153\n",
      "  Batch [980/1299] D_loss: -0.0045, G_loss: 0.1927\n",
      "  Batch [990/1299] D_loss: -1.0149, G_loss: -2.6987\n",
      "  Batch [1000/1299] D_loss: -0.2552, G_loss: -0.1095\n",
      "  Batch [1010/1299] D_loss: -0.7128, G_loss: 0.0089\n",
      "  Batch [1020/1299] D_loss: -0.2045, G_loss: 0.0948\n",
      "  Batch [1030/1299] D_loss: -0.8723, G_loss: 0.0865\n",
      "  Batch [1040/1299] D_loss: -0.1920, G_loss: 0.0290\n",
      "  Batch [1050/1299] D_loss: -0.2946, G_loss: -0.1442\n",
      "  Batch [1060/1299] D_loss: -0.0373, G_loss: 0.5805\n",
      "  Batch [1070/1299] D_loss: -0.1434, G_loss: 0.5121\n",
      "  Batch [1080/1299] D_loss: -0.0575, G_loss: 0.5767\n",
      "  Batch [1090/1299] D_loss: 0.0424, G_loss: 0.5820\n",
      "  Batch [1100/1299] D_loss: -0.0828, G_loss: 0.6101\n",
      "  Batch [1110/1299] D_loss: -3.5359, G_loss: -5.1370\n",
      "  Batch [1120/1299] D_loss: -0.2500, G_loss: 0.1018\n",
      "  Batch [1130/1299] D_loss: -0.3015, G_loss: -0.1629\n",
      "  Batch [1140/1299] D_loss: -0.1668, G_loss: 0.1391\n",
      "  Batch [1150/1299] D_loss: -1.0469, G_loss: -1.5094\n",
      "  Batch [1160/1299] D_loss: -1.2584, G_loss: -2.6714\n",
      "  Batch [1170/1299] D_loss: -0.0644, G_loss: 0.2365\n",
      "  Batch [1180/1299] D_loss: -0.1360, G_loss: 0.4531\n",
      "  Batch [1190/1299] D_loss: -0.1308, G_loss: 0.5383\n",
      "  Batch [1200/1299] D_loss: -0.0749, G_loss: 0.6332\n",
      "  Batch [1210/1299] D_loss: -0.0061, G_loss: 0.5499\n",
      "  Batch [1220/1299] D_loss: -0.0643, G_loss: 0.5194\n",
      "  Batch [1230/1299] D_loss: -4.8810, G_loss: -7.7019\n",
      "  Batch [1240/1299] D_loss: -0.0547, G_loss: 0.2903\n",
      "  Batch [1250/1299] D_loss: -1.0168, G_loss: -2.0057\n",
      "  Batch [1260/1299] D_loss: -0.0235, G_loss: 0.3064\n",
      "  Batch [1270/1299] D_loss: 0.0157, G_loss: 0.4132\n",
      "  Batch [1280/1299] D_loss: 0.0272, G_loss: 0.4065\n",
      "  Batch [1290/1299] D_loss: -0.0238, G_loss: 0.3795\n",
      "\n",
      "Epoch 68 Summary:\n",
      "  Average D_loss: -0.1523\n",
      "  Average G_loss: -0.0520\n",
      "\n",
      "Epoch [69/100]\n",
      "  Batch [0/1299] D_loss: -0.0742, G_loss: 0.3449\n",
      "  Batch [10/1299] D_loss: -0.0222, G_loss: 0.2514\n",
      "  Batch [20/1299] D_loss: -0.0602, G_loss: -0.0197\n",
      "  Batch [30/1299] D_loss: -0.0266, G_loss: 0.2146\n",
      "  Batch [40/1299] D_loss: 0.0659, G_loss: 0.1969\n",
      "  Batch [50/1299] D_loss: -0.0795, G_loss: 0.2818\n",
      "  Batch [60/1299] D_loss: -1.0481, G_loss: -5.2275\n",
      "  Batch [70/1299] D_loss: -0.3963, G_loss: -0.5909\n",
      "  Batch [80/1299] D_loss: -0.0473, G_loss: 0.3382\n",
      "  Batch [90/1299] D_loss: -0.0041, G_loss: 0.4282\n",
      "  Batch [100/1299] D_loss: -0.2858, G_loss: 0.5741\n",
      "  Batch [110/1299] D_loss: -0.3316, G_loss: 0.7604\n",
      "  Batch [120/1299] D_loss: 0.0013, G_loss: 0.4576\n",
      "  Batch [130/1299] D_loss: -0.0343, G_loss: 0.5001\n",
      "  Batch [140/1299] D_loss: -0.2559, G_loss: -1.1977\n",
      "  Batch [150/1299] D_loss: -0.0215, G_loss: 0.0756\n",
      "  Batch [160/1299] D_loss: -0.0031, G_loss: 0.1040\n",
      "  Batch [170/1299] D_loss: -0.1603, G_loss: -0.0501\n",
      "  Batch [180/1299] D_loss: -1.8217, G_loss: -3.3264\n",
      "  Batch [190/1299] D_loss: -0.0176, G_loss: 0.2237\n",
      "  Batch [200/1299] D_loss: -0.1031, G_loss: 0.4451\n",
      "  Batch [210/1299] D_loss: -0.1698, G_loss: 0.6482\n",
      "  Batch [220/1299] D_loss: -0.0690, G_loss: 0.4968\n",
      "  Batch [230/1299] D_loss: -0.0962, G_loss: 0.5445\n",
      "  Batch [240/1299] D_loss: 0.0101, G_loss: 0.3762\n",
      "  Batch [250/1299] D_loss: -0.0664, G_loss: 0.3655\n",
      "  Batch [260/1299] D_loss: -0.1906, G_loss: -0.1976\n",
      "  Batch [270/1299] D_loss: -0.9744, G_loss: -1.3529\n",
      "  Batch [280/1299] D_loss: -0.1270, G_loss: 0.0699\n",
      "  Batch [290/1299] D_loss: -0.0566, G_loss: 0.2779\n",
      "  Batch [300/1299] D_loss: -0.0658, G_loss: 0.3660\n",
      "  Batch [310/1299] D_loss: -0.0010, G_loss: 0.4392\n",
      "  Batch [320/1299] D_loss: -0.1442, G_loss: 0.4917\n",
      "  Batch [330/1299] D_loss: -0.1017, G_loss: 0.5157\n",
      "  Batch [340/1299] D_loss: -0.0621, G_loss: 0.4224\n",
      "  Batch [350/1299] D_loss: -1.1343, G_loss: -1.6783\n",
      "  Batch [360/1299] D_loss: -0.5888, G_loss: -0.3150\n",
      "  Batch [370/1299] D_loss: -0.3308, G_loss: -0.0143\n",
      "  Batch [380/1299] D_loss: -0.5942, G_loss: -0.0478\n",
      "  Batch [390/1299] D_loss: -0.1953, G_loss: 0.0251\n",
      "  Batch [400/1299] D_loss: -1.0079, G_loss: -1.2080\n",
      "  Batch [410/1299] D_loss: -0.5326, G_loss: 0.0804\n",
      "  Batch [420/1299] D_loss: -0.4381, G_loss: -0.0281\n",
      "  Batch [430/1299] D_loss: -0.1277, G_loss: 0.2626\n",
      "  Batch [440/1299] D_loss: -0.1306, G_loss: 0.4101\n",
      "  Batch [450/1299] D_loss: -0.4433, G_loss: 0.6717\n",
      "  Batch [460/1299] D_loss: -0.1997, G_loss: 0.6889\n",
      "  Batch [470/1299] D_loss: -0.3042, G_loss: 0.4694\n",
      "  Batch [480/1299] D_loss: -0.0271, G_loss: 0.3104\n",
      "  Batch [490/1299] D_loss: -0.7030, G_loss: -3.3769\n",
      "  Batch [500/1299] D_loss: -0.0764, G_loss: 0.1010\n",
      "  Batch [510/1299] D_loss: -0.0677, G_loss: -1.8143\n",
      "  Batch [520/1299] D_loss: -0.0297, G_loss: 0.1386\n",
      "  Batch [530/1299] D_loss: -0.1267, G_loss: 0.3264\n",
      "  Batch [540/1299] D_loss: 0.0505, G_loss: 0.3854\n",
      "  Batch [550/1299] D_loss: -0.1730, G_loss: 0.6298\n",
      "  Batch [560/1299] D_loss: -0.1928, G_loss: 0.6777\n",
      "  Batch [570/1299] D_loss: -0.1673, G_loss: 0.6428\n",
      "  Batch [580/1299] D_loss: -2.7839, G_loss: -2.0520\n",
      "  Batch [590/1299] D_loss: 0.0323, G_loss: 0.1197\n",
      "  Batch [600/1299] D_loss: -0.8228, G_loss: -0.2621\n",
      "  Batch [610/1299] D_loss: -0.0435, G_loss: 0.1686\n",
      "  Batch [620/1299] D_loss: -1.3139, G_loss: -1.5372\n",
      "  Batch [630/1299] D_loss: -0.2741, G_loss: 0.0662\n",
      "  Batch [640/1299] D_loss: -0.0386, G_loss: 0.3063\n",
      "  Batch [650/1299] D_loss: -0.1273, G_loss: 0.5030\n",
      "  Batch [660/1299] D_loss: -0.1228, G_loss: 0.5737\n",
      "  Batch [670/1299] D_loss: 0.0801, G_loss: 0.5409\n",
      "  Batch [680/1299] D_loss: 0.0177, G_loss: 0.4171\n",
      "  Batch [690/1299] D_loss: -0.0558, G_loss: 0.3058\n",
      "  Batch [700/1299] D_loss: -0.4533, G_loss: -0.3217\n",
      "  Batch [710/1299] D_loss: -0.8443, G_loss: -2.7776\n",
      "  Batch [720/1299] D_loss: -0.4780, G_loss: -0.0460\n",
      "  Batch [730/1299] D_loss: -0.0759, G_loss: 0.0640\n",
      "  Batch [740/1299] D_loss: -0.1044, G_loss: 0.1427\n",
      "  Batch [750/1299] D_loss: -0.1669, G_loss: 0.2279\n",
      "  Batch [760/1299] D_loss: -1.3124, G_loss: -1.6471\n",
      "  Batch [770/1299] D_loss: -1.4579, G_loss: -1.3257\n",
      "  Batch [780/1299] D_loss: -0.5382, G_loss: 0.1238\n",
      "  Batch [790/1299] D_loss: -0.0317, G_loss: 0.1672\n",
      "  Batch [800/1299] D_loss: -0.0643, G_loss: 0.4110\n",
      "  Batch [810/1299] D_loss: -0.1622, G_loss: 0.6412\n",
      "  Batch [820/1299] D_loss: -0.0474, G_loss: 0.9742\n",
      "  Batch [830/1299] D_loss: -0.0814, G_loss: 0.7778\n",
      "  Batch [840/1299] D_loss: 0.0187, G_loss: 0.6675\n",
      "  Batch [850/1299] D_loss: -0.1028, G_loss: 0.4251\n",
      "  Batch [860/1299] D_loss: -1.8186, G_loss: -6.5951\n",
      "  Batch [870/1299] D_loss: -0.0435, G_loss: 0.1957\n",
      "  Batch [880/1299] D_loss: -0.1247, G_loss: 0.3326\n",
      "  Batch [890/1299] D_loss: -0.0340, G_loss: 0.3951\n",
      "  Batch [900/1299] D_loss: -0.0112, G_loss: 0.5033\n",
      "  Batch [910/1299] D_loss: -0.0965, G_loss: 0.5144\n",
      "  Batch [920/1299] D_loss: -0.1969, G_loss: 0.4882\n",
      "  Batch [930/1299] D_loss: -0.4635, G_loss: -0.3284\n",
      "  Batch [940/1299] D_loss: -0.5835, G_loss: -0.4502\n",
      "  Batch [950/1299] D_loss: -0.0364, G_loss: 0.0711\n",
      "  Batch [960/1299] D_loss: -0.4589, G_loss: -0.2138\n",
      "  Batch [970/1299] D_loss: -0.6660, G_loss: -0.4598\n",
      "  Batch [980/1299] D_loss: -0.4285, G_loss: -0.1914\n",
      "  Batch [990/1299] D_loss: -0.4609, G_loss: 0.0268\n",
      "  Batch [1000/1299] D_loss: -0.0505, G_loss: 0.4091\n",
      "  Batch [1010/1299] D_loss: -0.0376, G_loss: 0.4034\n",
      "  Batch [1020/1299] D_loss: 0.0159, G_loss: 0.5597\n",
      "  Batch [1030/1299] D_loss: -0.1954, G_loss: 0.7424\n",
      "  Batch [1040/1299] D_loss: -0.0272, G_loss: 0.5530\n",
      "  Batch [1050/1299] D_loss: -0.0294, G_loss: 0.5351\n",
      "  Batch [1060/1299] D_loss: -0.3828, G_loss: -0.1448\n",
      "  Batch [1070/1299] D_loss: -0.6165, G_loss: -1.9263\n",
      "  Batch [1080/1299] D_loss: -0.3887, G_loss: -0.8357\n",
      "  Batch [1090/1299] D_loss: -0.2359, G_loss: 0.1193\n",
      "  Batch [1100/1299] D_loss: -0.3021, G_loss: -0.1049\n",
      "  Batch [1110/1299] D_loss: -1.1349, G_loss: -0.3708\n",
      "  Batch [1120/1299] D_loss: -1.0033, G_loss: -0.1870\n",
      "  Batch [1130/1299] D_loss: -0.0247, G_loss: 0.1504\n",
      "  Batch [1140/1299] D_loss: -0.6720, G_loss: -0.5756\n",
      "  Batch [1150/1299] D_loss: -0.1142, G_loss: 0.1950\n",
      "  Batch [1160/1299] D_loss: -0.2182, G_loss: -0.0059\n",
      "  Batch [1170/1299] D_loss: -0.1515, G_loss: 0.5559\n",
      "  Batch [1180/1299] D_loss: -0.1088, G_loss: 0.7784\n",
      "  Batch [1190/1299] D_loss: -0.0310, G_loss: 0.8148\n",
      "  Batch [1200/1299] D_loss: -0.1663, G_loss: 0.6054\n",
      "  Batch [1210/1299] D_loss: 0.0194, G_loss: 0.5957\n",
      "  Batch [1220/1299] D_loss: -0.1760, G_loss: 0.3464\n",
      "  Batch [1230/1299] D_loss: -0.7019, G_loss: -2.0369\n",
      "  Batch [1240/1299] D_loss: -0.1680, G_loss: 0.0706\n",
      "  Batch [1250/1299] D_loss: -0.1326, G_loss: 0.1346\n",
      "  Batch [1260/1299] D_loss: -0.0814, G_loss: 0.4178\n",
      "  Batch [1270/1299] D_loss: -0.0694, G_loss: 0.6332\n",
      "  Batch [1280/1299] D_loss: -0.2581, G_loss: 0.5028\n",
      "  Batch [1290/1299] D_loss: -0.2031, G_loss: 0.6586\n",
      "\n",
      "Epoch 69 Summary:\n",
      "  Average D_loss: -0.1649\n",
      "  Average G_loss: -0.0813\n",
      "\n",
      "Epoch [70/100]\n",
      "  Batch [0/1299] D_loss: -0.0559, G_loss: 0.3877\n",
      "  Batch [10/1299] D_loss: -0.0341, G_loss: 0.4132\n",
      "  Batch [20/1299] D_loss: -0.3160, G_loss: -0.9036\n",
      "  Batch [30/1299] D_loss: -1.5954, G_loss: -1.3953\n",
      "  Batch [40/1299] D_loss: -0.0906, G_loss: 0.0384\n",
      "  Batch [50/1299] D_loss: -0.0140, G_loss: 0.2502\n",
      "  Batch [60/1299] D_loss: -0.1010, G_loss: 0.3568\n",
      "  Batch [70/1299] D_loss: -0.0390, G_loss: 0.5755\n",
      "  Batch [80/1299] D_loss: -0.1728, G_loss: 0.5763\n",
      "  Batch [90/1299] D_loss: -0.0790, G_loss: 0.5859\n",
      "  Batch [100/1299] D_loss: -0.1294, G_loss: 0.3809\n",
      "  Batch [110/1299] D_loss: -1.5214, G_loss: -1.4503\n",
      "  Batch [120/1299] D_loss: -0.3823, G_loss: -0.6879\n",
      "  Batch [130/1299] D_loss: -0.5094, G_loss: -0.2000\n",
      "  Batch [140/1299] D_loss: -0.0146, G_loss: 0.1534\n",
      "  Batch [150/1299] D_loss: -0.8029, G_loss: -0.9603\n",
      "  Batch [160/1299] D_loss: -0.5443, G_loss: -0.5868\n",
      "  Batch [170/1299] D_loss: -2.1559, G_loss: -1.8566\n",
      "  Batch [180/1299] D_loss: -0.6509, G_loss: -0.0379\n",
      "  Batch [190/1299] D_loss: -0.3075, G_loss: -0.1468\n",
      "  Batch [200/1299] D_loss: -0.1555, G_loss: 0.4804\n",
      "  Batch [210/1299] D_loss: -0.1753, G_loss: 0.7792\n",
      "  Batch [220/1299] D_loss: -0.1447, G_loss: 0.9093\n",
      "  Batch [230/1299] D_loss: -0.1583, G_loss: 0.6864\n",
      "  Batch [240/1299] D_loss: -0.0743, G_loss: 0.6918\n",
      "  Batch [250/1299] D_loss: -0.0575, G_loss: 0.7169\n",
      "  Batch [260/1299] D_loss: -0.0023, G_loss: 0.4439\n",
      "  Batch [270/1299] D_loss: -1.7976, G_loss: -2.7608\n",
      "  Batch [280/1299] D_loss: -0.4586, G_loss: -1.0714\n",
      "  Batch [290/1299] D_loss: -0.1959, G_loss: 0.0737\n",
      "  Batch [300/1299] D_loss: -0.8086, G_loss: -3.5626\n",
      "  Batch [310/1299] D_loss: -0.0764, G_loss: 0.3131\n",
      "  Batch [320/1299] D_loss: -0.0926, G_loss: 0.4653\n",
      "  Batch [330/1299] D_loss: -0.1159, G_loss: 0.6411\n",
      "  Batch [340/1299] D_loss: -0.0487, G_loss: 0.5861\n",
      "  Batch [350/1299] D_loss: -0.0262, G_loss: 0.5322\n",
      "  Batch [360/1299] D_loss: -0.0653, G_loss: 0.4124\n",
      "  Batch [370/1299] D_loss: -3.5370, G_loss: -8.4678\n",
      "  Batch [380/1299] D_loss: -0.9755, G_loss: -1.7251\n",
      "  Batch [390/1299] D_loss: -0.1276, G_loss: 0.0693\n",
      "  Batch [400/1299] D_loss: -0.0338, G_loss: 0.3144\n",
      "  Batch [410/1299] D_loss: -0.0965, G_loss: 0.4988\n",
      "  Batch [420/1299] D_loss: -0.0069, G_loss: 0.4349\n",
      "  Batch [430/1299] D_loss: 0.0255, G_loss: 0.4472\n",
      "  Batch [440/1299] D_loss: 0.0017, G_loss: 0.3289\n",
      "  Batch [450/1299] D_loss: -2.2623, G_loss: -1.1260\n",
      "  Batch [460/1299] D_loss: -0.3297, G_loss: -0.1107\n",
      "  Batch [470/1299] D_loss: -0.4521, G_loss: 0.0938\n",
      "  Batch [480/1299] D_loss: -0.0986, G_loss: 0.1663\n",
      "  Batch [490/1299] D_loss: -0.0437, G_loss: 0.1381\n",
      "  Batch [500/1299] D_loss: -1.2431, G_loss: -0.6731\n",
      "  Batch [510/1299] D_loss: -0.5504, G_loss: 0.0183\n",
      "  Batch [520/1299] D_loss: -0.2326, G_loss: -0.2059\n",
      "  Batch [530/1299] D_loss: -0.1213, G_loss: 0.1707\n",
      "  Batch [540/1299] D_loss: -0.2023, G_loss: 0.1892\n",
      "  Batch [550/1299] D_loss: -0.1259, G_loss: 0.3325\n",
      "  Batch [560/1299] D_loss: -0.1579, G_loss: 0.4209\n",
      "  Batch [570/1299] D_loss: -0.1690, G_loss: 0.5184\n",
      "  Batch [580/1299] D_loss: 0.0110, G_loss: 0.5364\n",
      "  Batch [590/1299] D_loss: -0.1442, G_loss: 0.3072\n",
      "  Batch [600/1299] D_loss: -0.1082, G_loss: -0.0662\n",
      "  Batch [610/1299] D_loss: -0.2322, G_loss: 0.0401\n",
      "  Batch [620/1299] D_loss: -0.0076, G_loss: 0.1693\n",
      "  Batch [630/1299] D_loss: -0.1261, G_loss: 0.4644\n",
      "  Batch [640/1299] D_loss: -0.1275, G_loss: 0.5431\n",
      "  Batch [650/1299] D_loss: -0.1747, G_loss: 0.5609\n",
      "  Batch [660/1299] D_loss: -0.1675, G_loss: 0.5943\n",
      "  Batch [670/1299] D_loss: -0.0955, G_loss: 0.3777\n",
      "  Batch [680/1299] D_loss: -2.0974, G_loss: -3.8215\n",
      "  Batch [690/1299] D_loss: 0.0524, G_loss: 0.1927\n",
      "  Batch [700/1299] D_loss: -0.0390, G_loss: 0.3374\n",
      "  Batch [710/1299] D_loss: -0.0529, G_loss: 0.3608\n",
      "  Batch [720/1299] D_loss: -0.0880, G_loss: 0.4253\n",
      "  Batch [730/1299] D_loss: 0.0472, G_loss: 0.3320\n",
      "  Batch [740/1299] D_loss: -0.1369, G_loss: 0.2769\n",
      "  Batch [750/1299] D_loss: -0.2018, G_loss: -0.1999\n",
      "  Batch [760/1299] D_loss: -0.0865, G_loss: 0.0357\n",
      "  Batch [770/1299] D_loss: -0.1612, G_loss: 0.0683\n",
      "  Batch [780/1299] D_loss: -0.0538, G_loss: 0.2031\n",
      "  Batch [790/1299] D_loss: -0.0371, G_loss: 0.4822\n",
      "  Batch [800/1299] D_loss: -0.0152, G_loss: 0.3932\n",
      "  Batch [810/1299] D_loss: -0.1373, G_loss: 0.5474\n",
      "  Batch [820/1299] D_loss: -0.2267, G_loss: 0.6353\n",
      "  Batch [830/1299] D_loss: -0.1261, G_loss: 0.3021\n",
      "  Batch [840/1299] D_loss: -0.7594, G_loss: -1.4459\n",
      "  Batch [850/1299] D_loss: -0.6682, G_loss: -0.0443\n",
      "  Batch [860/1299] D_loss: -0.1227, G_loss: 0.1136\n",
      "  Batch [870/1299] D_loss: -0.1161, G_loss: 0.1671\n",
      "  Batch [880/1299] D_loss: -1.3377, G_loss: -1.4911\n",
      "  Batch [890/1299] D_loss: -0.0038, G_loss: 0.1531\n",
      "  Batch [900/1299] D_loss: -0.0345, G_loss: 0.4824\n",
      "  Batch [910/1299] D_loss: -0.0762, G_loss: 0.7349\n",
      "  Batch [920/1299] D_loss: -0.1426, G_loss: 0.8555\n",
      "  Batch [930/1299] D_loss: -0.1249, G_loss: 0.7327\n",
      "  Batch [940/1299] D_loss: 0.0807, G_loss: 0.3973\n",
      "  Batch [950/1299] D_loss: 0.0044, G_loss: 0.2617\n",
      "  Batch [960/1299] D_loss: -0.0620, G_loss: 0.0493\n",
      "  Batch [970/1299] D_loss: -0.2896, G_loss: 0.0315\n",
      "  Batch [980/1299] D_loss: -0.0349, G_loss: 0.1514\n",
      "  Batch [990/1299] D_loss: -0.0269, G_loss: 0.3214\n",
      "  Batch [1000/1299] D_loss: -0.0987, G_loss: 0.4299\n",
      "  Batch [1010/1299] D_loss: 0.0912, G_loss: 0.4010\n",
      "  Batch [1020/1299] D_loss: -0.1620, G_loss: 0.4616\n",
      "  Batch [1030/1299] D_loss: -0.1794, G_loss: 0.4075\n",
      "  Batch [1040/1299] D_loss: -0.0387, G_loss: 0.2562\n",
      "  Batch [1050/1299] D_loss: -0.2902, G_loss: 0.0838\n",
      "  Batch [1060/1299] D_loss: -0.0608, G_loss: -0.0988\n",
      "  Batch [1070/1299] D_loss: -0.4152, G_loss: -0.0658\n",
      "  Batch [1080/1299] D_loss: -0.4756, G_loss: -1.2151\n",
      "  Batch [1090/1299] D_loss: -0.3707, G_loss: -0.0427\n",
      "  Batch [1100/1299] D_loss: -0.0507, G_loss: 0.3187\n",
      "  Batch [1110/1299] D_loss: -0.1156, G_loss: 0.4890\n",
      "  Batch [1120/1299] D_loss: -0.0643, G_loss: 0.6975\n",
      "  Batch [1130/1299] D_loss: -0.2461, G_loss: 0.7412\n",
      "  Batch [1140/1299] D_loss: -0.1014, G_loss: 0.5640\n",
      "  Batch [1150/1299] D_loss: -0.0447, G_loss: 0.4704\n",
      "  Batch [1160/1299] D_loss: -4.5981, G_loss: -6.6419\n",
      "  Batch [1170/1299] D_loss: -0.6460, G_loss: 0.0593\n",
      "  Batch [1180/1299] D_loss: -0.3371, G_loss: 0.0192\n",
      "  Batch [1190/1299] D_loss: -0.2661, G_loss: 0.4329\n",
      "  Batch [1200/1299] D_loss: -0.1873, G_loss: 0.7707\n",
      "  Batch [1210/1299] D_loss: -0.0911, G_loss: 0.7204\n",
      "  Batch [1220/1299] D_loss: -0.1537, G_loss: 0.7322\n",
      "  Batch [1230/1299] D_loss: -0.0122, G_loss: 0.5235\n",
      "  Batch [1240/1299] D_loss: -0.0324, G_loss: 0.2749\n",
      "  Batch [1250/1299] D_loss: -0.0532, G_loss: 0.1891\n",
      "  Batch [1260/1299] D_loss: -0.3600, G_loss: -0.9720\n",
      "  Batch [1270/1299] D_loss: -0.8470, G_loss: -0.0568\n",
      "  Batch [1280/1299] D_loss: 0.0050, G_loss: 0.0663\n",
      "  Batch [1290/1299] D_loss: -1.0337, G_loss: -0.1952\n",
      "\n",
      "Epoch 70 Summary:\n",
      "  Average D_loss: -0.1784\n",
      "  Average G_loss: -0.1113\n",
      "\n",
      "Epoch [71/100]\n",
      "  Batch [0/1299] D_loss: -0.3293, G_loss: -0.2730\n",
      "  Batch [10/1299] D_loss: -0.0561, G_loss: 0.2953\n",
      "  Batch [20/1299] D_loss: -0.1605, G_loss: 0.5807\n",
      "  Batch [30/1299] D_loss: -0.3636, G_loss: 0.8286\n",
      "  Batch [40/1299] D_loss: -0.0637, G_loss: 0.6907\n",
      "  Batch [50/1299] D_loss: -0.0908, G_loss: 0.6133\n",
      "  Batch [60/1299] D_loss: -0.0304, G_loss: 0.5946\n",
      "  Batch [70/1299] D_loss: -0.0312, G_loss: 0.2991\n",
      "  Batch [80/1299] D_loss: 0.1825, G_loss: -3.0865\n",
      "  Batch [90/1299] D_loss: -0.0107, G_loss: 0.1396\n",
      "  Batch [100/1299] D_loss: -0.1109, G_loss: 0.2981\n",
      "  Batch [110/1299] D_loss: -0.0181, G_loss: 0.4024\n",
      "  Batch [120/1299] D_loss: -0.0814, G_loss: 0.4021\n",
      "  Batch [130/1299] D_loss: -0.0286, G_loss: 0.5721\n",
      "  Batch [140/1299] D_loss: -0.1656, G_loss: 0.4314\n",
      "  Batch [150/1299] D_loss: -0.3556, G_loss: -0.4047\n",
      "  Batch [160/1299] D_loss: -1.2236, G_loss: -1.7308\n",
      "  Batch [170/1299] D_loss: -0.7377, G_loss: -0.5010\n",
      "  Batch [180/1299] D_loss: -0.3982, G_loss: 0.0329\n",
      "  Batch [190/1299] D_loss: -0.6835, G_loss: -0.0247\n",
      "  Batch [200/1299] D_loss: -0.0368, G_loss: 0.2116\n",
      "  Batch [210/1299] D_loss: -0.1288, G_loss: 0.4595\n",
      "  Batch [220/1299] D_loss: -0.1208, G_loss: 0.6201\n",
      "  Batch [230/1299] D_loss: -0.2412, G_loss: 0.6763\n",
      "  Batch [240/1299] D_loss: -0.0619, G_loss: 0.7197\n",
      "  Batch [250/1299] D_loss: -0.0044, G_loss: 0.4285\n",
      "  Batch [260/1299] D_loss: 0.0021, G_loss: 0.3854\n",
      "  Batch [270/1299] D_loss: -2.0377, G_loss: -1.8523\n",
      "  Batch [280/1299] D_loss: -0.3502, G_loss: -0.3895\n",
      "  Batch [290/1299] D_loss: -0.0936, G_loss: 0.2649\n",
      "  Batch [300/1299] D_loss: -0.0638, G_loss: 0.3484\n",
      "  Batch [310/1299] D_loss: 0.0095, G_loss: 0.4253\n",
      "  Batch [320/1299] D_loss: -0.0415, G_loss: 0.3527\n",
      "  Batch [330/1299] D_loss: -0.1557, G_loss: 0.2993\n",
      "  Batch [340/1299] D_loss: -0.0400, G_loss: 0.2045\n",
      "  Batch [350/1299] D_loss: -0.0650, G_loss: 0.0096\n",
      "  Batch [360/1299] D_loss: -0.0054, G_loss: 0.1708\n",
      "  Batch [370/1299] D_loss: 0.0430, G_loss: 0.1879\n",
      "  Batch [380/1299] D_loss: -0.0680, G_loss: 0.3846\n",
      "  Batch [390/1299] D_loss: -0.0835, G_loss: 0.2807\n",
      "  Batch [400/1299] D_loss: -0.0394, G_loss: 0.1807\n",
      "  Batch [410/1299] D_loss: -0.0184, G_loss: 0.2465\n",
      "  Batch [420/1299] D_loss: -0.0081, G_loss: 0.4511\n",
      "  Batch [430/1299] D_loss: -0.0387, G_loss: 0.4023\n",
      "  Batch [440/1299] D_loss: 0.0296, G_loss: 0.3614\n",
      "  Batch [450/1299] D_loss: -0.0749, G_loss: 0.3694\n",
      "  Batch [460/1299] D_loss: -0.9080, G_loss: -0.8868\n",
      "  Batch [470/1299] D_loss: 0.0089, G_loss: 0.1484\n",
      "  Batch [480/1299] D_loss: -0.0935, G_loss: 0.3385\n",
      "  Batch [490/1299] D_loss: -0.1063, G_loss: 0.3760\n",
      "  Batch [500/1299] D_loss: -0.0392, G_loss: 0.3887\n",
      "  Batch [510/1299] D_loss: -0.1245, G_loss: 0.4816\n",
      "  Batch [520/1299] D_loss: -0.0790, G_loss: 0.3134\n",
      "  Batch [530/1299] D_loss: -1.8512, G_loss: -5.2792\n",
      "  Batch [540/1299] D_loss: -1.5808, G_loss: -2.6776\n",
      "  Batch [550/1299] D_loss: -0.1074, G_loss: 0.2112\n",
      "  Batch [560/1299] D_loss: -0.0766, G_loss: 0.3013\n",
      "  Batch [570/1299] D_loss: 0.0023, G_loss: 0.4744\n",
      "  Batch [580/1299] D_loss: -0.0890, G_loss: 0.5175\n",
      "  Batch [590/1299] D_loss: -0.0893, G_loss: 0.5396\n",
      "  Batch [600/1299] D_loss: -0.0832, G_loss: 0.3429\n",
      "  Batch [610/1299] D_loss: -1.8848, G_loss: -3.1560\n",
      "  Batch [620/1299] D_loss: -0.6120, G_loss: -1.0179\n",
      "  Batch [630/1299] D_loss: -0.6997, G_loss: -1.9876\n",
      "  Batch [640/1299] D_loss: -0.3096, G_loss: 0.0603\n",
      "  Batch [650/1299] D_loss: -0.5125, G_loss: -1.4325\n",
      "  Batch [660/1299] D_loss: -0.1119, G_loss: 0.3051\n",
      "  Batch [670/1299] D_loss: -0.0579, G_loss: 0.4339\n",
      "  Batch [680/1299] D_loss: -0.1921, G_loss: 0.6592\n",
      "  Batch [690/1299] D_loss: -0.1649, G_loss: 0.6578\n",
      "  Batch [700/1299] D_loss: -0.1559, G_loss: 0.6645\n",
      "  Batch [710/1299] D_loss: -0.0570, G_loss: 0.4128\n",
      "  Batch [720/1299] D_loss: -0.0757, G_loss: 0.1863\n",
      "  Batch [730/1299] D_loss: -0.0004, G_loss: 0.1383\n",
      "  Batch [740/1299] D_loss: 0.0055, G_loss: 0.1899\n",
      "  Batch [750/1299] D_loss: -0.0361, G_loss: 0.2707\n",
      "  Batch [760/1299] D_loss: -1.4671, G_loss: -1.9936\n",
      "  Batch [770/1299] D_loss: -0.7007, G_loss: -1.3046\n",
      "  Batch [780/1299] D_loss: -1.2323, G_loss: -4.3325\n",
      "  Batch [790/1299] D_loss: -0.0512, G_loss: 0.2152\n",
      "  Batch [800/1299] D_loss: -0.0870, G_loss: 0.4706\n",
      "  Batch [810/1299] D_loss: -0.1005, G_loss: 0.5775\n",
      "  Batch [820/1299] D_loss: -0.1836, G_loss: 0.5753\n",
      "  Batch [830/1299] D_loss: -0.1520, G_loss: 0.6588\n",
      "  Batch [840/1299] D_loss: -0.1285, G_loss: 0.4916\n",
      "  Batch [850/1299] D_loss: 0.0236, G_loss: 0.3542\n",
      "  Batch [860/1299] D_loss: -0.7366, G_loss: -0.8415\n",
      "  Batch [870/1299] D_loss: -0.0686, G_loss: 0.1534\n",
      "  Batch [880/1299] D_loss: -0.0846, G_loss: 0.2973\n",
      "  Batch [890/1299] D_loss: -0.0315, G_loss: 0.3522\n",
      "  Batch [900/1299] D_loss: -0.1208, G_loss: 0.4398\n",
      "  Batch [910/1299] D_loss: -0.1243, G_loss: 0.2636\n",
      "  Batch [920/1299] D_loss: -1.4514, G_loss: -2.3846\n",
      "  Batch [930/1299] D_loss: -0.0029, G_loss: 0.2568\n",
      "  Batch [940/1299] D_loss: -0.0459, G_loss: 0.2387\n",
      "  Batch [950/1299] D_loss: -0.0362, G_loss: 0.2693\n",
      "  Batch [960/1299] D_loss: -0.0191, G_loss: 0.2508\n",
      "  Batch [970/1299] D_loss: -0.0021, G_loss: 0.1752\n",
      "  Batch [980/1299] D_loss: -0.9200, G_loss: -1.0600\n",
      "  Batch [990/1299] D_loss: -0.0220, G_loss: 0.2323\n",
      "  Batch [1000/1299] D_loss: -0.0801, G_loss: 0.2647\n",
      "  Batch [1010/1299] D_loss: -0.0023, G_loss: 0.2676\n",
      "  Batch [1020/1299] D_loss: -0.1323, G_loss: -0.0769\n",
      "  Batch [1030/1299] D_loss: 0.0047, G_loss: 0.0829\n",
      "  Batch [1040/1299] D_loss: -0.0651, G_loss: 0.2092\n",
      "  Batch [1050/1299] D_loss: -0.0280, G_loss: 0.3724\n",
      "  Batch [1060/1299] D_loss: -0.0801, G_loss: 0.4862\n",
      "  Batch [1070/1299] D_loss: -0.0723, G_loss: 0.5174\n",
      "  Batch [1080/1299] D_loss: -0.1315, G_loss: 0.4254\n",
      "  Batch [1090/1299] D_loss: -1.6404, G_loss: -5.3314\n",
      "  Batch [1100/1299] D_loss: -0.2809, G_loss: 0.0688\n",
      "  Batch [1110/1299] D_loss: -0.0121, G_loss: 0.1127\n",
      "  Batch [1120/1299] D_loss: -0.0631, G_loss: 0.1119\n",
      "  Batch [1130/1299] D_loss: 0.0170, G_loss: 0.1405\n",
      "  Batch [1140/1299] D_loss: -0.0444, G_loss: 0.3160\n",
      "  Batch [1150/1299] D_loss: -0.0206, G_loss: 0.3741\n",
      "  Batch [1160/1299] D_loss: -0.0154, G_loss: 0.4688\n",
      "  Batch [1170/1299] D_loss: -0.0643, G_loss: 0.3183\n",
      "  Batch [1180/1299] D_loss: -0.0595, G_loss: 0.2585\n",
      "  Batch [1190/1299] D_loss: -0.9597, G_loss: -1.1585\n",
      "  Batch [1200/1299] D_loss: -0.1856, G_loss: 0.0708\n",
      "  Batch [1210/1299] D_loss: -0.0628, G_loss: 0.0686\n",
      "  Batch [1220/1299] D_loss: -0.8499, G_loss: -0.5889\n",
      "  Batch [1230/1299] D_loss: -0.0712, G_loss: 0.2617\n",
      "  Batch [1240/1299] D_loss: -0.0983, G_loss: 0.4062\n",
      "  Batch [1250/1299] D_loss: -0.0470, G_loss: 0.4559\n",
      "  Batch [1260/1299] D_loss: -0.0196, G_loss: 0.5515\n",
      "  Batch [1270/1299] D_loss: -0.0474, G_loss: 0.4938\n",
      "  Batch [1280/1299] D_loss: -0.0126, G_loss: 0.3955\n",
      "  Batch [1290/1299] D_loss: -0.2772, G_loss: -1.2954\n",
      "\n",
      "Epoch 71 Summary:\n",
      "  Average D_loss: -0.1363\n",
      "  Average G_loss: -0.0385\n",
      "\n",
      "Epoch [72/100]\n",
      "  Batch [0/1299] D_loss: -1.3338, G_loss: -1.5540\n",
      "  Batch [10/1299] D_loss: -1.0290, G_loss: -1.4848\n",
      "  Batch [20/1299] D_loss: -1.3370, G_loss: -1.7073\n",
      "  Batch [30/1299] D_loss: -0.9476, G_loss: -2.7378\n",
      "  Batch [40/1299] D_loss: -0.2752, G_loss: -0.1844\n",
      "  Batch [50/1299] D_loss: -0.2933, G_loss: 0.0803\n",
      "  Batch [60/1299] D_loss: -1.4289, G_loss: -1.0002\n",
      "  Batch [70/1299] D_loss: -0.8633, G_loss: -0.6466\n",
      "  Batch [80/1299] D_loss: -1.1234, G_loss: -2.0120\n",
      "  Batch [90/1299] D_loss: -0.1093, G_loss: 0.2639\n",
      "  Batch [100/1299] D_loss: -0.0960, G_loss: 0.6348\n",
      "  Batch [110/1299] D_loss: -0.1542, G_loss: 0.7404\n",
      "  Batch [120/1299] D_loss: -0.1178, G_loss: 0.9607\n",
      "  Batch [130/1299] D_loss: -0.1565, G_loss: 0.8001\n",
      "  Batch [140/1299] D_loss: -0.1164, G_loss: 0.5037\n",
      "  Batch [150/1299] D_loss: -2.0730, G_loss: -1.5185\n",
      "  Batch [160/1299] D_loss: -0.0535, G_loss: 0.1125\n",
      "  Batch [170/1299] D_loss: -0.3715, G_loss: 0.0598\n",
      "  Batch [180/1299] D_loss: -0.3092, G_loss: -0.0617\n",
      "  Batch [190/1299] D_loss: -0.1031, G_loss: 0.2315\n",
      "  Batch [200/1299] D_loss: -0.1190, G_loss: 0.5176\n",
      "  Batch [210/1299] D_loss: -0.3034, G_loss: 0.5126\n",
      "  Batch [220/1299] D_loss: -0.1044, G_loss: 0.4068\n",
      "  Batch [230/1299] D_loss: -0.2566, G_loss: 0.3770\n",
      "  Batch [240/1299] D_loss: -0.8008, G_loss: 0.0575\n",
      "  Batch [250/1299] D_loss: -0.2963, G_loss: -0.4726\n",
      "  Batch [260/1299] D_loss: -0.2176, G_loss: -0.1331\n",
      "  Batch [270/1299] D_loss: -0.0624, G_loss: 0.2450\n",
      "  Batch [280/1299] D_loss: -0.0297, G_loss: 0.3502\n",
      "  Batch [290/1299] D_loss: -0.0830, G_loss: 0.4824\n",
      "  Batch [300/1299] D_loss: 0.0473, G_loss: 0.5543\n",
      "  Batch [310/1299] D_loss: -0.0208, G_loss: 0.3478\n",
      "  Batch [320/1299] D_loss: -0.0005, G_loss: 0.3357\n",
      "  Batch [330/1299] D_loss: -1.1490, G_loss: -2.6604\n",
      "  Batch [340/1299] D_loss: -0.0369, G_loss: 0.1732\n",
      "  Batch [350/1299] D_loss: 0.0101, G_loss: 0.2450\n",
      "  Batch [360/1299] D_loss: -0.0085, G_loss: 0.2645\n",
      "  Batch [370/1299] D_loss: -1.6790, G_loss: -1.3340\n",
      "  Batch [380/1299] D_loss: -0.7343, G_loss: -1.4284\n",
      "  Batch [390/1299] D_loss: -0.3049, G_loss: 0.0246\n",
      "  Batch [400/1299] D_loss: -0.4280, G_loss: -0.0194\n",
      "  Batch [410/1299] D_loss: -0.0651, G_loss: 0.3689\n",
      "  Batch [420/1299] D_loss: -0.1919, G_loss: 0.4760\n",
      "  Batch [430/1299] D_loss: -0.1593, G_loss: 0.5699\n",
      "  Batch [440/1299] D_loss: -0.0845, G_loss: 0.4757\n",
      "  Batch [450/1299] D_loss: -0.0282, G_loss: 0.3747\n",
      "  Batch [460/1299] D_loss: -0.1877, G_loss: -0.2956\n",
      "  Batch [470/1299] D_loss: -0.9992, G_loss: -1.3510\n",
      "  Batch [480/1299] D_loss: 0.0022, G_loss: 0.1187\n",
      "  Batch [490/1299] D_loss: -0.0645, G_loss: 0.3482\n",
      "  Batch [500/1299] D_loss: -0.1008, G_loss: 0.2988\n",
      "  Batch [510/1299] D_loss: -0.0995, G_loss: 0.5551\n",
      "  Batch [520/1299] D_loss: -0.1003, G_loss: 0.4913\n",
      "  Batch [530/1299] D_loss: -0.0814, G_loss: 0.2871\n",
      "  Batch [540/1299] D_loss: -0.0404, G_loss: 0.1490\n",
      "  Batch [550/1299] D_loss: -0.0037, G_loss: 0.2919\n",
      "  Batch [560/1299] D_loss: -0.0278, G_loss: 0.3244\n",
      "  Batch [570/1299] D_loss: -0.0494, G_loss: 0.3275\n",
      "  Batch [580/1299] D_loss: -0.0521, G_loss: 0.1366\n",
      "  Batch [590/1299] D_loss: -0.0192, G_loss: -0.0181\n",
      "  Batch [600/1299] D_loss: -0.5210, G_loss: -0.9526\n",
      "  Batch [610/1299] D_loss: -0.0046, G_loss: 0.1805\n",
      "  Batch [620/1299] D_loss: -0.1069, G_loss: 0.2990\n",
      "  Batch [630/1299] D_loss: -0.0506, G_loss: 0.4569\n",
      "  Batch [640/1299] D_loss: 0.0308, G_loss: 0.4473\n",
      "  Batch [650/1299] D_loss: -0.0139, G_loss: 0.4620\n",
      "  Batch [660/1299] D_loss: -0.1187, G_loss: 0.3888\n",
      "  Batch [670/1299] D_loss: -2.7985, G_loss: -5.4879\n",
      "  Batch [680/1299] D_loss: -0.5782, G_loss: -1.6693\n",
      "  Batch [690/1299] D_loss: -1.1290, G_loss: -2.4948\n",
      "  Batch [700/1299] D_loss: -1.2284, G_loss: -2.1414\n",
      "  Batch [710/1299] D_loss: -0.4981, G_loss: 0.0274\n",
      "  Batch [720/1299] D_loss: -1.0418, G_loss: -2.2000\n",
      "  Batch [730/1299] D_loss: -0.1560, G_loss: 0.0089\n",
      "  Batch [740/1299] D_loss: -0.0600, G_loss: 0.3132\n",
      "  Batch [750/1299] D_loss: -0.1345, G_loss: 0.5124\n",
      "  Batch [760/1299] D_loss: 0.0440, G_loss: 0.5323\n",
      "  Batch [770/1299] D_loss: -0.0387, G_loss: 0.6238\n",
      "  Batch [780/1299] D_loss: -0.1049, G_loss: 0.5333\n",
      "  Batch [790/1299] D_loss: 0.0616, G_loss: 0.4029\n",
      "  Batch [800/1299] D_loss: -1.9504, G_loss: -5.5587\n",
      "  Batch [810/1299] D_loss: -0.5967, G_loss: -0.3793\n",
      "  Batch [820/1299] D_loss: -0.0098, G_loss: 0.1206\n",
      "  Batch [830/1299] D_loss: -0.0339, G_loss: 0.2213\n",
      "  Batch [840/1299] D_loss: -0.0452, G_loss: 0.1870\n",
      "  Batch [850/1299] D_loss: -1.6055, G_loss: -2.3597\n",
      "  Batch [860/1299] D_loss: -0.2606, G_loss: -0.4654\n",
      "  Batch [870/1299] D_loss: -0.0478, G_loss: 0.4104\n",
      "  Batch [880/1299] D_loss: -0.1196, G_loss: 0.6638\n",
      "  Batch [890/1299] D_loss: -0.0682, G_loss: 0.7911\n",
      "  Batch [900/1299] D_loss: -0.3370, G_loss: 0.7060\n",
      "  Batch [910/1299] D_loss: -0.0958, G_loss: 0.5524\n",
      "  Batch [920/1299] D_loss: 0.0379, G_loss: 0.4614\n",
      "  Batch [930/1299] D_loss: -0.0860, G_loss: 0.3529\n",
      "  Batch [940/1299] D_loss: -0.0544, G_loss: -1.7482\n",
      "  Batch [950/1299] D_loss: -0.5840, G_loss: -0.3767\n",
      "  Batch [960/1299] D_loss: -0.3760, G_loss: -0.6045\n",
      "  Batch [970/1299] D_loss: -0.1504, G_loss: 0.1672\n",
      "  Batch [980/1299] D_loss: -0.0662, G_loss: 0.3659\n",
      "  Batch [990/1299] D_loss: -0.0082, G_loss: 0.4165\n",
      "  Batch [1000/1299] D_loss: -0.0461, G_loss: 0.4567\n",
      "  Batch [1010/1299] D_loss: -0.2296, G_loss: 0.4965\n",
      "  Batch [1020/1299] D_loss: -0.0806, G_loss: 0.3003\n",
      "  Batch [1030/1299] D_loss: -0.1400, G_loss: 0.0984\n",
      "  Batch [1040/1299] D_loss: -0.0459, G_loss: 0.2924\n",
      "  Batch [1050/1299] D_loss: -0.0439, G_loss: 0.4515\n",
      "  Batch [1060/1299] D_loss: -0.0709, G_loss: 0.5338\n",
      "  Batch [1070/1299] D_loss: -0.0362, G_loss: 0.4053\n",
      "  Batch [1080/1299] D_loss: -0.1497, G_loss: 0.3981\n",
      "  Batch [1090/1299] D_loss: -1.4516, G_loss: -2.6043\n",
      "  Batch [1100/1299] D_loss: -0.9055, G_loss: -0.3137\n",
      "  Batch [1110/1299] D_loss: -0.2510, G_loss: 0.0408\n",
      "  Batch [1120/1299] D_loss: -0.0013, G_loss: 0.1415\n",
      "  Batch [1130/1299] D_loss: -1.0939, G_loss: -0.2747\n",
      "  Batch [1140/1299] D_loss: -1.1952, G_loss: -1.9337\n",
      "  Batch [1150/1299] D_loss: -0.0232, G_loss: 0.1977\n",
      "  Batch [1160/1299] D_loss: -0.0569, G_loss: 0.4435\n",
      "  Batch [1170/1299] D_loss: -0.1556, G_loss: 0.6209\n",
      "  Batch [1180/1299] D_loss: -0.1336, G_loss: 0.6425\n",
      "  Batch [1190/1299] D_loss: -0.2790, G_loss: 0.7223\n",
      "  Batch [1200/1299] D_loss: -0.0906, G_loss: 0.4991\n",
      "  Batch [1210/1299] D_loss: 0.1419, G_loss: 0.3773\n",
      "  Batch [1220/1299] D_loss: -0.0187, G_loss: 0.2419\n",
      "  Batch [1230/1299] D_loss: -1.2977, G_loss: -2.0803\n",
      "  Batch [1240/1299] D_loss: -0.0115, G_loss: 0.1942\n",
      "  Batch [1250/1299] D_loss: 0.0109, G_loss: 0.2532\n",
      "  Batch [1260/1299] D_loss: -0.1352, G_loss: 0.4492\n",
      "  Batch [1270/1299] D_loss: -0.0961, G_loss: 0.3397\n",
      "  Batch [1280/1299] D_loss: -1.8821, G_loss: -2.6902\n",
      "  Batch [1290/1299] D_loss: -0.0118, G_loss: 0.1327\n",
      "\n",
      "Epoch 72 Summary:\n",
      "  Average D_loss: -0.1690\n",
      "  Average G_loss: -0.0869\n",
      "\n",
      "Epoch [73/100]\n",
      "  Batch [0/1299] D_loss: -0.0555, G_loss: 0.2396\n",
      "  Batch [10/1299] D_loss: 0.0500, G_loss: 0.3019\n",
      "  Batch [20/1299] D_loss: -0.0576, G_loss: 0.2710\n",
      "  Batch [30/1299] D_loss: -0.0327, G_loss: 0.2696\n",
      "  Batch [40/1299] D_loss: -0.6099, G_loss: -0.2271\n",
      "  Batch [50/1299] D_loss: -0.0822, G_loss: 0.1180\n",
      "  Batch [60/1299] D_loss: -0.0601, G_loss: 0.1242\n",
      "  Batch [70/1299] D_loss: -0.0149, G_loss: 0.3547\n",
      "  Batch [80/1299] D_loss: -0.1023, G_loss: 0.4945\n",
      "  Batch [90/1299] D_loss: -0.2719, G_loss: 0.6473\n",
      "  Batch [100/1299] D_loss: -0.1525, G_loss: 0.5405\n",
      "  Batch [110/1299] D_loss: -0.0221, G_loss: 0.1368\n",
      "  Batch [120/1299] D_loss: -0.5771, G_loss: -1.1094\n",
      "  Batch [130/1299] D_loss: -0.9338, G_loss: -2.2144\n",
      "  Batch [140/1299] D_loss: -0.0161, G_loss: 0.1408\n",
      "  Batch [150/1299] D_loss: -0.3915, G_loss: -0.2334\n",
      "  Batch [160/1299] D_loss: -0.0728, G_loss: 0.2656\n",
      "  Batch [170/1299] D_loss: -0.1146, G_loss: 0.3954\n",
      "  Batch [180/1299] D_loss: -0.1639, G_loss: 0.5071\n",
      "  Batch [190/1299] D_loss: -0.1393, G_loss: 0.6079\n",
      "  Batch [200/1299] D_loss: -0.0302, G_loss: 0.4801\n",
      "  Batch [210/1299] D_loss: -0.0964, G_loss: 0.2792\n",
      "  Batch [220/1299] D_loss: -0.0160, G_loss: 0.1797\n",
      "  Batch [230/1299] D_loss: 0.0036, G_loss: 0.1907\n",
      "  Batch [240/1299] D_loss: -1.0229, G_loss: -0.1361\n",
      "  Batch [250/1299] D_loss: -0.0060, G_loss: 0.0877\n",
      "  Batch [260/1299] D_loss: -0.0609, G_loss: 0.2180\n",
      "  Batch [270/1299] D_loss: -0.0440, G_loss: 0.1018\n",
      "  Batch [280/1299] D_loss: -0.5028, G_loss: -0.9235\n",
      "  Batch [290/1299] D_loss: -0.0880, G_loss: 0.2094\n",
      "  Batch [300/1299] D_loss: -0.0936, G_loss: 0.4786\n",
      "  Batch [310/1299] D_loss: -0.2432, G_loss: 0.5308\n",
      "  Batch [320/1299] D_loss: -0.0552, G_loss: 0.5651\n",
      "  Batch [330/1299] D_loss: -0.6175, G_loss: 0.4832\n",
      "  Batch [340/1299] D_loss: -0.1508, G_loss: 0.4584\n",
      "  Batch [350/1299] D_loss: -1.4158, G_loss: -1.6795\n",
      "  Batch [360/1299] D_loss: -0.0202, G_loss: 0.1094\n",
      "  Batch [370/1299] D_loss: -0.4161, G_loss: -0.5257\n",
      "  Batch [380/1299] D_loss: -0.7606, G_loss: -0.8148\n",
      "  Batch [390/1299] D_loss: -1.5870, G_loss: -0.0261\n",
      "  Batch [400/1299] D_loss: -0.1608, G_loss: 0.2402\n",
      "  Batch [410/1299] D_loss: -0.0844, G_loss: 0.1782\n",
      "  Batch [420/1299] D_loss: -0.4592, G_loss: -0.0635\n",
      "  Batch [430/1299] D_loss: -0.0425, G_loss: 0.3431\n",
      "  Batch [440/1299] D_loss: -0.1520, G_loss: 0.6928\n",
      "  Batch [450/1299] D_loss: -0.1318, G_loss: 0.6509\n",
      "  Batch [460/1299] D_loss: -0.1216, G_loss: 0.6152\n",
      "  Batch [470/1299] D_loss: -0.0427, G_loss: 0.4620\n",
      "  Batch [480/1299] D_loss: -0.0257, G_loss: 0.2458\n",
      "  Batch [490/1299] D_loss: -2.9010, G_loss: -4.9413\n",
      "  Batch [500/1299] D_loss: -0.1123, G_loss: 0.0873\n",
      "  Batch [510/1299] D_loss: -1.5424, G_loss: -1.3734\n",
      "  Batch [520/1299] D_loss: -0.1297, G_loss: 0.1074\n",
      "  Batch [530/1299] D_loss: -0.1413, G_loss: 0.1052\n",
      "  Batch [540/1299] D_loss: -0.0061, G_loss: 0.2321\n",
      "  Batch [550/1299] D_loss: -0.0277, G_loss: 0.3285\n",
      "  Batch [560/1299] D_loss: -0.0827, G_loss: 0.4216\n",
      "  Batch [570/1299] D_loss: -0.0782, G_loss: 0.4889\n",
      "  Batch [580/1299] D_loss: -0.0670, G_loss: 0.5957\n",
      "  Batch [590/1299] D_loss: 0.0282, G_loss: 0.3525\n",
      "  Batch [600/1299] D_loss: -0.0726, G_loss: 0.1472\n",
      "  Batch [610/1299] D_loss: -0.0886, G_loss: -0.1432\n",
      "  Batch [620/1299] D_loss: -0.6892, G_loss: -0.2026\n",
      "  Batch [630/1299] D_loss: -0.4642, G_loss: -0.0774\n",
      "  Batch [640/1299] D_loss: -0.8926, G_loss: -0.4668\n",
      "  Batch [650/1299] D_loss: -0.6072, G_loss: -0.8416\n",
      "  Batch [660/1299] D_loss: -0.1545, G_loss: 0.1994\n",
      "  Batch [670/1299] D_loss: -0.1605, G_loss: 0.1268\n",
      "  Batch [680/1299] D_loss: -0.6425, G_loss: -0.4878\n",
      "  Batch [690/1299] D_loss: -0.3493, G_loss: 0.1427\n",
      "  Batch [700/1299] D_loss: -0.7692, G_loss: -0.7608\n",
      "  Batch [710/1299] D_loss: -0.3055, G_loss: 0.0816\n",
      "  Batch [720/1299] D_loss: -0.0009, G_loss: 0.5320\n",
      "  Batch [730/1299] D_loss: -0.0770, G_loss: 0.6309\n",
      "  Batch [740/1299] D_loss: -0.1653, G_loss: 0.8307\n",
      "  Batch [750/1299] D_loss: -0.1361, G_loss: 0.8250\n",
      "  Batch [760/1299] D_loss: -0.0647, G_loss: 0.5744\n",
      "  Batch [770/1299] D_loss: 0.0096, G_loss: 0.3617\n",
      "  Batch [780/1299] D_loss: -0.5491, G_loss: -0.0490\n",
      "  Batch [790/1299] D_loss: -0.0203, G_loss: 0.1710\n",
      "  Batch [800/1299] D_loss: -0.4887, G_loss: 0.0705\n",
      "  Batch [810/1299] D_loss: -0.7681, G_loss: -0.3899\n",
      "  Batch [820/1299] D_loss: -0.9150, G_loss: -0.2810\n",
      "  Batch [830/1299] D_loss: -1.1867, G_loss: -0.0974\n",
      "  Batch [840/1299] D_loss: -0.6867, G_loss: 0.1330\n",
      "  Batch [850/1299] D_loss: -0.1163, G_loss: 0.1951\n",
      "  Batch [860/1299] D_loss: -0.0597, G_loss: 0.3516\n",
      "  Batch [870/1299] D_loss: -0.0666, G_loss: 0.5290\n",
      "  Batch [880/1299] D_loss: 0.1556, G_loss: 0.6039\n",
      "  Batch [890/1299] D_loss: -0.3748, G_loss: 0.8105\n",
      "  Batch [900/1299] D_loss: -0.0907, G_loss: 0.7094\n",
      "  Batch [910/1299] D_loss: 0.0010, G_loss: 0.2643\n",
      "  Batch [920/1299] D_loss: 0.0100, G_loss: 0.1760\n",
      "  Batch [930/1299] D_loss: -0.9246, G_loss: -0.6559\n",
      "  Batch [940/1299] D_loss: -0.6016, G_loss: 0.0904\n",
      "  Batch [950/1299] D_loss: -0.0472, G_loss: 0.1171\n",
      "  Batch [960/1299] D_loss: -0.4565, G_loss: 0.1440\n",
      "  Batch [970/1299] D_loss: -0.0088, G_loss: 0.1884\n",
      "  Batch [980/1299] D_loss: -0.0547, G_loss: 0.2974\n",
      "  Batch [990/1299] D_loss: -0.0080, G_loss: 0.3778\n",
      "  Batch [1000/1299] D_loss: -0.0560, G_loss: 0.4051\n",
      "  Batch [1010/1299] D_loss: -0.1897, G_loss: 0.4668\n",
      "  Batch [1020/1299] D_loss: -0.0191, G_loss: 0.3324\n",
      "  Batch [1030/1299] D_loss: -1.0157, G_loss: -2.6468\n",
      "  Batch [1040/1299] D_loss: -1.1652, G_loss: -0.7617\n",
      "  Batch [1050/1299] D_loss: -0.1132, G_loss: 0.1045\n",
      "  Batch [1060/1299] D_loss: -0.0370, G_loss: 0.2605\n",
      "  Batch [1070/1299] D_loss: -0.1013, G_loss: 0.4607\n",
      "  Batch [1080/1299] D_loss: -0.0912, G_loss: 0.6315\n",
      "  Batch [1090/1299] D_loss: -0.2031, G_loss: 0.6446\n",
      "  Batch [1100/1299] D_loss: -0.0903, G_loss: 0.5915\n",
      "  Batch [1110/1299] D_loss: -0.0908, G_loss: 0.3911\n",
      "  Batch [1120/1299] D_loss: -0.7447, G_loss: -1.0602\n",
      "  Batch [1130/1299] D_loss: -0.4065, G_loss: -0.1478\n",
      "  Batch [1140/1299] D_loss: -2.0651, G_loss: -2.8469\n",
      "  Batch [1150/1299] D_loss: -1.4920, G_loss: -1.3903\n",
      "  Batch [1160/1299] D_loss: -0.0884, G_loss: 0.0786\n",
      "  Batch [1170/1299] D_loss: -0.0278, G_loss: 0.3966\n",
      "  Batch [1180/1299] D_loss: -0.0412, G_loss: 0.6213\n",
      "  Batch [1190/1299] D_loss: -0.0390, G_loss: 0.6986\n",
      "  Batch [1200/1299] D_loss: -0.1567, G_loss: 0.6838\n",
      "  Batch [1210/1299] D_loss: -0.1549, G_loss: 0.5095\n",
      "  Batch [1220/1299] D_loss: -0.0081, G_loss: 0.4068\n",
      "  Batch [1230/1299] D_loss: -1.9879, G_loss: -0.2359\n",
      "  Batch [1240/1299] D_loss: -0.1872, G_loss: -0.0531\n",
      "  Batch [1250/1299] D_loss: -0.7156, G_loss: -1.0574\n",
      "  Batch [1260/1299] D_loss: -0.4706, G_loss: -0.3653\n",
      "  Batch [1270/1299] D_loss: -0.4818, G_loss: 0.0401\n",
      "  Batch [1280/1299] D_loss: -0.8241, G_loss: 0.1220\n",
      "  Batch [1290/1299] D_loss: -1.2717, G_loss: -0.8171\n",
      "\n",
      "Epoch 73 Summary:\n",
      "  Average D_loss: -0.2014\n",
      "  Average G_loss: -0.1184\n",
      "\n",
      "Epoch [74/100]\n",
      "  Batch [0/1299] D_loss: -0.2446, G_loss: 0.0124\n",
      "  Batch [10/1299] D_loss: -0.4000, G_loss: -0.2260\n",
      "  Batch [20/1299] D_loss: -0.5730, G_loss: -0.1575\n",
      "  Batch [30/1299] D_loss: -1.3045, G_loss: -2.5216\n",
      "  Batch [40/1299] D_loss: -1.1388, G_loss: -0.0126\n",
      "  Batch [50/1299] D_loss: -0.8642, G_loss: -1.2561\n",
      "  Batch [60/1299] D_loss: -0.0024, G_loss: 0.1137\n",
      "  Batch [70/1299] D_loss: -0.5105, G_loss: -0.0923\n",
      "  Batch [80/1299] D_loss: -0.2298, G_loss: 0.1688\n",
      "  Batch [90/1299] D_loss: -0.1039, G_loss: 0.3648\n",
      "  Batch [100/1299] D_loss: -0.2316, G_loss: 0.5447\n",
      "  Batch [110/1299] D_loss: -0.2465, G_loss: 0.9246\n",
      "  Batch [120/1299] D_loss: -0.0235, G_loss: 0.7091\n",
      "  Batch [130/1299] D_loss: -0.2472, G_loss: 0.7025\n",
      "  Batch [140/1299] D_loss: 0.0170, G_loss: 0.4440\n",
      "  Batch [150/1299] D_loss: -0.1124, G_loss: 0.0803\n",
      "  Batch [160/1299] D_loss: -0.5417, G_loss: -0.0833\n",
      "  Batch [170/1299] D_loss: -0.6217, G_loss: 0.0906\n",
      "  Batch [180/1299] D_loss: -0.6786, G_loss: -0.0767\n",
      "  Batch [190/1299] D_loss: 0.0226, G_loss: 0.2893\n",
      "  Batch [200/1299] D_loss: -0.0915, G_loss: 0.4595\n",
      "  Batch [210/1299] D_loss: -0.1247, G_loss: 0.6110\n",
      "  Batch [220/1299] D_loss: -0.1883, G_loss: 0.6001\n",
      "  Batch [230/1299] D_loss: -0.1520, G_loss: 0.4959\n",
      "  Batch [240/1299] D_loss: -0.1223, G_loss: 0.5488\n",
      "  Batch [250/1299] D_loss: -1.8745, G_loss: -1.0881\n",
      "  Batch [260/1299] D_loss: -0.7115, G_loss: -1.4026\n",
      "  Batch [270/1299] D_loss: -0.8805, G_loss: -0.8819\n",
      "  Batch [280/1299] D_loss: -0.2116, G_loss: -0.0561\n",
      "  Batch [290/1299] D_loss: -1.0203, G_loss: -1.0100\n",
      "  Batch [300/1299] D_loss: -0.7780, G_loss: -0.0762\n",
      "  Batch [310/1299] D_loss: -1.0808, G_loss: -0.2850\n",
      "  Batch [320/1299] D_loss: -0.7338, G_loss: 0.1250\n",
      "  Batch [330/1299] D_loss: -0.1647, G_loss: 0.2179\n",
      "  Batch [340/1299] D_loss: -0.0774, G_loss: 0.4814\n",
      "  Batch [350/1299] D_loss: -0.1088, G_loss: 0.7271\n",
      "  Batch [360/1299] D_loss: -0.3336, G_loss: 0.7331\n",
      "  Batch [370/1299] D_loss: -0.1492, G_loss: 0.7154\n",
      "  Batch [380/1299] D_loss: 0.0672, G_loss: 0.3856\n",
      "  Batch [390/1299] D_loss: -0.0169, G_loss: 0.3972\n",
      "  Batch [400/1299] D_loss: -1.8470, G_loss: -2.9582\n",
      "  Batch [410/1299] D_loss: -0.4105, G_loss: -0.5732\n",
      "  Batch [420/1299] D_loss: -0.3622, G_loss: 0.1250\n",
      "  Batch [430/1299] D_loss: -0.7499, G_loss: 0.1059\n",
      "  Batch [440/1299] D_loss: -1.2141, G_loss: -0.5549\n",
      "  Batch [450/1299] D_loss: -0.1086, G_loss: 0.3243\n",
      "  Batch [460/1299] D_loss: -0.1496, G_loss: 0.3926\n",
      "  Batch [470/1299] D_loss: -0.1869, G_loss: 0.6430\n",
      "  Batch [480/1299] D_loss: -0.0551, G_loss: 0.7114\n",
      "  Batch [490/1299] D_loss: 0.0009, G_loss: 0.4987\n",
      "  Batch [500/1299] D_loss: -0.0753, G_loss: 0.3947\n",
      "  Batch [510/1299] D_loss: -3.7838, G_loss: -5.3077\n",
      "  Batch [520/1299] D_loss: -0.0488, G_loss: 0.1357\n",
      "  Batch [530/1299] D_loss: -0.1732, G_loss: 0.3029\n",
      "  Batch [540/1299] D_loss: -0.0977, G_loss: 0.4152\n",
      "  Batch [550/1299] D_loss: -0.1367, G_loss: 0.5170\n",
      "  Batch [560/1299] D_loss: -0.0447, G_loss: 0.4632\n",
      "  Batch [570/1299] D_loss: -0.1134, G_loss: 0.4052\n",
      "  Batch [580/1299] D_loss: -1.6664, G_loss: -2.1915\n",
      "  Batch [590/1299] D_loss: -0.1491, G_loss: 0.0732\n",
      "  Batch [600/1299] D_loss: 0.0135, G_loss: 0.0919\n",
      "  Batch [610/1299] D_loss: -0.0338, G_loss: 0.3036\n",
      "  Batch [620/1299] D_loss: -0.1091, G_loss: 0.4026\n",
      "  Batch [630/1299] D_loss: -0.0873, G_loss: 0.5649\n",
      "  Batch [640/1299] D_loss: -0.0318, G_loss: 0.4664\n",
      "  Batch [650/1299] D_loss: -0.0168, G_loss: 0.3723\n",
      "  Batch [660/1299] D_loss: -0.0292, G_loss: 0.2797\n",
      "  Batch [670/1299] D_loss: -0.2664, G_loss: -0.3057\n",
      "  Batch [680/1299] D_loss: -0.3856, G_loss: 0.0109\n",
      "  Batch [690/1299] D_loss: 0.0102, G_loss: 0.1866\n",
      "  Batch [700/1299] D_loss: 0.0071, G_loss: 0.3139\n",
      "  Batch [710/1299] D_loss: -0.0409, G_loss: 0.3025\n",
      "  Batch [720/1299] D_loss: 0.0185, G_loss: 0.3141\n",
      "  Batch [730/1299] D_loss: -0.6738, G_loss: 0.2258\n",
      "  Batch [740/1299] D_loss: -0.2214, G_loss: -0.0726\n",
      "  Batch [750/1299] D_loss: -0.8491, G_loss: -0.0209\n",
      "  Batch [760/1299] D_loss: -0.0520, G_loss: 0.1835\n",
      "  Batch [770/1299] D_loss: -0.0169, G_loss: 0.2738\n",
      "  Batch [780/1299] D_loss: -0.0854, G_loss: 0.4611\n",
      "  Batch [790/1299] D_loss: -0.1553, G_loss: 0.5493\n",
      "  Batch [800/1299] D_loss: -0.0763, G_loss: 0.4662\n",
      "  Batch [810/1299] D_loss: -0.0426, G_loss: 0.3006\n",
      "  Batch [820/1299] D_loss: -3.0738, G_loss: -6.5758\n",
      "  Batch [830/1299] D_loss: -0.1435, G_loss: 0.0095\n",
      "  Batch [840/1299] D_loss: -1.0549, G_loss: -2.0751\n",
      "  Batch [850/1299] D_loss: -0.0340, G_loss: 0.3288\n",
      "  Batch [860/1299] D_loss: -0.0629, G_loss: 0.3939\n",
      "  Batch [870/1299] D_loss: -0.1555, G_loss: 0.4887\n",
      "  Batch [880/1299] D_loss: -0.0040, G_loss: 0.4600\n",
      "  Batch [890/1299] D_loss: 0.0359, G_loss: 0.4405\n",
      "  Batch [900/1299] D_loss: -0.0513, G_loss: 0.1842\n",
      "  Batch [910/1299] D_loss: 0.0315, G_loss: 0.0839\n",
      "  Batch [920/1299] D_loss: -0.0147, G_loss: 0.1522\n",
      "  Batch [930/1299] D_loss: -0.0282, G_loss: 0.2717\n",
      "  Batch [940/1299] D_loss: 0.0066, G_loss: 0.2265\n",
      "  Batch [950/1299] D_loss: -1.4909, G_loss: -1.7515\n",
      "  Batch [960/1299] D_loss: -0.0509, G_loss: 0.1282\n",
      "  Batch [970/1299] D_loss: -1.3225, G_loss: -1.5685\n",
      "  Batch [980/1299] D_loss: -0.9446, G_loss: -1.5279\n",
      "  Batch [990/1299] D_loss: -0.1176, G_loss: 0.1948\n",
      "  Batch [1000/1299] D_loss: -0.0801, G_loss: 0.4084\n",
      "  Batch [1010/1299] D_loss: -0.0332, G_loss: 0.4315\n",
      "  Batch [1020/1299] D_loss: -0.0604, G_loss: 0.5166\n",
      "  Batch [1030/1299] D_loss: 0.0053, G_loss: 0.3841\n",
      "  Batch [1040/1299] D_loss: -1.9563, G_loss: -3.3573\n",
      "  Batch [1050/1299] D_loss: -0.1032, G_loss: -0.0411\n",
      "  Batch [1060/1299] D_loss: -0.0622, G_loss: 0.1958\n",
      "  Batch [1070/1299] D_loss: -0.1178, G_loss: 0.5081\n",
      "  Batch [1080/1299] D_loss: -0.1339, G_loss: 0.6268\n",
      "  Batch [1090/1299] D_loss: -0.3494, G_loss: 0.6793\n",
      "  Batch [1100/1299] D_loss: -0.1649, G_loss: 0.8977\n",
      "  Batch [1110/1299] D_loss: 0.1107, G_loss: 0.3857\n",
      "  Batch [1120/1299] D_loss: -1.2434, G_loss: -4.1357\n",
      "  Batch [1130/1299] D_loss: -0.0165, G_loss: 0.2253\n",
      "  Batch [1140/1299] D_loss: -0.0158, G_loss: 0.2775\n",
      "  Batch [1150/1299] D_loss: -0.0674, G_loss: 0.3186\n",
      "  Batch [1160/1299] D_loss: -0.0378, G_loss: 0.3188\n",
      "  Batch [1170/1299] D_loss: -0.1895, G_loss: -0.0646\n",
      "  Batch [1180/1299] D_loss: -0.5539, G_loss: -0.1732\n",
      "  Batch [1190/1299] D_loss: -0.1219, G_loss: 0.1172\n",
      "  Batch [1200/1299] D_loss: -0.0186, G_loss: 0.2931\n",
      "  Batch [1210/1299] D_loss: -0.1864, G_loss: 0.4616\n",
      "  Batch [1220/1299] D_loss: -0.0453, G_loss: 0.5003\n",
      "  Batch [1230/1299] D_loss: -0.0511, G_loss: 0.3075\n",
      "  Batch [1240/1299] D_loss: -1.1760, G_loss: -4.3805\n",
      "  Batch [1250/1299] D_loss: -0.9159, G_loss: -2.2565\n",
      "  Batch [1260/1299] D_loss: -0.2153, G_loss: 0.0832\n",
      "  Batch [1270/1299] D_loss: -0.1862, G_loss: 0.2560\n",
      "  Batch [1280/1299] D_loss: -0.1473, G_loss: 0.5289\n",
      "  Batch [1290/1299] D_loss: -0.2410, G_loss: 0.8693\n",
      "\n",
      "Epoch 74 Summary:\n",
      "  Average D_loss: -0.1812\n",
      "  Average G_loss: -0.0709\n",
      "\n",
      "Epoch [75/100]\n",
      "  Batch [0/1299] D_loss: -0.1206, G_loss: 0.9788\n",
      "  Batch [10/1299] D_loss: -0.1627, G_loss: 0.7752\n",
      "  Batch [20/1299] D_loss: -0.1306, G_loss: 0.7207\n",
      "  Batch [30/1299] D_loss: -0.0541, G_loss: 0.4624\n",
      "  Batch [40/1299] D_loss: -0.0484, G_loss: 0.3436\n",
      "  Batch [50/1299] D_loss: 0.0948, G_loss: 0.2171\n",
      "  Batch [60/1299] D_loss: -0.0467, G_loss: 0.1579\n",
      "  Batch [70/1299] D_loss: -1.1832, G_loss: -0.8087\n",
      "  Batch [80/1299] D_loss: -0.1992, G_loss: -0.2117\n",
      "  Batch [90/1299] D_loss: -0.2773, G_loss: -0.4842\n",
      "  Batch [100/1299] D_loss: -0.5287, G_loss: -0.1325\n",
      "  Batch [110/1299] D_loss: -0.9772, G_loss: -1.4359\n",
      "  Batch [120/1299] D_loss: -0.0645, G_loss: 0.1422\n",
      "  Batch [130/1299] D_loss: -0.5488, G_loss: 0.0844\n",
      "  Batch [140/1299] D_loss: -0.6911, G_loss: 0.0826\n",
      "  Batch [150/1299] D_loss: -0.0103, G_loss: 0.1790\n",
      "  Batch [160/1299] D_loss: -0.1860, G_loss: 0.5113\n",
      "  Batch [170/1299] D_loss: -0.3073, G_loss: 0.6680\n",
      "  Batch [180/1299] D_loss: -0.1105, G_loss: 0.6323\n",
      "  Batch [190/1299] D_loss: 0.0401, G_loss: 0.5172\n",
      "  Batch [200/1299] D_loss: -0.0131, G_loss: 0.4427\n",
      "  Batch [210/1299] D_loss: 0.0058, G_loss: 0.3143\n",
      "  Batch [220/1299] D_loss: -0.5526, G_loss: -0.6567\n",
      "  Batch [230/1299] D_loss: -0.1587, G_loss: -0.1878\n",
      "  Batch [240/1299] D_loss: -0.1463, G_loss: 0.1511\n",
      "  Batch [250/1299] D_loss: -0.0213, G_loss: 0.4341\n",
      "  Batch [260/1299] D_loss: -0.1091, G_loss: 0.4509\n",
      "  Batch [270/1299] D_loss: -0.1570, G_loss: 0.4836\n",
      "  Batch [280/1299] D_loss: -0.0894, G_loss: 0.5053\n",
      "  Batch [290/1299] D_loss: 0.0253, G_loss: 0.4004\n",
      "  Batch [300/1299] D_loss: -2.5641, G_loss: -0.3294\n",
      "  Batch [310/1299] D_loss: -0.0476, G_loss: 0.0065\n",
      "  Batch [320/1299] D_loss: -0.6136, G_loss: -0.4328\n",
      "  Batch [330/1299] D_loss: -0.3114, G_loss: -0.2421\n",
      "  Batch [340/1299] D_loss: -0.0043, G_loss: 0.2252\n",
      "  Batch [350/1299] D_loss: -0.0208, G_loss: 0.3221\n",
      "  Batch [360/1299] D_loss: 0.0046, G_loss: 0.4185\n",
      "  Batch [370/1299] D_loss: -0.0847, G_loss: 0.5474\n",
      "  Batch [380/1299] D_loss: 0.0275, G_loss: 0.4928\n",
      "  Batch [390/1299] D_loss: 0.0114, G_loss: 0.2087\n",
      "  Batch [400/1299] D_loss: -0.1160, G_loss: 0.0988\n",
      "  Batch [410/1299] D_loss: -0.0160, G_loss: 0.2672\n",
      "  Batch [420/1299] D_loss: -0.0077, G_loss: 0.3179\n",
      "  Batch [430/1299] D_loss: -0.1070, G_loss: 0.3660\n",
      "  Batch [440/1299] D_loss: -0.1723, G_loss: 0.4295\n",
      "  Batch [450/1299] D_loss: -0.0471, G_loss: 0.2578\n",
      "  Batch [460/1299] D_loss: -1.0091, G_loss: -1.3138\n",
      "  Batch [470/1299] D_loss: -0.1508, G_loss: 0.0412\n",
      "  Batch [480/1299] D_loss: -1.4389, G_loss: -1.4412\n",
      "  Batch [490/1299] D_loss: -0.3847, G_loss: 0.1504\n",
      "  Batch [500/1299] D_loss: -0.1404, G_loss: 0.0182\n",
      "  Batch [510/1299] D_loss: -0.7985, G_loss: 0.0330\n",
      "  Batch [520/1299] D_loss: -0.1410, G_loss: 0.3797\n",
      "  Batch [530/1299] D_loss: -0.2661, G_loss: 0.6107\n",
      "  Batch [540/1299] D_loss: -0.2335, G_loss: 0.8219\n",
      "  Batch [550/1299] D_loss: -0.1415, G_loss: 0.6841\n",
      "  Batch [560/1299] D_loss: -0.1253, G_loss: 0.5413\n",
      "  Batch [570/1299] D_loss: -0.1982, G_loss: -0.6472\n",
      "  Batch [580/1299] D_loss: -0.9987, G_loss: -0.2344\n",
      "  Batch [590/1299] D_loss: -0.0316, G_loss: 0.1153\n",
      "  Batch [600/1299] D_loss: -0.3459, G_loss: -0.1131\n",
      "  Batch [610/1299] D_loss: -0.0257, G_loss: 0.1938\n",
      "  Batch [620/1299] D_loss: -0.1075, G_loss: 0.1616\n",
      "  Batch [630/1299] D_loss: -1.3893, G_loss: -2.4740\n",
      "  Batch [640/1299] D_loss: -0.1752, G_loss: 0.0698\n",
      "  Batch [650/1299] D_loss: -0.1902, G_loss: 0.0691\n",
      "  Batch [660/1299] D_loss: -0.0240, G_loss: 0.0857\n",
      "  Batch [670/1299] D_loss: -0.0615, G_loss: 0.3264\n",
      "  Batch [680/1299] D_loss: -0.1339, G_loss: 0.5470\n",
      "  Batch [690/1299] D_loss: -0.2302, G_loss: 0.7466\n",
      "  Batch [700/1299] D_loss: -0.2222, G_loss: 0.8078\n",
      "  Batch [710/1299] D_loss: 0.0306, G_loss: 0.7575\n",
      "  Batch [720/1299] D_loss: 0.0801, G_loss: 0.5264\n",
      "  Batch [730/1299] D_loss: -0.9772, G_loss: -0.3802\n",
      "  Batch [740/1299] D_loss: -0.5159, G_loss: -0.1549\n",
      "  Batch [750/1299] D_loss: -0.2823, G_loss: 0.0443\n",
      "  Batch [760/1299] D_loss: -0.0389, G_loss: 0.0943\n",
      "  Batch [770/1299] D_loss: -0.8196, G_loss: -1.3406\n",
      "  Batch [780/1299] D_loss: -0.7846, G_loss: -0.0210\n",
      "  Batch [790/1299] D_loss: -1.2884, G_loss: -2.6600\n",
      "  Batch [800/1299] D_loss: -0.7972, G_loss: -0.0461\n",
      "  Batch [810/1299] D_loss: -0.3007, G_loss: 0.0779\n",
      "  Batch [820/1299] D_loss: -0.4065, G_loss: -0.4537\n",
      "  Batch [830/1299] D_loss: -0.0909, G_loss: 0.2472\n",
      "  Batch [840/1299] D_loss: -0.1382, G_loss: 0.5272\n",
      "  Batch [850/1299] D_loss: -0.0875, G_loss: 0.7094\n",
      "  Batch [860/1299] D_loss: -0.0786, G_loss: 0.7656\n",
      "  Batch [870/1299] D_loss: -0.0280, G_loss: 0.6786\n",
      "  Batch [880/1299] D_loss: 0.1075, G_loss: 0.5570\n",
      "  Batch [890/1299] D_loss: -0.0588, G_loss: 0.4585\n",
      "  Batch [900/1299] D_loss: -1.9153, G_loss: -1.3361\n",
      "  Batch [910/1299] D_loss: -1.4550, G_loss: -1.1840\n",
      "  Batch [920/1299] D_loss: -0.0243, G_loss: 0.1526\n",
      "  Batch [930/1299] D_loss: -0.0643, G_loss: 0.3001\n",
      "  Batch [940/1299] D_loss: -0.1439, G_loss: 0.3364\n",
      "  Batch [950/1299] D_loss: -0.0835, G_loss: 0.4466\n",
      "  Batch [960/1299] D_loss: -0.1116, G_loss: 0.5032\n",
      "  Batch [970/1299] D_loss: 0.0540, G_loss: 0.2927\n",
      "  Batch [980/1299] D_loss: -0.7316, G_loss: -1.1719\n",
      "  Batch [990/1299] D_loss: -1.1496, G_loss: -1.2409\n",
      "  Batch [1000/1299] D_loss: -0.0784, G_loss: 0.2403\n",
      "  Batch [1010/1299] D_loss: -0.1064, G_loss: 0.3820\n",
      "  Batch [1020/1299] D_loss: -0.0183, G_loss: 0.4346\n",
      "  Batch [1030/1299] D_loss: -0.0495, G_loss: 0.4199\n",
      "  Batch [1040/1299] D_loss: -0.0800, G_loss: 0.5783\n",
      "  Batch [1050/1299] D_loss: -0.0393, G_loss: 0.4857\n",
      "  Batch [1060/1299] D_loss: -3.8066, G_loss: -4.7401\n",
      "  Batch [1070/1299] D_loss: -0.0286, G_loss: 0.2080\n",
      "  Batch [1080/1299] D_loss: -0.0083, G_loss: 0.2374\n",
      "  Batch [1090/1299] D_loss: -0.0561, G_loss: 0.2862\n",
      "  Batch [1100/1299] D_loss: -0.0239, G_loss: 0.2929\n",
      "  Batch [1110/1299] D_loss: -0.1112, G_loss: 0.2336\n",
      "  Batch [1120/1299] D_loss: -1.6894, G_loss: -2.3358\n",
      "  Batch [1130/1299] D_loss: -0.0705, G_loss: 0.1602\n",
      "  Batch [1140/1299] D_loss: -0.0181, G_loss: 0.2459\n",
      "  Batch [1150/1299] D_loss: -0.0736, G_loss: 0.3966\n",
      "  Batch [1160/1299] D_loss: -0.0138, G_loss: 0.4067\n",
      "  Batch [1170/1299] D_loss: -0.0645, G_loss: 0.3118\n",
      "  Batch [1180/1299] D_loss: -0.8628, G_loss: -0.0759\n",
      "  Batch [1190/1299] D_loss: -0.0385, G_loss: 0.2681\n",
      "  Batch [1200/1299] D_loss: -0.0039, G_loss: 0.2648\n",
      "  Batch [1210/1299] D_loss: -0.0286, G_loss: 0.4540\n",
      "  Batch [1220/1299] D_loss: -0.0600, G_loss: 0.4294\n",
      "  Batch [1230/1299] D_loss: -0.0442, G_loss: 0.2780\n",
      "  Batch [1240/1299] D_loss: -1.0490, G_loss: -0.0976\n",
      "  Batch [1250/1299] D_loss: -0.0925, G_loss: 0.1796\n",
      "  Batch [1260/1299] D_loss: -0.1028, G_loss: 0.3675\n",
      "  Batch [1270/1299] D_loss: 0.0014, G_loss: 0.5191\n",
      "  Batch [1280/1299] D_loss: -0.1708, G_loss: 0.4993\n",
      "  Batch [1290/1299] D_loss: 0.0170, G_loss: 0.3211\n",
      "\n",
      "Epoch 75 Summary:\n",
      "  Average D_loss: -0.1715\n",
      "  Average G_loss: -0.0578\n",
      "\n",
      "Epoch [76/100]\n",
      "  Batch [0/1299] D_loss: -0.0389, G_loss: 0.3298\n",
      "  Batch [10/1299] D_loss: -3.5724, G_loss: -5.2966\n",
      "  Batch [20/1299] D_loss: -0.3978, G_loss: -0.0977\n",
      "  Batch [30/1299] D_loss: -0.9856, G_loss: -2.0659\n",
      "  Batch [40/1299] D_loss: -0.1525, G_loss: 0.0100\n",
      "  Batch [50/1299] D_loss: -0.2523, G_loss: -0.2136\n",
      "  Batch [60/1299] D_loss: -0.0322, G_loss: 0.1325\n",
      "  Batch [70/1299] D_loss: -0.0481, G_loss: 0.4313\n",
      "  Batch [80/1299] D_loss: -0.1840, G_loss: 0.6878\n",
      "  Batch [90/1299] D_loss: -0.0734, G_loss: 0.6652\n",
      "  Batch [100/1299] D_loss: -0.2148, G_loss: 0.7145\n",
      "  Batch [110/1299] D_loss: 0.0190, G_loss: 0.4563\n",
      "  Batch [120/1299] D_loss: -0.0266, G_loss: 0.2532\n",
      "  Batch [130/1299] D_loss: 0.0004, G_loss: 0.1910\n",
      "  Batch [140/1299] D_loss: -0.0744, G_loss: -0.0239\n",
      "  Batch [150/1299] D_loss: -0.2629, G_loss: -0.4068\n",
      "  Batch [160/1299] D_loss: -0.0551, G_loss: 0.1946\n",
      "  Batch [170/1299] D_loss: -0.0266, G_loss: 0.3302\n",
      "  Batch [180/1299] D_loss: -0.0804, G_loss: 0.4852\n",
      "  Batch [190/1299] D_loss: -0.1539, G_loss: 0.3740\n",
      "  Batch [200/1299] D_loss: -0.0272, G_loss: 0.1949\n",
      "  Batch [210/1299] D_loss: -2.7263, G_loss: -1.4168\n",
      "  Batch [220/1299] D_loss: -0.3024, G_loss: -0.3295\n",
      "  Batch [230/1299] D_loss: -0.8425, G_loss: -0.7119\n",
      "  Batch [240/1299] D_loss: -1.1065, G_loss: 0.0659\n",
      "  Batch [250/1299] D_loss: -0.0019, G_loss: 0.1805\n",
      "  Batch [260/1299] D_loss: -0.0980, G_loss: 0.3202\n",
      "  Batch [270/1299] D_loss: -0.0315, G_loss: 0.4213\n",
      "  Batch [280/1299] D_loss: -0.1286, G_loss: 0.4692\n",
      "  Batch [290/1299] D_loss: -0.0122, G_loss: 0.4652\n",
      "  Batch [300/1299] D_loss: -0.0913, G_loss: 0.4378\n",
      "  Batch [310/1299] D_loss: -0.0497, G_loss: 0.3558\n",
      "  Batch [320/1299] D_loss: -1.4589, G_loss: -2.7488\n",
      "  Batch [330/1299] D_loss: -0.2693, G_loss: -0.0501\n",
      "  Batch [340/1299] D_loss: -0.2398, G_loss: -0.5006\n",
      "  Batch [350/1299] D_loss: -1.1509, G_loss: -1.5510\n",
      "  Batch [360/1299] D_loss: -0.3199, G_loss: -0.1066\n",
      "  Batch [370/1299] D_loss: -0.0999, G_loss: 0.1356\n",
      "  Batch [380/1299] D_loss: -0.1374, G_loss: 0.2400\n",
      "  Batch [390/1299] D_loss: -0.2687, G_loss: 0.1785\n",
      "  Batch [400/1299] D_loss: -0.1276, G_loss: 0.2429\n",
      "  Batch [410/1299] D_loss: -0.1483, G_loss: 0.5379\n",
      "  Batch [420/1299] D_loss: -0.1208, G_loss: 0.7946\n",
      "  Batch [430/1299] D_loss: -0.2131, G_loss: 0.7593\n",
      "  Batch [440/1299] D_loss: -0.1532, G_loss: 0.8016\n",
      "  Batch [450/1299] D_loss: -0.0042, G_loss: 0.5618\n",
      "  Batch [460/1299] D_loss: 0.0388, G_loss: 0.2940\n",
      "  Batch [470/1299] D_loss: -0.5474, G_loss: -1.6588\n",
      "  Batch [480/1299] D_loss: -0.6227, G_loss: -1.8210\n",
      "  Batch [490/1299] D_loss: -0.6776, G_loss: -1.3320\n",
      "  Batch [500/1299] D_loss: -0.8806, G_loss: -0.2452\n",
      "  Batch [510/1299] D_loss: -0.0498, G_loss: 0.1331\n",
      "  Batch [520/1299] D_loss: -0.0439, G_loss: 0.3646\n",
      "  Batch [530/1299] D_loss: -0.1375, G_loss: 0.4303\n",
      "  Batch [540/1299] D_loss: -0.1519, G_loss: 0.5511\n",
      "  Batch [550/1299] D_loss: -0.1098, G_loss: 0.7065\n",
      "  Batch [560/1299] D_loss: -0.1548, G_loss: 0.6723\n",
      "  Batch [570/1299] D_loss: -0.0085, G_loss: 0.3130\n",
      "  Batch [580/1299] D_loss: -0.1064, G_loss: 0.2062\n",
      "  Batch [590/1299] D_loss: 0.0237, G_loss: 0.1106\n",
      "  Batch [600/1299] D_loss: -0.0618, G_loss: 0.1212\n",
      "  Batch [610/1299] D_loss: -1.2470, G_loss: -2.3373\n",
      "  Batch [620/1299] D_loss: -0.7174, G_loss: -1.1632\n",
      "  Batch [630/1299] D_loss: -0.9290, G_loss: 0.0707\n",
      "  Batch [640/1299] D_loss: -0.6640, G_loss: -0.9988\n",
      "  Batch [650/1299] D_loss: -0.4643, G_loss: -0.1196\n",
      "  Batch [660/1299] D_loss: -0.1438, G_loss: 0.3539\n",
      "  Batch [670/1299] D_loss: -0.2210, G_loss: 0.5956\n",
      "  Batch [680/1299] D_loss: -0.2218, G_loss: 0.7921\n",
      "  Batch [690/1299] D_loss: -0.2406, G_loss: 0.9311\n",
      "  Batch [700/1299] D_loss: -0.0663, G_loss: 0.6715\n",
      "  Batch [710/1299] D_loss: -0.0030, G_loss: 0.6122\n",
      "  Batch [720/1299] D_loss: -0.0700, G_loss: 0.2901\n",
      "  Batch [730/1299] D_loss: -1.3066, G_loss: -1.8095\n",
      "  Batch [740/1299] D_loss: 0.0034, G_loss: 0.1055\n",
      "  Batch [750/1299] D_loss: -0.4604, G_loss: -0.3227\n",
      "  Batch [760/1299] D_loss: -0.0026, G_loss: 0.1903\n",
      "  Batch [770/1299] D_loss: -0.0391, G_loss: 0.2547\n",
      "  Batch [780/1299] D_loss: -0.0861, G_loss: 0.4376\n",
      "  Batch [790/1299] D_loss: 0.0296, G_loss: 0.3066\n",
      "  Batch [800/1299] D_loss: -0.0130, G_loss: 0.2803\n",
      "  Batch [810/1299] D_loss: -3.6368, G_loss: -6.1830\n",
      "  Batch [820/1299] D_loss: 0.0514, G_loss: 0.0998\n",
      "  Batch [830/1299] D_loss: -0.0322, G_loss: 0.1816\n",
      "  Batch [840/1299] D_loss: -0.0344, G_loss: 0.3734\n",
      "  Batch [850/1299] D_loss: -0.1595, G_loss: 0.5465\n",
      "  Batch [860/1299] D_loss: -0.1061, G_loss: 0.6644\n",
      "  Batch [870/1299] D_loss: -0.0661, G_loss: 0.5962\n",
      "  Batch [880/1299] D_loss: -0.0506, G_loss: 0.4888\n",
      "  Batch [890/1299] D_loss: -0.1038, G_loss: 0.1618\n",
      "  Batch [900/1299] D_loss: -0.1526, G_loss: 0.0318\n",
      "  Batch [910/1299] D_loss: -0.8072, G_loss: -1.1262\n",
      "  Batch [920/1299] D_loss: -0.7078, G_loss: -2.5725\n",
      "  Batch [930/1299] D_loss: 0.0091, G_loss: 0.1612\n",
      "  Batch [940/1299] D_loss: -0.0782, G_loss: 0.3892\n",
      "  Batch [950/1299] D_loss: -0.1336, G_loss: 0.6465\n",
      "  Batch [960/1299] D_loss: -0.0698, G_loss: 0.6925\n",
      "  Batch [970/1299] D_loss: -0.0942, G_loss: 0.5050\n",
      "  Batch [980/1299] D_loss: -0.1096, G_loss: 0.4986\n",
      "  Batch [990/1299] D_loss: -0.0637, G_loss: 0.2736\n",
      "  Batch [1000/1299] D_loss: -0.1264, G_loss: -0.0161\n",
      "  Batch [1010/1299] D_loss: 0.0173, G_loss: 0.1572\n",
      "  Batch [1020/1299] D_loss: 0.0175, G_loss: 0.2179\n",
      "  Batch [1030/1299] D_loss: -0.0666, G_loss: 0.4020\n",
      "  Batch [1040/1299] D_loss: -0.0912, G_loss: 0.3331\n",
      "  Batch [1050/1299] D_loss: -0.1318, G_loss: 0.3676\n",
      "  Batch [1060/1299] D_loss: -0.0839, G_loss: 0.3557\n",
      "  Batch [1070/1299] D_loss: -0.2627, G_loss: -0.4735\n",
      "  Batch [1080/1299] D_loss: -1.1644, G_loss: -1.7111\n",
      "  Batch [1090/1299] D_loss: -0.0346, G_loss: 0.2467\n",
      "  Batch [1100/1299] D_loss: -0.1471, G_loss: 0.4624\n",
      "  Batch [1110/1299] D_loss: 0.0223, G_loss: 0.4964\n",
      "  Batch [1120/1299] D_loss: 0.0323, G_loss: 0.4208\n",
      "  Batch [1130/1299] D_loss: -0.1311, G_loss: 0.4902\n",
      "  Batch [1140/1299] D_loss: -0.0545, G_loss: 0.2343\n",
      "  Batch [1150/1299] D_loss: -0.5872, G_loss: -1.0216\n",
      "  Batch [1160/1299] D_loss: -0.5409, G_loss: -0.8662\n",
      "  Batch [1170/1299] D_loss: -0.5232, G_loss: -0.2496\n",
      "  Batch [1180/1299] D_loss: -0.0163, G_loss: 0.2334\n",
      "  Batch [1190/1299] D_loss: -0.0495, G_loss: 0.4142\n",
      "  Batch [1200/1299] D_loss: -0.1663, G_loss: 0.5711\n",
      "  Batch [1210/1299] D_loss: -0.0280, G_loss: 0.5641\n",
      "  Batch [1220/1299] D_loss: -0.0235, G_loss: 0.4784\n",
      "  Batch [1230/1299] D_loss: -0.0937, G_loss: 0.3656\n",
      "  Batch [1240/1299] D_loss: -2.8372, G_loss: -4.7421\n",
      "  Batch [1250/1299] D_loss: -0.7134, G_loss: -2.0076\n",
      "  Batch [1260/1299] D_loss: -1.0920, G_loss: -1.3930\n",
      "  Batch [1270/1299] D_loss: -0.0810, G_loss: 0.0747\n",
      "  Batch [1280/1299] D_loss: -1.8402, G_loss: -2.3301\n",
      "  Batch [1290/1299] D_loss: -0.2541, G_loss: 0.0479\n",
      "\n",
      "Epoch 76 Summary:\n",
      "  Average D_loss: -0.1636\n",
      "  Average G_loss: -0.0919\n",
      "\n",
      "Epoch [77/100]\n",
      "  Batch [0/1299] D_loss: -0.0822, G_loss: 0.3967\n",
      "  Batch [10/1299] D_loss: -0.0581, G_loss: 0.5425\n",
      "  Batch [20/1299] D_loss: -0.3145, G_loss: 0.6874\n",
      "  Batch [30/1299] D_loss: -0.0685, G_loss: 0.5278\n",
      "  Batch [40/1299] D_loss: -0.1432, G_loss: 0.4577\n",
      "  Batch [50/1299] D_loss: -0.1658, G_loss: 0.3806\n",
      "  Batch [60/1299] D_loss: -4.5588, G_loss: -6.5130\n",
      "  Batch [70/1299] D_loss: -0.1993, G_loss: -0.1008\n",
      "  Batch [80/1299] D_loss: -0.2978, G_loss: 0.0449\n",
      "  Batch [90/1299] D_loss: -0.0553, G_loss: 0.1479\n",
      "  Batch [100/1299] D_loss: -0.0146, G_loss: 0.3656\n",
      "  Batch [110/1299] D_loss: 0.0169, G_loss: 0.5509\n",
      "  Batch [120/1299] D_loss: -0.0168, G_loss: 0.5851\n",
      "  Batch [130/1299] D_loss: -0.0183, G_loss: 0.6828\n",
      "  Batch [140/1299] D_loss: -0.0781, G_loss: 0.4647\n",
      "  Batch [150/1299] D_loss: -2.0926, G_loss: -1.1721\n",
      "  Batch [160/1299] D_loss: -0.4587, G_loss: -0.0294\n",
      "  Batch [170/1299] D_loss: -0.1816, G_loss: -0.1073\n",
      "  Batch [180/1299] D_loss: -0.4414, G_loss: -0.4146\n",
      "  Batch [190/1299] D_loss: -0.7417, G_loss: -0.3251\n",
      "  Batch [200/1299] D_loss: -0.8006, G_loss: -0.4789\n",
      "  Batch [210/1299] D_loss: -0.1148, G_loss: 0.2705\n",
      "  Batch [220/1299] D_loss: -0.1017, G_loss: 0.4029\n",
      "  Batch [230/1299] D_loss: -0.0337, G_loss: 0.4728\n",
      "  Batch [240/1299] D_loss: -0.0653, G_loss: 0.5952\n",
      "  Batch [250/1299] D_loss: -0.0391, G_loss: 0.6123\n",
      "  Batch [260/1299] D_loss: -0.0958, G_loss: 0.4613\n",
      "  Batch [270/1299] D_loss: -0.1458, G_loss: 0.5337\n",
      "  Batch [280/1299] D_loss: -1.4800, G_loss: -4.7961\n",
      "  Batch [290/1299] D_loss: -0.4117, G_loss: -0.0720\n",
      "  Batch [300/1299] D_loss: -0.5897, G_loss: -0.0781\n",
      "  Batch [310/1299] D_loss: -0.0707, G_loss: 0.2712\n",
      "  Batch [320/1299] D_loss: -1.3034, G_loss: -1.4919\n",
      "  Batch [330/1299] D_loss: 0.0093, G_loss: 0.1356\n",
      "  Batch [340/1299] D_loss: -0.0678, G_loss: 0.2337\n",
      "  Batch [350/1299] D_loss: -0.1463, G_loss: 0.3534\n",
      "  Batch [360/1299] D_loss: -0.1216, G_loss: 0.4978\n",
      "  Batch [370/1299] D_loss: -0.1365, G_loss: 0.5465\n",
      "  Batch [380/1299] D_loss: -0.0529, G_loss: 0.4774\n",
      "  Batch [390/1299] D_loss: -0.1737, G_loss: 0.5720\n",
      "  Batch [400/1299] D_loss: -1.7570, G_loss: -2.5111\n",
      "  Batch [410/1299] D_loss: -0.1348, G_loss: -0.4387\n",
      "  Batch [420/1299] D_loss: -1.3797, G_loss: -3.8239\n",
      "  Batch [430/1299] D_loss: -0.0443, G_loss: 0.2112\n",
      "  Batch [440/1299] D_loss: -0.1103, G_loss: 0.4836\n",
      "  Batch [450/1299] D_loss: 0.0029, G_loss: 0.6197\n",
      "  Batch [460/1299] D_loss: -0.0193, G_loss: 0.7817\n",
      "  Batch [470/1299] D_loss: -0.1201, G_loss: 0.7836\n",
      "  Batch [480/1299] D_loss: -0.0553, G_loss: 0.3986\n",
      "  Batch [490/1299] D_loss: 0.0215, G_loss: 0.2820\n",
      "  Batch [500/1299] D_loss: -0.5241, G_loss: -0.4591\n",
      "  Batch [510/1299] D_loss: -0.5404, G_loss: -0.2227\n",
      "  Batch [520/1299] D_loss: 0.0011, G_loss: 0.3106\n",
      "  Batch [530/1299] D_loss: -0.0278, G_loss: 0.2738\n",
      "  Batch [540/1299] D_loss: -0.0983, G_loss: 0.4421\n",
      "  Batch [550/1299] D_loss: -0.0383, G_loss: 0.2180\n",
      "  Batch [560/1299] D_loss: 0.0116, G_loss: 0.1920\n",
      "  Batch [570/1299] D_loss: -0.6253, G_loss: -1.1772\n",
      "  Batch [580/1299] D_loss: -0.0937, G_loss: -0.0081\n",
      "  Batch [590/1299] D_loss: -0.9398, G_loss: -2.7660\n",
      "  Batch [600/1299] D_loss: -0.3662, G_loss: -0.2591\n",
      "  Batch [610/1299] D_loss: -0.0615, G_loss: 0.2634\n",
      "  Batch [620/1299] D_loss: -0.0750, G_loss: 0.4470\n",
      "  Batch [630/1299] D_loss: -0.0204, G_loss: 0.5443\n",
      "  Batch [640/1299] D_loss: -0.2801, G_loss: 0.5603\n",
      "  Batch [650/1299] D_loss: -0.0717, G_loss: 0.6829\n",
      "  Batch [660/1299] D_loss: -0.1843, G_loss: 0.6184\n",
      "  Batch [670/1299] D_loss: -0.1404, G_loss: 0.3971\n",
      "  Batch [680/1299] D_loss: -3.3087, G_loss: -4.4481\n",
      "  Batch [690/1299] D_loss: -0.0050, G_loss: 0.1840\n",
      "  Batch [700/1299] D_loss: -0.0455, G_loss: 0.3299\n",
      "  Batch [710/1299] D_loss: 0.0430, G_loss: 0.3678\n",
      "  Batch [720/1299] D_loss: -0.0217, G_loss: 0.4069\n",
      "  Batch [730/1299] D_loss: -0.0073, G_loss: 0.1970\n",
      "  Batch [740/1299] D_loss: -3.4675, G_loss: -3.2441\n",
      "  Batch [750/1299] D_loss: -0.6132, G_loss: -0.0145\n",
      "  Batch [760/1299] D_loss: -0.7162, G_loss: -1.8312\n",
      "  Batch [770/1299] D_loss: -0.0016, G_loss: 0.1344\n",
      "  Batch [780/1299] D_loss: -0.0730, G_loss: 0.3672\n",
      "  Batch [790/1299] D_loss: -0.0299, G_loss: 0.3238\n",
      "  Batch [800/1299] D_loss: 0.0037, G_loss: 0.5293\n",
      "  Batch [810/1299] D_loss: -0.2157, G_loss: 0.5562\n",
      "  Batch [820/1299] D_loss: -0.1699, G_loss: -0.7026\n",
      "  Batch [830/1299] D_loss: -0.0559, G_loss: 0.0731\n",
      "  Batch [840/1299] D_loss: -0.8396, G_loss: -0.1183\n",
      "  Batch [850/1299] D_loss: -0.0572, G_loss: 0.1432\n",
      "  Batch [860/1299] D_loss: -0.0751, G_loss: 0.0750\n",
      "  Batch [870/1299] D_loss: -0.0523, G_loss: 0.2081\n",
      "  Batch [880/1299] D_loss: -0.0926, G_loss: 0.3093\n",
      "  Batch [890/1299] D_loss: -0.0337, G_loss: 0.3786\n",
      "  Batch [900/1299] D_loss: -0.1167, G_loss: 0.4229\n",
      "  Batch [910/1299] D_loss: -2.3364, G_loss: -1.2002\n",
      "  Batch [920/1299] D_loss: 0.0142, G_loss: 0.1451\n",
      "  Batch [930/1299] D_loss: -0.1389, G_loss: 0.0897\n",
      "  Batch [940/1299] D_loss: -0.6112, G_loss: -0.3521\n",
      "  Batch [950/1299] D_loss: -0.1462, G_loss: 0.1582\n",
      "  Batch [960/1299] D_loss: 0.0147, G_loss: 0.3660\n",
      "  Batch [970/1299] D_loss: 0.0396, G_loss: 0.4979\n",
      "  Batch [980/1299] D_loss: -0.0519, G_loss: 0.5156\n",
      "  Batch [990/1299] D_loss: 0.0116, G_loss: 0.4376\n",
      "  Batch [1000/1299] D_loss: -0.1067, G_loss: 0.0422\n",
      "  Batch [1010/1299] D_loss: -0.0515, G_loss: 0.1219\n",
      "  Batch [1020/1299] D_loss: -0.3057, G_loss: -1.1189\n",
      "  Batch [1030/1299] D_loss: -1.0513, G_loss: -2.0702\n",
      "  Batch [1040/1299] D_loss: -0.5238, G_loss: 0.0374\n",
      "  Batch [1050/1299] D_loss: -0.2846, G_loss: 0.0942\n",
      "  Batch [1060/1299] D_loss: -0.2412, G_loss: 0.0117\n",
      "  Batch [1070/1299] D_loss: -0.0181, G_loss: 0.2202\n",
      "  Batch [1080/1299] D_loss: -0.0850, G_loss: 0.5884\n",
      "  Batch [1090/1299] D_loss: -0.1806, G_loss: 0.7461\n",
      "  Batch [1100/1299] D_loss: -0.0160, G_loss: 0.7926\n",
      "  Batch [1110/1299] D_loss: -0.2686, G_loss: 0.8499\n",
      "  Batch [1120/1299] D_loss: 0.0055, G_loss: 0.5020\n",
      "  Batch [1130/1299] D_loss: -1.2111, G_loss: -3.2546\n",
      "  Batch [1140/1299] D_loss: -0.3657, G_loss: -0.4157\n",
      "  Batch [1150/1299] D_loss: -0.0126, G_loss: 0.1274\n",
      "  Batch [1160/1299] D_loss: -0.0425, G_loss: 0.1084\n",
      "  Batch [1170/1299] D_loss: -0.0287, G_loss: 0.1464\n",
      "  Batch [1180/1299] D_loss: -0.0850, G_loss: 0.3708\n",
      "  Batch [1190/1299] D_loss: -0.1275, G_loss: 0.5231\n",
      "  Batch [1200/1299] D_loss: -0.0163, G_loss: 0.5499\n",
      "  Batch [1210/1299] D_loss: -0.0504, G_loss: 0.5953\n",
      "  Batch [1220/1299] D_loss: -0.0791, G_loss: 0.4444\n",
      "  Batch [1230/1299] D_loss: -0.0329, G_loss: 0.3582\n",
      "  Batch [1240/1299] D_loss: -0.0963, G_loss: -0.1023\n",
      "  Batch [1250/1299] D_loss: -0.3866, G_loss: 0.0506\n",
      "  Batch [1260/1299] D_loss: -0.3890, G_loss: -0.7143\n",
      "  Batch [1270/1299] D_loss: -0.7539, G_loss: -1.6703\n",
      "  Batch [1280/1299] D_loss: -0.0314, G_loss: 0.2810\n",
      "  Batch [1290/1299] D_loss: -0.1470, G_loss: 0.4591\n",
      "\n",
      "Epoch 77 Summary:\n",
      "  Average D_loss: -0.1648\n",
      "  Average G_loss: -0.0652\n",
      "\n",
      "Epoch [78/100]\n",
      "  Batch [0/1299] D_loss: -0.1870, G_loss: 0.6186\n",
      "  Batch [10/1299] D_loss: -0.0869, G_loss: 0.6568\n",
      "  Batch [20/1299] D_loss: -0.0418, G_loss: 0.4463\n",
      "  Batch [30/1299] D_loss: -0.0466, G_loss: 0.2825\n",
      "  Batch [40/1299] D_loss: -0.7861, G_loss: -0.8304\n",
      "  Batch [50/1299] D_loss: -0.0061, G_loss: 0.1471\n",
      "  Batch [60/1299] D_loss: -0.3121, G_loss: 0.0761\n",
      "  Batch [70/1299] D_loss: -0.8445, G_loss: -0.1759\n",
      "  Batch [80/1299] D_loss: -0.5121, G_loss: -1.0459\n",
      "  Batch [90/1299] D_loss: -0.0009, G_loss: 0.2548\n",
      "  Batch [100/1299] D_loss: 0.0285, G_loss: 0.4565\n",
      "  Batch [110/1299] D_loss: -0.1766, G_loss: 0.5832\n",
      "  Batch [120/1299] D_loss: 0.0087, G_loss: 0.4363\n",
      "  Batch [130/1299] D_loss: -0.0052, G_loss: 0.4364\n",
      "  Batch [140/1299] D_loss: -0.1099, G_loss: 0.4092\n",
      "  Batch [150/1299] D_loss: -0.0524, G_loss: 0.2488\n",
      "  Batch [160/1299] D_loss: -0.3694, G_loss: -0.0499\n",
      "  Batch [170/1299] D_loss: -0.1140, G_loss: 0.0515\n",
      "  Batch [180/1299] D_loss: 0.0045, G_loss: 0.1587\n",
      "  Batch [190/1299] D_loss: -1.1195, G_loss: -2.2734\n",
      "  Batch [200/1299] D_loss: -0.4295, G_loss: -0.6285\n",
      "  Batch [210/1299] D_loss: -0.7387, G_loss: 0.0784\n",
      "  Batch [220/1299] D_loss: -0.4616, G_loss: 0.0831\n",
      "  Batch [230/1299] D_loss: -0.0100, G_loss: 0.1910\n",
      "  Batch [240/1299] D_loss: -0.1097, G_loss: 0.3806\n",
      "  Batch [250/1299] D_loss: -0.2340, G_loss: 0.5516\n",
      "  Batch [260/1299] D_loss: -0.0022, G_loss: 0.7559\n",
      "  Batch [270/1299] D_loss: -0.1425, G_loss: 0.6430\n",
      "  Batch [280/1299] D_loss: -0.1247, G_loss: 0.5784\n",
      "  Batch [290/1299] D_loss: -0.0431, G_loss: 0.4873\n",
      "  Batch [300/1299] D_loss: -6.0609, G_loss: -9.5423\n",
      "  Batch [310/1299] D_loss: -0.1796, G_loss: -0.2531\n",
      "  Batch [320/1299] D_loss: -0.5469, G_loss: -0.4746\n",
      "  Batch [330/1299] D_loss: 0.0210, G_loss: 0.1587\n",
      "  Batch [340/1299] D_loss: -0.2517, G_loss: 0.0415\n",
      "  Batch [350/1299] D_loss: -0.0099, G_loss: 0.3175\n",
      "  Batch [360/1299] D_loss: 0.0052, G_loss: 0.4005\n",
      "  Batch [370/1299] D_loss: -0.0887, G_loss: 0.4865\n",
      "  Batch [380/1299] D_loss: -0.0380, G_loss: 0.6252\n",
      "  Batch [390/1299] D_loss: -0.0703, G_loss: 0.4255\n",
      "  Batch [400/1299] D_loss: -2.0685, G_loss: -3.3143\n",
      "  Batch [410/1299] D_loss: -1.3272, G_loss: -1.6609\n",
      "  Batch [420/1299] D_loss: -0.0522, G_loss: 0.2124\n",
      "  Batch [430/1299] D_loss: -1.0112, G_loss: -2.5580\n",
      "  Batch [440/1299] D_loss: -0.4029, G_loss: -0.1645\n",
      "  Batch [450/1299] D_loss: -1.0755, G_loss: -0.3380\n",
      "  Batch [460/1299] D_loss: -0.1689, G_loss: 0.0037\n",
      "  Batch [470/1299] D_loss: -0.4442, G_loss: 0.1501\n",
      "  Batch [480/1299] D_loss: -1.4518, G_loss: -0.1356\n",
      "  Batch [490/1299] D_loss: -0.4967, G_loss: 0.2180\n",
      "  Batch [500/1299] D_loss: -0.0752, G_loss: 0.4098\n",
      "  Batch [510/1299] D_loss: -0.0591, G_loss: 0.5571\n",
      "  Batch [520/1299] D_loss: -0.3057, G_loss: 0.6421\n",
      "  Batch [530/1299] D_loss: -0.0893, G_loss: 0.7449\n",
      "  Batch [540/1299] D_loss: -0.1284, G_loss: 0.5454\n",
      "  Batch [550/1299] D_loss: 0.0444, G_loss: 0.5046\n",
      "  Batch [560/1299] D_loss: -1.1872, G_loss: -3.8917\n",
      "  Batch [570/1299] D_loss: -0.0201, G_loss: 0.1232\n",
      "  Batch [580/1299] D_loss: 0.0121, G_loss: 0.2577\n",
      "  Batch [590/1299] D_loss: -0.0694, G_loss: 0.4303\n",
      "  Batch [600/1299] D_loss: -0.1484, G_loss: 0.6310\n",
      "  Batch [610/1299] D_loss: -0.0558, G_loss: 0.4868\n",
      "  Batch [620/1299] D_loss: 0.0624, G_loss: 0.3926\n",
      "  Batch [630/1299] D_loss: -0.0604, G_loss: 0.3677\n",
      "  Batch [640/1299] D_loss: 0.0633, G_loss: 0.1020\n",
      "  Batch [650/1299] D_loss: -0.0161, G_loss: 0.2745\n",
      "  Batch [660/1299] D_loss: -0.0244, G_loss: 0.3508\n",
      "  Batch [670/1299] D_loss: -0.0294, G_loss: 0.2635\n",
      "  Batch [680/1299] D_loss: -0.0802, G_loss: 0.3485\n",
      "  Batch [690/1299] D_loss: -0.0669, G_loss: 0.3035\n",
      "  Batch [700/1299] D_loss: -1.4396, G_loss: -1.7543\n",
      "  Batch [710/1299] D_loss: -0.5931, G_loss: -1.0671\n",
      "  Batch [720/1299] D_loss: 0.0016, G_loss: 0.1769\n",
      "  Batch [730/1299] D_loss: 0.0171, G_loss: 0.3006\n",
      "  Batch [740/1299] D_loss: -0.0219, G_loss: 0.3082\n",
      "  Batch [750/1299] D_loss: -0.0509, G_loss: 0.3145\n",
      "  Batch [760/1299] D_loss: -0.0509, G_loss: 0.3403\n",
      "  Batch [770/1299] D_loss: -3.5675, G_loss: -6.7279\n",
      "  Batch [780/1299] D_loss: -0.0700, G_loss: 0.0915\n",
      "  Batch [790/1299] D_loss: -0.8659, G_loss: -0.4040\n",
      "  Batch [800/1299] D_loss: -2.1178, G_loss: -4.1379\n",
      "  Batch [810/1299] D_loss: -0.0607, G_loss: 0.2775\n",
      "  Batch [820/1299] D_loss: -0.1361, G_loss: 0.4432\n",
      "  Batch [830/1299] D_loss: -0.0692, G_loss: 0.6211\n",
      "  Batch [840/1299] D_loss: -0.0016, G_loss: 0.6759\n",
      "  Batch [850/1299] D_loss: -0.0780, G_loss: 0.6559\n",
      "  Batch [860/1299] D_loss: -0.1926, G_loss: 0.6106\n",
      "  Batch [870/1299] D_loss: 0.0682, G_loss: 0.3551\n",
      "  Batch [880/1299] D_loss: -0.0182, G_loss: 0.2321\n",
      "  Batch [890/1299] D_loss: -1.8727, G_loss: -1.8454\n",
      "  Batch [900/1299] D_loss: -0.0222, G_loss: 0.1560\n",
      "  Batch [910/1299] D_loss: 0.0287, G_loss: 0.1447\n",
      "  Batch [920/1299] D_loss: -0.0172, G_loss: 0.1578\n",
      "  Batch [930/1299] D_loss: -1.4921, G_loss: -4.6256\n",
      "  Batch [940/1299] D_loss: -0.0483, G_loss: 0.2078\n",
      "  Batch [950/1299] D_loss: -0.0399, G_loss: 0.3416\n",
      "  Batch [960/1299] D_loss: -0.0130, G_loss: 0.5271\n",
      "  Batch [970/1299] D_loss: 0.0365, G_loss: 0.4396\n",
      "  Batch [980/1299] D_loss: -0.1511, G_loss: 0.4732\n",
      "  Batch [990/1299] D_loss: -2.1548, G_loss: -1.4194\n",
      "  Batch [1000/1299] D_loss: -0.3043, G_loss: -0.0342\n",
      "  Batch [1010/1299] D_loss: -0.6539, G_loss: -0.6010\n",
      "  Batch [1020/1299] D_loss: -0.0716, G_loss: 0.3240\n",
      "  Batch [1030/1299] D_loss: -0.1800, G_loss: 0.4406\n",
      "  Batch [1040/1299] D_loss: -0.0096, G_loss: 0.3327\n",
      "  Batch [1050/1299] D_loss: -0.0580, G_loss: 0.4556\n",
      "  Batch [1060/1299] D_loss: -0.0393, G_loss: 0.3040\n",
      "  Batch [1070/1299] D_loss: 0.0091, G_loss: 0.0980\n",
      "  Batch [1080/1299] D_loss: 0.0138, G_loss: 0.1546\n",
      "  Batch [1090/1299] D_loss: -0.0883, G_loss: 0.3029\n",
      "  Batch [1100/1299] D_loss: -0.0780, G_loss: 0.3518\n",
      "  Batch [1110/1299] D_loss: -2.3887, G_loss: -3.4172\n",
      "  Batch [1120/1299] D_loss: -0.0387, G_loss: 0.1872\n",
      "  Batch [1130/1299] D_loss: -0.0844, G_loss: 0.3180\n",
      "  Batch [1140/1299] D_loss: -0.0227, G_loss: 0.4251\n",
      "  Batch [1150/1299] D_loss: -0.1156, G_loss: 0.4545\n",
      "  Batch [1160/1299] D_loss: 0.0105, G_loss: 0.3478\n",
      "  Batch [1170/1299] D_loss: -0.0806, G_loss: 0.3850\n",
      "  Batch [1180/1299] D_loss: -0.9198, G_loss: -1.6478\n",
      "  Batch [1190/1299] D_loss: 0.0043, G_loss: 0.1268\n",
      "  Batch [1200/1299] D_loss: -0.9924, G_loss: -0.5091\n",
      "  Batch [1210/1299] D_loss: -0.0358, G_loss: 0.1464\n",
      "  Batch [1220/1299] D_loss: -0.0210, G_loss: 0.0998\n",
      "  Batch [1230/1299] D_loss: -0.5473, G_loss: -1.8059\n",
      "  Batch [1240/1299] D_loss: -1.5916, G_loss: -2.1449\n",
      "  Batch [1250/1299] D_loss: -0.1463, G_loss: 0.1021\n",
      "  Batch [1260/1299] D_loss: -0.0449, G_loss: 0.3216\n",
      "  Batch [1270/1299] D_loss: -0.1224, G_loss: 0.6424\n",
      "  Batch [1280/1299] D_loss: -0.2962, G_loss: 0.6688\n",
      "  Batch [1290/1299] D_loss: -0.1387, G_loss: 0.8298\n",
      "\n",
      "Epoch 78 Summary:\n",
      "  Average D_loss: -0.1586\n",
      "  Average G_loss: -0.0686\n",
      "\n",
      "Epoch [79/100]\n",
      "  Batch [0/1299] D_loss: -0.1882, G_loss: 0.9146\n",
      "  Batch [10/1299] D_loss: -0.0408, G_loss: 0.7083\n",
      "  Batch [20/1299] D_loss: 0.0017, G_loss: 0.2350\n",
      "  Batch [30/1299] D_loss: -0.4043, G_loss: -0.9584\n",
      "  Batch [40/1299] D_loss: -0.2249, G_loss: -0.1660\n",
      "  Batch [50/1299] D_loss: -0.0199, G_loss: 0.2276\n",
      "  Batch [60/1299] D_loss: -0.0608, G_loss: 0.3138\n",
      "  Batch [70/1299] D_loss: -0.0796, G_loss: 0.3460\n",
      "  Batch [80/1299] D_loss: -0.1824, G_loss: 0.2676\n",
      "  Batch [90/1299] D_loss: -0.5736, G_loss: -0.7470\n",
      "  Batch [100/1299] D_loss: -0.0428, G_loss: 0.2157\n",
      "  Batch [110/1299] D_loss: -0.0728, G_loss: 0.3426\n",
      "  Batch [120/1299] D_loss: -0.0533, G_loss: 0.2177\n",
      "  Batch [130/1299] D_loss: -0.0833, G_loss: 0.4002\n",
      "  Batch [140/1299] D_loss: -0.0370, G_loss: 0.1848\n",
      "  Batch [150/1299] D_loss: -0.4561, G_loss: -0.5567\n",
      "  Batch [160/1299] D_loss: -0.6241, G_loss: -0.0967\n",
      "  Batch [170/1299] D_loss: -0.4519, G_loss: -0.2549\n",
      "  Batch [180/1299] D_loss: -0.0552, G_loss: 0.2141\n",
      "  Batch [190/1299] D_loss: -0.0547, G_loss: 0.3747\n",
      "  Batch [200/1299] D_loss: -0.0772, G_loss: 0.6276\n",
      "  Batch [210/1299] D_loss: -0.2148, G_loss: 0.5490\n",
      "  Batch [220/1299] D_loss: -0.1734, G_loss: 0.6019\n",
      "  Batch [230/1299] D_loss: -0.1900, G_loss: 0.4664\n",
      "  Batch [240/1299] D_loss: -0.7984, G_loss: -0.1227\n",
      "  Batch [250/1299] D_loss: 0.2096, G_loss: -0.8467\n",
      "  Batch [260/1299] D_loss: -0.4623, G_loss: -0.1666\n",
      "  Batch [270/1299] D_loss: -0.0732, G_loss: 0.2648\n",
      "  Batch [280/1299] D_loss: -0.0579, G_loss: 0.4887\n",
      "  Batch [290/1299] D_loss: -0.0808, G_loss: 0.4727\n",
      "  Batch [300/1299] D_loss: -0.0415, G_loss: 0.5570\n",
      "  Batch [310/1299] D_loss: -0.1097, G_loss: 0.4836\n",
      "  Batch [320/1299] D_loss: -0.0677, G_loss: 0.4059\n",
      "  Batch [330/1299] D_loss: -0.2522, G_loss: -0.3330\n",
      "  Batch [340/1299] D_loss: -0.1445, G_loss: 0.0227\n",
      "  Batch [350/1299] D_loss: -0.3134, G_loss: 0.0503\n",
      "  Batch [360/1299] D_loss: -0.3885, G_loss: 0.0364\n",
      "  Batch [370/1299] D_loss: -0.4697, G_loss: -0.0918\n",
      "  Batch [380/1299] D_loss: -0.1906, G_loss: 0.1592\n",
      "  Batch [390/1299] D_loss: -0.0581, G_loss: 0.2180\n",
      "  Batch [400/1299] D_loss: -0.1374, G_loss: 0.6200\n",
      "  Batch [410/1299] D_loss: -0.0706, G_loss: 0.7696\n",
      "  Batch [420/1299] D_loss: -0.0305, G_loss: 0.5853\n",
      "  Batch [430/1299] D_loss: -0.1502, G_loss: 0.6261\n",
      "  Batch [440/1299] D_loss: -0.0704, G_loss: 0.4288\n",
      "  Batch [450/1299] D_loss: -0.9852, G_loss: -2.1181\n",
      "  Batch [460/1299] D_loss: -0.0132, G_loss: 0.2396\n",
      "  Batch [470/1299] D_loss: -0.0468, G_loss: 0.2386\n",
      "  Batch [480/1299] D_loss: -0.0776, G_loss: 0.3368\n",
      "  Batch [490/1299] D_loss: -0.1458, G_loss: 0.2961\n",
      "  Batch [500/1299] D_loss: -0.0179, G_loss: 0.1682\n",
      "  Batch [510/1299] D_loss: -0.2700, G_loss: -0.4906\n",
      "  Batch [520/1299] D_loss: -0.0221, G_loss: 0.4350\n",
      "  Batch [530/1299] D_loss: -0.0739, G_loss: 0.4313\n",
      "  Batch [540/1299] D_loss: -0.1383, G_loss: 0.4551\n",
      "  Batch [550/1299] D_loss: -0.0869, G_loss: 0.4194\n",
      "  Batch [560/1299] D_loss: -0.0244, G_loss: 0.2455\n",
      "  Batch [570/1299] D_loss: -1.1283, G_loss: -1.1201\n",
      "  Batch [580/1299] D_loss: -0.0088, G_loss: 0.1087\n",
      "  Batch [590/1299] D_loss: -1.0934, G_loss: -0.5477\n",
      "  Batch [600/1299] D_loss: -0.5594, G_loss: -0.3036\n",
      "  Batch [610/1299] D_loss: -0.6197, G_loss: -0.1330\n",
      "  Batch [620/1299] D_loss: -0.8608, G_loss: -0.1918\n",
      "  Batch [630/1299] D_loss: -0.1275, G_loss: 0.3022\n",
      "  Batch [640/1299] D_loss: 0.0638, G_loss: 0.4333\n",
      "  Batch [650/1299] D_loss: -0.0817, G_loss: 0.5032\n",
      "  Batch [660/1299] D_loss: -0.1740, G_loss: 0.5443\n",
      "  Batch [670/1299] D_loss: 0.0750, G_loss: 0.4105\n",
      "  Batch [680/1299] D_loss: -0.0116, G_loss: 0.2416\n",
      "  Batch [690/1299] D_loss: -1.2328, G_loss: -0.0976\n",
      "  Batch [700/1299] D_loss: -1.3402, G_loss: -0.3952\n",
      "  Batch [710/1299] D_loss: -0.0527, G_loss: 0.2021\n",
      "  Batch [720/1299] D_loss: -0.0796, G_loss: 0.3181\n",
      "  Batch [730/1299] D_loss: -0.0290, G_loss: 0.4246\n",
      "  Batch [740/1299] D_loss: -0.1348, G_loss: 0.7015\n",
      "  Batch [750/1299] D_loss: -0.1767, G_loss: 0.5089\n",
      "  Batch [760/1299] D_loss: -0.0370, G_loss: 0.2299\n",
      "  Batch [770/1299] D_loss: -1.1168, G_loss: -1.4242\n",
      "  Batch [780/1299] D_loss: -0.0612, G_loss: 0.2736\n",
      "  Batch [790/1299] D_loss: -0.2783, G_loss: 0.4792\n",
      "  Batch [800/1299] D_loss: -0.1072, G_loss: 0.4752\n",
      "  Batch [810/1299] D_loss: -0.1957, G_loss: 0.7066\n",
      "  Batch [820/1299] D_loss: -0.1116, G_loss: 0.6464\n",
      "  Batch [830/1299] D_loss: -0.0435, G_loss: 0.3835\n",
      "  Batch [840/1299] D_loss: -2.4339, G_loss: -7.3438\n",
      "  Batch [850/1299] D_loss: -0.3022, G_loss: -0.5069\n",
      "  Batch [860/1299] D_loss: -0.0372, G_loss: 0.2372\n",
      "  Batch [870/1299] D_loss: -0.1245, G_loss: 0.3572\n",
      "  Batch [880/1299] D_loss: -0.1127, G_loss: 0.3840\n",
      "  Batch [890/1299] D_loss: 0.0089, G_loss: 0.4004\n",
      "  Batch [900/1299] D_loss: -0.1041, G_loss: 0.3669\n",
      "  Batch [910/1299] D_loss: 0.0199, G_loss: 0.1126\n",
      "  Batch [920/1299] D_loss: -0.0514, G_loss: 0.1844\n",
      "  Batch [930/1299] D_loss: -0.0439, G_loss: 0.2959\n",
      "  Batch [940/1299] D_loss: -0.2288, G_loss: 0.5756\n",
      "  Batch [950/1299] D_loss: -0.0452, G_loss: 0.4736\n",
      "  Batch [960/1299] D_loss: -0.0050, G_loss: 0.3678\n",
      "  Batch [970/1299] D_loss: -0.0189, G_loss: 0.2726\n",
      "  Batch [980/1299] D_loss: -4.1582, G_loss: -5.2613\n",
      "  Batch [990/1299] D_loss: -0.0390, G_loss: 0.0953\n",
      "  Batch [1000/1299] D_loss: -0.4275, G_loss: -0.1158\n",
      "  Batch [1010/1299] D_loss: -1.9667, G_loss: -5.2143\n",
      "  Batch [1020/1299] D_loss: -0.0225, G_loss: 0.1717\n",
      "  Batch [1030/1299] D_loss: -0.1673, G_loss: 0.1702\n",
      "  Batch [1040/1299] D_loss: -0.0783, G_loss: 0.3852\n",
      "  Batch [1050/1299] D_loss: -0.1090, G_loss: 0.4436\n",
      "  Batch [1060/1299] D_loss: -0.0595, G_loss: 0.2700\n",
      "  Batch [1070/1299] D_loss: -0.4991, G_loss: -0.5196\n",
      "  Batch [1080/1299] D_loss: 0.0260, G_loss: 0.1282\n",
      "  Batch [1090/1299] D_loss: -0.0283, G_loss: 0.2105\n",
      "  Batch [1100/1299] D_loss: -0.0507, G_loss: 0.3017\n",
      "  Batch [1110/1299] D_loss: 0.0177, G_loss: 0.3619\n",
      "  Batch [1120/1299] D_loss: 0.0111, G_loss: 0.2945\n",
      "  Batch [1130/1299] D_loss: -1.6325, G_loss: -4.2830\n",
      "  Batch [1140/1299] D_loss: -0.2492, G_loss: -0.4731\n",
      "  Batch [1150/1299] D_loss: -0.0436, G_loss: 0.1685\n",
      "  Batch [1160/1299] D_loss: 0.0214, G_loss: 0.2499\n",
      "  Batch [1170/1299] D_loss: -0.1198, G_loss: 0.3785\n",
      "  Batch [1180/1299] D_loss: -0.0498, G_loss: 0.4648\n",
      "  Batch [1190/1299] D_loss: 0.0026, G_loss: 0.2794\n",
      "  Batch [1200/1299] D_loss: -0.2380, G_loss: -0.0307\n",
      "  Batch [1210/1299] D_loss: -0.0117, G_loss: 0.1333\n",
      "  Batch [1220/1299] D_loss: -0.4236, G_loss: 0.0393\n",
      "  Batch [1230/1299] D_loss: -1.4251, G_loss: -0.6080\n",
      "  Batch [1240/1299] D_loss: -1.0321, G_loss: 0.0046\n",
      "  Batch [1250/1299] D_loss: 0.0204, G_loss: 0.1063\n",
      "  Batch [1260/1299] D_loss: -0.0321, G_loss: 0.3352\n",
      "  Batch [1270/1299] D_loss: 0.1189, G_loss: 0.4677\n",
      "  Batch [1280/1299] D_loss: -0.2217, G_loss: 0.6354\n",
      "  Batch [1290/1299] D_loss: -0.2145, G_loss: 0.6818\n",
      "\n",
      "Epoch 79 Summary:\n",
      "  Average D_loss: -0.1438\n",
      "  Average G_loss: -0.0711\n",
      "\n",
      "Epoch [80/100]\n",
      "  Batch [0/1299] D_loss: -0.0822, G_loss: 0.4096\n",
      "  Batch [10/1299] D_loss: -0.0637, G_loss: 0.3757\n",
      "  Batch [20/1299] D_loss: -0.0585, G_loss: 0.1480\n",
      "  Batch [30/1299] D_loss: -0.6090, G_loss: -0.9161\n",
      "  Batch [40/1299] D_loss: -0.0140, G_loss: 0.2494\n",
      "  Batch [50/1299] D_loss: -0.2721, G_loss: 0.1395\n",
      "  Batch [60/1299] D_loss: -0.0004, G_loss: 0.1455\n",
      "  Batch [70/1299] D_loss: -0.0212, G_loss: 0.2346\n",
      "  Batch [80/1299] D_loss: -0.1064, G_loss: 0.3623\n",
      "  Batch [90/1299] D_loss: -0.0355, G_loss: 0.4394\n",
      "  Batch [100/1299] D_loss: -0.0883, G_loss: 0.5040\n",
      "  Batch [110/1299] D_loss: -0.1834, G_loss: 0.4788\n",
      "  Batch [120/1299] D_loss: -0.0747, G_loss: 0.2770\n",
      "  Batch [130/1299] D_loss: -1.5161, G_loss: -1.9494\n",
      "  Batch [140/1299] D_loss: -0.4930, G_loss: -2.4482\n",
      "  Batch [150/1299] D_loss: -0.0419, G_loss: 0.2375\n",
      "  Batch [160/1299] D_loss: -0.1083, G_loss: 0.4926\n",
      "  Batch [170/1299] D_loss: -0.0370, G_loss: 0.5070\n",
      "  Batch [180/1299] D_loss: -0.1310, G_loss: 0.6674\n",
      "  Batch [190/1299] D_loss: -0.0186, G_loss: 0.3271\n",
      "  Batch [200/1299] D_loss: -0.0586, G_loss: 0.2712\n",
      "  Batch [210/1299] D_loss: -0.3922, G_loss: -2.4814\n",
      "  Batch [220/1299] D_loss: -0.2411, G_loss: -0.0768\n",
      "  Batch [230/1299] D_loss: -0.0420, G_loss: 0.1234\n",
      "  Batch [240/1299] D_loss: -0.0700, G_loss: 0.1472\n",
      "  Batch [250/1299] D_loss: -0.0278, G_loss: 0.2973\n",
      "  Batch [260/1299] D_loss: -0.0663, G_loss: 0.4206\n",
      "  Batch [270/1299] D_loss: -0.0763, G_loss: 0.5151\n",
      "  Batch [280/1299] D_loss: -0.0848, G_loss: 0.3915\n",
      "  Batch [290/1299] D_loss: -0.0171, G_loss: 0.4150\n",
      "  Batch [300/1299] D_loss: -0.0288, G_loss: 0.3317\n",
      "  Batch [310/1299] D_loss: -0.8790, G_loss: -1.6468\n",
      "  Batch [320/1299] D_loss: -0.5046, G_loss: -0.0259\n",
      "  Batch [330/1299] D_loss: -1.0796, G_loss: -2.5193\n",
      "  Batch [340/1299] D_loss: -0.8679, G_loss: -0.3577\n",
      "  Batch [350/1299] D_loss: -0.3703, G_loss: 0.0871\n",
      "  Batch [360/1299] D_loss: -0.7986, G_loss: -0.4652\n",
      "  Batch [370/1299] D_loss: -0.0836, G_loss: 0.4310\n",
      "  Batch [380/1299] D_loss: -0.0622, G_loss: 0.6913\n",
      "  Batch [390/1299] D_loss: -0.2069, G_loss: 0.7158\n",
      "  Batch [400/1299] D_loss: -0.1253, G_loss: 0.9071\n",
      "  Batch [410/1299] D_loss: -0.0450, G_loss: 0.7723\n",
      "  Batch [420/1299] D_loss: -0.0190, G_loss: 0.4309\n",
      "  Batch [430/1299] D_loss: 0.0428, G_loss: 0.3242\n",
      "  Batch [440/1299] D_loss: -0.5212, G_loss: -1.4438\n",
      "  Batch [450/1299] D_loss: -0.0949, G_loss: 0.1467\n",
      "  Batch [460/1299] D_loss: -0.3798, G_loss: -0.1145\n",
      "  Batch [470/1299] D_loss: -0.8224, G_loss: -1.1530\n",
      "  Batch [480/1299] D_loss: -0.0474, G_loss: 0.2791\n",
      "  Batch [490/1299] D_loss: -0.0785, G_loss: 0.3919\n",
      "  Batch [500/1299] D_loss: 0.0037, G_loss: 0.5154\n",
      "  Batch [510/1299] D_loss: -0.1088, G_loss: 0.5539\n",
      "  Batch [520/1299] D_loss: -0.0808, G_loss: 0.4793\n",
      "  Batch [530/1299] D_loss: -0.0302, G_loss: 0.4242\n",
      "  Batch [540/1299] D_loss: -1.5639, G_loss: -1.7626\n",
      "  Batch [550/1299] D_loss: -0.0750, G_loss: 0.0650\n",
      "  Batch [560/1299] D_loss: -0.0295, G_loss: 0.1902\n",
      "  Batch [570/1299] D_loss: -0.0562, G_loss: 0.2944\n",
      "  Batch [580/1299] D_loss: -0.0272, G_loss: 0.2330\n",
      "  Batch [590/1299] D_loss: -0.1135, G_loss: 0.3738\n",
      "  Batch [600/1299] D_loss: 0.0062, G_loss: 0.1649\n",
      "  Batch [610/1299] D_loss: -0.1194, G_loss: -0.1307\n",
      "  Batch [620/1299] D_loss: -0.0351, G_loss: 0.3137\n",
      "  Batch [630/1299] D_loss: -0.0212, G_loss: 0.3924\n",
      "  Batch [640/1299] D_loss: -0.1400, G_loss: 0.4373\n",
      "  Batch [650/1299] D_loss: -0.0324, G_loss: 0.3787\n",
      "  Batch [660/1299] D_loss: 0.0275, G_loss: 0.2647\n",
      "  Batch [670/1299] D_loss: -0.8702, G_loss: -1.7436\n",
      "  Batch [680/1299] D_loss: -1.5881, G_loss: -3.1841\n",
      "  Batch [690/1299] D_loss: -0.5257, G_loss: -0.1859\n",
      "  Batch [700/1299] D_loss: -1.1691, G_loss: -0.9965\n",
      "  Batch [710/1299] D_loss: -0.0262, G_loss: 0.2487\n",
      "  Batch [720/1299] D_loss: -0.1669, G_loss: 0.5090\n",
      "  Batch [730/1299] D_loss: 0.0668, G_loss: 0.5005\n",
      "  Batch [740/1299] D_loss: -0.1242, G_loss: 0.5671\n",
      "  Batch [750/1299] D_loss: -0.0959, G_loss: 0.8186\n",
      "  Batch [760/1299] D_loss: -0.1113, G_loss: 0.4911\n",
      "  Batch [770/1299] D_loss: -0.0325, G_loss: 0.3542\n",
      "  Batch [780/1299] D_loss: -2.0102, G_loss: -3.4220\n",
      "  Batch [790/1299] D_loss: -0.4158, G_loss: -1.0501\n",
      "  Batch [800/1299] D_loss: -1.3384, G_loss: -6.1258\n",
      "  Batch [810/1299] D_loss: -0.0503, G_loss: 0.0753\n",
      "  Batch [820/1299] D_loss: -0.3548, G_loss: -0.1367\n",
      "  Batch [830/1299] D_loss: -0.0743, G_loss: 0.4530\n",
      "  Batch [840/1299] D_loss: -0.1135, G_loss: 0.6654\n",
      "  Batch [850/1299] D_loss: -0.1029, G_loss: 0.6421\n",
      "  Batch [860/1299] D_loss: -0.1571, G_loss: 0.6760\n",
      "  Batch [870/1299] D_loss: -0.1281, G_loss: 0.6925\n",
      "  Batch [880/1299] D_loss: -0.0072, G_loss: 0.2217\n",
      "  Batch [890/1299] D_loss: -0.4183, G_loss: -0.4458\n",
      "  Batch [900/1299] D_loss: -0.8393, G_loss: -0.3909\n",
      "  Batch [910/1299] D_loss: -0.2021, G_loss: 0.0579\n",
      "  Batch [920/1299] D_loss: -0.3483, G_loss: -0.2631\n",
      "  Batch [930/1299] D_loss: -0.0286, G_loss: 0.2402\n",
      "  Batch [940/1299] D_loss: -0.1843, G_loss: 0.3868\n",
      "  Batch [950/1299] D_loss: -0.0006, G_loss: 0.5604\n",
      "  Batch [960/1299] D_loss: -0.0101, G_loss: 0.4160\n",
      "  Batch [970/1299] D_loss: 0.0059, G_loss: 0.5731\n",
      "  Batch [980/1299] D_loss: -0.0373, G_loss: 0.2337\n",
      "  Batch [990/1299] D_loss: -0.2194, G_loss: -0.2064\n",
      "  Batch [1000/1299] D_loss: -0.6953, G_loss: -0.8070\n",
      "  Batch [1010/1299] D_loss: -0.8097, G_loss: -0.2238\n",
      "  Batch [1020/1299] D_loss: -0.0015, G_loss: 0.1021\n",
      "  Batch [1030/1299] D_loss: -0.0843, G_loss: 0.2490\n",
      "  Batch [1040/1299] D_loss: -0.1018, G_loss: 0.4637\n",
      "  Batch [1050/1299] D_loss: -0.1897, G_loss: 0.6165\n",
      "  Batch [1060/1299] D_loss: 0.0036, G_loss: 0.4875\n",
      "  Batch [1070/1299] D_loss: -0.0885, G_loss: 0.4299\n",
      "  Batch [1080/1299] D_loss: 0.0127, G_loss: 0.3593\n",
      "  Batch [1090/1299] D_loss: 0.0229, G_loss: 0.2200\n",
      "  Batch [1100/1299] D_loss: -0.5701, G_loss: -0.7820\n",
      "  Batch [1110/1299] D_loss: -0.3541, G_loss: -0.7523\n",
      "  Batch [1120/1299] D_loss: -0.1276, G_loss: -0.0475\n",
      "  Batch [1130/1299] D_loss: -0.3548, G_loss: 0.0589\n",
      "  Batch [1140/1299] D_loss: -0.6263, G_loss: -0.7205\n",
      "  Batch [1150/1299] D_loss: -0.0383, G_loss: 0.2598\n",
      "  Batch [1160/1299] D_loss: -0.0803, G_loss: 0.3500\n",
      "  Batch [1170/1299] D_loss: 0.0299, G_loss: 0.4213\n",
      "  Batch [1180/1299] D_loss: -0.0689, G_loss: 0.5628\n",
      "  Batch [1190/1299] D_loss: -0.1176, G_loss: 0.6031\n",
      "  Batch [1200/1299] D_loss: -0.0199, G_loss: 0.2901\n",
      "  Batch [1210/1299] D_loss: -3.5032, G_loss: -4.7550\n",
      "  Batch [1220/1299] D_loss: -0.0460, G_loss: 0.1458\n",
      "  Batch [1230/1299] D_loss: -0.0295, G_loss: 0.2568\n",
      "  Batch [1240/1299] D_loss: -0.0787, G_loss: 0.3583\n",
      "  Batch [1250/1299] D_loss: -0.0511, G_loss: 0.2500\n",
      "  Batch [1260/1299] D_loss: -0.7724, G_loss: -1.1068\n",
      "  Batch [1270/1299] D_loss: -0.6131, G_loss: -0.6176\n",
      "  Batch [1280/1299] D_loss: -0.1338, G_loss: 0.1266\n",
      "  Batch [1290/1299] D_loss: -0.0812, G_loss: 0.3215\n",
      "\n",
      "Epoch 80 Summary:\n",
      "  Average D_loss: -0.1569\n",
      "  Average G_loss: -0.0842\n",
      "\n",
      "Epoch [81/100]\n",
      "  Batch [0/1299] D_loss: -0.0127, G_loss: 0.3330\n",
      "  Batch [10/1299] D_loss: -0.0653, G_loss: 0.4457\n",
      "  Batch [20/1299] D_loss: -0.0528, G_loss: 0.4621\n",
      "  Batch [30/1299] D_loss: -0.0606, G_loss: 0.4541\n",
      "  Batch [40/1299] D_loss: -1.3948, G_loss: -3.7602\n",
      "  Batch [50/1299] D_loss: -0.2882, G_loss: 0.0587\n",
      "  Batch [60/1299] D_loss: -0.4074, G_loss: -0.8481\n",
      "  Batch [70/1299] D_loss: -0.0311, G_loss: 0.2098\n",
      "  Batch [80/1299] D_loss: -0.0818, G_loss: 0.5303\n",
      "  Batch [90/1299] D_loss: -0.0963, G_loss: 0.6371\n",
      "  Batch [100/1299] D_loss: -0.0713, G_loss: 0.5781\n",
      "  Batch [110/1299] D_loss: -0.2074, G_loss: 0.6168\n",
      "  Batch [120/1299] D_loss: -0.0690, G_loss: 0.5523\n",
      "  Batch [130/1299] D_loss: -3.5662, G_loss: -2.1074\n",
      "  Batch [140/1299] D_loss: -0.6404, G_loss: 0.0173\n",
      "  Batch [150/1299] D_loss: -0.1193, G_loss: 0.1168\n",
      "  Batch [160/1299] D_loss: -0.5397, G_loss: -0.0355\n",
      "  Batch [170/1299] D_loss: -1.2140, G_loss: -0.6494\n",
      "  Batch [180/1299] D_loss: -0.0161, G_loss: 0.2722\n",
      "  Batch [190/1299] D_loss: -0.0933, G_loss: 0.3699\n",
      "  Batch [200/1299] D_loss: -0.0967, G_loss: 0.4806\n",
      "  Batch [210/1299] D_loss: -0.0020, G_loss: 0.4615\n",
      "  Batch [220/1299] D_loss: -0.0058, G_loss: 0.4054\n",
      "  Batch [230/1299] D_loss: -0.0644, G_loss: 0.3558\n",
      "  Batch [240/1299] D_loss: -0.0481, G_loss: 0.3102\n",
      "  Batch [250/1299] D_loss: -0.5018, G_loss: -2.0982\n",
      "  Batch [260/1299] D_loss: -0.0495, G_loss: 0.1272\n",
      "  Batch [270/1299] D_loss: -0.5059, G_loss: 0.0047\n",
      "  Batch [280/1299] D_loss: -0.3549, G_loss: -0.2080\n",
      "  Batch [290/1299] D_loss: -0.0175, G_loss: 0.4105\n",
      "  Batch [300/1299] D_loss: -0.1052, G_loss: 0.5881\n",
      "  Batch [310/1299] D_loss: -0.0263, G_loss: 0.5971\n",
      "  Batch [320/1299] D_loss: -0.0172, G_loss: 0.5794\n",
      "  Batch [330/1299] D_loss: -0.0582, G_loss: 0.5323\n",
      "  Batch [340/1299] D_loss: -0.5257, G_loss: -3.8398\n",
      "  Batch [350/1299] D_loss: -0.3355, G_loss: -0.1944\n",
      "  Batch [360/1299] D_loss: -0.4689, G_loss: -1.0047\n",
      "  Batch [370/1299] D_loss: -0.0254, G_loss: 0.2198\n",
      "  Batch [380/1299] D_loss: -0.1835, G_loss: 0.2734\n",
      "  Batch [390/1299] D_loss: -0.0742, G_loss: 0.3201\n",
      "  Batch [400/1299] D_loss: -0.1278, G_loss: 0.5616\n",
      "  Batch [410/1299] D_loss: -0.0205, G_loss: 0.5451\n",
      "  Batch [420/1299] D_loss: -0.3025, G_loss: 0.1450\n",
      "  Batch [430/1299] D_loss: -0.3436, G_loss: -0.2316\n",
      "  Batch [440/1299] D_loss: -0.5331, G_loss: -1.6411\n",
      "  Batch [450/1299] D_loss: -1.4005, G_loss: -0.9579\n",
      "  Batch [460/1299] D_loss: -0.5745, G_loss: -0.2168\n",
      "  Batch [470/1299] D_loss: -0.1536, G_loss: 0.0987\n",
      "  Batch [480/1299] D_loss: -0.0586, G_loss: 0.2769\n",
      "  Batch [490/1299] D_loss: -0.1525, G_loss: 0.5270\n",
      "  Batch [500/1299] D_loss: -0.1498, G_loss: 0.6179\n",
      "  Batch [510/1299] D_loss: -0.0608, G_loss: 0.6926\n",
      "  Batch [520/1299] D_loss: -0.2275, G_loss: 0.7037\n",
      "  Batch [530/1299] D_loss: -0.0317, G_loss: 0.4985\n",
      "  Batch [540/1299] D_loss: -5.5576, G_loss: -7.9769\n",
      "  Batch [550/1299] D_loss: -0.3807, G_loss: -1.1298\n",
      "  Batch [560/1299] D_loss: -0.1496, G_loss: 0.1573\n",
      "  Batch [570/1299] D_loss: 0.0548, G_loss: 0.1753\n",
      "  Batch [580/1299] D_loss: 0.0031, G_loss: 0.4944\n",
      "  Batch [590/1299] D_loss: 0.0015, G_loss: 0.4621\n",
      "  Batch [600/1299] D_loss: -0.0159, G_loss: 0.5126\n",
      "  Batch [610/1299] D_loss: -0.0338, G_loss: 0.3730\n",
      "  Batch [620/1299] D_loss: -0.1222, G_loss: 0.3202\n",
      "  Batch [630/1299] D_loss: -0.2832, G_loss: -0.1450\n",
      "  Batch [640/1299] D_loss: -0.0825, G_loss: 0.1357\n",
      "  Batch [650/1299] D_loss: 0.0002, G_loss: 0.1357\n",
      "  Batch [660/1299] D_loss: -0.0239, G_loss: 0.2648\n",
      "  Batch [670/1299] D_loss: -0.0406, G_loss: 0.3452\n",
      "  Batch [680/1299] D_loss: -0.0412, G_loss: 0.3402\n",
      "  Batch [690/1299] D_loss: -0.0348, G_loss: 0.3681\n",
      "  Batch [700/1299] D_loss: -0.0815, G_loss: 0.3334\n",
      "  Batch [710/1299] D_loss: -1.9379, G_loss: -10.4242\n",
      "  Batch [720/1299] D_loss: -0.0405, G_loss: 0.0966\n",
      "  Batch [730/1299] D_loss: -0.0445, G_loss: 0.1977\n",
      "  Batch [740/1299] D_loss: -0.0815, G_loss: 0.3323\n",
      "  Batch [750/1299] D_loss: 0.0019, G_loss: 0.3300\n",
      "  Batch [760/1299] D_loss: -0.0106, G_loss: 0.4557\n",
      "  Batch [770/1299] D_loss: -0.0884, G_loss: 0.4343\n",
      "  Batch [780/1299] D_loss: -1.3492, G_loss: -3.5675\n",
      "  Batch [790/1299] D_loss: -0.9775, G_loss: -0.5207\n",
      "  Batch [800/1299] D_loss: -0.6109, G_loss: -0.0877\n",
      "  Batch [810/1299] D_loss: -0.4979, G_loss: 0.0221\n",
      "  Batch [820/1299] D_loss: 0.0198, G_loss: 0.3794\n",
      "  Batch [830/1299] D_loss: -0.2555, G_loss: 0.5911\n",
      "  Batch [840/1299] D_loss: -0.2652, G_loss: 0.6910\n",
      "  Batch [850/1299] D_loss: -0.1448, G_loss: 0.6360\n",
      "  Batch [860/1299] D_loss: 0.0146, G_loss: 0.2156\n",
      "  Batch [870/1299] D_loss: -0.1162, G_loss: 0.4795\n",
      "  Batch [880/1299] D_loss: -0.0055, G_loss: 0.2248\n",
      "  Batch [890/1299] D_loss: -0.1159, G_loss: -0.0202\n",
      "  Batch [900/1299] D_loss: -1.0747, G_loss: -1.0895\n",
      "  Batch [910/1299] D_loss: -0.1903, G_loss: 0.0937\n",
      "  Batch [920/1299] D_loss: -0.0034, G_loss: 0.1365\n",
      "  Batch [930/1299] D_loss: -0.0931, G_loss: 0.3149\n",
      "  Batch [940/1299] D_loss: -0.1257, G_loss: 0.4599\n",
      "  Batch [950/1299] D_loss: -0.1660, G_loss: 0.5322\n",
      "  Batch [960/1299] D_loss: -0.0718, G_loss: 0.6809\n",
      "  Batch [970/1299] D_loss: 0.0478, G_loss: 0.4973\n",
      "  Batch [980/1299] D_loss: -0.0032, G_loss: 0.5268\n",
      "  Batch [990/1299] D_loss: -2.2552, G_loss: -2.9306\n",
      "  Batch [1000/1299] D_loss: -0.5576, G_loss: -0.5418\n",
      "  Batch [1010/1299] D_loss: -0.1122, G_loss: 0.0968\n",
      "  Batch [1020/1299] D_loss: 0.0007, G_loss: 0.0063\n",
      "  Batch [1030/1299] D_loss: -1.5285, G_loss: -0.5525\n",
      "  Batch [1040/1299] D_loss: -0.9055, G_loss: -1.4138\n",
      "  Batch [1050/1299] D_loss: -0.1095, G_loss: 0.3858\n",
      "  Batch [1060/1299] D_loss: -0.1187, G_loss: 0.6527\n",
      "  Batch [1070/1299] D_loss: -0.1117, G_loss: 0.6760\n",
      "  Batch [1080/1299] D_loss: -0.1025, G_loss: 0.7555\n",
      "  Batch [1090/1299] D_loss: 0.0096, G_loss: 0.6396\n",
      "  Batch [1100/1299] D_loss: -0.1515, G_loss: 0.7580\n",
      "  Batch [1110/1299] D_loss: -0.0241, G_loss: 0.3165\n",
      "  Batch [1120/1299] D_loss: -0.5503, G_loss: -0.0745\n",
      "  Batch [1130/1299] D_loss: -0.5208, G_loss: -0.2744\n",
      "  Batch [1140/1299] D_loss: -0.1651, G_loss: -0.0039\n",
      "  Batch [1150/1299] D_loss: -0.3624, G_loss: 0.0350\n",
      "  Batch [1160/1299] D_loss: -0.5043, G_loss: -0.5970\n",
      "  Batch [1170/1299] D_loss: -0.0443, G_loss: 0.0960\n",
      "  Batch [1180/1299] D_loss: -0.7567, G_loss: -0.1506\n",
      "  Batch [1190/1299] D_loss: -0.0471, G_loss: 0.1736\n",
      "  Batch [1200/1299] D_loss: -0.0667, G_loss: 0.2575\n",
      "  Batch [1210/1299] D_loss: -0.1255, G_loss: 0.5032\n",
      "  Batch [1220/1299] D_loss: -0.1485, G_loss: 0.5583\n",
      "  Batch [1230/1299] D_loss: 0.0929, G_loss: 0.6391\n",
      "  Batch [1240/1299] D_loss: 0.0386, G_loss: 0.4702\n",
      "  Batch [1250/1299] D_loss: -0.0517, G_loss: 0.3140\n",
      "  Batch [1260/1299] D_loss: -0.4408, G_loss: -0.5276\n",
      "  Batch [1270/1299] D_loss: -0.2997, G_loss: -0.0097\n",
      "  Batch [1280/1299] D_loss: -0.7073, G_loss: -2.2804\n",
      "  Batch [1290/1299] D_loss: -0.5515, G_loss: -0.3576\n",
      "\n",
      "Epoch 81 Summary:\n",
      "  Average D_loss: -0.1630\n",
      "  Average G_loss: -0.1128\n",
      "\n",
      "Models saved at epoch 81:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250120_153549_dataset+cell_type/generator_20250120_153549_dataset+cell_type_epoch_81.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250120_153549_dataset+cell_type/discriminator_20250120_153549_dataset+cell_type_epoch_81.pt\n",
      "\n",
      "Epoch [82/100]\n",
      "  Batch [0/1299] D_loss: -0.1138, G_loss: 0.4183\n",
      "  Batch [10/1299] D_loss: -0.0556, G_loss: 0.5897\n",
      "  Batch [20/1299] D_loss: -0.1723, G_loss: 0.8961\n",
      "  Batch [30/1299] D_loss: -0.0308, G_loss: 0.9837\n",
      "  Batch [40/1299] D_loss: -0.1434, G_loss: 0.5508\n",
      "  Batch [50/1299] D_loss: -0.0464, G_loss: 0.4266\n",
      "  Batch [60/1299] D_loss: -0.4719, G_loss: -1.1075\n",
      "  Batch [70/1299] D_loss: -1.0581, G_loss: -0.9494\n",
      "  Batch [80/1299] D_loss: -0.0222, G_loss: 0.1935\n",
      "  Batch [90/1299] D_loss: -0.0259, G_loss: 0.3110\n",
      "  Batch [100/1299] D_loss: -0.0094, G_loss: 0.3227\n",
      "  Batch [110/1299] D_loss: 0.0334, G_loss: 0.3836\n",
      "  Batch [120/1299] D_loss: -0.0345, G_loss: 0.4202\n",
      "  Batch [130/1299] D_loss: -0.0687, G_loss: 0.2483\n",
      "  Batch [140/1299] D_loss: -0.0532, G_loss: 0.1976\n",
      "  Batch [150/1299] D_loss: 0.0315, G_loss: 0.0962\n",
      "  Batch [160/1299] D_loss: 0.0285, G_loss: 0.0750\n",
      "  Batch [170/1299] D_loss: -1.4644, G_loss: -0.0944\n",
      "  Batch [180/1299] D_loss: -0.0849, G_loss: 0.0048\n",
      "  Batch [190/1299] D_loss: -0.1554, G_loss: -0.1256\n",
      "  Batch [200/1299] D_loss: -0.5053, G_loss: 0.0465\n",
      "  Batch [210/1299] D_loss: -0.5229, G_loss: -0.7037\n",
      "  Batch [220/1299] D_loss: -0.3268, G_loss: 0.0281\n",
      "  Batch [230/1299] D_loss: -0.0718, G_loss: 0.1961\n",
      "  Batch [240/1299] D_loss: -0.1091, G_loss: 0.4629\n",
      "  Batch [250/1299] D_loss: -0.0977, G_loss: 0.5564\n",
      "  Batch [260/1299] D_loss: -0.2282, G_loss: 0.8104\n",
      "  Batch [270/1299] D_loss: -0.0547, G_loss: 0.5929\n",
      "  Batch [280/1299] D_loss: -0.0796, G_loss: 0.6338\n",
      "  Batch [290/1299] D_loss: -0.0196, G_loss: 0.2754\n",
      "  Batch [300/1299] D_loss: -1.3907, G_loss: -0.5119\n",
      "  Batch [310/1299] D_loss: 0.0072, G_loss: 0.1194\n",
      "  Batch [320/1299] D_loss: -0.0260, G_loss: 0.2039\n",
      "  Batch [330/1299] D_loss: -0.0413, G_loss: 0.4512\n",
      "  Batch [340/1299] D_loss: 0.0170, G_loss: 0.5174\n",
      "  Batch [350/1299] D_loss: -0.0546, G_loss: 0.5054\n",
      "  Batch [360/1299] D_loss: -0.0726, G_loss: 0.3491\n",
      "  Batch [370/1299] D_loss: 0.0106, G_loss: 0.2515\n",
      "  Batch [380/1299] D_loss: -0.3630, G_loss: -0.2032\n",
      "  Batch [390/1299] D_loss: -0.0117, G_loss: 0.2691\n",
      "  Batch [400/1299] D_loss: 0.0129, G_loss: 0.3530\n",
      "  Batch [410/1299] D_loss: -0.0630, G_loss: 0.3965\n",
      "  Batch [420/1299] D_loss: -0.0142, G_loss: 0.2505\n",
      "  Batch [430/1299] D_loss: -0.0218, G_loss: 0.1094\n",
      "  Batch [440/1299] D_loss: -0.2484, G_loss: -0.0132\n",
      "  Batch [450/1299] D_loss: -0.0406, G_loss: 0.1530\n",
      "  Batch [460/1299] D_loss: -1.2116, G_loss: -0.3496\n",
      "  Batch [470/1299] D_loss: -0.0170, G_loss: 0.0919\n",
      "  Batch [480/1299] D_loss: -0.0607, G_loss: 0.0994\n",
      "  Batch [490/1299] D_loss: -1.3714, G_loss: -0.4748\n",
      "  Batch [500/1299] D_loss: -0.1420, G_loss: 0.0480\n",
      "  Batch [510/1299] D_loss: -0.3843, G_loss: -0.2476\n",
      "  Batch [520/1299] D_loss: -0.0910, G_loss: 0.2757\n",
      "  Batch [530/1299] D_loss: -0.0828, G_loss: 0.4920\n",
      "  Batch [540/1299] D_loss: -0.1144, G_loss: 0.5253\n",
      "  Batch [550/1299] D_loss: -0.0972, G_loss: 0.6266\n",
      "  Batch [560/1299] D_loss: -0.1172, G_loss: 0.4551\n",
      "  Batch [570/1299] D_loss: -0.0140, G_loss: 0.3365\n",
      "  Batch [580/1299] D_loss: -0.0330, G_loss: 0.0829\n",
      "  Batch [590/1299] D_loss: -0.9591, G_loss: -1.8481\n",
      "  Batch [600/1299] D_loss: -0.9851, G_loss: -1.3774\n",
      "  Batch [610/1299] D_loss: -0.0192, G_loss: 0.3441\n",
      "  Batch [620/1299] D_loss: -0.1122, G_loss: 0.3759\n",
      "  Batch [630/1299] D_loss: 0.0391, G_loss: 0.4123\n",
      "  Batch [640/1299] D_loss: -0.0311, G_loss: 0.3634\n",
      "  Batch [650/1299] D_loss: -0.7292, G_loss: -1.1889\n",
      "  Batch [660/1299] D_loss: -0.4824, G_loss: -0.3541\n",
      "  Batch [670/1299] D_loss: -0.1404, G_loss: 0.0745\n",
      "  Batch [680/1299] D_loss: -0.0603, G_loss: 0.2401\n",
      "  Batch [690/1299] D_loss: -0.9480, G_loss: -1.4821\n",
      "  Batch [700/1299] D_loss: -0.0153, G_loss: 0.2780\n",
      "  Batch [710/1299] D_loss: -0.1535, G_loss: 0.5857\n",
      "  Batch [720/1299] D_loss: -0.1854, G_loss: 0.5874\n",
      "  Batch [730/1299] D_loss: -0.1017, G_loss: 0.5370\n",
      "  Batch [740/1299] D_loss: 0.0143, G_loss: 0.3727\n",
      "  Batch [750/1299] D_loss: -0.0332, G_loss: 0.2946\n",
      "  Batch [760/1299] D_loss: -0.0295, G_loss: 0.1483\n",
      "  Batch [770/1299] D_loss: 0.0374, G_loss: 0.2606\n",
      "  Batch [780/1299] D_loss: -0.0727, G_loss: 0.2855\n",
      "  Batch [790/1299] D_loss: -0.0558, G_loss: 0.3395\n",
      "  Batch [800/1299] D_loss: 0.0428, G_loss: 0.2181\n",
      "  Batch [810/1299] D_loss: -0.0560, G_loss: 0.1915\n",
      "  Batch [820/1299] D_loss: -0.4714, G_loss: -3.5871\n",
      "  Batch [830/1299] D_loss: -0.0566, G_loss: 0.0346\n",
      "  Batch [840/1299] D_loss: -0.5329, G_loss: -1.1165\n",
      "  Batch [850/1299] D_loss: -1.3084, G_loss: -1.2307\n",
      "  Batch [860/1299] D_loss: -0.3994, G_loss: -0.0167\n",
      "  Batch [870/1299] D_loss: -0.8591, G_loss: -2.4990\n",
      "  Batch [880/1299] D_loss: -0.6082, G_loss: -0.4539\n",
      "  Batch [890/1299] D_loss: -0.6714, G_loss: 0.0742\n",
      "  Batch [900/1299] D_loss: -0.1198, G_loss: 0.1159\n",
      "  Batch [910/1299] D_loss: -0.6778, G_loss: -0.4950\n",
      "  Batch [920/1299] D_loss: -0.0235, G_loss: 0.1011\n",
      "  Batch [930/1299] D_loss: -0.0794, G_loss: 0.3157\n",
      "  Batch [940/1299] D_loss: -0.0574, G_loss: 0.5830\n",
      "  Batch [950/1299] D_loss: -0.2723, G_loss: 0.7903\n",
      "  Batch [960/1299] D_loss: -0.1584, G_loss: 0.7474\n",
      "  Batch [970/1299] D_loss: -0.1023, G_loss: 0.5722\n",
      "  Batch [980/1299] D_loss: -0.1521, G_loss: 0.4291\n",
      "  Batch [990/1299] D_loss: -0.6324, G_loss: -0.7611\n",
      "  Batch [1000/1299] D_loss: -0.6725, G_loss: -0.0152\n",
      "  Batch [1010/1299] D_loss: -0.0172, G_loss: 0.2048\n",
      "  Batch [1020/1299] D_loss: -0.0344, G_loss: 0.2546\n",
      "  Batch [1030/1299] D_loss: -0.1221, G_loss: 0.4186\n",
      "  Batch [1040/1299] D_loss: -0.0331, G_loss: 0.2965\n",
      "  Batch [1050/1299] D_loss: -0.0223, G_loss: 0.2833\n",
      "  Batch [1060/1299] D_loss: -2.7566, G_loss: -2.9141\n",
      "  Batch [1070/1299] D_loss: 0.0274, G_loss: 0.1497\n",
      "  Batch [1080/1299] D_loss: 0.0129, G_loss: 0.1927\n",
      "  Batch [1090/1299] D_loss: -0.0635, G_loss: 0.3146\n",
      "  Batch [1100/1299] D_loss: -0.0234, G_loss: 0.3370\n",
      "  Batch [1110/1299] D_loss: 0.0049, G_loss: 0.3649\n",
      "  Batch [1120/1299] D_loss: -0.0390, G_loss: 0.3395\n",
      "  Batch [1130/1299] D_loss: -1.7602, G_loss: -2.8880\n",
      "  Batch [1140/1299] D_loss: -0.3432, G_loss: -0.5647\n",
      "  Batch [1150/1299] D_loss: -0.0304, G_loss: 0.0966\n",
      "  Batch [1160/1299] D_loss: -0.2149, G_loss: -0.5102\n",
      "  Batch [1170/1299] D_loss: -0.0302, G_loss: 0.1956\n",
      "  Batch [1180/1299] D_loss: -0.0841, G_loss: 0.4963\n",
      "  Batch [1190/1299] D_loss: -0.2051, G_loss: 0.7051\n",
      "  Batch [1200/1299] D_loss: -0.0102, G_loss: 0.4865\n",
      "  Batch [1210/1299] D_loss: -0.1447, G_loss: 0.6467\n",
      "  Batch [1220/1299] D_loss: 0.0684, G_loss: 0.3545\n",
      "  Batch [1230/1299] D_loss: -1.5078, G_loss: -2.7373\n",
      "  Batch [1240/1299] D_loss: 0.0153, G_loss: 0.0816\n",
      "  Batch [1250/1299] D_loss: -0.8744, G_loss: -1.2461\n",
      "  Batch [1260/1299] D_loss: -0.8419, G_loss: -4.1619\n",
      "  Batch [1270/1299] D_loss: -0.0575, G_loss: 0.1004\n",
      "  Batch [1280/1299] D_loss: -0.6084, G_loss: -0.6012\n",
      "  Batch [1290/1299] D_loss: -0.0130, G_loss: 0.2651\n",
      "\n",
      "Epoch 82 Summary:\n",
      "  Average D_loss: -0.1552\n",
      "  Average G_loss: -0.0779\n",
      "\n",
      "Epoch [83/100]\n",
      "  Batch [0/1299] D_loss: -0.0174, G_loss: 0.2414\n",
      "  Batch [10/1299] D_loss: -0.0336, G_loss: 0.2659\n",
      "  Batch [20/1299] D_loss: -0.0484, G_loss: 0.5409\n",
      "  Batch [30/1299] D_loss: -0.0080, G_loss: 0.8318\n",
      "  Batch [40/1299] D_loss: -0.2811, G_loss: 0.8133\n",
      "  Batch [50/1299] D_loss: -0.0711, G_loss: 0.5926\n",
      "  Batch [60/1299] D_loss: -0.0140, G_loss: 0.3198\n",
      "  Batch [70/1299] D_loss: -0.5611, G_loss: -2.9629\n",
      "  Batch [80/1299] D_loss: -0.4745, G_loss: -0.5433\n",
      "  Batch [90/1299] D_loss: -0.0044, G_loss: 0.1981\n",
      "  Batch [100/1299] D_loss: -0.0775, G_loss: 0.3906\n",
      "  Batch [110/1299] D_loss: -0.0078, G_loss: 0.4104\n",
      "  Batch [120/1299] D_loss: -0.1806, G_loss: 0.3985\n",
      "  Batch [130/1299] D_loss: -0.0344, G_loss: 0.3546\n",
      "  Batch [140/1299] D_loss: -0.1635, G_loss: -0.2300\n",
      "  Batch [150/1299] D_loss: -0.0956, G_loss: 0.0184\n",
      "  Batch [160/1299] D_loss: -0.0325, G_loss: 0.2133\n",
      "  Batch [170/1299] D_loss: -0.0775, G_loss: 0.2807\n",
      "  Batch [180/1299] D_loss: -0.0880, G_loss: 0.3699\n",
      "  Batch [190/1299] D_loss: -0.0573, G_loss: 0.0203\n",
      "  Batch [200/1299] D_loss: 0.0110, G_loss: 0.1316\n",
      "  Batch [210/1299] D_loss: -0.1017, G_loss: 0.2513\n",
      "  Batch [220/1299] D_loss: -0.0839, G_loss: 0.3360\n",
      "  Batch [230/1299] D_loss: -1.8104, G_loss: -2.6305\n",
      "  Batch [240/1299] D_loss: -0.4541, G_loss: -0.8027\n",
      "  Batch [250/1299] D_loss: -0.3902, G_loss: -1.0262\n",
      "  Batch [260/1299] D_loss: -0.5379, G_loss: -0.5197\n",
      "  Batch [270/1299] D_loss: -1.2604, G_loss: -0.5397\n",
      "  Batch [280/1299] D_loss: -0.3768, G_loss: -0.0063\n",
      "  Batch [290/1299] D_loss: -0.2311, G_loss: 0.0847\n",
      "  Batch [300/1299] D_loss: -0.6284, G_loss: -0.2776\n",
      "  Batch [310/1299] D_loss: -0.0787, G_loss: 0.2147\n",
      "  Batch [320/1299] D_loss: -0.1487, G_loss: 0.5971\n",
      "  Batch [330/1299] D_loss: -0.1385, G_loss: 0.6957\n",
      "  Batch [340/1299] D_loss: -0.1126, G_loss: 0.7999\n",
      "  Batch [350/1299] D_loss: -0.1189, G_loss: 0.7772\n",
      "  Batch [360/1299] D_loss: -0.0439, G_loss: 0.5642\n",
      "  Batch [370/1299] D_loss: -0.0882, G_loss: 0.4120\n",
      "  Batch [380/1299] D_loss: -0.0519, G_loss: 0.2210\n",
      "  Batch [390/1299] D_loss: -0.1467, G_loss: 0.0762\n",
      "  Batch [400/1299] D_loss: -0.0113, G_loss: 0.1447\n",
      "  Batch [410/1299] D_loss: -1.1102, G_loss: -0.3395\n",
      "  Batch [420/1299] D_loss: -0.3223, G_loss: 0.0673\n",
      "  Batch [430/1299] D_loss: -1.8563, G_loss: -1.8511\n",
      "  Batch [440/1299] D_loss: -0.2112, G_loss: 0.0986\n",
      "  Batch [450/1299] D_loss: -0.1719, G_loss: 0.0475\n",
      "  Batch [460/1299] D_loss: -0.0564, G_loss: 0.2656\n",
      "  Batch [470/1299] D_loss: -0.0540, G_loss: 0.4377\n",
      "  Batch [480/1299] D_loss: -0.2003, G_loss: 0.6717\n",
      "  Batch [490/1299] D_loss: -0.0271, G_loss: 0.5807\n",
      "  Batch [500/1299] D_loss: -0.1692, G_loss: 0.6537\n",
      "  Batch [510/1299] D_loss: -0.0179, G_loss: 0.4079\n",
      "  Batch [520/1299] D_loss: -2.9877, G_loss: -1.3305\n",
      "  Batch [530/1299] D_loss: -0.1762, G_loss: -0.4571\n",
      "  Batch [540/1299] D_loss: -0.3021, G_loss: -0.0550\n",
      "  Batch [550/1299] D_loss: -1.0893, G_loss: -0.5006\n",
      "  Batch [560/1299] D_loss: -0.4161, G_loss: -0.1306\n",
      "  Batch [570/1299] D_loss: -0.1140, G_loss: -0.0471\n",
      "  Batch [580/1299] D_loss: -0.0841, G_loss: 0.3471\n",
      "  Batch [590/1299] D_loss: -0.0095, G_loss: 0.5359\n",
      "  Batch [600/1299] D_loss: -0.0764, G_loss: 0.6652\n",
      "  Batch [610/1299] D_loss: -0.0551, G_loss: 0.6101\n",
      "  Batch [620/1299] D_loss: -0.1975, G_loss: 0.4655\n",
      "  Batch [630/1299] D_loss: -4.2758, G_loss: -8.3735\n",
      "  Batch [640/1299] D_loss: -1.3604, G_loss: -1.3299\n",
      "  Batch [650/1299] D_loss: -0.3717, G_loss: -0.0622\n",
      "  Batch [660/1299] D_loss: -0.4927, G_loss: -0.5547\n",
      "  Batch [670/1299] D_loss: -0.3509, G_loss: -0.1302\n",
      "  Batch [680/1299] D_loss: -0.0465, G_loss: 0.1878\n",
      "  Batch [690/1299] D_loss: -0.1914, G_loss: 0.3947\n",
      "  Batch [700/1299] D_loss: -0.0634, G_loss: 0.4345\n",
      "  Batch [710/1299] D_loss: -0.1064, G_loss: 0.7067\n",
      "  Batch [720/1299] D_loss: -0.1042, G_loss: 0.6655\n",
      "  Batch [730/1299] D_loss: -0.0545, G_loss: 0.5961\n",
      "  Batch [740/1299] D_loss: -0.0452, G_loss: 0.3181\n",
      "  Batch [750/1299] D_loss: -2.7928, G_loss: -3.5039\n",
      "  Batch [760/1299] D_loss: -0.5768, G_loss: 0.0924\n",
      "  Batch [770/1299] D_loss: -0.1694, G_loss: 0.0476\n",
      "  Batch [780/1299] D_loss: -0.4642, G_loss: -0.0325\n",
      "  Batch [790/1299] D_loss: -0.3113, G_loss: -0.5900\n",
      "  Batch [800/1299] D_loss: -1.0260, G_loss: -0.6351\n",
      "  Batch [810/1299] D_loss: -0.0801, G_loss: 0.2170\n",
      "  Batch [820/1299] D_loss: -0.1600, G_loss: 0.4146\n",
      "  Batch [830/1299] D_loss: -0.2072, G_loss: 0.5452\n",
      "  Batch [840/1299] D_loss: -0.1263, G_loss: 0.6857\n",
      "  Batch [850/1299] D_loss: -0.1418, G_loss: 0.7073\n",
      "  Batch [860/1299] D_loss: -0.0374, G_loss: 0.5473\n",
      "  Batch [870/1299] D_loss: -0.0020, G_loss: 0.1591\n",
      "  Batch [880/1299] D_loss: -0.0321, G_loss: 0.1842\n",
      "  Batch [890/1299] D_loss: -0.6382, G_loss: 0.0650\n",
      "  Batch [900/1299] D_loss: -0.1436, G_loss: -0.3320\n",
      "  Batch [910/1299] D_loss: -0.5535, G_loss: -0.7029\n",
      "  Batch [920/1299] D_loss: -0.0649, G_loss: 0.0901\n",
      "  Batch [930/1299] D_loss: -0.5913, G_loss: -0.5247\n",
      "  Batch [940/1299] D_loss: -0.5824, G_loss: -0.1788\n",
      "  Batch [950/1299] D_loss: -0.5425, G_loss: -0.3323\n",
      "  Batch [960/1299] D_loss: -0.3212, G_loss: 0.1203\n",
      "  Batch [970/1299] D_loss: -0.8565, G_loss: -0.0244\n",
      "  Batch [980/1299] D_loss: -0.1434, G_loss: 0.5991\n",
      "  Batch [990/1299] D_loss: -0.2448, G_loss: 0.7950\n",
      "  Batch [1000/1299] D_loss: 0.0236, G_loss: 0.8964\n",
      "  Batch [1010/1299] D_loss: -0.2912, G_loss: 1.0153\n",
      "  Batch [1020/1299] D_loss: -0.1392, G_loss: 0.7488\n",
      "  Batch [1030/1299] D_loss: 0.0605, G_loss: 0.3986\n",
      "  Batch [1040/1299] D_loss: -0.1532, G_loss: 0.4704\n",
      "  Batch [1050/1299] D_loss: -1.7105, G_loss: -4.1998\n",
      "  Batch [1060/1299] D_loss: -0.0399, G_loss: 0.1863\n",
      "  Batch [1070/1299] D_loss: -0.1072, G_loss: 0.3957\n",
      "  Batch [1080/1299] D_loss: -0.1001, G_loss: 0.4686\n",
      "  Batch [1090/1299] D_loss: -0.1018, G_loss: 0.5599\n",
      "  Batch [1100/1299] D_loss: -0.1380, G_loss: 0.4463\n",
      "  Batch [1110/1299] D_loss: -0.1086, G_loss: 0.5088\n",
      "  Batch [1120/1299] D_loss: -0.0694, G_loss: 0.2244\n",
      "  Batch [1130/1299] D_loss: -1.2676, G_loss: -2.7344\n",
      "  Batch [1140/1299] D_loss: -0.5746, G_loss: -0.9262\n",
      "  Batch [1150/1299] D_loss: -0.5353, G_loss: -1.6343\n",
      "  Batch [1160/1299] D_loss: -1.3267, G_loss: -0.1968\n",
      "  Batch [1170/1299] D_loss: -0.4055, G_loss: -0.6454\n",
      "  Batch [1180/1299] D_loss: -0.3232, G_loss: -0.2076\n",
      "  Batch [1190/1299] D_loss: -0.0870, G_loss: 0.3808\n",
      "  Batch [1200/1299] D_loss: -0.0637, G_loss: 0.5183\n",
      "  Batch [1210/1299] D_loss: -0.1323, G_loss: 0.6156\n",
      "  Batch [1220/1299] D_loss: -0.1686, G_loss: 0.5863\n",
      "  Batch [1230/1299] D_loss: -0.0684, G_loss: 0.6474\n",
      "  Batch [1240/1299] D_loss: -0.0657, G_loss: 0.3947\n",
      "  Batch [1250/1299] D_loss: -1.1022, G_loss: -0.1163\n",
      "  Batch [1260/1299] D_loss: -0.6199, G_loss: -0.0800\n",
      "  Batch [1270/1299] D_loss: -1.1344, G_loss: -0.1705\n",
      "  Batch [1280/1299] D_loss: -0.6491, G_loss: -1.5607\n",
      "  Batch [1290/1299] D_loss: -0.8561, G_loss: -0.2319\n",
      "\n",
      "Epoch 83 Summary:\n",
      "  Average D_loss: -0.2015\n",
      "  Average G_loss: -0.0965\n",
      "\n",
      "Epoch [84/100]\n",
      "  Batch [0/1299] D_loss: -0.6568, G_loss: 0.0889\n",
      "  Batch [10/1299] D_loss: -0.1083, G_loss: 0.1423\n",
      "  Batch [20/1299] D_loss: -0.2187, G_loss: 0.0518\n",
      "  Batch [30/1299] D_loss: -0.1274, G_loss: 0.3149\n",
      "  Batch [40/1299] D_loss: -0.1653, G_loss: 0.6587\n",
      "  Batch [50/1299] D_loss: -0.1286, G_loss: 0.6565\n",
      "  Batch [60/1299] D_loss: -0.1157, G_loss: 0.6735\n",
      "  Batch [70/1299] D_loss: -0.2579, G_loss: 0.7059\n",
      "  Batch [80/1299] D_loss: -0.1390, G_loss: 0.4713\n",
      "  Batch [90/1299] D_loss: -0.1042, G_loss: 0.3196\n",
      "  Batch [100/1299] D_loss: -0.8587, G_loss: -0.0597\n",
      "  Batch [110/1299] D_loss: -0.4139, G_loss: -0.5016\n",
      "  Batch [120/1299] D_loss: -0.4495, G_loss: -0.2242\n",
      "  Batch [130/1299] D_loss: -0.6063, G_loss: -1.0171\n",
      "  Batch [140/1299] D_loss: -0.1004, G_loss: 0.1141\n",
      "  Batch [150/1299] D_loss: -0.5770, G_loss: 0.0703\n",
      "  Batch [160/1299] D_loss: -0.4180, G_loss: -0.6308\n",
      "  Batch [170/1299] D_loss: -2.0409, G_loss: -1.1383\n",
      "  Batch [180/1299] D_loss: -0.1723, G_loss: 0.0124\n",
      "  Batch [190/1299] D_loss: -0.0942, G_loss: 0.3605\n",
      "  Batch [200/1299] D_loss: -0.2340, G_loss: 0.5933\n",
      "  Batch [210/1299] D_loss: -0.0576, G_loss: 0.7799\n",
      "  Batch [220/1299] D_loss: 0.0086, G_loss: 0.8906\n",
      "  Batch [230/1299] D_loss: -0.1419, G_loss: 0.8249\n",
      "  Batch [240/1299] D_loss: -0.0257, G_loss: 0.6194\n",
      "  Batch [250/1299] D_loss: -0.0671, G_loss: 0.3650\n",
      "  Batch [260/1299] D_loss: -1.6438, G_loss: -3.2052\n",
      "  Batch [270/1299] D_loss: -0.4053, G_loss: 0.0393\n",
      "  Batch [280/1299] D_loss: -1.2881, G_loss: -0.6677\n",
      "  Batch [290/1299] D_loss: -0.5409, G_loss: -0.6022\n",
      "  Batch [300/1299] D_loss: -0.2175, G_loss: 0.0985\n",
      "  Batch [310/1299] D_loss: -0.1789, G_loss: 0.0702\n",
      "  Batch [320/1299] D_loss: -0.9129, G_loss: -1.0648\n",
      "  Batch [330/1299] D_loss: -0.7497, G_loss: -0.4288\n",
      "  Batch [340/1299] D_loss: -0.0606, G_loss: 0.1362\n",
      "  Batch [350/1299] D_loss: -0.1340, G_loss: 0.3951\n",
      "  Batch [360/1299] D_loss: -0.0813, G_loss: 0.6333\n",
      "  Batch [370/1299] D_loss: -0.0734, G_loss: 0.9192\n",
      "  Batch [380/1299] D_loss: -0.0675, G_loss: 0.7552\n",
      "  Batch [390/1299] D_loss: -0.2567, G_loss: 0.8610\n",
      "  Batch [400/1299] D_loss: -0.0172, G_loss: 0.7035\n",
      "  Batch [410/1299] D_loss: 0.0252, G_loss: 0.2243\n",
      "  Batch [420/1299] D_loss: -0.4191, G_loss: -3.3993\n",
      "  Batch [430/1299] D_loss: -0.9761, G_loss: -0.8730\n",
      "  Batch [440/1299] D_loss: -0.0171, G_loss: 0.1202\n",
      "  Batch [450/1299] D_loss: -0.2579, G_loss: -0.1978\n",
      "  Batch [460/1299] D_loss: -0.0956, G_loss: 0.3014\n",
      "  Batch [470/1299] D_loss: -0.1014, G_loss: 0.4114\n",
      "  Batch [480/1299] D_loss: 0.0554, G_loss: 0.5506\n",
      "  Batch [490/1299] D_loss: -0.0614, G_loss: 0.5561\n",
      "  Batch [500/1299] D_loss: -0.1346, G_loss: 0.6317\n",
      "  Batch [510/1299] D_loss: -0.0939, G_loss: 0.4360\n",
      "  Batch [520/1299] D_loss: -2.3780, G_loss: -4.6581\n",
      "  Batch [530/1299] D_loss: -0.1090, G_loss: 0.2329\n",
      "  Batch [540/1299] D_loss: -0.0224, G_loss: 0.3986\n",
      "  Batch [550/1299] D_loss: -0.1474, G_loss: 0.4664\n",
      "  Batch [560/1299] D_loss: -0.0438, G_loss: 0.3655\n",
      "  Batch [570/1299] D_loss: -0.1058, G_loss: 0.4604\n",
      "  Batch [580/1299] D_loss: -0.1922, G_loss: 0.4759\n",
      "  Batch [590/1299] D_loss: -0.0686, G_loss: 0.1631\n",
      "  Batch [600/1299] D_loss: 0.0044, G_loss: 0.1547\n",
      "  Batch [610/1299] D_loss: -0.0233, G_loss: 0.2591\n",
      "  Batch [620/1299] D_loss: -0.0532, G_loss: 0.3163\n",
      "  Batch [630/1299] D_loss: -0.0279, G_loss: 0.2834\n",
      "  Batch [640/1299] D_loss: -0.0267, G_loss: 0.1626\n",
      "  Batch [650/1299] D_loss: -2.5625, G_loss: -5.0950\n",
      "  Batch [660/1299] D_loss: -0.1316, G_loss: 0.0037\n",
      "  Batch [670/1299] D_loss: 0.0082, G_loss: 0.0828\n",
      "  Batch [680/1299] D_loss: -0.0655, G_loss: 0.1945\n",
      "  Batch [690/1299] D_loss: -0.0524, G_loss: 0.2820\n",
      "  Batch [700/1299] D_loss: -0.0977, G_loss: 0.4336\n",
      "  Batch [710/1299] D_loss: -0.1461, G_loss: 0.4020\n",
      "  Batch [720/1299] D_loss: -0.0912, G_loss: 0.2912\n",
      "  Batch [730/1299] D_loss: -3.5722, G_loss: -5.2604\n",
      "  Batch [740/1299] D_loss: -0.0132, G_loss: 0.1017\n",
      "  Batch [750/1299] D_loss: 0.0187, G_loss: 0.1351\n",
      "  Batch [760/1299] D_loss: 0.0267, G_loss: 0.1952\n",
      "  Batch [770/1299] D_loss: -0.0825, G_loss: 0.3064\n",
      "  Batch [780/1299] D_loss: -0.0741, G_loss: 0.3682\n",
      "  Batch [790/1299] D_loss: -0.8128, G_loss: -2.6126\n",
      "  Batch [800/1299] D_loss: -0.0307, G_loss: 0.1571\n",
      "  Batch [810/1299] D_loss: -0.0479, G_loss: 0.0894\n",
      "  Batch [820/1299] D_loss: -0.0548, G_loss: 0.1081\n",
      "  Batch [830/1299] D_loss: -0.0226, G_loss: 0.2044\n",
      "  Batch [840/1299] D_loss: -0.0749, G_loss: 0.4071\n",
      "  Batch [850/1299] D_loss: -0.1640, G_loss: 0.6070\n",
      "  Batch [860/1299] D_loss: -0.0318, G_loss: 0.4755\n",
      "  Batch [870/1299] D_loss: -0.0128, G_loss: 0.4473\n",
      "  Batch [880/1299] D_loss: -0.0329, G_loss: 0.2118\n",
      "  Batch [890/1299] D_loss: -0.5772, G_loss: -2.2122\n",
      "  Batch [900/1299] D_loss: -0.3701, G_loss: 0.0549\n",
      "  Batch [910/1299] D_loss: -0.0004, G_loss: 0.3363\n",
      "  Batch [920/1299] D_loss: -0.0347, G_loss: 0.4360\n",
      "  Batch [930/1299] D_loss: -0.0740, G_loss: 0.6027\n",
      "  Batch [940/1299] D_loss: -0.0776, G_loss: 0.4239\n",
      "  Batch [950/1299] D_loss: -0.0115, G_loss: 0.3213\n",
      "  Batch [960/1299] D_loss: -0.0511, G_loss: 0.1961\n",
      "  Batch [970/1299] D_loss: -1.5332, G_loss: -3.9020\n",
      "  Batch [980/1299] D_loss: -0.1337, G_loss: -0.0248\n",
      "  Batch [990/1299] D_loss: -0.0445, G_loss: 0.1395\n",
      "  Batch [1000/1299] D_loss: -0.5790, G_loss: 0.0141\n",
      "  Batch [1010/1299] D_loss: -0.1169, G_loss: 0.1069\n",
      "  Batch [1020/1299] D_loss: -0.0630, G_loss: 0.2531\n",
      "  Batch [1030/1299] D_loss: -0.0917, G_loss: 0.5471\n",
      "  Batch [1040/1299] D_loss: -0.1517, G_loss: 0.7321\n",
      "  Batch [1050/1299] D_loss: -0.2591, G_loss: 0.6351\n",
      "  Batch [1060/1299] D_loss: -0.0637, G_loss: 0.6357\n",
      "  Batch [1070/1299] D_loss: -0.0131, G_loss: 0.3298\n",
      "  Batch [1080/1299] D_loss: -0.0163, G_loss: 0.2555\n",
      "  Batch [1090/1299] D_loss: -0.4736, G_loss: -0.4299\n",
      "  Batch [1100/1299] D_loss: -0.9657, G_loss: -1.4984\n",
      "  Batch [1110/1299] D_loss: -0.1906, G_loss: 0.0353\n",
      "  Batch [1120/1299] D_loss: -0.2834, G_loss: -0.0275\n",
      "  Batch [1130/1299] D_loss: -0.3737, G_loss: 0.0325\n",
      "  Batch [1140/1299] D_loss: -1.1131, G_loss: -1.1548\n",
      "  Batch [1150/1299] D_loss: -0.5313, G_loss: 0.1738\n",
      "  Batch [1160/1299] D_loss: -0.2316, G_loss: 0.2467\n",
      "  Batch [1170/1299] D_loss: -0.1676, G_loss: 0.3895\n",
      "  Batch [1180/1299] D_loss: -0.0822, G_loss: 0.6058\n",
      "  Batch [1190/1299] D_loss: -0.1848, G_loss: 0.7308\n",
      "  Batch [1200/1299] D_loss: -0.1768, G_loss: 0.5810\n",
      "  Batch [1210/1299] D_loss: -0.0863, G_loss: 0.6622\n",
      "  Batch [1220/1299] D_loss: -0.0802, G_loss: 0.3805\n",
      "  Batch [1230/1299] D_loss: -1.7743, G_loss: -3.8859\n",
      "  Batch [1240/1299] D_loss: -0.0150, G_loss: 0.0976\n",
      "  Batch [1250/1299] D_loss: -0.0563, G_loss: 0.2019\n",
      "  Batch [1260/1299] D_loss: 0.0228, G_loss: 0.2909\n",
      "  Batch [1270/1299] D_loss: -0.1200, G_loss: 0.3083\n",
      "  Batch [1280/1299] D_loss: -0.0817, G_loss: 0.3722\n",
      "  Batch [1290/1299] D_loss: 0.0684, G_loss: 0.2206\n",
      "\n",
      "Epoch 84 Summary:\n",
      "  Average D_loss: -0.1770\n",
      "  Average G_loss: -0.0623\n",
      "\n",
      "Epoch [85/100]\n",
      "  Batch [0/1299] D_loss: -0.2904, G_loss: -0.4800\n",
      "  Batch [10/1299] D_loss: -0.3494, G_loss: -0.0547\n",
      "  Batch [20/1299] D_loss: -0.0407, G_loss: 0.1145\n",
      "  Batch [30/1299] D_loss: -0.0527, G_loss: 0.1915\n",
      "  Batch [40/1299] D_loss: -0.2143, G_loss: 0.1135\n",
      "  Batch [50/1299] D_loss: -0.0653, G_loss: -0.2464\n",
      "  Batch [60/1299] D_loss: -0.6416, G_loss: -0.0788\n",
      "  Batch [70/1299] D_loss: -0.1141, G_loss: 0.1298\n",
      "  Batch [80/1299] D_loss: -0.2068, G_loss: 0.1358\n",
      "  Batch [90/1299] D_loss: -0.0615, G_loss: 0.1818\n",
      "  Batch [100/1299] D_loss: -0.1646, G_loss: 0.5694\n",
      "  Batch [110/1299] D_loss: -0.3298, G_loss: 0.7012\n",
      "  Batch [120/1299] D_loss: 0.0363, G_loss: 0.6461\n",
      "  Batch [130/1299] D_loss: -0.0845, G_loss: 0.6382\n",
      "  Batch [140/1299] D_loss: -0.0743, G_loss: 0.4364\n",
      "  Batch [150/1299] D_loss: -0.0119, G_loss: 0.0947\n",
      "  Batch [160/1299] D_loss: -1.0728, G_loss: -1.4750\n",
      "  Batch [170/1299] D_loss: -0.1565, G_loss: 0.1308\n",
      "  Batch [180/1299] D_loss: -0.5809, G_loss: -0.8807\n",
      "  Batch [190/1299] D_loss: -0.2318, G_loss: 0.0615\n",
      "  Batch [200/1299] D_loss: -0.0196, G_loss: 0.1933\n",
      "  Batch [210/1299] D_loss: -0.2831, G_loss: 0.1526\n",
      "  Batch [220/1299] D_loss: -0.0306, G_loss: 0.3709\n",
      "  Batch [230/1299] D_loss: -1.9061, G_loss: -1.3955\n",
      "  Batch [240/1299] D_loss: -1.3645, G_loss: -2.5593\n",
      "  Batch [250/1299] D_loss: -0.0207, G_loss: 0.1377\n",
      "  Batch [260/1299] D_loss: 0.0053, G_loss: 0.3197\n",
      "  Batch [270/1299] D_loss: -0.1136, G_loss: 0.4983\n",
      "  Batch [280/1299] D_loss: -0.0213, G_loss: 0.5364\n",
      "  Batch [290/1299] D_loss: -0.0650, G_loss: 0.4500\n",
      "  Batch [300/1299] D_loss: -0.1454, G_loss: 0.4820\n",
      "  Batch [310/1299] D_loss: -0.0662, G_loss: 0.2682\n",
      "  Batch [320/1299] D_loss: -0.1474, G_loss: -0.1917\n",
      "  Batch [330/1299] D_loss: -1.1597, G_loss: -1.9319\n",
      "  Batch [340/1299] D_loss: -0.0020, G_loss: 0.2195\n",
      "  Batch [350/1299] D_loss: -0.0523, G_loss: 0.3052\n",
      "  Batch [360/1299] D_loss: -0.0609, G_loss: 0.3493\n",
      "  Batch [370/1299] D_loss: -0.0782, G_loss: 0.3494\n",
      "  Batch [380/1299] D_loss: -0.0528, G_loss: 0.3117\n",
      "  Batch [390/1299] D_loss: 0.0010, G_loss: 0.3120\n",
      "  Batch [400/1299] D_loss: -0.3225, G_loss: -0.7747\n",
      "  Batch [410/1299] D_loss: -0.0440, G_loss: 0.1970\n",
      "  Batch [420/1299] D_loss: -0.0641, G_loss: 0.3317\n",
      "  Batch [430/1299] D_loss: -0.0548, G_loss: 0.3858\n",
      "  Batch [440/1299] D_loss: 0.0189, G_loss: 0.3363\n",
      "  Batch [450/1299] D_loss: -0.0966, G_loss: 0.4454\n",
      "  Batch [460/1299] D_loss: -0.0921, G_loss: 0.2005\n",
      "  Batch [470/1299] D_loss: -0.3780, G_loss: -0.2260\n",
      "  Batch [480/1299] D_loss: -0.3204, G_loss: -0.3985\n",
      "  Batch [490/1299] D_loss: -0.6845, G_loss: -1.6228\n",
      "  Batch [500/1299] D_loss: -0.1933, G_loss: 0.0290\n",
      "  Batch [510/1299] D_loss: -0.5327, G_loss: -0.3083\n",
      "  Batch [520/1299] D_loss: -0.0539, G_loss: 0.2226\n",
      "  Batch [530/1299] D_loss: -0.0374, G_loss: 0.3642\n",
      "  Batch [540/1299] D_loss: -0.0713, G_loss: 0.5399\n",
      "  Batch [550/1299] D_loss: -0.1062, G_loss: 0.6841\n",
      "  Batch [560/1299] D_loss: -0.1286, G_loss: 0.6726\n",
      "  Batch [570/1299] D_loss: -0.0128, G_loss: 0.4607\n",
      "  Batch [580/1299] D_loss: 0.0707, G_loss: 0.3444\n",
      "  Batch [590/1299] D_loss: -2.0315, G_loss: -2.6466\n",
      "  Batch [600/1299] D_loss: -0.2899, G_loss: -1.0036\n",
      "  Batch [610/1299] D_loss: -0.2625, G_loss: 0.1305\n",
      "  Batch [620/1299] D_loss: -0.5101, G_loss: -0.0881\n",
      "  Batch [630/1299] D_loss: -0.0708, G_loss: 0.1827\n",
      "  Batch [640/1299] D_loss: -0.0839, G_loss: 0.1765\n",
      "  Batch [650/1299] D_loss: -0.0620, G_loss: 0.3264\n",
      "  Batch [660/1299] D_loss: 0.0418, G_loss: 0.3612\n",
      "  Batch [670/1299] D_loss: -0.0719, G_loss: 0.5029\n",
      "  Batch [680/1299] D_loss: -0.1208, G_loss: 0.3845\n",
      "  Batch [690/1299] D_loss: -0.1054, G_loss: 0.3454\n",
      "  Batch [700/1299] D_loss: -0.0161, G_loss: 0.2993\n",
      "  Batch [710/1299] D_loss: 0.0309, G_loss: 0.2338\n",
      "  Batch [720/1299] D_loss: -0.0162, G_loss: 0.4109\n",
      "  Batch [730/1299] D_loss: -0.1481, G_loss: 0.4664\n",
      "  Batch [740/1299] D_loss: -0.0425, G_loss: 0.2752\n",
      "  Batch [750/1299] D_loss: 0.0058, G_loss: 0.3609\n",
      "  Batch [760/1299] D_loss: -0.0167, G_loss: 0.0964\n",
      "  Batch [770/1299] D_loss: -0.2743, G_loss: -0.2459\n",
      "  Batch [780/1299] D_loss: -0.3439, G_loss: -0.2805\n",
      "  Batch [790/1299] D_loss: -0.0729, G_loss: 0.1054\n",
      "  Batch [800/1299] D_loss: 0.0094, G_loss: 0.1318\n",
      "  Batch [810/1299] D_loss: -0.2324, G_loss: 0.0873\n",
      "  Batch [820/1299] D_loss: -0.0853, G_loss: 0.3347\n",
      "  Batch [830/1299] D_loss: -0.0988, G_loss: 0.6069\n",
      "  Batch [840/1299] D_loss: 0.0173, G_loss: 0.5377\n",
      "  Batch [850/1299] D_loss: -0.0667, G_loss: 0.6665\n",
      "  Batch [860/1299] D_loss: -0.0776, G_loss: 0.5435\n",
      "  Batch [870/1299] D_loss: -0.0951, G_loss: 0.3798\n",
      "  Batch [880/1299] D_loss: -0.5090, G_loss: -3.5977\n",
      "  Batch [890/1299] D_loss: -0.3954, G_loss: -0.7557\n",
      "  Batch [900/1299] D_loss: -0.3460, G_loss: -0.2862\n",
      "  Batch [910/1299] D_loss: -0.7867, G_loss: -0.5685\n",
      "  Batch [920/1299] D_loss: -0.1216, G_loss: 0.0058\n",
      "  Batch [930/1299] D_loss: -0.0818, G_loss: 0.2944\n",
      "  Batch [940/1299] D_loss: -0.0708, G_loss: 0.4310\n",
      "  Batch [950/1299] D_loss: -0.1308, G_loss: 0.5635\n",
      "  Batch [960/1299] D_loss: -0.2103, G_loss: 0.6198\n",
      "  Batch [970/1299] D_loss: -0.0773, G_loss: 0.3946\n",
      "  Batch [980/1299] D_loss: -0.0338, G_loss: 0.2807\n",
      "  Batch [990/1299] D_loss: -1.7567, G_loss: 0.0373\n",
      "  Batch [1000/1299] D_loss: -0.0051, G_loss: 0.1876\n",
      "  Batch [1010/1299] D_loss: -0.0354, G_loss: 0.3166\n",
      "  Batch [1020/1299] D_loss: -0.0077, G_loss: 0.3147\n",
      "  Batch [1030/1299] D_loss: 0.0010, G_loss: 0.3061\n",
      "  Batch [1040/1299] D_loss: -0.0005, G_loss: 0.2140\n",
      "  Batch [1050/1299] D_loss: -1.1084, G_loss: -3.1006\n",
      "  Batch [1060/1299] D_loss: -0.4376, G_loss: -0.1658\n",
      "  Batch [1070/1299] D_loss: -0.0535, G_loss: 0.2072\n",
      "  Batch [1080/1299] D_loss: -0.0350, G_loss: 0.3919\n",
      "  Batch [1090/1299] D_loss: -0.0161, G_loss: 0.4812\n",
      "  Batch [1100/1299] D_loss: -0.0835, G_loss: 0.5122\n",
      "  Batch [1110/1299] D_loss: -0.0358, G_loss: 0.3805\n",
      "  Batch [1120/1299] D_loss: -0.0274, G_loss: 0.3977\n",
      "  Batch [1130/1299] D_loss: -0.0393, G_loss: -0.1141\n",
      "  Batch [1140/1299] D_loss: -0.4708, G_loss: -0.0033\n",
      "  Batch [1150/1299] D_loss: -0.4779, G_loss: -0.3671\n",
      "  Batch [1160/1299] D_loss: -0.2831, G_loss: 0.0789\n",
      "  Batch [1170/1299] D_loss: -0.4655, G_loss: -0.0847\n",
      "  Batch [1180/1299] D_loss: -0.1021, G_loss: 0.3926\n",
      "  Batch [1190/1299] D_loss: -0.0646, G_loss: 0.5086\n",
      "  Batch [1200/1299] D_loss: -0.0533, G_loss: 0.4820\n",
      "  Batch [1210/1299] D_loss: -0.1630, G_loss: 0.5675\n",
      "  Batch [1220/1299] D_loss: -0.0260, G_loss: 0.2966\n",
      "  Batch [1230/1299] D_loss: -0.0628, G_loss: 0.1952\n",
      "  Batch [1240/1299] D_loss: -0.5185, G_loss: -1.5180\n",
      "  Batch [1250/1299] D_loss: -0.0662, G_loss: 0.2399\n",
      "  Batch [1260/1299] D_loss: -0.0451, G_loss: 0.3949\n",
      "  Batch [1270/1299] D_loss: -0.0640, G_loss: 0.5681\n",
      "  Batch [1280/1299] D_loss: 0.0013, G_loss: 0.5456\n",
      "  Batch [1290/1299] D_loss: -0.1275, G_loss: 0.4682\n",
      "\n",
      "Epoch 85 Summary:\n",
      "  Average D_loss: -0.1572\n",
      "  Average G_loss: -0.0682\n",
      "\n",
      "Epoch [86/100]\n",
      "  Batch [0/1299] D_loss: -0.0626, G_loss: 0.3824\n",
      "  Batch [10/1299] D_loss: -0.1076, G_loss: 0.4543\n",
      "  Batch [20/1299] D_loss: -0.1304, G_loss: -1.4247\n",
      "  Batch [30/1299] D_loss: -0.3787, G_loss: -0.5741\n",
      "  Batch [40/1299] D_loss: -0.3628, G_loss: -0.8484\n",
      "  Batch [50/1299] D_loss: -0.0261, G_loss: 0.2765\n",
      "  Batch [60/1299] D_loss: -0.0439, G_loss: 0.3854\n",
      "  Batch [70/1299] D_loss: -0.0679, G_loss: 0.4897\n",
      "  Batch [80/1299] D_loss: -0.0373, G_loss: 0.5272\n",
      "  Batch [90/1299] D_loss: 0.0209, G_loss: 0.4346\n",
      "  Batch [100/1299] D_loss: -0.0628, G_loss: 0.4211\n",
      "  Batch [110/1299] D_loss: -2.2041, G_loss: -4.6888\n",
      "  Batch [120/1299] D_loss: -0.0362, G_loss: 0.1488\n",
      "  Batch [130/1299] D_loss: -0.0242, G_loss: 0.2274\n",
      "  Batch [140/1299] D_loss: -0.0575, G_loss: 0.3619\n",
      "  Batch [150/1299] D_loss: -0.0496, G_loss: 0.3305\n",
      "  Batch [160/1299] D_loss: 0.0413, G_loss: 0.2240\n",
      "  Batch [170/1299] D_loss: -1.8107, G_loss: -6.2989\n",
      "  Batch [180/1299] D_loss: -0.1649, G_loss: -0.2275\n",
      "  Batch [190/1299] D_loss: -0.0137, G_loss: 0.1423\n",
      "  Batch [200/1299] D_loss: -0.0036, G_loss: 0.0739\n",
      "  Batch [210/1299] D_loss: -0.3608, G_loss: -0.6894\n",
      "  Batch [220/1299] D_loss: -0.0402, G_loss: 0.1237\n",
      "  Batch [230/1299] D_loss: 0.0072, G_loss: 0.2395\n",
      "  Batch [240/1299] D_loss: -0.0378, G_loss: 0.3264\n",
      "  Batch [250/1299] D_loss: -0.0611, G_loss: 0.4156\n",
      "  Batch [260/1299] D_loss: -0.0047, G_loss: 0.2777\n",
      "  Batch [270/1299] D_loss: -1.4441, G_loss: -4.0397\n",
      "  Batch [280/1299] D_loss: 0.0071, G_loss: 0.2551\n",
      "  Batch [290/1299] D_loss: -0.1268, G_loss: 0.4449\n",
      "  Batch [300/1299] D_loss: -0.0981, G_loss: 0.5844\n",
      "  Batch [310/1299] D_loss: 0.0002, G_loss: 0.1775\n",
      "  Batch [320/1299] D_loss: -0.0251, G_loss: 0.0890\n",
      "  Batch [330/1299] D_loss: -0.0590, G_loss: -1.6618\n",
      "  Batch [340/1299] D_loss: -0.0498, G_loss: 0.1535\n",
      "  Batch [350/1299] D_loss: -0.0695, G_loss: 0.3439\n",
      "  Batch [360/1299] D_loss: -0.1563, G_loss: 0.5084\n",
      "  Batch [370/1299] D_loss: 0.0027, G_loss: 0.4905\n",
      "  Batch [380/1299] D_loss: -0.0852, G_loss: 0.5191\n",
      "  Batch [390/1299] D_loss: -0.0184, G_loss: 0.2889\n",
      "  Batch [400/1299] D_loss: -0.1744, G_loss: 0.2194\n",
      "  Batch [410/1299] D_loss: -0.4453, G_loss: -1.8792\n",
      "  Batch [420/1299] D_loss: -0.0223, G_loss: 0.2124\n",
      "  Batch [430/1299] D_loss: 0.0588, G_loss: 0.2881\n",
      "  Batch [440/1299] D_loss: -0.0949, G_loss: 0.3697\n",
      "  Batch [450/1299] D_loss: 0.0218, G_loss: 0.3254\n",
      "  Batch [460/1299] D_loss: -0.0024, G_loss: 0.1850\n",
      "  Batch [470/1299] D_loss: -0.0484, G_loss: 0.0920\n",
      "  Batch [480/1299] D_loss: 0.0060, G_loss: 0.2050\n",
      "  Batch [490/1299] D_loss: 0.0057, G_loss: 0.3069\n",
      "  Batch [500/1299] D_loss: 0.0282, G_loss: 0.2731\n",
      "  Batch [510/1299] D_loss: -0.0166, G_loss: 0.2330\n",
      "  Batch [520/1299] D_loss: -0.6681, G_loss: -0.5056\n",
      "  Batch [530/1299] D_loss: -0.4534, G_loss: -0.3261\n",
      "  Batch [540/1299] D_loss: -0.0305, G_loss: 0.2858\n",
      "  Batch [550/1299] D_loss: -0.0109, G_loss: 0.4005\n",
      "  Batch [560/1299] D_loss: -0.0157, G_loss: 0.4561\n",
      "  Batch [570/1299] D_loss: -0.0535, G_loss: 0.3136\n",
      "  Batch [580/1299] D_loss: 0.0062, G_loss: 0.2039\n",
      "  Batch [590/1299] D_loss: -0.0963, G_loss: 0.2219\n",
      "  Batch [600/1299] D_loss: -0.2313, G_loss: -0.3791\n",
      "  Batch [610/1299] D_loss: -0.0462, G_loss: 0.2027\n",
      "  Batch [620/1299] D_loss: 0.0197, G_loss: 0.1441\n",
      "  Batch [630/1299] D_loss: -0.0280, G_loss: 0.1769\n",
      "  Batch [640/1299] D_loss: -0.8756, G_loss: -1.1820\n",
      "  Batch [650/1299] D_loss: -0.6310, G_loss: -1.0312\n",
      "  Batch [660/1299] D_loss: -0.0953, G_loss: 0.3016\n",
      "  Batch [670/1299] D_loss: -0.0743, G_loss: 0.4311\n",
      "  Batch [680/1299] D_loss: -0.0496, G_loss: 0.4646\n",
      "  Batch [690/1299] D_loss: -0.0734, G_loss: 0.5341\n",
      "  Batch [700/1299] D_loss: -0.0037, G_loss: 0.5378\n",
      "  Batch [710/1299] D_loss: -0.0404, G_loss: 0.2433\n",
      "  Batch [720/1299] D_loss: -2.4811, G_loss: -4.5317\n",
      "  Batch [730/1299] D_loss: -0.1319, G_loss: -0.0152\n",
      "  Batch [740/1299] D_loss: -0.3220, G_loss: 0.0010\n",
      "  Batch [750/1299] D_loss: -0.8145, G_loss: -1.7754\n",
      "  Batch [760/1299] D_loss: -0.2174, G_loss: -0.0420\n",
      "  Batch [770/1299] D_loss: -0.2595, G_loss: -0.0758\n",
      "  Batch [780/1299] D_loss: -0.9578, G_loss: -0.7258\n",
      "  Batch [790/1299] D_loss: -0.2293, G_loss: -0.5296\n",
      "  Batch [800/1299] D_loss: -0.8716, G_loss: 0.0932\n",
      "  Batch [810/1299] D_loss: -0.1494, G_loss: 0.1437\n",
      "  Batch [820/1299] D_loss: -0.1093, G_loss: -0.2161\n",
      "  Batch [830/1299] D_loss: -0.0588, G_loss: 0.2517\n",
      "  Batch [840/1299] D_loss: -0.0304, G_loss: 0.5332\n",
      "  Batch [850/1299] D_loss: -0.2594, G_loss: 0.6458\n",
      "  Batch [860/1299] D_loss: -0.2193, G_loss: 0.8363\n",
      "  Batch [870/1299] D_loss: -0.0991, G_loss: 0.5668\n",
      "  Batch [880/1299] D_loss: -0.0932, G_loss: 0.5113\n",
      "  Batch [890/1299] D_loss: -0.1190, G_loss: 0.3893\n",
      "  Batch [900/1299] D_loss: -0.6405, G_loss: -0.6203\n",
      "  Batch [910/1299] D_loss: -0.4384, G_loss: -0.2612\n",
      "  Batch [920/1299] D_loss: -0.0564, G_loss: 0.1511\n",
      "  Batch [930/1299] D_loss: -0.0273, G_loss: 0.2611\n",
      "  Batch [940/1299] D_loss: -0.1356, G_loss: 0.3220\n",
      "  Batch [950/1299] D_loss: -0.0455, G_loss: 0.4226\n",
      "  Batch [960/1299] D_loss: -0.0410, G_loss: 0.3787\n",
      "  Batch [970/1299] D_loss: -0.0319, G_loss: 0.2684\n",
      "  Batch [980/1299] D_loss: -2.0789, G_loss: -6.8020\n",
      "  Batch [990/1299] D_loss: 0.0109, G_loss: 0.2229\n",
      "  Batch [1000/1299] D_loss: 0.0079, G_loss: 0.3243\n",
      "  Batch [1010/1299] D_loss: -0.0393, G_loss: 0.3291\n",
      "  Batch [1020/1299] D_loss: -0.0816, G_loss: 0.4817\n",
      "  Batch [1030/1299] D_loss: 0.0109, G_loss: 0.2529\n",
      "  Batch [1040/1299] D_loss: -1.4969, G_loss: -3.1454\n",
      "  Batch [1050/1299] D_loss: -0.6779, G_loss: -0.1111\n",
      "  Batch [1060/1299] D_loss: -0.9544, G_loss: -0.3013\n",
      "  Batch [1070/1299] D_loss: -0.2250, G_loss: 0.0247\n",
      "  Batch [1080/1299] D_loss: -0.5631, G_loss: -0.4315\n",
      "  Batch [1090/1299] D_loss: -0.0371, G_loss: 0.2401\n",
      "  Batch [1100/1299] D_loss: -0.1512, G_loss: 0.5279\n",
      "  Batch [1110/1299] D_loss: -0.2074, G_loss: 0.6394\n",
      "  Batch [1120/1299] D_loss: -0.0309, G_loss: 0.7150\n",
      "  Batch [1130/1299] D_loss: -0.0322, G_loss: 0.4819\n",
      "  Batch [1140/1299] D_loss: 0.0007, G_loss: 0.5787\n",
      "  Batch [1150/1299] D_loss: -0.0382, G_loss: 0.2450\n",
      "  Batch [1160/1299] D_loss: 0.1422, G_loss: -0.0197\n",
      "  Batch [1170/1299] D_loss: -0.0069, G_loss: 0.1086\n",
      "  Batch [1180/1299] D_loss: -0.0202, G_loss: 0.2119\n",
      "  Batch [1190/1299] D_loss: -0.0289, G_loss: 0.3672\n",
      "  Batch [1200/1299] D_loss: 0.0134, G_loss: 0.3457\n",
      "  Batch [1210/1299] D_loss: 0.0827, G_loss: 0.4598\n",
      "  Batch [1220/1299] D_loss: -0.0623, G_loss: 0.2811\n",
      "  Batch [1230/1299] D_loss: -1.3488, G_loss: -2.3336\n",
      "  Batch [1240/1299] D_loss: -0.0114, G_loss: 0.1531\n",
      "  Batch [1250/1299] D_loss: 0.0047, G_loss: 0.1539\n",
      "  Batch [1260/1299] D_loss: -0.0021, G_loss: 0.1181\n",
      "  Batch [1270/1299] D_loss: -1.0185, G_loss: -1.0811\n",
      "  Batch [1280/1299] D_loss: 0.0425, G_loss: 0.1681\n",
      "  Batch [1290/1299] D_loss: -0.2015, G_loss: 0.0119\n",
      "\n",
      "Epoch 86 Summary:\n",
      "  Average D_loss: -0.1364\n",
      "  Average G_loss: -0.0705\n",
      "\n",
      "Epoch [87/100]\n",
      "  Batch [0/1299] D_loss: -0.0320, G_loss: 0.1023\n",
      "  Batch [10/1299] D_loss: -1.5437, G_loss: -2.0506\n",
      "  Batch [20/1299] D_loss: -0.5478, G_loss: -0.1694\n",
      "  Batch [30/1299] D_loss: -0.0276, G_loss: 0.1183\n",
      "  Batch [40/1299] D_loss: -1.1333, G_loss: -1.1561\n",
      "  Batch [50/1299] D_loss: -0.1560, G_loss: 0.1270\n",
      "  Batch [60/1299] D_loss: -0.0475, G_loss: 0.4566\n",
      "  Batch [70/1299] D_loss: -0.0208, G_loss: 0.5968\n",
      "  Batch [80/1299] D_loss: -0.0180, G_loss: 0.8206\n",
      "  Batch [90/1299] D_loss: -0.1339, G_loss: 0.7412\n",
      "  Batch [100/1299] D_loss: -0.0147, G_loss: 0.4523\n",
      "  Batch [110/1299] D_loss: -0.0623, G_loss: -0.9381\n",
      "  Batch [120/1299] D_loss: -0.7379, G_loss: -1.6104\n",
      "  Batch [130/1299] D_loss: -0.0857, G_loss: 0.2546\n",
      "  Batch [140/1299] D_loss: 0.0039, G_loss: 0.2903\n",
      "  Batch [150/1299] D_loss: -0.1530, G_loss: 0.3896\n",
      "  Batch [160/1299] D_loss: -0.0158, G_loss: 0.3414\n",
      "  Batch [170/1299] D_loss: 0.0255, G_loss: 0.2943\n",
      "  Batch [180/1299] D_loss: -2.6266, G_loss: -4.5396\n",
      "  Batch [190/1299] D_loss: -0.1674, G_loss: -0.1933\n",
      "  Batch [200/1299] D_loss: -2.1299, G_loss: -3.3524\n",
      "  Batch [210/1299] D_loss: -0.0009, G_loss: 0.1642\n",
      "  Batch [220/1299] D_loss: -0.0752, G_loss: 0.3076\n",
      "  Batch [230/1299] D_loss: -0.1200, G_loss: 0.4257\n",
      "  Batch [240/1299] D_loss: 0.0173, G_loss: 0.4376\n",
      "  Batch [250/1299] D_loss: -0.2849, G_loss: 0.5950\n",
      "  Batch [260/1299] D_loss: -0.0267, G_loss: 0.4075\n",
      "  Batch [270/1299] D_loss: -1.4639, G_loss: -1.6468\n",
      "  Batch [280/1299] D_loss: -0.0130, G_loss: 0.0627\n",
      "  Batch [290/1299] D_loss: 0.0041, G_loss: 0.1016\n",
      "  Batch [300/1299] D_loss: -0.4318, G_loss: -0.2980\n",
      "  Batch [310/1299] D_loss: -0.5358, G_loss: -1.1440\n",
      "  Batch [320/1299] D_loss: -0.0705, G_loss: 0.1368\n",
      "  Batch [330/1299] D_loss: -0.8406, G_loss: 0.0019\n",
      "  Batch [340/1299] D_loss: -0.0022, G_loss: 0.1696\n",
      "  Batch [350/1299] D_loss: -0.0907, G_loss: 0.4850\n",
      "  Batch [360/1299] D_loss: -0.1569, G_loss: 0.5310\n",
      "  Batch [370/1299] D_loss: -0.0872, G_loss: 0.6552\n",
      "  Batch [380/1299] D_loss: -0.0058, G_loss: 0.5404\n",
      "  Batch [390/1299] D_loss: -0.1257, G_loss: 0.4060\n",
      "  Batch [400/1299] D_loss: -1.0542, G_loss: -0.4496\n",
      "  Batch [410/1299] D_loss: -0.7431, G_loss: 0.0036\n",
      "  Batch [420/1299] D_loss: -0.0462, G_loss: 0.0425\n",
      "  Batch [430/1299] D_loss: -0.9684, G_loss: -1.8998\n",
      "  Batch [440/1299] D_loss: -0.5555, G_loss: -0.5048\n",
      "  Batch [450/1299] D_loss: -0.0828, G_loss: 0.3021\n",
      "  Batch [460/1299] D_loss: -0.1983, G_loss: 0.6505\n",
      "  Batch [470/1299] D_loss: -0.1544, G_loss: 0.7154\n",
      "  Batch [480/1299] D_loss: -0.1252, G_loss: 0.7273\n",
      "  Batch [490/1299] D_loss: -0.0738, G_loss: 0.3629\n",
      "  Batch [500/1299] D_loss: -0.1717, G_loss: 0.5476\n",
      "  Batch [510/1299] D_loss: -3.2246, G_loss: -7.3139\n",
      "  Batch [520/1299] D_loss: -0.3556, G_loss: -0.4851\n",
      "  Batch [530/1299] D_loss: -0.0525, G_loss: 0.2841\n",
      "  Batch [540/1299] D_loss: -0.2501, G_loss: 0.3539\n",
      "  Batch [550/1299] D_loss: -0.7803, G_loss: -0.4719\n",
      "  Batch [560/1299] D_loss: -1.0488, G_loss: -0.6489\n",
      "  Batch [570/1299] D_loss: -1.3041, G_loss: -0.6946\n",
      "  Batch [580/1299] D_loss: -0.8424, G_loss: 0.0617\n",
      "  Batch [590/1299] D_loss: -1.1822, G_loss: -0.0140\n",
      "  Batch [600/1299] D_loss: -0.1056, G_loss: 0.3295\n",
      "  Batch [610/1299] D_loss: -0.2117, G_loss: 0.5861\n",
      "  Batch [620/1299] D_loss: -0.0537, G_loss: 0.5483\n",
      "  Batch [630/1299] D_loss: -0.0608, G_loss: 0.6054\n",
      "  Batch [640/1299] D_loss: -0.0982, G_loss: 0.4488\n",
      "  Batch [650/1299] D_loss: -0.0389, G_loss: 0.3056\n",
      "  Batch [660/1299] D_loss: -1.7882, G_loss: -3.9339\n",
      "  Batch [670/1299] D_loss: -0.0373, G_loss: 0.2609\n",
      "  Batch [680/1299] D_loss: -0.0530, G_loss: 0.3637\n",
      "  Batch [690/1299] D_loss: -0.0983, G_loss: 0.4188\n",
      "  Batch [700/1299] D_loss: -0.0648, G_loss: 0.3628\n",
      "  Batch [710/1299] D_loss: 0.0145, G_loss: 0.2321\n",
      "  Batch [720/1299] D_loss: -0.4811, G_loss: -2.7562\n",
      "  Batch [730/1299] D_loss: -0.5124, G_loss: -0.1619\n",
      "  Batch [740/1299] D_loss: -0.4929, G_loss: -0.6636\n",
      "  Batch [750/1299] D_loss: -0.8227, G_loss: -1.0826\n",
      "  Batch [760/1299] D_loss: 0.0066, G_loss: 0.4419\n",
      "  Batch [770/1299] D_loss: -0.0649, G_loss: 0.5463\n",
      "  Batch [780/1299] D_loss: -0.1483, G_loss: 0.6877\n",
      "  Batch [790/1299] D_loss: -0.1192, G_loss: 0.7195\n",
      "  Batch [800/1299] D_loss: 0.0488, G_loss: 0.4080\n",
      "  Batch [810/1299] D_loss: -0.0347, G_loss: 0.1895\n",
      "  Batch [820/1299] D_loss: -3.0336, G_loss: -5.6537\n",
      "  Batch [830/1299] D_loss: 0.0255, G_loss: 0.0682\n",
      "  Batch [840/1299] D_loss: -0.0094, G_loss: 0.1588\n",
      "  Batch [850/1299] D_loss: -0.0180, G_loss: 0.2713\n",
      "  Batch [860/1299] D_loss: -0.0149, G_loss: 0.3516\n",
      "  Batch [870/1299] D_loss: -0.0582, G_loss: 0.3663\n",
      "  Batch [880/1299] D_loss: -0.0881, G_loss: 0.3923\n",
      "  Batch [890/1299] D_loss: -0.8007, G_loss: -0.2326\n",
      "  Batch [900/1299] D_loss: -0.6007, G_loss: -1.6474\n",
      "  Batch [910/1299] D_loss: -0.0221, G_loss: 0.1552\n",
      "  Batch [920/1299] D_loss: -0.3557, G_loss: -1.4062\n",
      "  Batch [930/1299] D_loss: -0.0068, G_loss: 0.1361\n",
      "  Batch [940/1299] D_loss: -0.0758, G_loss: 0.2865\n",
      "  Batch [950/1299] D_loss: -0.0626, G_loss: 0.4246\n",
      "  Batch [960/1299] D_loss: -0.0822, G_loss: 0.5055\n",
      "  Batch [970/1299] D_loss: -0.2087, G_loss: 0.6873\n",
      "  Batch [980/1299] D_loss: -0.0642, G_loss: 0.5031\n",
      "  Batch [990/1299] D_loss: -0.0190, G_loss: 0.3624\n",
      "  Batch [1000/1299] D_loss: 0.9592, G_loss: -2.4448\n",
      "  Batch [1010/1299] D_loss: -0.2370, G_loss: -0.3599\n",
      "  Batch [1020/1299] D_loss: -0.0229, G_loss: 0.1385\n",
      "  Batch [1030/1299] D_loss: -0.0928, G_loss: 0.2996\n",
      "  Batch [1040/1299] D_loss: -0.0618, G_loss: 0.4361\n",
      "  Batch [1050/1299] D_loss: -0.0082, G_loss: 0.4346\n",
      "  Batch [1060/1299] D_loss: -0.0985, G_loss: 0.4469\n",
      "  Batch [1070/1299] D_loss: -0.0421, G_loss: 0.4100\n",
      "  Batch [1080/1299] D_loss: -0.0253, G_loss: 0.2717\n",
      "  Batch [1090/1299] D_loss: 0.0157, G_loss: 0.0918\n",
      "  Batch [1100/1299] D_loss: -0.0082, G_loss: 0.1854\n",
      "  Batch [1110/1299] D_loss: -0.0291, G_loss: 0.2654\n",
      "  Batch [1120/1299] D_loss: -0.0729, G_loss: 0.2852\n",
      "  Batch [1130/1299] D_loss: 0.0283, G_loss: 0.1343\n",
      "  Batch [1140/1299] D_loss: -0.7062, G_loss: -1.6859\n",
      "  Batch [1150/1299] D_loss: -0.4537, G_loss: -0.8473\n",
      "  Batch [1160/1299] D_loss: -1.1243, G_loss: -1.0215\n",
      "  Batch [1170/1299] D_loss: -0.3251, G_loss: -0.0857\n",
      "  Batch [1180/1299] D_loss: -0.0369, G_loss: 0.1128\n",
      "  Batch [1190/1299] D_loss: -0.0877, G_loss: 0.4725\n",
      "  Batch [1200/1299] D_loss: -0.0263, G_loss: 0.7875\n",
      "  Batch [1210/1299] D_loss: 0.0241, G_loss: 0.6903\n",
      "  Batch [1220/1299] D_loss: -0.1004, G_loss: 0.7398\n",
      "  Batch [1230/1299] D_loss: -0.1687, G_loss: 0.7579\n",
      "  Batch [1240/1299] D_loss: -0.0496, G_loss: 0.4291\n",
      "  Batch [1250/1299] D_loss: -0.1093, G_loss: 0.2564\n",
      "  Batch [1260/1299] D_loss: -0.7176, G_loss: -0.5594\n",
      "  Batch [1270/1299] D_loss: -0.1893, G_loss: -0.3928\n",
      "  Batch [1280/1299] D_loss: 0.0064, G_loss: 0.1758\n",
      "  Batch [1290/1299] D_loss: -0.0722, G_loss: 0.2654\n",
      "\n",
      "Epoch 87 Summary:\n",
      "  Average D_loss: -0.1581\n",
      "  Average G_loss: -0.0728\n",
      "\n",
      "Epoch [88/100]\n",
      "  Batch [0/1299] D_loss: 0.0110, G_loss: 0.2739\n",
      "  Batch [10/1299] D_loss: -0.0615, G_loss: 0.3911\n",
      "  Batch [20/1299] D_loss: -0.0031, G_loss: 0.3107\n",
      "  Batch [30/1299] D_loss: -2.6051, G_loss: -2.1527\n",
      "  Batch [40/1299] D_loss: -0.2640, G_loss: -0.3116\n",
      "  Batch [50/1299] D_loss: -0.0348, G_loss: 0.2018\n",
      "  Batch [60/1299] D_loss: -0.0480, G_loss: 0.4348\n",
      "  Batch [70/1299] D_loss: -0.0868, G_loss: 0.5585\n",
      "  Batch [80/1299] D_loss: -0.1895, G_loss: 0.5681\n",
      "  Batch [90/1299] D_loss: -0.1558, G_loss: 0.5218\n",
      "  Batch [100/1299] D_loss: -2.8045, G_loss: -2.2650\n",
      "  Batch [110/1299] D_loss: -0.0259, G_loss: 0.1156\n",
      "  Batch [120/1299] D_loss: -0.9002, G_loss: -3.2352\n",
      "  Batch [130/1299] D_loss: -0.5305, G_loss: -0.3955\n",
      "  Batch [140/1299] D_loss: -0.0799, G_loss: -0.0078\n",
      "  Batch [150/1299] D_loss: -0.3743, G_loss: -0.1125\n",
      "  Batch [160/1299] D_loss: -0.0464, G_loss: 0.2578\n",
      "  Batch [170/1299] D_loss: -0.0531, G_loss: 0.5061\n",
      "  Batch [180/1299] D_loss: -0.0299, G_loss: 0.3928\n",
      "  Batch [190/1299] D_loss: -0.0492, G_loss: 0.4090\n",
      "  Batch [200/1299] D_loss: -0.0191, G_loss: 0.2819\n",
      "  Batch [210/1299] D_loss: -1.4944, G_loss: -2.0487\n",
      "  Batch [220/1299] D_loss: -0.4250, G_loss: -0.2474\n",
      "  Batch [230/1299] D_loss: -1.6501, G_loss: -1.7570\n",
      "  Batch [240/1299] D_loss: -0.6930, G_loss: -0.6391\n",
      "  Batch [250/1299] D_loss: -0.7528, G_loss: -0.4854\n",
      "  Batch [260/1299] D_loss: -0.6719, G_loss: -0.1207\n",
      "  Batch [270/1299] D_loss: -0.3862, G_loss: -0.2037\n",
      "  Batch [280/1299] D_loss: -0.0144, G_loss: 0.3600\n",
      "  Batch [290/1299] D_loss: -0.0251, G_loss: 0.5260\n",
      "  Batch [300/1299] D_loss: -0.0766, G_loss: 0.7493\n",
      "  Batch [310/1299] D_loss: -0.1132, G_loss: 0.7021\n",
      "  Batch [320/1299] D_loss: -0.0701, G_loss: 0.5601\n",
      "  Batch [330/1299] D_loss: -0.0871, G_loss: 0.4342\n",
      "  Batch [340/1299] D_loss: -3.1715, G_loss: -1.3118\n",
      "  Batch [350/1299] D_loss: -0.1535, G_loss: 0.0034\n",
      "  Batch [360/1299] D_loss: -1.1964, G_loss: -0.7504\n",
      "  Batch [370/1299] D_loss: -0.1019, G_loss: 0.1112\n",
      "  Batch [380/1299] D_loss: -0.3533, G_loss: 0.0251\n",
      "  Batch [390/1299] D_loss: -0.1525, G_loss: 0.0696\n",
      "  Batch [400/1299] D_loss: -1.4417, G_loss: -1.0301\n",
      "  Batch [410/1299] D_loss: -0.4615, G_loss: -0.3312\n",
      "  Batch [420/1299] D_loss: -0.7466, G_loss: 0.1752\n",
      "  Batch [430/1299] D_loss: -0.0920, G_loss: 0.3850\n",
      "  Batch [440/1299] D_loss: -0.0659, G_loss: 0.6021\n",
      "  Batch [450/1299] D_loss: -0.1932, G_loss: 0.6508\n",
      "  Batch [460/1299] D_loss: -0.0627, G_loss: 0.6317\n",
      "  Batch [470/1299] D_loss: -0.0631, G_loss: 0.6984\n",
      "  Batch [480/1299] D_loss: 0.0479, G_loss: 0.4438\n",
      "  Batch [490/1299] D_loss: -0.0207, G_loss: 0.2474\n",
      "  Batch [500/1299] D_loss: -1.6285, G_loss: -4.2137\n",
      "  Batch [510/1299] D_loss: -0.7655, G_loss: -0.9443\n",
      "  Batch [520/1299] D_loss: -0.1158, G_loss: 0.0037\n",
      "  Batch [530/1299] D_loss: -1.2178, G_loss: -2.6196\n",
      "  Batch [540/1299] D_loss: -0.6267, G_loss: 0.0540\n",
      "  Batch [550/1299] D_loss: -0.3226, G_loss: 0.1304\n",
      "  Batch [560/1299] D_loss: -0.2352, G_loss: 0.1278\n",
      "  Batch [570/1299] D_loss: -0.8934, G_loss: -0.4146\n",
      "  Batch [580/1299] D_loss: -0.7755, G_loss: -2.1116\n",
      "  Batch [590/1299] D_loss: -0.0047, G_loss: 0.1839\n",
      "  Batch [600/1299] D_loss: -0.2091, G_loss: 0.1781\n",
      "  Batch [610/1299] D_loss: -0.1570, G_loss: 0.5068\n",
      "  Batch [620/1299] D_loss: -0.2810, G_loss: 0.7912\n",
      "  Batch [630/1299] D_loss: -0.3255, G_loss: 0.9912\n",
      "  Batch [640/1299] D_loss: -0.0981, G_loss: 1.0042\n",
      "  Batch [650/1299] D_loss: 0.0279, G_loss: 0.6155\n",
      "  Batch [660/1299] D_loss: -0.2137, G_loss: 0.4565\n",
      "  Batch [670/1299] D_loss: -2.4279, G_loss: -2.6267\n",
      "  Batch [680/1299] D_loss: -0.3326, G_loss: -0.0092\n",
      "  Batch [690/1299] D_loss: -1.1539, G_loss: -1.8126\n",
      "  Batch [700/1299] D_loss: -0.7759, G_loss: -0.5864\n",
      "  Batch [710/1299] D_loss: -0.5658, G_loss: -0.3140\n",
      "  Batch [720/1299] D_loss: -0.0733, G_loss: 0.2189\n",
      "  Batch [730/1299] D_loss: -0.1022, G_loss: 0.3571\n",
      "  Batch [740/1299] D_loss: -0.0377, G_loss: 0.5102\n",
      "  Batch [750/1299] D_loss: -0.2192, G_loss: 0.7222\n",
      "  Batch [760/1299] D_loss: -0.0308, G_loss: 0.7254\n",
      "  Batch [770/1299] D_loss: -0.1535, G_loss: 0.6538\n",
      "  Batch [780/1299] D_loss: -0.0895, G_loss: 0.4840\n",
      "  Batch [790/1299] D_loss: -0.5503, G_loss: -1.0674\n",
      "  Batch [800/1299] D_loss: -1.3586, G_loss: -2.8901\n",
      "  Batch [810/1299] D_loss: -0.1608, G_loss: 0.0108\n",
      "  Batch [820/1299] D_loss: -1.1567, G_loss: -1.5294\n",
      "  Batch [830/1299] D_loss: -0.8855, G_loss: 0.0460\n",
      "  Batch [840/1299] D_loss: -0.5284, G_loss: -0.1305\n",
      "  Batch [850/1299] D_loss: -0.2341, G_loss: 0.0924\n",
      "  Batch [860/1299] D_loss: -0.6927, G_loss: 0.0773\n",
      "  Batch [870/1299] D_loss: -1.1100, G_loss: -0.6470\n",
      "  Batch [880/1299] D_loss: -0.1955, G_loss: -0.1793\n",
      "  Batch [890/1299] D_loss: -0.1589, G_loss: 0.4128\n",
      "  Batch [900/1299] D_loss: -0.0793, G_loss: 0.6202\n",
      "  Batch [910/1299] D_loss: -0.1977, G_loss: 0.7139\n",
      "  Batch [920/1299] D_loss: -0.0831, G_loss: 0.8480\n",
      "  Batch [930/1299] D_loss: -0.0012, G_loss: 0.6449\n",
      "  Batch [940/1299] D_loss: 0.0143, G_loss: 0.3540\n",
      "  Batch [950/1299] D_loss: -0.0589, G_loss: 0.3225\n",
      "  Batch [960/1299] D_loss: -1.1746, G_loss: -2.3533\n",
      "  Batch [970/1299] D_loss: 0.0028, G_loss: 0.1576\n",
      "  Batch [980/1299] D_loss: -0.0358, G_loss: 0.3341\n",
      "  Batch [990/1299] D_loss: -0.0461, G_loss: 0.3290\n",
      "  Batch [1000/1299] D_loss: -0.0060, G_loss: 0.3137\n",
      "  Batch [1010/1299] D_loss: -0.0334, G_loss: 0.3228\n",
      "  Batch [1020/1299] D_loss: -0.0380, G_loss: 0.2740\n",
      "  Batch [1030/1299] D_loss: -1.1829, G_loss: -1.6017\n",
      "  Batch [1040/1299] D_loss: -0.2084, G_loss: 0.0580\n",
      "  Batch [1050/1299] D_loss: -0.3931, G_loss: -0.0084\n",
      "  Batch [1060/1299] D_loss: 0.0073, G_loss: 0.0718\n",
      "  Batch [1070/1299] D_loss: -0.0469, G_loss: 0.2166\n",
      "  Batch [1080/1299] D_loss: -0.0572, G_loss: 0.4757\n",
      "  Batch [1090/1299] D_loss: -0.0681, G_loss: 0.6358\n",
      "  Batch [1100/1299] D_loss: -0.0276, G_loss: 0.6927\n",
      "  Batch [1110/1299] D_loss: -0.1803, G_loss: 0.6422\n",
      "  Batch [1120/1299] D_loss: -0.0692, G_loss: 0.4460\n",
      "  Batch [1130/1299] D_loss: -1.2632, G_loss: -2.7145\n",
      "  Batch [1140/1299] D_loss: 0.0307, G_loss: 0.1863\n",
      "  Batch [1150/1299] D_loss: -0.0122, G_loss: 0.1350\n",
      "  Batch [1160/1299] D_loss: -0.0036, G_loss: 0.1662\n",
      "  Batch [1170/1299] D_loss: -0.0293, G_loss: 0.1883\n",
      "  Batch [1180/1299] D_loss: -0.0824, G_loss: 0.2941\n",
      "  Batch [1190/1299] D_loss: -0.0619, G_loss: 0.1344\n",
      "  Batch [1200/1299] D_loss: 0.0172, G_loss: 0.1437\n",
      "  Batch [1210/1299] D_loss: -0.2907, G_loss: 0.0455\n",
      "  Batch [1220/1299] D_loss: -0.1585, G_loss: -0.0329\n",
      "  Batch [1230/1299] D_loss: -0.0310, G_loss: 0.1536\n",
      "  Batch [1240/1299] D_loss: -0.0809, G_loss: 0.3152\n",
      "  Batch [1250/1299] D_loss: -0.0499, G_loss: 0.3799\n",
      "  Batch [1260/1299] D_loss: -0.0772, G_loss: 0.4577\n",
      "  Batch [1270/1299] D_loss: -0.0781, G_loss: 0.4572\n",
      "  Batch [1280/1299] D_loss: -0.4746, G_loss: -1.1837\n",
      "  Batch [1290/1299] D_loss: -0.6590, G_loss: -0.7252\n",
      "\n",
      "Epoch 88 Summary:\n",
      "  Average D_loss: -0.1924\n",
      "  Average G_loss: -0.1030\n",
      "\n",
      "Epoch [89/100]\n",
      "  Batch [0/1299] D_loss: -0.6544, G_loss: -1.2047\n",
      "  Batch [10/1299] D_loss: -0.1241, G_loss: 0.1720\n",
      "  Batch [20/1299] D_loss: -0.0518, G_loss: 0.2716\n",
      "  Batch [30/1299] D_loss: -0.1600, G_loss: 0.4643\n",
      "  Batch [40/1299] D_loss: 0.0233, G_loss: 0.3195\n",
      "  Batch [50/1299] D_loss: -0.0169, G_loss: 0.3562\n",
      "  Batch [60/1299] D_loss: -2.3397, G_loss: -2.3674\n",
      "  Batch [70/1299] D_loss: -0.7529, G_loss: -0.5041\n",
      "  Batch [80/1299] D_loss: -0.0430, G_loss: 0.0968\n",
      "  Batch [90/1299] D_loss: -0.0127, G_loss: 0.2346\n",
      "  Batch [100/1299] D_loss: -0.1023, G_loss: 0.3237\n",
      "  Batch [110/1299] D_loss: -0.0571, G_loss: 0.4708\n",
      "  Batch [120/1299] D_loss: -0.0777, G_loss: 0.4666\n",
      "  Batch [130/1299] D_loss: -0.1813, G_loss: 0.4656\n",
      "  Batch [140/1299] D_loss: -0.0773, G_loss: 0.2608\n",
      "  Batch [150/1299] D_loss: -0.3925, G_loss: -2.9959\n",
      "  Batch [160/1299] D_loss: -0.6911, G_loss: -2.5431\n",
      "  Batch [170/1299] D_loss: -0.7113, G_loss: -0.3698\n",
      "  Batch [180/1299] D_loss: -0.0095, G_loss: 0.2211\n",
      "  Batch [190/1299] D_loss: 0.0071, G_loss: 0.3585\n",
      "  Batch [200/1299] D_loss: -0.1241, G_loss: 0.4658\n",
      "  Batch [210/1299] D_loss: -0.0573, G_loss: 0.4887\n",
      "  Batch [220/1299] D_loss: -0.0981, G_loss: 0.4559\n",
      "  Batch [230/1299] D_loss: 0.0788, G_loss: 0.2963\n",
      "  Batch [240/1299] D_loss: -0.3622, G_loss: -0.1307\n",
      "  Batch [250/1299] D_loss: -1.1092, G_loss: -1.4187\n",
      "  Batch [260/1299] D_loss: -1.7638, G_loss: -5.5944\n",
      "  Batch [270/1299] D_loss: -0.0870, G_loss: 0.3187\n",
      "  Batch [280/1299] D_loss: -0.2020, G_loss: 0.5021\n",
      "  Batch [290/1299] D_loss: -0.1854, G_loss: 0.8235\n",
      "  Batch [300/1299] D_loss: -0.1666, G_loss: 0.8150\n",
      "  Batch [310/1299] D_loss: -0.0925, G_loss: 0.6537\n",
      "  Batch [320/1299] D_loss: -0.0442, G_loss: 0.5934\n",
      "  Batch [330/1299] D_loss: -0.2600, G_loss: 0.6024\n",
      "  Batch [340/1299] D_loss: -0.6023, G_loss: -2.7913\n",
      "  Batch [350/1299] D_loss: -0.4489, G_loss: -1.1157\n",
      "  Batch [360/1299] D_loss: -1.7256, G_loss: -3.1823\n",
      "  Batch [370/1299] D_loss: -0.2946, G_loss: 0.0923\n",
      "  Batch [380/1299] D_loss: -1.0790, G_loss: -1.0402\n",
      "  Batch [390/1299] D_loss: -0.4881, G_loss: -0.1138\n",
      "  Batch [400/1299] D_loss: -0.2851, G_loss: -0.0255\n",
      "  Batch [410/1299] D_loss: -0.2818, G_loss: 0.1474\n",
      "  Batch [420/1299] D_loss: -1.0317, G_loss: -0.1293\n",
      "  Batch [430/1299] D_loss: -0.8683, G_loss: 0.0443\n",
      "  Batch [440/1299] D_loss: -0.8322, G_loss: -1.4269\n",
      "  Batch [450/1299] D_loss: -0.2708, G_loss: 0.1240\n",
      "  Batch [460/1299] D_loss: -0.7653, G_loss: -0.3622\n",
      "  Batch [470/1299] D_loss: -0.4687, G_loss: -0.5494\n",
      "  Batch [480/1299] D_loss: -0.4709, G_loss: 0.0534\n",
      "  Batch [490/1299] D_loss: -0.0175, G_loss: 0.2445\n",
      "  Batch [500/1299] D_loss: -0.0375, G_loss: 0.5188\n",
      "  Batch [510/1299] D_loss: -0.1878, G_loss: 0.7508\n",
      "  Batch [520/1299] D_loss: 0.0012, G_loss: 0.6629\n",
      "  Batch [530/1299] D_loss: -0.1444, G_loss: 0.6498\n",
      "  Batch [540/1299] D_loss: -0.1419, G_loss: 0.3796\n",
      "  Batch [550/1299] D_loss: -2.1873, G_loss: -3.2361\n",
      "  Batch [560/1299] D_loss: -1.5193, G_loss: -1.0408\n",
      "  Batch [570/1299] D_loss: -0.3070, G_loss: 0.0315\n",
      "  Batch [580/1299] D_loss: -0.6061, G_loss: -0.8482\n",
      "  Batch [590/1299] D_loss: -0.0234, G_loss: 0.1512\n",
      "  Batch [600/1299] D_loss: -1.1717, G_loss: -1.0726\n",
      "  Batch [610/1299] D_loss: -0.0390, G_loss: 0.4291\n",
      "  Batch [620/1299] D_loss: -0.1771, G_loss: 0.7235\n",
      "  Batch [630/1299] D_loss: -0.1313, G_loss: 0.7453\n",
      "  Batch [640/1299] D_loss: 0.0308, G_loss: 0.6230\n",
      "  Batch [650/1299] D_loss: 0.0313, G_loss: 0.5033\n",
      "  Batch [660/1299] D_loss: -0.0187, G_loss: 0.3798\n",
      "  Batch [670/1299] D_loss: -0.9913, G_loss: -2.0244\n",
      "  Batch [680/1299] D_loss: -0.5353, G_loss: -2.0004\n",
      "  Batch [690/1299] D_loss: -0.4483, G_loss: -0.4467\n",
      "  Batch [700/1299] D_loss: -1.6464, G_loss: -0.5682\n",
      "  Batch [710/1299] D_loss: -0.0587, G_loss: 0.2588\n",
      "  Batch [720/1299] D_loss: -0.0563, G_loss: 0.4589\n",
      "  Batch [730/1299] D_loss: -0.0899, G_loss: 0.6560\n",
      "  Batch [740/1299] D_loss: 0.0864, G_loss: 0.5393\n",
      "  Batch [750/1299] D_loss: -0.0972, G_loss: 0.6113\n",
      "  Batch [760/1299] D_loss: -0.0385, G_loss: 0.6116\n",
      "  Batch [770/1299] D_loss: 0.0154, G_loss: 0.1920\n",
      "  Batch [780/1299] D_loss: -0.9095, G_loss: -1.7764\n",
      "  Batch [790/1299] D_loss: -0.9700, G_loss: -0.1550\n",
      "  Batch [800/1299] D_loss: -1.0696, G_loss: -1.1297\n",
      "  Batch [810/1299] D_loss: -1.2179, G_loss: -0.1905\n",
      "  Batch [820/1299] D_loss: -0.7548, G_loss: 0.0030\n",
      "  Batch [830/1299] D_loss: -0.0608, G_loss: 0.1524\n",
      "  Batch [840/1299] D_loss: -0.0190, G_loss: 0.2218\n",
      "  Batch [850/1299] D_loss: -0.2756, G_loss: 0.1996\n",
      "  Batch [860/1299] D_loss: -0.4465, G_loss: 0.1408\n",
      "  Batch [870/1299] D_loss: -0.0757, G_loss: 0.4520\n",
      "  Batch [880/1299] D_loss: -0.0794, G_loss: 0.5651\n",
      "  Batch [890/1299] D_loss: -0.1304, G_loss: 0.5909\n",
      "  Batch [900/1299] D_loss: -0.2096, G_loss: 0.4520\n",
      "  Batch [910/1299] D_loss: -0.0412, G_loss: 0.3367\n",
      "  Batch [920/1299] D_loss: -1.3106, G_loss: -0.3243\n",
      "  Batch [930/1299] D_loss: -0.0088, G_loss: 0.2321\n",
      "  Batch [940/1299] D_loss: 0.0367, G_loss: 0.2860\n",
      "  Batch [950/1299] D_loss: -0.0419, G_loss: 0.3192\n",
      "  Batch [960/1299] D_loss: -0.0363, G_loss: 0.2419\n",
      "  Batch [970/1299] D_loss: -2.5538, G_loss: -5.9153\n",
      "  Batch [980/1299] D_loss: -0.0414, G_loss: 0.0744\n",
      "  Batch [990/1299] D_loss: -1.5127, G_loss: -1.9527\n",
      "  Batch [1000/1299] D_loss: -0.0544, G_loss: 0.2253\n",
      "  Batch [1010/1299] D_loss: -0.1019, G_loss: 0.5653\n",
      "  Batch [1020/1299] D_loss: -0.1548, G_loss: 0.7040\n",
      "  Batch [1030/1299] D_loss: -0.1210, G_loss: 0.8826\n",
      "  Batch [1040/1299] D_loss: -0.1247, G_loss: 1.0130\n",
      "  Batch [1050/1299] D_loss: 0.0219, G_loss: 0.5754\n",
      "  Batch [1060/1299] D_loss: -0.1436, G_loss: 0.5139\n",
      "  Batch [1070/1299] D_loss: -0.3109, G_loss: -0.1353\n",
      "  Batch [1080/1299] D_loss: -0.0279, G_loss: 0.1604\n",
      "  Batch [1090/1299] D_loss: -0.7148, G_loss: -0.7321\n",
      "  Batch [1100/1299] D_loss: -0.5973, G_loss: 0.0285\n",
      "  Batch [1110/1299] D_loss: -0.0468, G_loss: 0.1772\n",
      "  Batch [1120/1299] D_loss: -0.0112, G_loss: 0.3142\n",
      "  Batch [1130/1299] D_loss: -0.0510, G_loss: 0.4898\n",
      "  Batch [1140/1299] D_loss: -0.1262, G_loss: 0.5883\n",
      "  Batch [1150/1299] D_loss: -0.1276, G_loss: 0.5400\n",
      "  Batch [1160/1299] D_loss: -0.0877, G_loss: 0.5391\n",
      "  Batch [1170/1299] D_loss: -0.1064, G_loss: 0.6051\n",
      "  Batch [1180/1299] D_loss: -0.0442, G_loss: 0.2919\n",
      "  Batch [1190/1299] D_loss: -0.9950, G_loss: -0.7803\n",
      "  Batch [1200/1299] D_loss: -0.7045, G_loss: -0.6081\n",
      "  Batch [1210/1299] D_loss: -0.0991, G_loss: 0.0190\n",
      "  Batch [1220/1299] D_loss: -0.1203, G_loss: 0.1591\n",
      "  Batch [1230/1299] D_loss: -0.0999, G_loss: 0.1547\n",
      "  Batch [1240/1299] D_loss: -1.1213, G_loss: -2.1785\n",
      "  Batch [1250/1299] D_loss: 0.0010, G_loss: 0.2362\n",
      "  Batch [1260/1299] D_loss: -0.0862, G_loss: 0.5078\n",
      "  Batch [1270/1299] D_loss: -0.1143, G_loss: 0.6484\n",
      "  Batch [1280/1299] D_loss: -0.1050, G_loss: 0.7683\n",
      "  Batch [1290/1299] D_loss: -0.0063, G_loss: 0.8204\n",
      "\n",
      "Epoch 89 Summary:\n",
      "  Average D_loss: -0.2027\n",
      "  Average G_loss: -0.0834\n",
      "\n",
      "Epoch [90/100]\n",
      "  Batch [0/1299] D_loss: -0.0854, G_loss: 0.5476\n",
      "  Batch [10/1299] D_loss: -0.0648, G_loss: 0.5834\n",
      "  Batch [20/1299] D_loss: -0.0767, G_loss: 0.3302\n",
      "  Batch [30/1299] D_loss: -1.9986, G_loss: -5.3986\n",
      "  Batch [40/1299] D_loss: -0.3686, G_loss: -0.7411\n",
      "  Batch [50/1299] D_loss: -0.1374, G_loss: 0.1477\n",
      "  Batch [60/1299] D_loss: -0.5445, G_loss: -1.8863\n",
      "  Batch [70/1299] D_loss: -0.9108, G_loss: -0.3835\n",
      "  Batch [80/1299] D_loss: -0.0815, G_loss: 0.1426\n",
      "  Batch [90/1299] D_loss: -0.3003, G_loss: -0.1138\n",
      "  Batch [100/1299] D_loss: -0.0260, G_loss: 0.3566\n",
      "  Batch [110/1299] D_loss: -0.1868, G_loss: 0.5500\n",
      "  Batch [120/1299] D_loss: -0.0163, G_loss: 0.6432\n",
      "  Batch [130/1299] D_loss: -0.1080, G_loss: 0.7067\n",
      "  Batch [140/1299] D_loss: 0.0093, G_loss: 0.6269\n",
      "  Batch [150/1299] D_loss: -0.0048, G_loss: 0.4782\n",
      "  Batch [160/1299] D_loss: -0.0582, G_loss: 0.3538\n",
      "  Batch [170/1299] D_loss: -0.7335, G_loss: -0.1509\n",
      "  Batch [180/1299] D_loss: -0.1422, G_loss: -0.0472\n",
      "  Batch [190/1299] D_loss: -0.0282, G_loss: 0.2629\n",
      "  Batch [200/1299] D_loss: -0.0803, G_loss: 0.4827\n",
      "  Batch [210/1299] D_loss: -0.0357, G_loss: 0.4877\n",
      "  Batch [220/1299] D_loss: -0.0997, G_loss: 0.5871\n",
      "  Batch [230/1299] D_loss: -0.0768, G_loss: 0.5105\n",
      "  Batch [240/1299] D_loss: -0.1498, G_loss: 0.3075\n",
      "  Batch [250/1299] D_loss: -2.1568, G_loss: -3.9694\n",
      "  Batch [260/1299] D_loss: -1.1223, G_loss: -2.9979\n",
      "  Batch [270/1299] D_loss: -0.0493, G_loss: 0.2289\n",
      "  Batch [280/1299] D_loss: -0.0363, G_loss: 0.3495\n",
      "  Batch [290/1299] D_loss: -0.0148, G_loss: 0.4587\n",
      "  Batch [300/1299] D_loss: -0.2011, G_loss: 0.7374\n",
      "  Batch [310/1299] D_loss: -0.1141, G_loss: 0.6484\n",
      "  Batch [320/1299] D_loss: -0.1117, G_loss: 0.5590\n",
      "  Batch [330/1299] D_loss: -0.1175, G_loss: 0.5192\n",
      "  Batch [340/1299] D_loss: -1.6192, G_loss: -1.8325\n",
      "  Batch [350/1299] D_loss: 0.0512, G_loss: 0.1142\n",
      "  Batch [360/1299] D_loss: -0.2592, G_loss: -0.5570\n",
      "  Batch [370/1299] D_loss: -0.0444, G_loss: 0.2340\n",
      "  Batch [380/1299] D_loss: -0.0068, G_loss: 0.3411\n",
      "  Batch [390/1299] D_loss: -0.0050, G_loss: 0.4728\n",
      "  Batch [400/1299] D_loss: -0.0837, G_loss: 0.3985\n",
      "  Batch [410/1299] D_loss: -0.2929, G_loss: -0.8741\n",
      "  Batch [420/1299] D_loss: 0.0014, G_loss: 0.2317\n",
      "  Batch [430/1299] D_loss: 0.0033, G_loss: 0.2811\n",
      "  Batch [440/1299] D_loss: -0.0070, G_loss: 0.4446\n",
      "  Batch [450/1299] D_loss: -0.0474, G_loss: 0.6362\n",
      "  Batch [460/1299] D_loss: -0.0177, G_loss: 0.4771\n",
      "  Batch [470/1299] D_loss: 0.1071, G_loss: 0.3802\n",
      "  Batch [480/1299] D_loss: -0.0289, G_loss: 0.1816\n",
      "  Batch [490/1299] D_loss: -0.0186, G_loss: 0.1097\n",
      "  Batch [500/1299] D_loss: -0.5978, G_loss: -1.0252\n",
      "  Batch [510/1299] D_loss: -0.4908, G_loss: -0.5135\n",
      "  Batch [520/1299] D_loss: -0.2928, G_loss: 0.0612\n",
      "  Batch [530/1299] D_loss: -0.9118, G_loss: -2.1594\n",
      "  Batch [540/1299] D_loss: -0.1397, G_loss: 0.0730\n",
      "  Batch [550/1299] D_loss: -1.0007, G_loss: -0.8384\n",
      "  Batch [560/1299] D_loss: -0.5757, G_loss: 0.0201\n",
      "  Batch [570/1299] D_loss: -0.4835, G_loss: 0.0255\n",
      "  Batch [580/1299] D_loss: -0.4953, G_loss: -0.2774\n",
      "  Batch [590/1299] D_loss: -0.1149, G_loss: 0.3383\n",
      "  Batch [600/1299] D_loss: -0.1331, G_loss: 0.5767\n",
      "  Batch [610/1299] D_loss: -0.1198, G_loss: 0.7414\n",
      "  Batch [620/1299] D_loss: -0.2470, G_loss: 0.8260\n",
      "  Batch [630/1299] D_loss: -0.2065, G_loss: 0.8225\n",
      "  Batch [640/1299] D_loss: -0.0350, G_loss: 0.7009\n",
      "  Batch [650/1299] D_loss: -0.1924, G_loss: 0.6092\n",
      "  Batch [660/1299] D_loss: 0.0178, G_loss: 0.2214\n",
      "  Batch [670/1299] D_loss: -0.6159, G_loss: -0.8604\n",
      "  Batch [680/1299] D_loss: -0.0159, G_loss: 0.1433\n",
      "  Batch [690/1299] D_loss: -0.2570, G_loss: -0.3677\n",
      "  Batch [700/1299] D_loss: -0.0990, G_loss: 0.0539\n",
      "  Batch [710/1299] D_loss: 0.0070, G_loss: 0.2076\n",
      "  Batch [720/1299] D_loss: -0.3872, G_loss: -0.5995\n",
      "  Batch [730/1299] D_loss: -0.0995, G_loss: 0.2424\n",
      "  Batch [740/1299] D_loss: -0.0444, G_loss: 0.3393\n",
      "  Batch [750/1299] D_loss: -0.1236, G_loss: 0.3077\n",
      "  Batch [760/1299] D_loss: -0.0937, G_loss: 0.3647\n",
      "  Batch [770/1299] D_loss: -0.7091, G_loss: -0.0889\n",
      "  Batch [780/1299] D_loss: -0.0090, G_loss: 0.1507\n",
      "  Batch [790/1299] D_loss: -0.0252, G_loss: 0.2231\n",
      "  Batch [800/1299] D_loss: -0.0615, G_loss: 0.2930\n",
      "  Batch [810/1299] D_loss: 0.0074, G_loss: 0.3942\n",
      "  Batch [820/1299] D_loss: -0.0468, G_loss: 0.3066\n",
      "  Batch [830/1299] D_loss: -0.1671, G_loss: 0.3056\n",
      "  Batch [840/1299] D_loss: -2.5797, G_loss: -6.9913\n",
      "  Batch [850/1299] D_loss: -0.1106, G_loss: 0.0762\n",
      "  Batch [860/1299] D_loss: -0.0168, G_loss: 0.2363\n",
      "  Batch [870/1299] D_loss: -0.0146, G_loss: 0.2823\n",
      "  Batch [880/1299] D_loss: 0.0125, G_loss: 0.3523\n",
      "  Batch [890/1299] D_loss: -0.0146, G_loss: 0.3282\n",
      "  Batch [900/1299] D_loss: -0.0190, G_loss: 0.3405\n",
      "  Batch [910/1299] D_loss: -0.0484, G_loss: 0.2955\n",
      "  Batch [920/1299] D_loss: -1.0081, G_loss: -0.3802\n",
      "  Batch [930/1299] D_loss: -0.2374, G_loss: 0.0282\n",
      "  Batch [940/1299] D_loss: -0.0667, G_loss: 0.0772\n",
      "  Batch [950/1299] D_loss: -0.7423, G_loss: -0.0249\n",
      "  Batch [960/1299] D_loss: -0.6301, G_loss: -0.0767\n",
      "  Batch [970/1299] D_loss: -0.0771, G_loss: 0.3203\n",
      "  Batch [980/1299] D_loss: -0.1373, G_loss: 0.5298\n",
      "  Batch [990/1299] D_loss: -0.1794, G_loss: 0.6691\n",
      "  Batch [1000/1299] D_loss: -0.1241, G_loss: 0.6250\n",
      "  Batch [1010/1299] D_loss: -0.0527, G_loss: 0.6934\n",
      "  Batch [1020/1299] D_loss: 0.0226, G_loss: 0.4367\n",
      "  Batch [1030/1299] D_loss: -0.6217, G_loss: -1.3564\n",
      "  Batch [1040/1299] D_loss: -0.4877, G_loss: -0.8194\n",
      "  Batch [1050/1299] D_loss: -0.5855, G_loss: -0.2671\n",
      "  Batch [1060/1299] D_loss: -0.4769, G_loss: -0.3053\n",
      "  Batch [1070/1299] D_loss: -0.0180, G_loss: 0.2752\n",
      "  Batch [1080/1299] D_loss: -0.0873, G_loss: 0.4600\n",
      "  Batch [1090/1299] D_loss: -0.1163, G_loss: 0.5634\n",
      "  Batch [1100/1299] D_loss: -0.1007, G_loss: 0.6146\n",
      "  Batch [1110/1299] D_loss: -0.0060, G_loss: 0.5145\n",
      "  Batch [1120/1299] D_loss: 0.0236, G_loss: 0.3623\n",
      "  Batch [1130/1299] D_loss: -0.8502, G_loss: -0.6554\n",
      "  Batch [1140/1299] D_loss: -0.3051, G_loss: -0.2723\n",
      "  Batch [1150/1299] D_loss: -0.6177, G_loss: -0.3635\n",
      "  Batch [1160/1299] D_loss: -0.9479, G_loss: -0.3909\n",
      "  Batch [1170/1299] D_loss: -0.0094, G_loss: 0.0903\n",
      "  Batch [1180/1299] D_loss: -0.9419, G_loss: -0.0878\n",
      "  Batch [1190/1299] D_loss: -0.3761, G_loss: -0.1450\n",
      "  Batch [1200/1299] D_loss: -0.4439, G_loss: -0.0000\n",
      "  Batch [1210/1299] D_loss: -0.1059, G_loss: 0.1353\n",
      "  Batch [1220/1299] D_loss: -0.4062, G_loss: -0.0747\n",
      "  Batch [1230/1299] D_loss: -0.0466, G_loss: 0.1443\n",
      "  Batch [1240/1299] D_loss: -0.0728, G_loss: 0.4635\n",
      "  Batch [1250/1299] D_loss: -0.1314, G_loss: 0.6421\n",
      "  Batch [1260/1299] D_loss: -0.0943, G_loss: 0.6915\n",
      "  Batch [1270/1299] D_loss: -0.1679, G_loss: 0.7196\n",
      "  Batch [1280/1299] D_loss: -0.1219, G_loss: 0.6538\n",
      "  Batch [1290/1299] D_loss: -0.0195, G_loss: 0.3111\n",
      "\n",
      "Epoch 90 Summary:\n",
      "  Average D_loss: -0.1739\n",
      "  Average G_loss: -0.0900\n",
      "\n",
      "Epoch [91/100]\n",
      "  Batch [0/1299] D_loss: -1.6117, G_loss: -1.5593\n",
      "  Batch [10/1299] D_loss: 0.0259, G_loss: 0.1884\n",
      "  Batch [20/1299] D_loss: -0.0930, G_loss: 0.2558\n",
      "  Batch [30/1299] D_loss: -0.1506, G_loss: 0.4797\n",
      "  Batch [40/1299] D_loss: -0.0647, G_loss: 0.6010\n",
      "  Batch [50/1299] D_loss: -0.0813, G_loss: 0.4528\n",
      "  Batch [60/1299] D_loss: -0.0906, G_loss: 0.4425\n",
      "  Batch [70/1299] D_loss: -0.1211, G_loss: 0.2854\n",
      "  Batch [80/1299] D_loss: -0.0125, G_loss: 0.1562\n",
      "  Batch [90/1299] D_loss: -0.6239, G_loss: -0.3708\n",
      "  Batch [100/1299] D_loss: -0.3807, G_loss: -0.3669\n",
      "  Batch [110/1299] D_loss: 0.0212, G_loss: 0.1685\n",
      "  Batch [120/1299] D_loss: -0.0237, G_loss: 0.3423\n",
      "  Batch [130/1299] D_loss: -0.0762, G_loss: 0.4499\n",
      "  Batch [140/1299] D_loss: -0.0957, G_loss: 0.4897\n",
      "  Batch [150/1299] D_loss: -0.0238, G_loss: 0.4343\n",
      "  Batch [160/1299] D_loss: -0.0771, G_loss: 0.4117\n",
      "  Batch [170/1299] D_loss: -0.0362, G_loss: 0.2977\n",
      "  Batch [180/1299] D_loss: -0.3806, G_loss: -0.0228\n",
      "  Batch [190/1299] D_loss: -0.0229, G_loss: 0.0655\n",
      "  Batch [200/1299] D_loss: -0.7519, G_loss: -1.8664\n",
      "  Batch [210/1299] D_loss: -0.0850, G_loss: 0.0054\n",
      "  Batch [220/1299] D_loss: -0.0024, G_loss: 0.0965\n",
      "  Batch [230/1299] D_loss: -0.1148, G_loss: 0.2788\n",
      "  Batch [240/1299] D_loss: -0.1164, G_loss: 0.4530\n",
      "  Batch [250/1299] D_loss: -0.0744, G_loss: 0.5889\n",
      "  Batch [260/1299] D_loss: -0.0714, G_loss: 0.6212\n",
      "  Batch [270/1299] D_loss: -0.2168, G_loss: 0.5598\n",
      "  Batch [280/1299] D_loss: -0.1818, G_loss: 0.3822\n",
      "  Batch [290/1299] D_loss: -1.4816, G_loss: -6.1843\n",
      "  Batch [300/1299] D_loss: -0.2911, G_loss: 0.0126\n",
      "  Batch [310/1299] D_loss: -0.7290, G_loss: -0.9864\n",
      "  Batch [320/1299] D_loss: -0.8324, G_loss: -0.4703\n",
      "  Batch [330/1299] D_loss: -0.6896, G_loss: -0.7242\n",
      "  Batch [340/1299] D_loss: -0.6323, G_loss: -0.2373\n",
      "  Batch [350/1299] D_loss: -0.3470, G_loss: -0.1306\n",
      "  Batch [360/1299] D_loss: -0.7436, G_loss: -0.5364\n",
      "  Batch [370/1299] D_loss: -0.1808, G_loss: 0.0063\n",
      "  Batch [380/1299] D_loss: -0.7938, G_loss: 0.2038\n",
      "  Batch [390/1299] D_loss: -0.0794, G_loss: 0.2421\n",
      "  Batch [400/1299] D_loss: -0.1846, G_loss: 0.4477\n",
      "  Batch [410/1299] D_loss: -0.1783, G_loss: 0.6045\n",
      "  Batch [420/1299] D_loss: -0.1268, G_loss: 0.7033\n",
      "  Batch [430/1299] D_loss: -0.0943, G_loss: 0.7842\n",
      "  Batch [440/1299] D_loss: -0.1154, G_loss: 0.7333\n",
      "  Batch [450/1299] D_loss: -0.0891, G_loss: 0.4667\n",
      "  Batch [460/1299] D_loss: -0.1708, G_loss: -0.0043\n",
      "  Batch [470/1299] D_loss: -0.5973, G_loss: -0.6990\n",
      "  Batch [480/1299] D_loss: -0.0999, G_loss: 0.1578\n",
      "  Batch [490/1299] D_loss: -0.0909, G_loss: 0.1790\n",
      "  Batch [500/1299] D_loss: -0.1053, G_loss: 0.2976\n",
      "  Batch [510/1299] D_loss: -0.1416, G_loss: 0.5722\n",
      "  Batch [520/1299] D_loss: -0.1887, G_loss: 0.6660\n",
      "  Batch [530/1299] D_loss: -0.0804, G_loss: 0.5633\n",
      "  Batch [540/1299] D_loss: -0.0689, G_loss: 0.4584\n",
      "  Batch [550/1299] D_loss: -0.0414, G_loss: 0.3062\n",
      "  Batch [560/1299] D_loss: -0.1390, G_loss: -0.2856\n",
      "  Batch [570/1299] D_loss: -0.0062, G_loss: 0.0798\n",
      "  Batch [580/1299] D_loss: -0.0089, G_loss: 0.1449\n",
      "  Batch [590/1299] D_loss: -0.0566, G_loss: 0.2233\n",
      "  Batch [600/1299] D_loss: -0.0930, G_loss: 0.3113\n",
      "  Batch [610/1299] D_loss: 0.0340, G_loss: 0.3918\n",
      "  Batch [620/1299] D_loss: -0.0401, G_loss: 0.3096\n",
      "  Batch [630/1299] D_loss: -0.2560, G_loss: -0.2700\n",
      "  Batch [640/1299] D_loss: -0.6339, G_loss: -2.4693\n",
      "  Batch [650/1299] D_loss: -0.5128, G_loss: 0.0699\n",
      "  Batch [660/1299] D_loss: -0.0558, G_loss: 0.1202\n",
      "  Batch [670/1299] D_loss: -0.0682, G_loss: -0.0200\n",
      "  Batch [680/1299] D_loss: -0.0286, G_loss: 0.1682\n",
      "  Batch [690/1299] D_loss: -0.1137, G_loss: 0.3800\n",
      "  Batch [700/1299] D_loss: 0.0136, G_loss: 0.4769\n",
      "  Batch [710/1299] D_loss: -0.0912, G_loss: 0.6854\n",
      "  Batch [720/1299] D_loss: -0.0556, G_loss: 0.6151\n",
      "  Batch [730/1299] D_loss: -0.0551, G_loss: 0.5866\n",
      "  Batch [740/1299] D_loss: -0.0712, G_loss: 0.4069\n",
      "  Batch [750/1299] D_loss: -1.9428, G_loss: -4.5977\n",
      "  Batch [760/1299] D_loss: -0.0990, G_loss: 0.0063\n",
      "  Batch [770/1299] D_loss: -0.0107, G_loss: 0.2358\n",
      "  Batch [780/1299] D_loss: -0.0457, G_loss: 0.2773\n",
      "  Batch [790/1299] D_loss: -0.0722, G_loss: 0.4975\n",
      "  Batch [800/1299] D_loss: -0.0598, G_loss: 0.5080\n",
      "  Batch [810/1299] D_loss: -0.0429, G_loss: 0.4553\n",
      "  Batch [820/1299] D_loss: -0.0237, G_loss: 0.2540\n",
      "  Batch [830/1299] D_loss: -1.3666, G_loss: -1.8835\n",
      "  Batch [840/1299] D_loss: -0.0931, G_loss: 0.0416\n",
      "  Batch [850/1299] D_loss: -0.0014, G_loss: 0.1126\n",
      "  Batch [860/1299] D_loss: -0.0199, G_loss: 0.2130\n",
      "  Batch [870/1299] D_loss: -0.0378, G_loss: 0.2920\n",
      "  Batch [880/1299] D_loss: -0.0667, G_loss: 0.3600\n",
      "  Batch [890/1299] D_loss: -0.0221, G_loss: 0.2329\n",
      "  Batch [900/1299] D_loss: -0.0637, G_loss: 0.2523\n",
      "  Batch [910/1299] D_loss: -0.4410, G_loss: -0.6446\n",
      "  Batch [920/1299] D_loss: -1.0716, G_loss: -0.6676\n",
      "  Batch [930/1299] D_loss: -1.1442, G_loss: 0.0508\n",
      "  Batch [940/1299] D_loss: -0.0932, G_loss: -0.3025\n",
      "  Batch [950/1299] D_loss: -0.3461, G_loss: -0.8465\n",
      "  Batch [960/1299] D_loss: -0.0159, G_loss: 0.3427\n",
      "  Batch [970/1299] D_loss: -0.1287, G_loss: 0.5105\n",
      "  Batch [980/1299] D_loss: -0.0964, G_loss: 0.6055\n",
      "  Batch [990/1299] D_loss: -0.0937, G_loss: 0.5353\n",
      "  Batch [1000/1299] D_loss: -0.0792, G_loss: 0.4302\n",
      "  Batch [1010/1299] D_loss: -0.0468, G_loss: 0.2836\n",
      "  Batch [1020/1299] D_loss: -1.6750, G_loss: -4.3335\n",
      "  Batch [1030/1299] D_loss: -0.9122, G_loss: -1.7149\n",
      "  Batch [1040/1299] D_loss: -0.1622, G_loss: 0.0500\n",
      "  Batch [1050/1299] D_loss: -0.0419, G_loss: 0.2831\n",
      "  Batch [1060/1299] D_loss: -0.0174, G_loss: 0.4332\n",
      "  Batch [1070/1299] D_loss: -0.1673, G_loss: 0.5142\n",
      "  Batch [1080/1299] D_loss: 0.0529, G_loss: 0.4184\n",
      "  Batch [1090/1299] D_loss: -0.0927, G_loss: 0.3010\n",
      "  Batch [1100/1299] D_loss: -0.0402, G_loss: 0.1751\n",
      "  Batch [1110/1299] D_loss: -0.1754, G_loss: -0.2034\n",
      "  Batch [1120/1299] D_loss: -0.4515, G_loss: -0.0225\n",
      "  Batch [1130/1299] D_loss: -0.3799, G_loss: -0.3954\n",
      "  Batch [1140/1299] D_loss: -1.5153, G_loss: -3.0791\n",
      "  Batch [1150/1299] D_loss: -0.5829, G_loss: -0.0496\n",
      "  Batch [1160/1299] D_loss: -0.8862, G_loss: -2.1722\n",
      "  Batch [1170/1299] D_loss: -0.0101, G_loss: 0.2346\n",
      "  Batch [1180/1299] D_loss: -0.1283, G_loss: 0.4163\n",
      "  Batch [1190/1299] D_loss: -0.0308, G_loss: 0.5329\n",
      "  Batch [1200/1299] D_loss: -0.1315, G_loss: 0.5866\n",
      "  Batch [1210/1299] D_loss: -0.1513, G_loss: 0.5952\n",
      "  Batch [1220/1299] D_loss: -0.1912, G_loss: 0.5478\n",
      "  Batch [1230/1299] D_loss: -0.0479, G_loss: 0.2438\n",
      "  Batch [1240/1299] D_loss: -0.8291, G_loss: -0.7775\n",
      "  Batch [1250/1299] D_loss: 0.0132, G_loss: 0.1950\n",
      "  Batch [1260/1299] D_loss: -0.6017, G_loss: -0.7226\n",
      "  Batch [1270/1299] D_loss: -0.1211, G_loss: 0.1173\n",
      "  Batch [1280/1299] D_loss: -0.4567, G_loss: -0.9135\n",
      "  Batch [1290/1299] D_loss: -0.0631, G_loss: 0.2891\n",
      "\n",
      "Epoch 91 Summary:\n",
      "  Average D_loss: -0.1589\n",
      "  Average G_loss: -0.0785\n",
      "\n",
      "Epoch [92/100]\n",
      "  Batch [0/1299] D_loss: -0.0949, G_loss: 0.4341\n",
      "  Batch [10/1299] D_loss: -0.0129, G_loss: 0.5588\n",
      "  Batch [20/1299] D_loss: -0.1482, G_loss: 0.6405\n",
      "  Batch [30/1299] D_loss: -0.0105, G_loss: 0.3169\n",
      "  Batch [40/1299] D_loss: -0.0633, G_loss: 0.3494\n",
      "  Batch [50/1299] D_loss: -1.1522, G_loss: -2.1931\n",
      "  Batch [60/1299] D_loss: -1.0011, G_loss: -0.4198\n",
      "  Batch [70/1299] D_loss: -0.2631, G_loss: 0.0564\n",
      "  Batch [80/1299] D_loss: -0.1018, G_loss: 0.1084\n",
      "  Batch [90/1299] D_loss: -0.0187, G_loss: 0.1918\n",
      "  Batch [100/1299] D_loss: -0.1041, G_loss: 0.3603\n",
      "  Batch [110/1299] D_loss: -0.1851, G_loss: 0.6273\n",
      "  Batch [120/1299] D_loss: -0.1218, G_loss: 0.6440\n",
      "  Batch [130/1299] D_loss: -0.0514, G_loss: 0.6317\n",
      "  Batch [140/1299] D_loss: 0.0497, G_loss: 0.4921\n",
      "  Batch [150/1299] D_loss: -0.0303, G_loss: 0.3199\n",
      "  Batch [160/1299] D_loss: -2.0232, G_loss: -4.1809\n",
      "  Batch [170/1299] D_loss: -0.2962, G_loss: -0.0678\n",
      "  Batch [180/1299] D_loss: -0.0403, G_loss: 0.1665\n",
      "  Batch [190/1299] D_loss: -0.1535, G_loss: 0.3453\n",
      "  Batch [200/1299] D_loss: -0.0532, G_loss: 0.4329\n",
      "  Batch [210/1299] D_loss: -0.0148, G_loss: 0.4681\n",
      "  Batch [220/1299] D_loss: -0.2624, G_loss: 0.5592\n",
      "  Batch [230/1299] D_loss: -0.1075, G_loss: 0.3578\n",
      "  Batch [240/1299] D_loss: -0.0546, G_loss: 0.0533\n",
      "  Batch [250/1299] D_loss: 0.0101, G_loss: 0.1323\n",
      "  Batch [260/1299] D_loss: -0.0395, G_loss: 0.1653\n",
      "  Batch [270/1299] D_loss: -0.0325, G_loss: 0.2642\n",
      "  Batch [280/1299] D_loss: -0.0424, G_loss: 0.5136\n",
      "  Batch [290/1299] D_loss: -0.1561, G_loss: 0.6657\n",
      "  Batch [300/1299] D_loss: -0.0985, G_loss: 0.5786\n",
      "  Batch [310/1299] D_loss: -0.0210, G_loss: 0.5121\n",
      "  Batch [320/1299] D_loss: -0.0899, G_loss: 0.4938\n",
      "  Batch [330/1299] D_loss: -1.1943, G_loss: -0.5528\n",
      "  Batch [340/1299] D_loss: 0.0310, G_loss: -0.0899\n",
      "  Batch [350/1299] D_loss: -2.0288, G_loss: -1.6083\n",
      "  Batch [360/1299] D_loss: -0.4972, G_loss: -0.1345\n",
      "  Batch [370/1299] D_loss: -0.0253, G_loss: 0.1771\n",
      "  Batch [380/1299] D_loss: -0.0366, G_loss: 0.2102\n",
      "  Batch [390/1299] D_loss: -0.0648, G_loss: 0.4063\n",
      "  Batch [400/1299] D_loss: -0.0211, G_loss: 0.4240\n",
      "  Batch [410/1299] D_loss: -0.0951, G_loss: 0.4355\n",
      "  Batch [420/1299] D_loss: -0.0598, G_loss: 0.2464\n",
      "  Batch [430/1299] D_loss: -1.4726, G_loss: -4.2906\n",
      "  Batch [440/1299] D_loss: -0.3739, G_loss: -0.1525\n",
      "  Batch [450/1299] D_loss: -0.0319, G_loss: 0.2244\n",
      "  Batch [460/1299] D_loss: -0.0592, G_loss: 0.4670\n",
      "  Batch [470/1299] D_loss: -0.0593, G_loss: 0.5130\n",
      "  Batch [480/1299] D_loss: -0.1109, G_loss: 0.5070\n",
      "  Batch [490/1299] D_loss: 0.0199, G_loss: 0.4591\n",
      "  Batch [500/1299] D_loss: -0.0318, G_loss: 0.3073\n",
      "  Batch [510/1299] D_loss: -2.8212, G_loss: -2.1160\n",
      "  Batch [520/1299] D_loss: -0.9734, G_loss: -0.4611\n",
      "  Batch [530/1299] D_loss: -0.2261, G_loss: -0.3157\n",
      "  Batch [540/1299] D_loss: -0.0401, G_loss: 0.2059\n",
      "  Batch [550/1299] D_loss: -0.0458, G_loss: 0.3795\n",
      "  Batch [560/1299] D_loss: -0.0470, G_loss: 0.3499\n",
      "  Batch [570/1299] D_loss: -0.0352, G_loss: 0.4408\n",
      "  Batch [580/1299] D_loss: -0.0383, G_loss: 0.5360\n",
      "  Batch [590/1299] D_loss: -0.0105, G_loss: 0.4187\n",
      "  Batch [600/1299] D_loss: -3.3491, G_loss: -4.3717\n",
      "  Batch [610/1299] D_loss: -0.1567, G_loss: 0.0422\n",
      "  Batch [620/1299] D_loss: -0.5153, G_loss: -1.5242\n",
      "  Batch [630/1299] D_loss: -0.0590, G_loss: 0.1112\n",
      "  Batch [640/1299] D_loss: -0.1525, G_loss: 0.4371\n",
      "  Batch [650/1299] D_loss: -0.1915, G_loss: 0.7153\n",
      "  Batch [660/1299] D_loss: -0.1165, G_loss: 0.7054\n",
      "  Batch [670/1299] D_loss: -0.0934, G_loss: 0.7266\n",
      "  Batch [680/1299] D_loss: -0.0751, G_loss: 0.5937\n",
      "  Batch [690/1299] D_loss: -0.1318, G_loss: 0.4972\n",
      "  Batch [700/1299] D_loss: -0.0574, G_loss: 0.1628\n",
      "  Batch [710/1299] D_loss: 0.0229, G_loss: 0.1499\n",
      "  Batch [720/1299] D_loss: -0.0047, G_loss: 0.2623\n",
      "  Batch [730/1299] D_loss: -0.1509, G_loss: 0.3820\n",
      "  Batch [740/1299] D_loss: -0.0957, G_loss: 0.3657\n",
      "  Batch [750/1299] D_loss: -0.0406, G_loss: 0.4051\n",
      "  Batch [760/1299] D_loss: -0.1420, G_loss: 0.3297\n",
      "  Batch [770/1299] D_loss: -2.1013, G_loss: -8.3275\n",
      "  Batch [780/1299] D_loss: -0.0183, G_loss: 0.0998\n",
      "  Batch [790/1299] D_loss: 0.0005, G_loss: 0.0741\n",
      "  Batch [800/1299] D_loss: -0.8467, G_loss: -0.9949\n",
      "  Batch [810/1299] D_loss: 0.0009, G_loss: 0.1997\n",
      "  Batch [820/1299] D_loss: -0.0195, G_loss: 0.4670\n",
      "  Batch [830/1299] D_loss: -0.1544, G_loss: 0.6669\n",
      "  Batch [840/1299] D_loss: -0.1446, G_loss: 0.6508\n",
      "  Batch [850/1299] D_loss: 0.0298, G_loss: 0.4688\n",
      "  Batch [860/1299] D_loss: -0.0864, G_loss: 0.4870\n",
      "  Batch [870/1299] D_loss: -0.0107, G_loss: 0.2388\n",
      "  Batch [880/1299] D_loss: -0.3287, G_loss: -0.2046\n",
      "  Batch [890/1299] D_loss: -1.3814, G_loss: -3.2566\n",
      "  Batch [900/1299] D_loss: -1.2495, G_loss: -1.2251\n",
      "  Batch [910/1299] D_loss: -0.3179, G_loss: -0.0110\n",
      "  Batch [920/1299] D_loss: -0.2054, G_loss: -0.2784\n",
      "  Batch [930/1299] D_loss: -0.0836, G_loss: 0.4245\n",
      "  Batch [940/1299] D_loss: -0.2078, G_loss: 0.5420\n",
      "  Batch [950/1299] D_loss: -0.2480, G_loss: 0.7429\n",
      "  Batch [960/1299] D_loss: -0.1696, G_loss: 0.8843\n",
      "  Batch [970/1299] D_loss: 0.0802, G_loss: 0.4959\n",
      "  Batch [980/1299] D_loss: -0.0933, G_loss: 0.4198\n",
      "  Batch [990/1299] D_loss: -0.0652, G_loss: 0.3840\n",
      "  Batch [1000/1299] D_loss: -3.0498, G_loss: -4.4560\n",
      "  Batch [1010/1299] D_loss: -1.6903, G_loss: -2.6883\n",
      "  Batch [1020/1299] D_loss: -0.2516, G_loss: 0.0659\n",
      "  Batch [1030/1299] D_loss: -0.0524, G_loss: 0.3091\n",
      "  Batch [1040/1299] D_loss: -0.0885, G_loss: 0.4805\n",
      "  Batch [1050/1299] D_loss: -0.0739, G_loss: 0.5411\n",
      "  Batch [1060/1299] D_loss: -0.0390, G_loss: 0.4891\n",
      "  Batch [1070/1299] D_loss: -0.0177, G_loss: 0.6025\n",
      "  Batch [1080/1299] D_loss: -0.0612, G_loss: 0.3429\n",
      "  Batch [1090/1299] D_loss: -1.0664, G_loss: -2.5576\n",
      "  Batch [1100/1299] D_loss: -0.0855, G_loss: 0.1102\n",
      "  Batch [1110/1299] D_loss: -0.6325, G_loss: -1.3265\n",
      "  Batch [1120/1299] D_loss: -0.4899, G_loss: -0.1966\n",
      "  Batch [1130/1299] D_loss: -0.0329, G_loss: 0.1609\n",
      "  Batch [1140/1299] D_loss: -0.0631, G_loss: 0.3002\n",
      "  Batch [1150/1299] D_loss: -0.1243, G_loss: 0.3199\n",
      "  Batch [1160/1299] D_loss: -0.0178, G_loss: 0.4129\n",
      "  Batch [1170/1299] D_loss: -0.0702, G_loss: 0.5277\n",
      "  Batch [1180/1299] D_loss: -0.0522, G_loss: 0.4140\n",
      "  Batch [1190/1299] D_loss: -0.0656, G_loss: 0.2757\n",
      "  Batch [1200/1299] D_loss: -1.5144, G_loss: -3.3170\n",
      "  Batch [1210/1299] D_loss: -0.0498, G_loss: 0.1423\n",
      "  Batch [1220/1299] D_loss: -0.1362, G_loss: 0.2733\n",
      "  Batch [1230/1299] D_loss: -0.0149, G_loss: 0.4759\n",
      "  Batch [1240/1299] D_loss: -0.1011, G_loss: 0.4908\n",
      "  Batch [1250/1299] D_loss: -0.0723, G_loss: 0.3206\n",
      "  Batch [1260/1299] D_loss: -0.0713, G_loss: 0.4549\n",
      "  Batch [1270/1299] D_loss: -1.6453, G_loss: -1.4162\n",
      "  Batch [1280/1299] D_loss: -0.1425, G_loss: -0.0762\n",
      "  Batch [1290/1299] D_loss: -0.2273, G_loss: -0.1794\n",
      "\n",
      "Epoch 92 Summary:\n",
      "  Average D_loss: -0.1579\n",
      "  Average G_loss: -0.0776\n",
      "\n",
      "Epoch [93/100]\n",
      "  Batch [0/1299] D_loss: -1.4375, G_loss: -1.4615\n",
      "  Batch [10/1299] D_loss: -0.2818, G_loss: -0.1936\n",
      "  Batch [20/1299] D_loss: -0.7697, G_loss: 0.0211\n",
      "  Batch [30/1299] D_loss: -0.9390, G_loss: -0.5517\n",
      "  Batch [40/1299] D_loss: -0.8481, G_loss: 0.0435\n",
      "  Batch [50/1299] D_loss: -0.3052, G_loss: -0.0505\n",
      "  Batch [60/1299] D_loss: -0.1513, G_loss: 0.0756\n",
      "  Batch [70/1299] D_loss: -0.2477, G_loss: 0.0966\n",
      "  Batch [80/1299] D_loss: -0.4731, G_loss: -0.2523\n",
      "  Batch [90/1299] D_loss: -0.1252, G_loss: 0.2152\n",
      "  Batch [100/1299] D_loss: -0.2012, G_loss: 0.4779\n",
      "  Batch [110/1299] D_loss: -0.2319, G_loss: 0.8225\n",
      "  Batch [120/1299] D_loss: -0.1148, G_loss: 0.8457\n",
      "  Batch [130/1299] D_loss: -0.0505, G_loss: 1.0963\n",
      "  Batch [140/1299] D_loss: 0.0550, G_loss: 0.8123\n",
      "  Batch [150/1299] D_loss: -0.0600, G_loss: 0.6313\n",
      "  Batch [160/1299] D_loss: 0.0015, G_loss: 0.3495\n",
      "  Batch [170/1299] D_loss: -0.3833, G_loss: -0.1053\n",
      "  Batch [180/1299] D_loss: -0.5123, G_loss: -0.5664\n",
      "  Batch [190/1299] D_loss: -0.1129, G_loss: -0.1931\n",
      "  Batch [200/1299] D_loss: -0.2055, G_loss: -0.1530\n",
      "  Batch [210/1299] D_loss: -0.0392, G_loss: 0.2455\n",
      "  Batch [220/1299] D_loss: -0.1330, G_loss: 0.3535\n",
      "  Batch [230/1299] D_loss: -0.0821, G_loss: 0.5579\n",
      "  Batch [240/1299] D_loss: -0.0822, G_loss: 0.6894\n",
      "  Batch [250/1299] D_loss: -0.0296, G_loss: 0.4922\n",
      "  Batch [260/1299] D_loss: -0.1506, G_loss: 0.4677\n",
      "  Batch [270/1299] D_loss: -1.9494, G_loss: -2.0295\n",
      "  Batch [280/1299] D_loss: -0.2827, G_loss: 0.0132\n",
      "  Batch [290/1299] D_loss: -0.2581, G_loss: 0.0982\n",
      "  Batch [300/1299] D_loss: -0.1710, G_loss: 0.1299\n",
      "  Batch [310/1299] D_loss: -1.3805, G_loss: -1.0272\n",
      "  Batch [320/1299] D_loss: -0.0285, G_loss: 0.1466\n",
      "  Batch [330/1299] D_loss: -0.0790, G_loss: 0.2619\n",
      "  Batch [340/1299] D_loss: -0.0776, G_loss: 0.4091\n",
      "  Batch [350/1299] D_loss: -0.1405, G_loss: 0.5511\n",
      "  Batch [360/1299] D_loss: 0.0946, G_loss: 0.4379\n",
      "  Batch [370/1299] D_loss: 0.0452, G_loss: 0.5151\n",
      "  Batch [380/1299] D_loss: -0.1334, G_loss: 0.3248\n",
      "  Batch [390/1299] D_loss: -0.2696, G_loss: -0.1032\n",
      "  Batch [400/1299] D_loss: -0.4204, G_loss: -0.3918\n",
      "  Batch [410/1299] D_loss: -0.0313, G_loss: 0.1362\n",
      "  Batch [420/1299] D_loss: -1.2113, G_loss: -0.9776\n",
      "  Batch [430/1299] D_loss: -0.7107, G_loss: -0.1380\n",
      "  Batch [440/1299] D_loss: -0.2976, G_loss: -0.1361\n",
      "  Batch [450/1299] D_loss: -0.3311, G_loss: -0.1847\n",
      "  Batch [460/1299] D_loss: -0.8952, G_loss: -0.0580\n",
      "  Batch [470/1299] D_loss: -0.0204, G_loss: 0.1124\n",
      "  Batch [480/1299] D_loss: -0.7283, G_loss: 0.1331\n",
      "  Batch [490/1299] D_loss: -0.3466, G_loss: 0.0838\n",
      "  Batch [500/1299] D_loss: -0.0858, G_loss: 0.3037\n",
      "  Batch [510/1299] D_loss: -0.1335, G_loss: 0.6136\n",
      "  Batch [520/1299] D_loss: -0.2990, G_loss: 0.8170\n",
      "  Batch [530/1299] D_loss: -0.0468, G_loss: 0.5613\n",
      "  Batch [540/1299] D_loss: -0.0709, G_loss: 0.5537\n",
      "  Batch [550/1299] D_loss: -0.0909, G_loss: 0.3720\n",
      "  Batch [560/1299] D_loss: -0.0510, G_loss: 0.2870\n",
      "  Batch [570/1299] D_loss: -1.6979, G_loss: -0.6146\n",
      "  Batch [580/1299] D_loss: -0.5198, G_loss: -0.0541\n",
      "  Batch [590/1299] D_loss: -0.7633, G_loss: -1.1299\n",
      "  Batch [600/1299] D_loss: -0.0527, G_loss: 0.2295\n",
      "  Batch [610/1299] D_loss: -0.0766, G_loss: 0.3009\n",
      "  Batch [620/1299] D_loss: 0.0459, G_loss: 0.3756\n",
      "  Batch [630/1299] D_loss: -0.1155, G_loss: 0.4729\n",
      "  Batch [640/1299] D_loss: -0.0961, G_loss: 0.5164\n",
      "  Batch [650/1299] D_loss: -0.0321, G_loss: 0.3288\n",
      "  Batch [660/1299] D_loss: -1.9176, G_loss: -1.9789\n",
      "  Batch [670/1299] D_loss: -0.0476, G_loss: 0.1701\n",
      "  Batch [680/1299] D_loss: -0.0676, G_loss: 0.2946\n",
      "  Batch [690/1299] D_loss: -0.0159, G_loss: 0.2912\n",
      "  Batch [700/1299] D_loss: -0.0691, G_loss: 0.4262\n",
      "  Batch [710/1299] D_loss: -0.0653, G_loss: 0.4413\n",
      "  Batch [720/1299] D_loss: -0.0913, G_loss: 0.4633\n",
      "  Batch [730/1299] D_loss: -1.3900, G_loss: -0.8897\n",
      "  Batch [740/1299] D_loss: -0.6471, G_loss: -1.5131\n",
      "  Batch [750/1299] D_loss: -0.3341, G_loss: -0.5425\n",
      "  Batch [760/1299] D_loss: 0.0275, G_loss: 0.2439\n",
      "  Batch [770/1299] D_loss: -0.0894, G_loss: 0.2520\n",
      "  Batch [780/1299] D_loss: -0.0171, G_loss: 0.3054\n",
      "  Batch [790/1299] D_loss: -0.0846, G_loss: 0.3411\n",
      "  Batch [800/1299] D_loss: -0.0457, G_loss: 0.2620\n",
      "  Batch [810/1299] D_loss: -0.0726, G_loss: 0.3349\n",
      "  Batch [820/1299] D_loss: -1.6845, G_loss: -4.4723\n",
      "  Batch [830/1299] D_loss: -0.0197, G_loss: 0.1961\n",
      "  Batch [840/1299] D_loss: -0.2005, G_loss: -0.7577\n",
      "  Batch [850/1299] D_loss: -1.5475, G_loss: -1.2395\n",
      "  Batch [860/1299] D_loss: -0.7903, G_loss: -0.1903\n",
      "  Batch [870/1299] D_loss: -0.7037, G_loss: -0.1769\n",
      "  Batch [880/1299] D_loss: -0.4170, G_loss: 0.0927\n",
      "  Batch [890/1299] D_loss: -0.1051, G_loss: 0.1696\n",
      "  Batch [900/1299] D_loss: -0.0462, G_loss: 0.2546\n",
      "  Batch [910/1299] D_loss: -0.1838, G_loss: 0.5773\n",
      "  Batch [920/1299] D_loss: -0.3559, G_loss: 0.9053\n",
      "  Batch [930/1299] D_loss: -0.2511, G_loss: 1.1572\n",
      "  Batch [940/1299] D_loss: -0.3154, G_loss: 1.0768\n",
      "  Batch [950/1299] D_loss: -0.2579, G_loss: 1.0542\n",
      "  Batch [960/1299] D_loss: -0.0933, G_loss: 0.7539\n",
      "  Batch [970/1299] D_loss: 0.0341, G_loss: 0.4079\n",
      "  Batch [980/1299] D_loss: -1.3900, G_loss: -2.9906\n",
      "  Batch [990/1299] D_loss: -0.3251, G_loss: 0.0778\n",
      "  Batch [1000/1299] D_loss: -0.7558, G_loss: -0.2300\n",
      "  Batch [1010/1299] D_loss: 0.0436, G_loss: 0.1314\n",
      "  Batch [1020/1299] D_loss: -0.1920, G_loss: -0.2289\n",
      "  Batch [1030/1299] D_loss: -0.0859, G_loss: 0.2843\n",
      "  Batch [1040/1299] D_loss: -0.0281, G_loss: 0.4428\n",
      "  Batch [1050/1299] D_loss: -0.0222, G_loss: 0.5612\n",
      "  Batch [1060/1299] D_loss: -0.1684, G_loss: 0.7438\n",
      "  Batch [1070/1299] D_loss: -0.1321, G_loss: 0.5825\n",
      "  Batch [1080/1299] D_loss: -0.0853, G_loss: 0.5303\n",
      "  Batch [1090/1299] D_loss: -2.8446, G_loss: -2.7469\n",
      "  Batch [1100/1299] D_loss: -0.1962, G_loss: 0.0610\n",
      "  Batch [1110/1299] D_loss: -0.0608, G_loss: 0.1346\n",
      "  Batch [1120/1299] D_loss: -0.4863, G_loss: -0.1006\n",
      "  Batch [1130/1299] D_loss: -0.2964, G_loss: -0.4129\n",
      "  Batch [1140/1299] D_loss: -0.0161, G_loss: 0.1580\n",
      "  Batch [1150/1299] D_loss: -0.1269, G_loss: 0.2769\n",
      "  Batch [1160/1299] D_loss: -0.1252, G_loss: 0.3364\n",
      "  Batch [1170/1299] D_loss: -0.0050, G_loss: 0.3823\n",
      "  Batch [1180/1299] D_loss: -0.1779, G_loss: 0.4006\n",
      "  Batch [1190/1299] D_loss: -0.1063, G_loss: 0.2951\n",
      "  Batch [1200/1299] D_loss: -0.7998, G_loss: -0.9095\n",
      "  Batch [1210/1299] D_loss: -0.7751, G_loss: 0.1132\n",
      "  Batch [1220/1299] D_loss: -0.4031, G_loss: -0.1381\n",
      "  Batch [1230/1299] D_loss: -0.0251, G_loss: 0.1721\n",
      "  Batch [1240/1299] D_loss: -0.0758, G_loss: 0.4638\n",
      "  Batch [1250/1299] D_loss: -0.1490, G_loss: 0.6164\n",
      "  Batch [1260/1299] D_loss: -0.1056, G_loss: 0.6320\n",
      "  Batch [1270/1299] D_loss: -0.1601, G_loss: 0.6702\n",
      "  Batch [1280/1299] D_loss: -0.0648, G_loss: 0.5635\n",
      "  Batch [1290/1299] D_loss: -0.0014, G_loss: 0.2805\n",
      "\n",
      "Epoch 93 Summary:\n",
      "  Average D_loss: -0.1995\n",
      "  Average G_loss: -0.0802\n",
      "\n",
      "Epoch [94/100]\n",
      "  Batch [0/1299] D_loss: -1.6659, G_loss: -2.3702\n",
      "  Batch [10/1299] D_loss: -0.8073, G_loss: -0.1926\n",
      "  Batch [20/1299] D_loss: -0.0281, G_loss: 0.1504\n",
      "  Batch [30/1299] D_loss: -1.1140, G_loss: -0.6255\n",
      "  Batch [40/1299] D_loss: -0.8134, G_loss: -0.1249\n",
      "  Batch [50/1299] D_loss: -0.0065, G_loss: 0.1191\n",
      "  Batch [60/1299] D_loss: -0.2053, G_loss: 0.0476\n",
      "  Batch [70/1299] D_loss: -1.7766, G_loss: -2.0345\n",
      "  Batch [80/1299] D_loss: -0.0778, G_loss: 0.0937\n",
      "  Batch [90/1299] D_loss: -0.4870, G_loss: -0.2548\n",
      "  Batch [100/1299] D_loss: -0.1486, G_loss: 0.3573\n",
      "  Batch [110/1299] D_loss: -0.1495, G_loss: 0.6285\n",
      "  Batch [120/1299] D_loss: -0.1114, G_loss: 0.6599\n",
      "  Batch [130/1299] D_loss: 0.0313, G_loss: 0.7011\n",
      "  Batch [140/1299] D_loss: -0.1844, G_loss: 0.6483\n",
      "  Batch [150/1299] D_loss: 0.0159, G_loss: 0.3384\n",
      "  Batch [160/1299] D_loss: -0.1402, G_loss: 0.1059\n",
      "  Batch [170/1299] D_loss: -1.2137, G_loss: -1.7501\n",
      "  Batch [180/1299] D_loss: -0.2618, G_loss: -0.2449\n",
      "  Batch [190/1299] D_loss: -1.8126, G_loss: -2.2633\n",
      "  Batch [200/1299] D_loss: -0.7345, G_loss: -0.0050\n",
      "  Batch [210/1299] D_loss: -0.5725, G_loss: -0.5313\n",
      "  Batch [220/1299] D_loss: -0.6971, G_loss: -0.7302\n",
      "  Batch [230/1299] D_loss: -0.1486, G_loss: -0.0868\n",
      "  Batch [240/1299] D_loss: -0.0882, G_loss: 0.1991\n",
      "  Batch [250/1299] D_loss: -0.0295, G_loss: 0.1993\n",
      "  Batch [260/1299] D_loss: -0.0938, G_loss: 0.4480\n",
      "  Batch [270/1299] D_loss: -0.1016, G_loss: 0.4841\n",
      "  Batch [280/1299] D_loss: -0.0724, G_loss: 0.8017\n",
      "  Batch [290/1299] D_loss: -0.1999, G_loss: 0.9657\n",
      "  Batch [300/1299] D_loss: -0.0035, G_loss: 0.6213\n",
      "  Batch [310/1299] D_loss: -0.0010, G_loss: 0.3510\n",
      "  Batch [320/1299] D_loss: -0.0536, G_loss: 0.3345\n",
      "  Batch [330/1299] D_loss: -0.4967, G_loss: -0.6552\n",
      "  Batch [340/1299] D_loss: -1.3374, G_loss: 0.0705\n",
      "  Batch [350/1299] D_loss: -1.0732, G_loss: -0.7420\n",
      "  Batch [360/1299] D_loss: -0.8794, G_loss: -0.4924\n",
      "  Batch [370/1299] D_loss: -0.0167, G_loss: 0.2213\n",
      "  Batch [380/1299] D_loss: -0.0378, G_loss: 0.3176\n",
      "  Batch [390/1299] D_loss: -0.0034, G_loss: 0.4642\n",
      "  Batch [400/1299] D_loss: -0.1545, G_loss: 0.5084\n",
      "  Batch [410/1299] D_loss: -0.1143, G_loss: 0.4413\n",
      "  Batch [420/1299] D_loss: -0.0270, G_loss: 0.3074\n",
      "  Batch [430/1299] D_loss: -1.2452, G_loss: -2.3784\n",
      "  Batch [440/1299] D_loss: -0.0436, G_loss: 0.1993\n",
      "  Batch [450/1299] D_loss: -0.0099, G_loss: 0.3276\n",
      "  Batch [460/1299] D_loss: -0.0269, G_loss: 0.4002\n",
      "  Batch [470/1299] D_loss: -0.0890, G_loss: 0.4390\n",
      "  Batch [480/1299] D_loss: -0.0224, G_loss: 0.3393\n",
      "  Batch [490/1299] D_loss: -0.0800, G_loss: 0.3893\n",
      "  Batch [500/1299] D_loss: -1.9437, G_loss: -3.4215\n",
      "  Batch [510/1299] D_loss: -3.8727, G_loss: -4.8262\n",
      "  Batch [520/1299] D_loss: -1.1149, G_loss: -1.9519\n",
      "  Batch [530/1299] D_loss: -0.2026, G_loss: -0.0902\n",
      "  Batch [540/1299] D_loss: -0.6697, G_loss: -0.5556\n",
      "  Batch [550/1299] D_loss: -0.0444, G_loss: 0.3534\n",
      "  Batch [560/1299] D_loss: -0.1076, G_loss: 0.5597\n",
      "  Batch [570/1299] D_loss: -0.1263, G_loss: 0.6389\n",
      "  Batch [580/1299] D_loss: 0.0555, G_loss: 0.5593\n",
      "  Batch [590/1299] D_loss: -0.1415, G_loss: 0.8228\n",
      "  Batch [600/1299] D_loss: -0.0674, G_loss: 0.4998\n",
      "  Batch [610/1299] D_loss: -0.0618, G_loss: 0.2713\n",
      "  Batch [620/1299] D_loss: -0.1660, G_loss: -0.1775\n",
      "  Batch [630/1299] D_loss: 0.0172, G_loss: 0.1538\n",
      "  Batch [640/1299] D_loss: -0.0289, G_loss: 0.2557\n",
      "  Batch [650/1299] D_loss: -0.0446, G_loss: 0.2598\n",
      "  Batch [660/1299] D_loss: -0.0750, G_loss: 0.3008\n",
      "  Batch [670/1299] D_loss: 0.0036, G_loss: 0.2281\n",
      "  Batch [680/1299] D_loss: -1.5415, G_loss: -1.8123\n",
      "  Batch [690/1299] D_loss: -0.6967, G_loss: -2.2418\n",
      "  Batch [700/1299] D_loss: -0.0212, G_loss: 0.2082\n",
      "  Batch [710/1299] D_loss: -0.0396, G_loss: 0.3715\n",
      "  Batch [720/1299] D_loss: -0.2069, G_loss: 0.5252\n",
      "  Batch [730/1299] D_loss: -0.1130, G_loss: 0.5129\n",
      "  Batch [740/1299] D_loss: -0.1081, G_loss: 0.5765\n",
      "  Batch [750/1299] D_loss: -0.0334, G_loss: 0.5601\n",
      "  Batch [760/1299] D_loss: -0.0295, G_loss: 0.2966\n",
      "  Batch [770/1299] D_loss: -0.3672, G_loss: -1.7286\n",
      "  Batch [780/1299] D_loss: -0.4295, G_loss: -0.2603\n",
      "  Batch [790/1299] D_loss: 0.0104, G_loss: 0.2076\n",
      "  Batch [800/1299] D_loss: -0.0716, G_loss: 0.3437\n",
      "  Batch [810/1299] D_loss: -0.0477, G_loss: 0.5034\n",
      "  Batch [820/1299] D_loss: -0.1586, G_loss: 0.5008\n",
      "  Batch [830/1299] D_loss: -0.0451, G_loss: 0.5229\n",
      "  Batch [840/1299] D_loss: -0.9696, G_loss: -1.2702\n",
      "  Batch [850/1299] D_loss: -0.3714, G_loss: -0.5623\n",
      "  Batch [860/1299] D_loss: -0.6495, G_loss: -1.4176\n",
      "  Batch [870/1299] D_loss: -0.0243, G_loss: 0.2689\n",
      "  Batch [880/1299] D_loss: -0.0292, G_loss: 0.4190\n",
      "  Batch [890/1299] D_loss: -0.1450, G_loss: 0.5384\n",
      "  Batch [900/1299] D_loss: -0.0517, G_loss: 0.3440\n",
      "  Batch [910/1299] D_loss: -0.0831, G_loss: 0.2829\n",
      "  Batch [920/1299] D_loss: -2.2782, G_loss: -2.0198\n",
      "  Batch [930/1299] D_loss: -0.0069, G_loss: 0.0902\n",
      "  Batch [940/1299] D_loss: -0.0078, G_loss: 0.2036\n",
      "  Batch [950/1299] D_loss: 0.0010, G_loss: 0.2812\n",
      "  Batch [960/1299] D_loss: -2.0731, G_loss: -1.4570\n",
      "  Batch [970/1299] D_loss: -0.0351, G_loss: 0.2377\n",
      "  Batch [980/1299] D_loss: -0.0256, G_loss: 0.3460\n",
      "  Batch [990/1299] D_loss: -0.1248, G_loss: 0.5001\n",
      "  Batch [1000/1299] D_loss: -0.0797, G_loss: 0.4857\n",
      "  Batch [1010/1299] D_loss: -0.0712, G_loss: 0.5042\n",
      "  Batch [1020/1299] D_loss: -0.0956, G_loss: 0.2779\n",
      "  Batch [1030/1299] D_loss: -0.9002, G_loss: -0.8043\n",
      "  Batch [1040/1299] D_loss: -0.0067, G_loss: 0.1559\n",
      "  Batch [1050/1299] D_loss: -0.0255, G_loss: 0.3013\n",
      "  Batch [1060/1299] D_loss: -0.0463, G_loss: 0.2759\n",
      "  Batch [1070/1299] D_loss: -1.4722, G_loss: -1.4128\n",
      "  Batch [1080/1299] D_loss: -0.0982, G_loss: 0.1097\n",
      "  Batch [1090/1299] D_loss: -1.3009, G_loss: 0.0598\n",
      "  Batch [1100/1299] D_loss: -0.2505, G_loss: 0.1112\n",
      "  Batch [1110/1299] D_loss: -0.2940, G_loss: -0.1333\n",
      "  Batch [1120/1299] D_loss: -0.5917, G_loss: -0.5417\n",
      "  Batch [1130/1299] D_loss: -0.0954, G_loss: 0.3842\n",
      "  Batch [1140/1299] D_loss: -0.1111, G_loss: 0.5626\n",
      "  Batch [1150/1299] D_loss: -0.1961, G_loss: 0.7595\n",
      "  Batch [1160/1299] D_loss: -0.2024, G_loss: 0.7825\n",
      "  Batch [1170/1299] D_loss: -0.0709, G_loss: 0.5547\n",
      "  Batch [1180/1299] D_loss: -0.1041, G_loss: 0.6888\n",
      "  Batch [1190/1299] D_loss: -0.0960, G_loss: 0.4119\n",
      "  Batch [1200/1299] D_loss: -2.0087, G_loss: -7.0885\n",
      "  Batch [1210/1299] D_loss: -0.6790, G_loss: -0.3105\n",
      "  Batch [1220/1299] D_loss: -0.0243, G_loss: 0.1777\n",
      "  Batch [1230/1299] D_loss: -0.0544, G_loss: 0.3163\n",
      "  Batch [1240/1299] D_loss: -0.0213, G_loss: 0.4976\n",
      "  Batch [1250/1299] D_loss: -0.0694, G_loss: 0.3967\n",
      "  Batch [1260/1299] D_loss: -0.0436, G_loss: 0.4515\n",
      "  Batch [1270/1299] D_loss: -0.3531, G_loss: -0.3384\n",
      "  Batch [1280/1299] D_loss: -0.0433, G_loss: 0.0834\n",
      "  Batch [1290/1299] D_loss: 0.0152, G_loss: 0.1813\n",
      "\n",
      "Epoch 94 Summary:\n",
      "  Average D_loss: -0.1734\n",
      "  Average G_loss: -0.0888\n",
      "\n",
      "Epoch [95/100]\n",
      "  Batch [0/1299] D_loss: -0.0473, G_loss: 0.2837\n",
      "  Batch [10/1299] D_loss: -0.0457, G_loss: 0.3885\n",
      "  Batch [20/1299] D_loss: -0.0624, G_loss: 0.3420\n",
      "  Batch [30/1299] D_loss: 0.0116, G_loss: 0.3122\n",
      "  Batch [40/1299] D_loss: -1.1691, G_loss: -0.0445\n",
      "  Batch [50/1299] D_loss: -0.0693, G_loss: 0.0279\n",
      "  Batch [60/1299] D_loss: -1.0355, G_loss: -0.4729\n",
      "  Batch [70/1299] D_loss: -0.8114, G_loss: -0.1845\n",
      "  Batch [80/1299] D_loss: -0.8675, G_loss: -0.4835\n",
      "  Batch [90/1299] D_loss: -0.5730, G_loss: -0.4524\n",
      "  Batch [100/1299] D_loss: -0.0804, G_loss: 0.1126\n",
      "  Batch [110/1299] D_loss: -0.0137, G_loss: 0.2474\n",
      "  Batch [120/1299] D_loss: -0.1944, G_loss: 0.5127\n",
      "  Batch [130/1299] D_loss: -0.2694, G_loss: 0.6946\n",
      "  Batch [140/1299] D_loss: -0.1430, G_loss: 0.8075\n",
      "  Batch [150/1299] D_loss: -0.1070, G_loss: 0.7543\n",
      "  Batch [160/1299] D_loss: -0.0705, G_loss: 0.5486\n",
      "  Batch [170/1299] D_loss: -0.1062, G_loss: 0.3009\n",
      "  Batch [180/1299] D_loss: -1.1859, G_loss: -1.8200\n",
      "  Batch [190/1299] D_loss: -0.0161, G_loss: 0.0712\n",
      "  Batch [200/1299] D_loss: -0.9958, G_loss: -1.4552\n",
      "  Batch [210/1299] D_loss: -1.4841, G_loss: -1.7909\n",
      "  Batch [220/1299] D_loss: -1.4190, G_loss: -1.1958\n",
      "  Batch [230/1299] D_loss: -0.3690, G_loss: -0.2005\n",
      "  Batch [240/1299] D_loss: -0.2269, G_loss: -0.1995\n",
      "  Batch [250/1299] D_loss: -0.1077, G_loss: 0.3750\n",
      "  Batch [260/1299] D_loss: -0.0342, G_loss: 0.5605\n",
      "  Batch [270/1299] D_loss: -0.1726, G_loss: 0.8030\n",
      "  Batch [280/1299] D_loss: -0.1141, G_loss: 0.8395\n",
      "  Batch [290/1299] D_loss: -0.0804, G_loss: 0.6576\n",
      "  Batch [300/1299] D_loss: -0.0641, G_loss: 0.5166\n",
      "  Batch [310/1299] D_loss: -0.0246, G_loss: 0.3706\n",
      "  Batch [320/1299] D_loss: -0.4247, G_loss: -0.2948\n",
      "  Batch [330/1299] D_loss: -0.0443, G_loss: 0.1452\n",
      "  Batch [340/1299] D_loss: -0.0090, G_loss: 0.1906\n",
      "  Batch [350/1299] D_loss: -0.0815, G_loss: 0.3503\n",
      "  Batch [360/1299] D_loss: -0.0599, G_loss: 0.3451\n",
      "  Batch [370/1299] D_loss: -0.0860, G_loss: 0.3396\n",
      "  Batch [380/1299] D_loss: -2.2259, G_loss: -1.1570\n",
      "  Batch [390/1299] D_loss: -0.2495, G_loss: -0.1454\n",
      "  Batch [400/1299] D_loss: -0.4544, G_loss: -0.4756\n",
      "  Batch [410/1299] D_loss: -0.5045, G_loss: -0.5543\n",
      "  Batch [420/1299] D_loss: -0.1313, G_loss: 0.1215\n",
      "  Batch [430/1299] D_loss: -0.9841, G_loss: -0.2992\n",
      "  Batch [440/1299] D_loss: -0.5206, G_loss: -0.4760\n",
      "  Batch [450/1299] D_loss: -0.2073, G_loss: 0.0352\n",
      "  Batch [460/1299] D_loss: -0.1039, G_loss: 0.3603\n",
      "  Batch [470/1299] D_loss: -0.2955, G_loss: 0.6520\n",
      "  Batch [480/1299] D_loss: -0.0211, G_loss: 0.6890\n",
      "  Batch [490/1299] D_loss: -0.0579, G_loss: 0.6120\n",
      "  Batch [500/1299] D_loss: 0.0422, G_loss: 0.5573\n",
      "  Batch [510/1299] D_loss: -0.1125, G_loss: 0.3906\n",
      "  Batch [520/1299] D_loss: -1.0316, G_loss: -2.9599\n",
      "  Batch [530/1299] D_loss: -0.4326, G_loss: -0.9576\n",
      "  Batch [540/1299] D_loss: -0.0597, G_loss: 0.0783\n",
      "  Batch [550/1299] D_loss: -0.1024, G_loss: 0.3107\n",
      "  Batch [560/1299] D_loss: -0.1599, G_loss: 0.5347\n",
      "  Batch [570/1299] D_loss: -0.2399, G_loss: 0.6778\n",
      "  Batch [580/1299] D_loss: 0.0187, G_loss: 0.6692\n",
      "  Batch [590/1299] D_loss: -0.0563, G_loss: 0.7188\n",
      "  Batch [600/1299] D_loss: 0.0486, G_loss: 0.5348\n",
      "  Batch [610/1299] D_loss: -0.0283, G_loss: 0.4136\n",
      "  Batch [620/1299] D_loss: -1.4079, G_loss: -0.7616\n",
      "  Batch [630/1299] D_loss: -0.5039, G_loss: -0.9140\n",
      "  Batch [640/1299] D_loss: -0.0568, G_loss: 0.2811\n",
      "  Batch [650/1299] D_loss: -0.0847, G_loss: 0.3322\n",
      "  Batch [660/1299] D_loss: -0.0581, G_loss: 0.3218\n",
      "  Batch [670/1299] D_loss: -0.0093, G_loss: 0.3600\n",
      "  Batch [680/1299] D_loss: -0.1080, G_loss: 0.4709\n",
      "  Batch [690/1299] D_loss: 0.0022, G_loss: 0.2259\n",
      "  Batch [700/1299] D_loss: -0.4838, G_loss: -1.5897\n",
      "  Batch [710/1299] D_loss: -1.1893, G_loss: -1.7363\n",
      "  Batch [720/1299] D_loss: -0.0583, G_loss: 0.1668\n",
      "  Batch [730/1299] D_loss: -0.4058, G_loss: 0.0187\n",
      "  Batch [740/1299] D_loss: -0.3674, G_loss: 0.0231\n",
      "  Batch [750/1299] D_loss: -0.2202, G_loss: 0.1168\n",
      "  Batch [760/1299] D_loss: -1.0699, G_loss: 0.0181\n",
      "  Batch [770/1299] D_loss: -0.2160, G_loss: 0.0837\n",
      "  Batch [780/1299] D_loss: -0.0601, G_loss: 0.4400\n",
      "  Batch [790/1299] D_loss: -0.0638, G_loss: 0.5949\n",
      "  Batch [800/1299] D_loss: -0.1777, G_loss: 0.7066\n",
      "  Batch [810/1299] D_loss: -0.2212, G_loss: 0.7696\n",
      "  Batch [820/1299] D_loss: -0.1877, G_loss: 0.6857\n",
      "  Batch [830/1299] D_loss: 0.0234, G_loss: 0.7663\n",
      "  Batch [840/1299] D_loss: -0.7120, G_loss: -0.0975\n",
      "  Batch [850/1299] D_loss: 0.0101, G_loss: 0.1598\n",
      "  Batch [860/1299] D_loss: 0.0321, G_loss: 0.1841\n",
      "  Batch [870/1299] D_loss: -0.0462, G_loss: 0.2804\n",
      "  Batch [880/1299] D_loss: -0.0808, G_loss: 0.4682\n",
      "  Batch [890/1299] D_loss: -0.0929, G_loss: 0.4550\n",
      "  Batch [900/1299] D_loss: -0.1472, G_loss: 0.5153\n",
      "  Batch [910/1299] D_loss: -2.8863, G_loss: -2.5348\n",
      "  Batch [920/1299] D_loss: -0.7067, G_loss: -0.0228\n",
      "  Batch [930/1299] D_loss: -0.1380, G_loss: 0.1485\n",
      "  Batch [940/1299] D_loss: -0.1931, G_loss: 0.1346\n",
      "  Batch [950/1299] D_loss: -1.6547, G_loss: -4.2747\n",
      "  Batch [960/1299] D_loss: -0.0393, G_loss: 0.0643\n",
      "  Batch [970/1299] D_loss: -0.9602, G_loss: -3.1211\n",
      "  Batch [980/1299] D_loss: -0.4798, G_loss: -0.3400\n",
      "  Batch [990/1299] D_loss: -0.7013, G_loss: -1.6114\n",
      "  Batch [1000/1299] D_loss: -0.8454, G_loss: -0.3832\n",
      "  Batch [1010/1299] D_loss: -1.1994, G_loss: -0.4169\n",
      "  Batch [1020/1299] D_loss: -0.0952, G_loss: 0.3501\n",
      "  Batch [1030/1299] D_loss: -0.1138, G_loss: 0.6115\n",
      "  Batch [1040/1299] D_loss: -0.1162, G_loss: 0.6540\n",
      "  Batch [1050/1299] D_loss: -0.1687, G_loss: 0.7932\n",
      "  Batch [1060/1299] D_loss: -0.0588, G_loss: 0.7458\n",
      "  Batch [1070/1299] D_loss: 0.0014, G_loss: 0.5042\n",
      "  Batch [1080/1299] D_loss: -0.2183, G_loss: 0.4094\n",
      "  Batch [1090/1299] D_loss: -2.3446, G_loss: -1.1039\n",
      "  Batch [1100/1299] D_loss: -0.1674, G_loss: 0.1776\n",
      "  Batch [1110/1299] D_loss: 0.0106, G_loss: 0.1373\n",
      "  Batch [1120/1299] D_loss: -0.0497, G_loss: 0.2250\n",
      "  Batch [1130/1299] D_loss: -0.0669, G_loss: 0.3363\n",
      "  Batch [1140/1299] D_loss: -0.0680, G_loss: 0.2949\n",
      "  Batch [1150/1299] D_loss: -0.0078, G_loss: 0.3155\n",
      "  Batch [1160/1299] D_loss: -0.0929, G_loss: 0.2144\n",
      "  Batch [1170/1299] D_loss: -0.0683, G_loss: 0.1872\n",
      "  Batch [1180/1299] D_loss: -0.7426, G_loss: -1.6777\n",
      "  Batch [1190/1299] D_loss: -0.7513, G_loss: -0.2349\n",
      "  Batch [1200/1299] D_loss: -0.0336, G_loss: 0.1100\n",
      "  Batch [1210/1299] D_loss: -1.0425, G_loss: -0.1395\n",
      "  Batch [1220/1299] D_loss: -0.1519, G_loss: 0.3219\n",
      "  Batch [1230/1299] D_loss: -0.0517, G_loss: 0.4491\n",
      "  Batch [1240/1299] D_loss: -0.1354, G_loss: 0.6757\n",
      "  Batch [1250/1299] D_loss: -0.0652, G_loss: 0.6636\n",
      "  Batch [1260/1299] D_loss: 0.0040, G_loss: 0.4276\n",
      "  Batch [1270/1299] D_loss: -0.0748, G_loss: 0.3395\n",
      "  Batch [1280/1299] D_loss: -1.2888, G_loss: -0.8755\n",
      "  Batch [1290/1299] D_loss: -0.5609, G_loss: -1.5299\n",
      "\n",
      "Epoch 95 Summary:\n",
      "  Average D_loss: -0.1803\n",
      "  Average G_loss: -0.1129\n",
      "\n",
      "Epoch [96/100]\n",
      "  Batch [0/1299] D_loss: -1.7615, G_loss: -2.0130\n",
      "  Batch [10/1299] D_loss: -0.7638, G_loss: -2.8886\n",
      "  Batch [20/1299] D_loss: -0.0151, G_loss: 0.2871\n",
      "  Batch [30/1299] D_loss: -0.1209, G_loss: 0.5983\n",
      "  Batch [40/1299] D_loss: -0.0507, G_loss: 0.6725\n",
      "  Batch [50/1299] D_loss: -0.1717, G_loss: 0.5629\n",
      "  Batch [60/1299] D_loss: -0.0293, G_loss: 0.5901\n",
      "  Batch [70/1299] D_loss: -0.0430, G_loss: 0.4936\n",
      "  Batch [80/1299] D_loss: -0.0899, G_loss: 0.3888\n",
      "  Batch [90/1299] D_loss: -1.5860, G_loss: -2.9723\n",
      "  Batch [100/1299] D_loss: -1.8486, G_loss: -1.2284\n",
      "  Batch [110/1299] D_loss: -0.8157, G_loss: -2.2618\n",
      "  Batch [120/1299] D_loss: -0.0202, G_loss: 0.2393\n",
      "  Batch [130/1299] D_loss: -0.0409, G_loss: 0.4812\n",
      "  Batch [140/1299] D_loss: -0.0822, G_loss: 0.5421\n",
      "  Batch [150/1299] D_loss: -0.2069, G_loss: 0.5808\n",
      "  Batch [160/1299] D_loss: -0.0028, G_loss: 0.5415\n",
      "  Batch [170/1299] D_loss: -0.0885, G_loss: 0.4279\n",
      "  Batch [180/1299] D_loss: -0.0787, G_loss: 0.3067\n",
      "  Batch [190/1299] D_loss: -1.3558, G_loss: -2.1653\n",
      "  Batch [200/1299] D_loss: -0.0245, G_loss: 0.1193\n",
      "  Batch [210/1299] D_loss: -0.2773, G_loss: -0.1301\n",
      "  Batch [220/1299] D_loss: -0.2050, G_loss: 0.0705\n",
      "  Batch [230/1299] D_loss: -1.0233, G_loss: 0.0230\n",
      "  Batch [240/1299] D_loss: -0.0709, G_loss: 0.2452\n",
      "  Batch [250/1299] D_loss: -0.0555, G_loss: 0.3880\n",
      "  Batch [260/1299] D_loss: -0.0134, G_loss: 0.5688\n",
      "  Batch [270/1299] D_loss: -0.1141, G_loss: 0.6005\n",
      "  Batch [280/1299] D_loss: -0.1544, G_loss: 0.5636\n",
      "  Batch [290/1299] D_loss: -0.0314, G_loss: 0.5192\n",
      "  Batch [300/1299] D_loss: -0.1476, G_loss: 0.2442\n",
      "  Batch [310/1299] D_loss: -0.5777, G_loss: -1.3445\n",
      "  Batch [320/1299] D_loss: -0.2517, G_loss: -0.0477\n",
      "  Batch [330/1299] D_loss: -0.1469, G_loss: 0.2574\n",
      "  Batch [340/1299] D_loss: -0.0416, G_loss: 0.4214\n",
      "  Batch [350/1299] D_loss: -0.0135, G_loss: 0.3985\n",
      "  Batch [360/1299] D_loss: -0.0767, G_loss: 0.4571\n",
      "  Batch [370/1299] D_loss: -0.0339, G_loss: 0.3918\n",
      "  Batch [380/1299] D_loss: -1.2021, G_loss: -0.3658\n",
      "  Batch [390/1299] D_loss: -0.7534, G_loss: -0.7177\n",
      "  Batch [400/1299] D_loss: -0.0717, G_loss: 0.1975\n",
      "  Batch [410/1299] D_loss: -0.0657, G_loss: -0.0396\n",
      "  Batch [420/1299] D_loss: -0.9494, G_loss: -0.8654\n",
      "  Batch [430/1299] D_loss: -1.7030, G_loss: -0.7412\n",
      "  Batch [440/1299] D_loss: -0.1155, G_loss: 0.2272\n",
      "  Batch [450/1299] D_loss: -0.1215, G_loss: 0.3857\n",
      "  Batch [460/1299] D_loss: -0.1437, G_loss: 0.5291\n",
      "  Batch [470/1299] D_loss: -0.0185, G_loss: 0.4649\n",
      "  Batch [480/1299] D_loss: -0.1487, G_loss: 0.4878\n",
      "  Batch [490/1299] D_loss: -0.1507, G_loss: 0.0409\n",
      "  Batch [500/1299] D_loss: -1.5103, G_loss: -3.9831\n",
      "  Batch [510/1299] D_loss: -0.0038, G_loss: 0.2001\n",
      "  Batch [520/1299] D_loss: -0.0680, G_loss: 0.4205\n",
      "  Batch [530/1299] D_loss: -0.0999, G_loss: 0.4432\n",
      "  Batch [540/1299] D_loss: -0.0246, G_loss: 0.3976\n",
      "  Batch [550/1299] D_loss: -0.0533, G_loss: 0.4322\n",
      "  Batch [560/1299] D_loss: -0.5563, G_loss: -2.2872\n",
      "  Batch [570/1299] D_loss: -2.0330, G_loss: -4.1074\n",
      "  Batch [580/1299] D_loss: -0.0348, G_loss: 0.1720\n",
      "  Batch [590/1299] D_loss: -0.0600, G_loss: 0.2909\n",
      "  Batch [600/1299] D_loss: -0.1206, G_loss: 0.4692\n",
      "  Batch [610/1299] D_loss: -0.1390, G_loss: 0.5782\n",
      "  Batch [620/1299] D_loss: 0.0049, G_loss: 0.4309\n",
      "  Batch [630/1299] D_loss: -0.1133, G_loss: 0.3617\n",
      "  Batch [640/1299] D_loss: -0.2697, G_loss: -3.8394\n",
      "  Batch [650/1299] D_loss: 0.0139, G_loss: 0.1286\n",
      "  Batch [660/1299] D_loss: -0.2721, G_loss: 0.0387\n",
      "  Batch [670/1299] D_loss: -0.0979, G_loss: -0.0081\n",
      "  Batch [680/1299] D_loss: -0.1623, G_loss: 0.0599\n",
      "  Batch [690/1299] D_loss: -0.8688, G_loss: -2.3787\n",
      "  Batch [700/1299] D_loss: -0.5482, G_loss: 0.0070\n",
      "  Batch [710/1299] D_loss: -2.0862, G_loss: -2.1368\n",
      "  Batch [720/1299] D_loss: -0.0537, G_loss: 0.1642\n",
      "  Batch [730/1299] D_loss: -0.7196, G_loss: -0.4091\n",
      "  Batch [740/1299] D_loss: -0.1466, G_loss: 0.5573\n",
      "  Batch [750/1299] D_loss: -0.1885, G_loss: 0.7099\n",
      "  Batch [760/1299] D_loss: -0.1091, G_loss: 0.7423\n",
      "  Batch [770/1299] D_loss: -0.2365, G_loss: 0.8435\n",
      "  Batch [780/1299] D_loss: -0.0470, G_loss: 0.5622\n",
      "  Batch [790/1299] D_loss: 0.0258, G_loss: 0.5048\n",
      "  Batch [800/1299] D_loss: 0.0816, G_loss: 0.4643\n",
      "  Batch [810/1299] D_loss: -3.1511, G_loss: -6.9788\n",
      "  Batch [820/1299] D_loss: -0.0150, G_loss: 0.1572\n",
      "  Batch [830/1299] D_loss: -0.0295, G_loss: 0.1897\n",
      "  Batch [840/1299] D_loss: -0.0276, G_loss: 0.3097\n",
      "  Batch [850/1299] D_loss: -0.0472, G_loss: 0.3353\n",
      "  Batch [860/1299] D_loss: -0.0731, G_loss: 0.5382\n",
      "  Batch [870/1299] D_loss: -1.4213, G_loss: -0.6688\n",
      "  Batch [880/1299] D_loss: -0.2244, G_loss: -0.0471\n",
      "  Batch [890/1299] D_loss: 0.0119, G_loss: 0.1524\n",
      "  Batch [900/1299] D_loss: -0.1632, G_loss: 0.2777\n",
      "  Batch [910/1299] D_loss: -0.0538, G_loss: 0.3140\n",
      "  Batch [920/1299] D_loss: -0.0656, G_loss: 0.3931\n",
      "  Batch [930/1299] D_loss: -0.0438, G_loss: 0.1182\n",
      "  Batch [940/1299] D_loss: -0.5031, G_loss: -0.5748\n",
      "  Batch [950/1299] D_loss: -0.5824, G_loss: -0.1669\n",
      "  Batch [960/1299] D_loss: -0.6503, G_loss: -0.2486\n",
      "  Batch [970/1299] D_loss: -0.1597, G_loss: -0.0002\n",
      "  Batch [980/1299] D_loss: -0.0047, G_loss: 0.1577\n",
      "  Batch [990/1299] D_loss: -0.7656, G_loss: -1.1070\n",
      "  Batch [1000/1299] D_loss: -0.6283, G_loss: -0.1554\n",
      "  Batch [1010/1299] D_loss: -0.3299, G_loss: -0.0030\n",
      "  Batch [1020/1299] D_loss: -0.8846, G_loss: -0.0258\n",
      "  Batch [1030/1299] D_loss: -0.1076, G_loss: 0.4063\n",
      "  Batch [1040/1299] D_loss: -0.1795, G_loss: 0.6256\n",
      "  Batch [1050/1299] D_loss: -0.2544, G_loss: 0.6866\n",
      "  Batch [1060/1299] D_loss: -0.0827, G_loss: 0.6909\n",
      "  Batch [1070/1299] D_loss: 0.0247, G_loss: 0.5656\n",
      "  Batch [1080/1299] D_loss: -0.1448, G_loss: 0.7150\n",
      "  Batch [1090/1299] D_loss: -0.0002, G_loss: 0.2212\n",
      "  Batch [1100/1299] D_loss: -1.4628, G_loss: -2.8594\n",
      "  Batch [1110/1299] D_loss: -0.8051, G_loss: -0.1890\n",
      "  Batch [1120/1299] D_loss: -0.2390, G_loss: 0.0668\n",
      "  Batch [1130/1299] D_loss: -0.9049, G_loss: -2.4061\n",
      "  Batch [1140/1299] D_loss: -0.0181, G_loss: 0.3662\n",
      "  Batch [1150/1299] D_loss: -0.0732, G_loss: 0.4866\n",
      "  Batch [1160/1299] D_loss: -0.1996, G_loss: 0.6658\n",
      "  Batch [1170/1299] D_loss: -0.1264, G_loss: 0.6269\n",
      "  Batch [1180/1299] D_loss: -0.1614, G_loss: 0.4910\n",
      "  Batch [1190/1299] D_loss: -0.0999, G_loss: 0.6001\n",
      "  Batch [1200/1299] D_loss: -0.0337, G_loss: 0.1801\n",
      "  Batch [1210/1299] D_loss: -0.8969, G_loss: -2.3500\n",
      "  Batch [1220/1299] D_loss: -0.7199, G_loss: 0.0501\n",
      "  Batch [1230/1299] D_loss: -0.0203, G_loss: 0.1397\n",
      "  Batch [1240/1299] D_loss: -0.9267, G_loss: -0.6956\n",
      "  Batch [1250/1299] D_loss: -0.8218, G_loss: -0.4774\n",
      "  Batch [1260/1299] D_loss: -0.2091, G_loss: -0.1323\n",
      "  Batch [1270/1299] D_loss: -0.2372, G_loss: -0.0036\n",
      "  Batch [1280/1299] D_loss: -0.8097, G_loss: -1.1544\n",
      "  Batch [1290/1299] D_loss: -2.0717, G_loss: -1.6334\n",
      "\n",
      "Epoch 96 Summary:\n",
      "  Average D_loss: -0.1921\n",
      "  Average G_loss: -0.1052\n",
      "\n",
      "Epoch [97/100]\n",
      "  Batch [0/1299] D_loss: -0.7577, G_loss: -0.6552\n",
      "  Batch [10/1299] D_loss: -0.3946, G_loss: 0.1712\n",
      "  Batch [20/1299] D_loss: -0.7288, G_loss: -0.4171\n",
      "  Batch [30/1299] D_loss: -0.1655, G_loss: 0.5201\n",
      "  Batch [40/1299] D_loss: -0.3249, G_loss: 0.8263\n",
      "  Batch [50/1299] D_loss: -0.2967, G_loss: 1.0712\n",
      "  Batch [60/1299] D_loss: -0.0716, G_loss: 0.6991\n",
      "  Batch [70/1299] D_loss: 0.0054, G_loss: 0.8861\n",
      "  Batch [80/1299] D_loss: -0.0933, G_loss: 0.5982\n",
      "  Batch [90/1299] D_loss: -0.0971, G_loss: 0.3622\n",
      "  Batch [100/1299] D_loss: -0.9275, G_loss: -1.1168\n",
      "  Batch [110/1299] D_loss: -0.1309, G_loss: 0.1168\n",
      "  Batch [120/1299] D_loss: 0.0273, G_loss: 0.2654\n",
      "  Batch [130/1299] D_loss: -0.0629, G_loss: 0.3914\n",
      "  Batch [140/1299] D_loss: -0.1489, G_loss: 0.6427\n",
      "  Batch [150/1299] D_loss: -0.1040, G_loss: 0.4746\n",
      "  Batch [160/1299] D_loss: -0.0052, G_loss: 0.4178\n",
      "  Batch [170/1299] D_loss: -0.0464, G_loss: 0.2168\n",
      "  Batch [180/1299] D_loss: -0.7538, G_loss: -1.2858\n",
      "  Batch [190/1299] D_loss: -0.0472, G_loss: 0.1689\n",
      "  Batch [200/1299] D_loss: -0.0364, G_loss: 0.2960\n",
      "  Batch [210/1299] D_loss: -0.0642, G_loss: 0.5172\n",
      "  Batch [220/1299] D_loss: -0.0357, G_loss: 0.5675\n",
      "  Batch [230/1299] D_loss: -0.1685, G_loss: 0.6851\n",
      "  Batch [240/1299] D_loss: -0.0078, G_loss: 0.4526\n",
      "  Batch [250/1299] D_loss: -3.8399, G_loss: -5.4686\n",
      "  Batch [260/1299] D_loss: -0.8725, G_loss: -2.3799\n",
      "  Batch [270/1299] D_loss: -0.1709, G_loss: 0.1503\n",
      "  Batch [280/1299] D_loss: -0.7994, G_loss: -0.9199\n",
      "  Batch [290/1299] D_loss: -0.5441, G_loss: 0.0223\n",
      "  Batch [300/1299] D_loss: -0.0432, G_loss: 0.1761\n",
      "  Batch [310/1299] D_loss: -0.1107, G_loss: 0.3046\n",
      "  Batch [320/1299] D_loss: -0.1378, G_loss: 0.4367\n",
      "  Batch [330/1299] D_loss: -0.1916, G_loss: 0.7369\n",
      "  Batch [340/1299] D_loss: 0.0738, G_loss: 0.4958\n",
      "  Batch [350/1299] D_loss: 0.0099, G_loss: 0.5395\n",
      "  Batch [360/1299] D_loss: -0.0842, G_loss: 0.3949\n",
      "  Batch [370/1299] D_loss: -0.0349, G_loss: 0.2064\n",
      "  Batch [380/1299] D_loss: -0.5297, G_loss: -0.6708\n",
      "  Batch [390/1299] D_loss: -0.3620, G_loss: -0.2862\n",
      "  Batch [400/1299] D_loss: -0.9060, G_loss: -1.4369\n",
      "  Batch [410/1299] D_loss: -0.1134, G_loss: 0.2566\n",
      "  Batch [420/1299] D_loss: -0.0118, G_loss: 0.3812\n",
      "  Batch [430/1299] D_loss: -0.2059, G_loss: 0.5463\n",
      "  Batch [440/1299] D_loss: -0.1503, G_loss: 0.6947\n",
      "  Batch [450/1299] D_loss: 0.0148, G_loss: 0.5292\n",
      "  Batch [460/1299] D_loss: -0.1110, G_loss: 0.5198\n",
      "  Batch [470/1299] D_loss: -0.1068, G_loss: 0.5506\n",
      "  Batch [480/1299] D_loss: -1.8343, G_loss: -2.2894\n",
      "  Batch [490/1299] D_loss: -0.0011, G_loss: -2.2959\n",
      "  Batch [500/1299] D_loss: -0.3609, G_loss: -1.1037\n",
      "  Batch [510/1299] D_loss: -0.0864, G_loss: 0.2540\n",
      "  Batch [520/1299] D_loss: 0.0285, G_loss: 0.3558\n",
      "  Batch [530/1299] D_loss: -0.1514, G_loss: 0.5677\n",
      "  Batch [540/1299] D_loss: -0.1112, G_loss: 0.6164\n",
      "  Batch [550/1299] D_loss: -0.1670, G_loss: 0.6001\n",
      "  Batch [560/1299] D_loss: -0.1201, G_loss: 0.4789\n",
      "  Batch [570/1299] D_loss: -0.0066, G_loss: 0.3295\n",
      "  Batch [580/1299] D_loss: -0.7027, G_loss: -0.0934\n",
      "  Batch [590/1299] D_loss: -0.5183, G_loss: -0.3006\n",
      "  Batch [600/1299] D_loss: -0.2974, G_loss: -0.0631\n",
      "  Batch [610/1299] D_loss: -0.0388, G_loss: 0.1665\n",
      "  Batch [620/1299] D_loss: -0.0463, G_loss: 0.3308\n",
      "  Batch [630/1299] D_loss: -0.0828, G_loss: 0.4280\n",
      "  Batch [640/1299] D_loss: -0.0817, G_loss: 0.4887\n",
      "  Batch [650/1299] D_loss: -0.1653, G_loss: 0.5037\n",
      "  Batch [660/1299] D_loss: -0.1287, G_loss: 0.6274\n",
      "  Batch [670/1299] D_loss: -0.0592, G_loss: 0.4377\n",
      "  Batch [680/1299] D_loss: -0.7585, G_loss: -0.5856\n",
      "  Batch [690/1299] D_loss: 0.0173, G_loss: 0.1720\n",
      "  Batch [700/1299] D_loss: -0.0146, G_loss: 0.4097\n",
      "  Batch [710/1299] D_loss: -0.0184, G_loss: 0.5264\n",
      "  Batch [720/1299] D_loss: -0.1697, G_loss: 0.5514\n",
      "  Batch [730/1299] D_loss: -0.0247, G_loss: 0.5558\n",
      "  Batch [740/1299] D_loss: -0.0418, G_loss: 0.3293\n",
      "  Batch [750/1299] D_loss: -0.0572, G_loss: 0.2474\n",
      "  Batch [760/1299] D_loss: -0.1891, G_loss: -0.2653\n",
      "  Batch [770/1299] D_loss: -0.3365, G_loss: -0.7624\n",
      "  Batch [780/1299] D_loss: -0.0667, G_loss: 0.3891\n",
      "  Batch [790/1299] D_loss: -0.1182, G_loss: 0.5734\n",
      "  Batch [800/1299] D_loss: -0.1754, G_loss: 0.5858\n",
      "  Batch [810/1299] D_loss: -0.0657, G_loss: 0.5898\n",
      "  Batch [820/1299] D_loss: 0.0446, G_loss: 0.5636\n",
      "  Batch [830/1299] D_loss: 0.0645, G_loss: 0.3050\n",
      "  Batch [840/1299] D_loss: -0.0181, G_loss: 0.2061\n",
      "  Batch [850/1299] D_loss: -0.7593, G_loss: -0.9121\n",
      "  Batch [860/1299] D_loss: -0.6881, G_loss: -1.2338\n",
      "  Batch [870/1299] D_loss: -0.0765, G_loss: 0.2961\n",
      "  Batch [880/1299] D_loss: -0.0421, G_loss: 0.4281\n",
      "  Batch [890/1299] D_loss: -0.1120, G_loss: 0.5092\n",
      "  Batch [900/1299] D_loss: 0.0172, G_loss: 0.4614\n",
      "  Batch [910/1299] D_loss: -0.0197, G_loss: 0.5530\n",
      "  Batch [920/1299] D_loss: 0.0393, G_loss: 0.3359\n",
      "  Batch [930/1299] D_loss: -0.2149, G_loss: 0.1782\n",
      "  Batch [940/1299] D_loss: -0.3203, G_loss: -0.4830\n",
      "  Batch [950/1299] D_loss: -0.0127, G_loss: 0.1744\n",
      "  Batch [960/1299] D_loss: -0.2489, G_loss: -0.1808\n",
      "  Batch [970/1299] D_loss: -0.8693, G_loss: -0.1241\n",
      "  Batch [980/1299] D_loss: -0.3853, G_loss: -0.6339\n",
      "  Batch [990/1299] D_loss: -0.6008, G_loss: 0.0740\n",
      "  Batch [1000/1299] D_loss: -0.8118, G_loss: -0.4911\n",
      "  Batch [1010/1299] D_loss: -0.0402, G_loss: 0.4043\n",
      "  Batch [1020/1299] D_loss: -0.1370, G_loss: 0.6343\n",
      "  Batch [1030/1299] D_loss: -0.0469, G_loss: 0.7170\n",
      "  Batch [1040/1299] D_loss: -0.0854, G_loss: 0.7116\n",
      "  Batch [1050/1299] D_loss: 0.0011, G_loss: 0.6177\n",
      "  Batch [1060/1299] D_loss: -0.1974, G_loss: 0.5515\n",
      "  Batch [1070/1299] D_loss: -0.0476, G_loss: 0.2796\n",
      "  Batch [1080/1299] D_loss: -1.3030, G_loss: -4.8537\n",
      "  Batch [1090/1299] D_loss: -0.5841, G_loss: -2.6009\n",
      "  Batch [1100/1299] D_loss: -0.2323, G_loss: 0.1889\n",
      "  Batch [1110/1299] D_loss: -0.0455, G_loss: 0.3041\n",
      "  Batch [1120/1299] D_loss: -0.0081, G_loss: 0.4247\n",
      "  Batch [1130/1299] D_loss: -0.1025, G_loss: 0.5218\n",
      "  Batch [1140/1299] D_loss: -0.0726, G_loss: 0.4933\n",
      "  Batch [1150/1299] D_loss: -0.0863, G_loss: 0.3878\n",
      "  Batch [1160/1299] D_loss: -1.8618, G_loss: -1.3210\n",
      "  Batch [1170/1299] D_loss: -0.0683, G_loss: 0.2320\n",
      "  Batch [1180/1299] D_loss: 0.0137, G_loss: 0.2357\n",
      "  Batch [1190/1299] D_loss: -0.0156, G_loss: 0.2490\n",
      "  Batch [1200/1299] D_loss: -0.0997, G_loss: 0.3901\n",
      "  Batch [1210/1299] D_loss: -0.1035, G_loss: 0.4081\n",
      "  Batch [1220/1299] D_loss: -0.0352, G_loss: 0.2566\n",
      "  Batch [1230/1299] D_loss: -0.9217, G_loss: -1.5770\n",
      "  Batch [1240/1299] D_loss: 0.0004, G_loss: 0.1221\n",
      "  Batch [1250/1299] D_loss: 0.0127, G_loss: 0.1730\n",
      "  Batch [1260/1299] D_loss: -0.0686, G_loss: 0.2748\n",
      "  Batch [1270/1299] D_loss: 0.0259, G_loss: 0.2731\n",
      "  Batch [1280/1299] D_loss: -0.0052, G_loss: 0.3818\n",
      "  Batch [1290/1299] D_loss: -0.0406, G_loss: 0.3218\n",
      "\n",
      "Epoch 97 Summary:\n",
      "  Average D_loss: -0.1675\n",
      "  Average G_loss: -0.0484\n",
      "\n",
      "Epoch [98/100]\n",
      "  Batch [0/1299] D_loss: -0.4836, G_loss: -2.0589\n",
      "  Batch [10/1299] D_loss: -0.2749, G_loss: -0.3293\n",
      "  Batch [20/1299] D_loss: -0.8681, G_loss: -0.3392\n",
      "  Batch [30/1299] D_loss: -0.0568, G_loss: 0.0654\n",
      "  Batch [40/1299] D_loss: -0.2374, G_loss: 0.0259\n",
      "  Batch [50/1299] D_loss: -0.3972, G_loss: 0.1244\n",
      "  Batch [60/1299] D_loss: -0.6509, G_loss: -0.8648\n",
      "  Batch [70/1299] D_loss: -0.1580, G_loss: 0.1169\n",
      "  Batch [80/1299] D_loss: -0.0729, G_loss: 0.3350\n",
      "  Batch [90/1299] D_loss: -0.1201, G_loss: 0.5616\n",
      "  Batch [100/1299] D_loss: -0.0428, G_loss: 0.6301\n",
      "  Batch [110/1299] D_loss: -0.0759, G_loss: 0.5653\n",
      "  Batch [120/1299] D_loss: -0.0270, G_loss: 0.5357\n",
      "  Batch [130/1299] D_loss: -0.0413, G_loss: 0.3577\n",
      "  Batch [140/1299] D_loss: -2.4114, G_loss: -2.7921\n",
      "  Batch [150/1299] D_loss: -0.8917, G_loss: -0.4452\n",
      "  Batch [160/1299] D_loss: -0.0227, G_loss: 0.2828\n",
      "  Batch [170/1299] D_loss: -0.0155, G_loss: 0.3379\n",
      "  Batch [180/1299] D_loss: -0.0021, G_loss: 0.4641\n",
      "  Batch [190/1299] D_loss: -0.1086, G_loss: 0.5835\n",
      "  Batch [200/1299] D_loss: -0.0782, G_loss: 0.2951\n",
      "  Batch [210/1299] D_loss: -0.0970, G_loss: 0.2603\n",
      "  Batch [220/1299] D_loss: -0.0443, G_loss: 0.1182\n",
      "  Batch [230/1299] D_loss: -1.0634, G_loss: -1.5988\n",
      "  Batch [240/1299] D_loss: -0.3056, G_loss: -0.1712\n",
      "  Batch [250/1299] D_loss: -0.0163, G_loss: 0.1453\n",
      "  Batch [260/1299] D_loss: -0.1109, G_loss: 0.3595\n",
      "  Batch [270/1299] D_loss: -0.0492, G_loss: 0.4909\n",
      "  Batch [280/1299] D_loss: 0.0190, G_loss: 0.2845\n",
      "  Batch [290/1299] D_loss: -0.1853, G_loss: 0.4149\n",
      "  Batch [300/1299] D_loss: -0.0279, G_loss: 0.3238\n",
      "  Batch [310/1299] D_loss: -0.0826, G_loss: 0.2732\n",
      "  Batch [320/1299] D_loss: -1.7123, G_loss: -3.2254\n",
      "  Batch [330/1299] D_loss: 0.0247, G_loss: 0.1170\n",
      "  Batch [340/1299] D_loss: -0.0667, G_loss: 0.0744\n",
      "  Batch [350/1299] D_loss: -0.2562, G_loss: 0.0095\n",
      "  Batch [360/1299] D_loss: -1.0248, G_loss: -0.0319\n",
      "  Batch [370/1299] D_loss: -1.1723, G_loss: -1.1553\n",
      "  Batch [380/1299] D_loss: -0.5304, G_loss: 0.0856\n",
      "  Batch [390/1299] D_loss: -0.4381, G_loss: -0.6725\n",
      "  Batch [400/1299] D_loss: -1.4774, G_loss: -2.7600\n",
      "  Batch [410/1299] D_loss: -0.5121, G_loss: 0.0309\n",
      "  Batch [420/1299] D_loss: -0.6358, G_loss: 0.0966\n",
      "  Batch [430/1299] D_loss: -0.1094, G_loss: 0.2865\n",
      "  Batch [440/1299] D_loss: -0.1082, G_loss: 0.6375\n",
      "  Batch [450/1299] D_loss: -0.2437, G_loss: 0.9293\n",
      "  Batch [460/1299] D_loss: -0.0854, G_loss: 0.7779\n",
      "  Batch [470/1299] D_loss: -0.1820, G_loss: 0.8377\n",
      "  Batch [480/1299] D_loss: -0.0917, G_loss: 0.8407\n",
      "  Batch [490/1299] D_loss: -0.1680, G_loss: 0.6179\n",
      "  Batch [500/1299] D_loss: -0.0143, G_loss: 0.2709\n",
      "  Batch [510/1299] D_loss: -0.0985, G_loss: -1.4989\n",
      "  Batch [520/1299] D_loss: -0.6934, G_loss: -1.3900\n",
      "  Batch [530/1299] D_loss: -0.1174, G_loss: 0.2929\n",
      "  Batch [540/1299] D_loss: -0.0220, G_loss: 0.4345\n",
      "  Batch [550/1299] D_loss: -0.0594, G_loss: 0.4313\n",
      "  Batch [560/1299] D_loss: 0.0391, G_loss: 0.4356\n",
      "  Batch [570/1299] D_loss: 0.0062, G_loss: 0.4642\n",
      "  Batch [580/1299] D_loss: -0.0465, G_loss: 0.4533\n",
      "  Batch [590/1299] D_loss: -2.7190, G_loss: -1.3414\n",
      "  Batch [600/1299] D_loss: -0.9618, G_loss: -1.2022\n",
      "  Batch [610/1299] D_loss: -0.6413, G_loss: -0.3152\n",
      "  Batch [620/1299] D_loss: -1.0229, G_loss: -0.3477\n",
      "  Batch [630/1299] D_loss: -0.8781, G_loss: -0.9092\n",
      "  Batch [640/1299] D_loss: -0.3355, G_loss: -0.6075\n",
      "  Batch [650/1299] D_loss: -0.0616, G_loss: 0.2436\n",
      "  Batch [660/1299] D_loss: -0.1215, G_loss: 0.4215\n",
      "  Batch [670/1299] D_loss: -0.1445, G_loss: 0.5985\n",
      "  Batch [680/1299] D_loss: -0.0863, G_loss: 0.5927\n",
      "  Batch [690/1299] D_loss: -0.0957, G_loss: 0.7425\n",
      "  Batch [700/1299] D_loss: -0.0763, G_loss: 0.5758\n",
      "  Batch [710/1299] D_loss: -0.0727, G_loss: 0.4378\n",
      "  Batch [720/1299] D_loss: -0.0541, G_loss: 0.2202\n",
      "  Batch [730/1299] D_loss: -1.4030, G_loss: -2.8456\n",
      "  Batch [740/1299] D_loss: -0.4447, G_loss: -1.0299\n",
      "  Batch [750/1299] D_loss: -0.6946, G_loss: -0.0829\n",
      "  Batch [760/1299] D_loss: -0.7046, G_loss: -0.4307\n",
      "  Batch [770/1299] D_loss: -0.0172, G_loss: 0.2893\n",
      "  Batch [780/1299] D_loss: -0.5093, G_loss: -0.1086\n",
      "  Batch [790/1299] D_loss: -1.1348, G_loss: -2.4910\n",
      "  Batch [800/1299] D_loss: -1.0303, G_loss: -0.3183\n",
      "  Batch [810/1299] D_loss: -0.9768, G_loss: -0.9508\n",
      "  Batch [820/1299] D_loss: -0.5943, G_loss: -0.3046\n",
      "  Batch [830/1299] D_loss: -0.1018, G_loss: 0.2047\n",
      "  Batch [840/1299] D_loss: -0.1216, G_loss: 0.2603\n",
      "  Batch [850/1299] D_loss: -0.1790, G_loss: 0.5471\n",
      "  Batch [860/1299] D_loss: -0.2507, G_loss: 0.8752\n",
      "  Batch [870/1299] D_loss: -0.2105, G_loss: 0.9263\n",
      "  Batch [880/1299] D_loss: -0.0904, G_loss: 0.8804\n",
      "  Batch [890/1299] D_loss: -0.0841, G_loss: 0.6364\n",
      "  Batch [900/1299] D_loss: -0.1181, G_loss: 0.3765\n",
      "  Batch [910/1299] D_loss: -1.3819, G_loss: -4.8610\n",
      "  Batch [920/1299] D_loss: -0.3828, G_loss: -0.3732\n",
      "  Batch [930/1299] D_loss: -0.0105, G_loss: 0.0777\n",
      "  Batch [940/1299] D_loss: -0.4599, G_loss: -0.2365\n",
      "  Batch [950/1299] D_loss: -0.4559, G_loss: -1.3581\n",
      "  Batch [960/1299] D_loss: -0.1148, G_loss: 0.0340\n",
      "  Batch [970/1299] D_loss: -0.1391, G_loss: 0.5045\n",
      "  Batch [980/1299] D_loss: -0.1206, G_loss: 0.6418\n",
      "  Batch [990/1299] D_loss: -0.1879, G_loss: 0.7067\n",
      "  Batch [1000/1299] D_loss: -0.1447, G_loss: 0.8659\n",
      "  Batch [1010/1299] D_loss: -0.1170, G_loss: 0.8933\n",
      "  Batch [1020/1299] D_loss: -0.1073, G_loss: 0.6907\n",
      "  Batch [1030/1299] D_loss: -0.0310, G_loss: 0.3269\n",
      "  Batch [1040/1299] D_loss: -2.5154, G_loss: -7.3633\n",
      "  Batch [1050/1299] D_loss: -0.5886, G_loss: -0.0314\n",
      "  Batch [1060/1299] D_loss: -0.0289, G_loss: 0.0238\n",
      "  Batch [1070/1299] D_loss: -0.6553, G_loss: -0.6140\n",
      "  Batch [1080/1299] D_loss: -0.6343, G_loss: 0.0272\n",
      "  Batch [1090/1299] D_loss: -0.2754, G_loss: 0.0061\n",
      "  Batch [1100/1299] D_loss: -0.8482, G_loss: -0.7833\n",
      "  Batch [1110/1299] D_loss: 0.0122, G_loss: 0.1647\n",
      "  Batch [1120/1299] D_loss: -0.0150, G_loss: 0.1968\n",
      "  Batch [1130/1299] D_loss: -0.1133, G_loss: 0.4747\n",
      "  Batch [1140/1299] D_loss: -0.1100, G_loss: 0.5922\n",
      "  Batch [1150/1299] D_loss: -0.0991, G_loss: 0.6284\n",
      "  Batch [1160/1299] D_loss: -0.1494, G_loss: 0.6153\n",
      "  Batch [1170/1299] D_loss: -0.1341, G_loss: 0.5438\n",
      "  Batch [1180/1299] D_loss: -0.0862, G_loss: 0.3511\n",
      "  Batch [1190/1299] D_loss: -2.8997, G_loss: -2.2152\n",
      "  Batch [1200/1299] D_loss: -0.3629, G_loss: 0.0287\n",
      "  Batch [1210/1299] D_loss: -0.5248, G_loss: -0.4209\n",
      "  Batch [1220/1299] D_loss: -0.5986, G_loss: -0.3207\n",
      "  Batch [1230/1299] D_loss: -0.6046, G_loss: -0.2301\n",
      "  Batch [1240/1299] D_loss: -0.6600, G_loss: -0.2781\n",
      "  Batch [1250/1299] D_loss: -0.0019, G_loss: 0.2533\n",
      "  Batch [1260/1299] D_loss: -0.0291, G_loss: 0.3892\n",
      "  Batch [1270/1299] D_loss: -0.2324, G_loss: 0.6207\n",
      "  Batch [1280/1299] D_loss: -0.1087, G_loss: 0.5234\n",
      "  Batch [1290/1299] D_loss: -0.0664, G_loss: 0.6181\n",
      "\n",
      "Epoch 98 Summary:\n",
      "  Average D_loss: -0.2027\n",
      "  Average G_loss: -0.1061\n",
      "\n",
      "Epoch [99/100]\n",
      "  Batch [0/1299] D_loss: -0.1156, G_loss: 0.4132\n",
      "  Batch [10/1299] D_loss: -1.4728, G_loss: -3.1031\n",
      "  Batch [20/1299] D_loss: -0.3179, G_loss: 0.0827\n",
      "  Batch [30/1299] D_loss: -0.5332, G_loss: -1.1753\n",
      "  Batch [40/1299] D_loss: -0.4590, G_loss: -0.5223\n",
      "  Batch [50/1299] D_loss: -0.4564, G_loss: 0.0219\n",
      "  Batch [60/1299] D_loss: -0.0221, G_loss: 0.1453\n",
      "  Batch [70/1299] D_loss: -0.2038, G_loss: 0.3333\n",
      "  Batch [80/1299] D_loss: -0.2615, G_loss: 0.6268\n",
      "  Batch [90/1299] D_loss: 0.0172, G_loss: 0.6057\n",
      "  Batch [100/1299] D_loss: 0.0489, G_loss: 0.5390\n",
      "  Batch [110/1299] D_loss: -0.1625, G_loss: 0.6771\n",
      "  Batch [120/1299] D_loss: -0.1459, G_loss: 0.4573\n",
      "  Batch [130/1299] D_loss: -1.7157, G_loss: -2.5284\n",
      "  Batch [140/1299] D_loss: -0.7096, G_loss: -0.7077\n",
      "  Batch [150/1299] D_loss: -0.1181, G_loss: 0.0877\n",
      "  Batch [160/1299] D_loss: -0.0263, G_loss: 0.2192\n",
      "  Batch [170/1299] D_loss: 0.0044, G_loss: 0.2372\n",
      "  Batch [180/1299] D_loss: -0.0317, G_loss: 0.3494\n",
      "  Batch [190/1299] D_loss: -0.0961, G_loss: 0.4481\n",
      "  Batch [200/1299] D_loss: 0.0732, G_loss: 0.4354\n",
      "  Batch [210/1299] D_loss: -0.1476, G_loss: 0.4624\n",
      "  Batch [220/1299] D_loss: -0.1057, G_loss: 0.3892\n",
      "  Batch [230/1299] D_loss: -1.5026, G_loss: -0.1362\n",
      "  Batch [240/1299] D_loss: -0.2328, G_loss: -0.0552\n",
      "  Batch [250/1299] D_loss: -0.0289, G_loss: 0.1372\n",
      "  Batch [260/1299] D_loss: -0.0715, G_loss: 0.2915\n",
      "  Batch [270/1299] D_loss: -0.1399, G_loss: 0.4266\n",
      "  Batch [280/1299] D_loss: -0.0951, G_loss: 0.5302\n",
      "  Batch [290/1299] D_loss: -0.0789, G_loss: 0.5785\n",
      "  Batch [300/1299] D_loss: -0.0698, G_loss: 0.5326\n",
      "  Batch [310/1299] D_loss: -0.0039, G_loss: 0.1787\n",
      "  Batch [320/1299] D_loss: -0.8863, G_loss: -1.8604\n",
      "  Batch [330/1299] D_loss: -0.0345, G_loss: 0.1562\n",
      "  Batch [340/1299] D_loss: -0.5205, G_loss: -1.3297\n",
      "  Batch [350/1299] D_loss: -0.0631, G_loss: 0.2756\n",
      "  Batch [360/1299] D_loss: -0.0387, G_loss: 0.3927\n",
      "  Batch [370/1299] D_loss: -0.0612, G_loss: 0.5814\n",
      "  Batch [380/1299] D_loss: -0.0750, G_loss: 0.5744\n",
      "  Batch [390/1299] D_loss: -0.0672, G_loss: 0.6070\n",
      "  Batch [400/1299] D_loss: 0.0872, G_loss: 0.5044\n",
      "  Batch [410/1299] D_loss: -0.0474, G_loss: 0.3189\n",
      "  Batch [420/1299] D_loss: -0.6432, G_loss: -2.5807\n",
      "  Batch [430/1299] D_loss: -0.5918, G_loss: -0.5898\n",
      "  Batch [440/1299] D_loss: -0.0324, G_loss: 0.0836\n",
      "  Batch [450/1299] D_loss: -0.2874, G_loss: -0.1575\n",
      "  Batch [460/1299] D_loss: -0.1502, G_loss: 0.0112\n",
      "  Batch [470/1299] D_loss: -0.0274, G_loss: 0.3311\n",
      "  Batch [480/1299] D_loss: -0.0763, G_loss: 0.5003\n",
      "  Batch [490/1299] D_loss: -0.0961, G_loss: 0.6366\n",
      "  Batch [500/1299] D_loss: -0.1458, G_loss: 0.7182\n",
      "  Batch [510/1299] D_loss: -0.0081, G_loss: 0.4468\n",
      "  Batch [520/1299] D_loss: -0.1290, G_loss: 0.5343\n",
      "  Batch [530/1299] D_loss: -3.0045, G_loss: -1.0594\n",
      "  Batch [540/1299] D_loss: 0.0083, G_loss: 0.1086\n",
      "  Batch [550/1299] D_loss: -0.0051, G_loss: 0.1718\n",
      "  Batch [560/1299] D_loss: -0.1401, G_loss: 0.1918\n",
      "  Batch [570/1299] D_loss: -0.0708, G_loss: 0.2081\n",
      "  Batch [580/1299] D_loss: -0.1482, G_loss: -0.2805\n",
      "  Batch [590/1299] D_loss: -0.1280, G_loss: 0.3770\n",
      "  Batch [600/1299] D_loss: -0.0833, G_loss: 0.5645\n",
      "  Batch [610/1299] D_loss: -0.0403, G_loss: 0.5770\n",
      "  Batch [620/1299] D_loss: -0.1089, G_loss: 0.5775\n",
      "  Batch [630/1299] D_loss: -0.0432, G_loss: 0.4518\n",
      "  Batch [640/1299] D_loss: -0.0197, G_loss: 0.2826\n",
      "  Batch [650/1299] D_loss: -2.2863, G_loss: -2.6854\n",
      "  Batch [660/1299] D_loss: -0.1279, G_loss: 0.0478\n",
      "  Batch [670/1299] D_loss: -0.3278, G_loss: -0.1668\n",
      "  Batch [680/1299] D_loss: -0.4095, G_loss: -0.7394\n",
      "  Batch [690/1299] D_loss: -0.1106, G_loss: 0.0333\n",
      "  Batch [700/1299] D_loss: -0.9201, G_loss: -0.7409\n",
      "  Batch [710/1299] D_loss: -0.1461, G_loss: 0.1582\n",
      "  Batch [720/1299] D_loss: -0.1298, G_loss: 0.4690\n",
      "  Batch [730/1299] D_loss: -0.2260, G_loss: 0.7416\n",
      "  Batch [740/1299] D_loss: -0.1961, G_loss: 0.8114\n",
      "  Batch [750/1299] D_loss: -0.1994, G_loss: 0.9736\n",
      "  Batch [760/1299] D_loss: -0.1605, G_loss: 0.7369\n",
      "  Batch [770/1299] D_loss: 0.0234, G_loss: 0.3406\n",
      "  Batch [780/1299] D_loss: -0.0295, G_loss: 0.3656\n",
      "  Batch [790/1299] D_loss: -1.0569, G_loss: -1.2928\n",
      "  Batch [800/1299] D_loss: -0.0137, G_loss: 0.1340\n",
      "  Batch [810/1299] D_loss: -0.0046, G_loss: 0.1874\n",
      "  Batch [820/1299] D_loss: -0.0782, G_loss: 0.2196\n",
      "  Batch [830/1299] D_loss: -0.0385, G_loss: 0.2844\n",
      "  Batch [840/1299] D_loss: -0.6248, G_loss: -1.2403\n",
      "  Batch [850/1299] D_loss: 0.0109, G_loss: 0.0741\n",
      "  Batch [860/1299] D_loss: -0.0390, G_loss: 0.2185\n",
      "  Batch [870/1299] D_loss: -0.0580, G_loss: 0.2662\n",
      "  Batch [880/1299] D_loss: -0.1065, G_loss: 0.4639\n",
      "  Batch [890/1299] D_loss: -0.0049, G_loss: 0.3455\n",
      "  Batch [900/1299] D_loss: -0.0055, G_loss: 0.3667\n",
      "  Batch [910/1299] D_loss: -0.4168, G_loss: 0.0807\n",
      "  Batch [920/1299] D_loss: -0.0186, G_loss: -0.0220\n",
      "  Batch [930/1299] D_loss: -0.0160, G_loss: 0.0656\n",
      "  Batch [940/1299] D_loss: -0.0017, G_loss: 0.1050\n",
      "  Batch [950/1299] D_loss: -0.0289, G_loss: 0.1748\n",
      "  Batch [960/1299] D_loss: -0.0721, G_loss: 0.2632\n",
      "  Batch [970/1299] D_loss: -0.0839, G_loss: 0.3971\n",
      "  Batch [980/1299] D_loss: -0.1215, G_loss: 0.4045\n",
      "  Batch [990/1299] D_loss: -0.0265, G_loss: 0.4039\n",
      "  Batch [1000/1299] D_loss: -0.4946, G_loss: 0.0798\n",
      "  Batch [1010/1299] D_loss: -0.0302, G_loss: 0.2318\n",
      "  Batch [1020/1299] D_loss: -0.0527, G_loss: 0.2811\n",
      "  Batch [1030/1299] D_loss: -0.0547, G_loss: 0.2771\n",
      "  Batch [1040/1299] D_loss: -0.9953, G_loss: -2.1531\n",
      "  Batch [1050/1299] D_loss: -0.0030, G_loss: 0.0581\n",
      "  Batch [1060/1299] D_loss: -0.0372, G_loss: 0.1012\n",
      "  Batch [1070/1299] D_loss: -0.0285, G_loss: 0.1762\n",
      "  Batch [1080/1299] D_loss: -0.0432, G_loss: 0.2938\n",
      "  Batch [1090/1299] D_loss: 0.0349, G_loss: 0.4796\n",
      "  Batch [1100/1299] D_loss: -0.0952, G_loss: 0.4620\n",
      "  Batch [1110/1299] D_loss: -0.1119, G_loss: 0.4101\n",
      "  Batch [1120/1299] D_loss: -1.6530, G_loss: -2.7553\n",
      "  Batch [1130/1299] D_loss: -0.0841, G_loss: 0.1159\n",
      "  Batch [1140/1299] D_loss: -0.0504, G_loss: 0.2093\n",
      "  Batch [1150/1299] D_loss: -0.0766, G_loss: 0.3163\n",
      "  Batch [1160/1299] D_loss: -0.0321, G_loss: 0.3452\n",
      "  Batch [1170/1299] D_loss: -0.0275, G_loss: 0.2992\n",
      "  Batch [1180/1299] D_loss: -0.7562, G_loss: -0.9016\n",
      "  Batch [1190/1299] D_loss: -0.2980, G_loss: -1.5395\n",
      "  Batch [1200/1299] D_loss: -0.0215, G_loss: 0.1436\n",
      "  Batch [1210/1299] D_loss: -0.0203, G_loss: 0.3524\n",
      "  Batch [1220/1299] D_loss: 0.0226, G_loss: 0.4751\n",
      "  Batch [1230/1299] D_loss: -0.1024, G_loss: 0.4630\n",
      "  Batch [1240/1299] D_loss: -0.0276, G_loss: 0.4879\n",
      "  Batch [1250/1299] D_loss: -0.0457, G_loss: 0.3951\n",
      "  Batch [1260/1299] D_loss: -0.0070, G_loss: 0.3764\n",
      "  Batch [1270/1299] D_loss: -2.0818, G_loss: -1.8035\n",
      "  Batch [1280/1299] D_loss: -0.2656, G_loss: -1.6481\n",
      "  Batch [1290/1299] D_loss: -1.0557, G_loss: -1.8544\n",
      "\n",
      "Epoch 99 Summary:\n",
      "  Average D_loss: -0.1475\n",
      "  Average G_loss: -0.0918\n",
      "\n",
      "Epoch [100/100]\n",
      "  Batch [0/1299] D_loss: -0.8102, G_loss: -0.6206\n",
      "  Batch [10/1299] D_loss: -0.0160, G_loss: 0.2413\n",
      "  Batch [20/1299] D_loss: -0.0346, G_loss: 0.4590\n",
      "  Batch [30/1299] D_loss: -0.1043, G_loss: 0.5953\n",
      "  Batch [40/1299] D_loss: -0.1502, G_loss: 0.6212\n",
      "  Batch [50/1299] D_loss: -0.0203, G_loss: 0.5075\n",
      "  Batch [60/1299] D_loss: -0.0471, G_loss: 0.4472\n",
      "  Batch [70/1299] D_loss: -0.0530, G_loss: 0.2427\n",
      "  Batch [80/1299] D_loss: -0.0911, G_loss: 0.0746\n",
      "  Batch [90/1299] D_loss: -0.0249, G_loss: 0.2053\n",
      "  Batch [100/1299] D_loss: -0.0420, G_loss: 0.3211\n",
      "  Batch [110/1299] D_loss: -0.0849, G_loss: 0.4676\n",
      "  Batch [120/1299] D_loss: 0.0229, G_loss: 0.2039\n",
      "  Batch [130/1299] D_loss: -0.0284, G_loss: 0.2533\n",
      "  Batch [140/1299] D_loss: -1.6934, G_loss: -1.7871\n",
      "  Batch [150/1299] D_loss: -0.0035, G_loss: 0.0695\n",
      "  Batch [160/1299] D_loss: -0.1805, G_loss: -0.2222\n",
      "  Batch [170/1299] D_loss: -0.6135, G_loss: -1.0473\n",
      "  Batch [180/1299] D_loss: -1.2880, G_loss: -0.0578\n",
      "  Batch [190/1299] D_loss: -0.0423, G_loss: 0.0965\n",
      "  Batch [200/1299] D_loss: -0.4494, G_loss: -0.0377\n",
      "  Batch [210/1299] D_loss: -0.0018, G_loss: 0.1748\n",
      "  Batch [220/1299] D_loss: -0.0640, G_loss: 0.3284\n",
      "  Batch [230/1299] D_loss: 0.0675, G_loss: 0.5494\n",
      "  Batch [240/1299] D_loss: 0.0176, G_loss: 0.4231\n",
      "  Batch [250/1299] D_loss: 0.0125, G_loss: 0.4595\n",
      "  Batch [260/1299] D_loss: 0.0164, G_loss: 0.2654\n",
      "  Batch [270/1299] D_loss: -1.6452, G_loss: -2.5633\n",
      "  Batch [280/1299] D_loss: -0.3290, G_loss: -0.0859\n",
      "  Batch [290/1299] D_loss: -0.0942, G_loss: 0.2079\n",
      "  Batch [300/1299] D_loss: -0.1174, G_loss: 0.3964\n",
      "  Batch [310/1299] D_loss: -0.0256, G_loss: 0.4285\n",
      "  Batch [320/1299] D_loss: -0.0227, G_loss: 0.4068\n",
      "  Batch [330/1299] D_loss: -0.1658, G_loss: 0.3505\n",
      "  Batch [340/1299] D_loss: -0.0639, G_loss: 0.3692\n",
      "  Batch [350/1299] D_loss: -0.0019, G_loss: 0.2399\n",
      "  Batch [360/1299] D_loss: -0.9706, G_loss: -1.7260\n",
      "  Batch [370/1299] D_loss: -0.6731, G_loss: -1.0274\n",
      "  Batch [380/1299] D_loss: -0.1581, G_loss: 0.1657\n",
      "  Batch [390/1299] D_loss: -0.3697, G_loss: 0.0460\n",
      "  Batch [400/1299] D_loss: -0.7717, G_loss: -0.1634\n",
      "  Batch [410/1299] D_loss: -0.4046, G_loss: 0.1427\n",
      "  Batch [420/1299] D_loss: -0.0025, G_loss: 0.0949\n",
      "  Batch [430/1299] D_loss: -0.0854, G_loss: 0.1848\n",
      "  Batch [440/1299] D_loss: -0.0622, G_loss: 0.2936\n",
      "  Batch [450/1299] D_loss: 0.0482, G_loss: 0.3682\n",
      "  Batch [460/1299] D_loss: -0.1328, G_loss: 0.6707\n",
      "  Batch [470/1299] D_loss: -0.0310, G_loss: 0.6100\n",
      "  Batch [480/1299] D_loss: -0.0232, G_loss: 0.4361\n",
      "  Batch [490/1299] D_loss: -0.1984, G_loss: 0.3823\n",
      "  Batch [500/1299] D_loss: -3.1201, G_loss: -6.1293\n",
      "  Batch [510/1299] D_loss: -0.0200, G_loss: 0.2158\n",
      "  Batch [520/1299] D_loss: 0.0350, G_loss: 0.2604\n",
      "  Batch [530/1299] D_loss: -0.0694, G_loss: 0.3278\n",
      "  Batch [540/1299] D_loss: -0.0611, G_loss: 0.3248\n",
      "  Batch [550/1299] D_loss: -0.0862, G_loss: 0.4617\n",
      "  Batch [560/1299] D_loss: -0.0543, G_loss: 0.2770\n",
      "  Batch [570/1299] D_loss: -0.8917, G_loss: -2.1705\n",
      "  Batch [580/1299] D_loss: -0.5767, G_loss: -1.0258\n",
      "  Batch [590/1299] D_loss: -0.0616, G_loss: 0.0898\n",
      "  Batch [600/1299] D_loss: -1.0455, G_loss: -0.8500\n",
      "  Batch [610/1299] D_loss: -0.0267, G_loss: 0.1871\n",
      "  Batch [620/1299] D_loss: -0.1032, G_loss: 0.3402\n",
      "  Batch [630/1299] D_loss: -0.0551, G_loss: 0.3653\n",
      "  Batch [640/1299] D_loss: 0.1129, G_loss: 0.4002\n",
      "  Batch [650/1299] D_loss: -0.0049, G_loss: 0.4577\n",
      "  Batch [660/1299] D_loss: -0.0325, G_loss: 0.3242\n",
      "  Batch [670/1299] D_loss: -0.0867, G_loss: 0.3691\n",
      "  Batch [680/1299] D_loss: -0.1332, G_loss: -0.3039\n",
      "  Batch [690/1299] D_loss: -0.4904, G_loss: -0.5052\n",
      "  Batch [700/1299] D_loss: 0.0956, G_loss: -0.0298\n",
      "  Batch [710/1299] D_loss: -0.0437, G_loss: 0.1348\n",
      "  Batch [720/1299] D_loss: -0.0796, G_loss: 0.3203\n",
      "  Batch [730/1299] D_loss: -0.1805, G_loss: 0.5819\n",
      "  Batch [740/1299] D_loss: -0.2833, G_loss: 0.8340\n",
      "  Batch [750/1299] D_loss: -0.1105, G_loss: 0.6884\n",
      "  Batch [760/1299] D_loss: 0.0003, G_loss: 0.5317\n",
      "  Batch [770/1299] D_loss: 0.0522, G_loss: 0.4924\n",
      "  Batch [780/1299] D_loss: -0.1338, G_loss: 0.2422\n",
      "  Batch [790/1299] D_loss: -0.6567, G_loss: -3.0038\n",
      "  Batch [800/1299] D_loss: -0.6082, G_loss: -0.6328\n",
      "  Batch [810/1299] D_loss: -0.0459, G_loss: 0.1850\n",
      "  Batch [820/1299] D_loss: -0.0516, G_loss: 0.3975\n",
      "  Batch [830/1299] D_loss: -0.0811, G_loss: 0.4265\n",
      "  Batch [840/1299] D_loss: -0.0295, G_loss: 0.4481\n",
      "  Batch [850/1299] D_loss: -0.0057, G_loss: 0.5489\n",
      "  Batch [860/1299] D_loss: -0.1085, G_loss: 0.4812\n",
      "  Batch [870/1299] D_loss: -0.1995, G_loss: 0.6769\n",
      "  Batch [880/1299] D_loss: -0.0031, G_loss: 0.4396\n",
      "  Batch [890/1299] D_loss: 0.0250, G_loss: 0.2344\n",
      "  Batch [900/1299] D_loss: -0.1011, G_loss: 0.2802\n",
      "  Batch [910/1299] D_loss: -0.0036, G_loss: 0.1323\n",
      "  Batch [920/1299] D_loss: -0.0369, G_loss: 0.1612\n",
      "  Batch [930/1299] D_loss: -0.0507, G_loss: 0.1067\n",
      "  Batch [940/1299] D_loss: -0.0286, G_loss: 0.1705\n",
      "  Batch [950/1299] D_loss: -0.0037, G_loss: 0.2419\n",
      "  Batch [960/1299] D_loss: -0.0057, G_loss: 0.2835\n",
      "  Batch [970/1299] D_loss: -0.0469, G_loss: 0.3147\n",
      "  Batch [980/1299] D_loss: -2.3429, G_loss: -7.2578\n",
      "  Batch [990/1299] D_loss: -1.1498, G_loss: -1.2436\n",
      "  Batch [1000/1299] D_loss: -0.0691, G_loss: 0.2906\n",
      "  Batch [1010/1299] D_loss: -0.0944, G_loss: 0.5421\n",
      "  Batch [1020/1299] D_loss: -0.0223, G_loss: 0.5280\n",
      "  Batch [1030/1299] D_loss: -0.0060, G_loss: 0.4308\n",
      "  Batch [1040/1299] D_loss: -0.1452, G_loss: 0.4202\n",
      "  Batch [1050/1299] D_loss: -0.0840, G_loss: 0.2826\n",
      "  Batch [1060/1299] D_loss: -0.1587, G_loss: 0.1958\n",
      "  Batch [1070/1299] D_loss: 0.0326, G_loss: 0.1151\n",
      "  Batch [1080/1299] D_loss: -0.9935, G_loss: -1.6482\n",
      "  Batch [1090/1299] D_loss: -0.0121, G_loss: 0.1374\n",
      "  Batch [1100/1299] D_loss: -0.0152, G_loss: 0.2003\n",
      "  Batch [1110/1299] D_loss: -0.0821, G_loss: 0.3041\n",
      "  Batch [1120/1299] D_loss: -0.1260, G_loss: 0.4165\n",
      "  Batch [1130/1299] D_loss: -0.0867, G_loss: 0.3661\n",
      "  Batch [1140/1299] D_loss: -0.0656, G_loss: 0.2475\n",
      "  Batch [1150/1299] D_loss: -3.1099, G_loss: -6.8963\n",
      "  Batch [1160/1299] D_loss: -0.0285, G_loss: 0.0719\n",
      "  Batch [1170/1299] D_loss: -0.7808, G_loss: -1.3861\n",
      "  Batch [1180/1299] D_loss: -1.5133, G_loss: -3.4094\n",
      "  Batch [1190/1299] D_loss: -0.0229, G_loss: 0.0740\n",
      "  Batch [1200/1299] D_loss: 0.0321, G_loss: 0.1858\n",
      "  Batch [1210/1299] D_loss: -0.1050, G_loss: 0.4086\n",
      "  Batch [1220/1299] D_loss: -0.1037, G_loss: 0.4243\n",
      "  Batch [1230/1299] D_loss: -0.0364, G_loss: 0.3471\n",
      "  Batch [1240/1299] D_loss: -0.0607, G_loss: 0.4614\n",
      "  Batch [1250/1299] D_loss: -2.4674, G_loss: -1.5282\n",
      "  Batch [1260/1299] D_loss: 0.0198, G_loss: 0.1184\n",
      "  Batch [1270/1299] D_loss: -0.0797, G_loss: 0.0014\n",
      "  Batch [1280/1299] D_loss: -0.0260, G_loss: 0.1338\n",
      "  Batch [1290/1299] D_loss: -0.0609, G_loss: 0.2868\n",
      "\n",
      "Epoch 100 Summary:\n",
      "  Average D_loss: -0.1381\n",
      "  Average G_loss: -0.0553\n"
     ]
    }
   ],
   "source": [
    "def main(selected_categories=None):\n",
    "    \"\"\"\n",
    "    Train the GAN with selected categorical variables\n",
    "    Args:\n",
    "        selected_categories: List of column names to use as categorical variables.\n",
    "                           If None, uses all columns except 'cell_id'\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'epochs': 100,\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 32,\n",
    "        'nb_layers': 3,\n",
    "        'hdim': 256,\n",
    "        'lr': 1e-4,\n",
    "        'nb_critic': 5,\n",
    "        'lambda_gp': 10  # Gradient penalty coefficient\n",
    "    }\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/\"\n",
    "    \n",
    "    # Load expression matrix\n",
    "    # matrix with cells as columns and genes as rows\n",
    "    with h5py.File(data_path+'combined_normalized_data.h5', 'r') as f:\n",
    "        x_train = f['matrix'][:]\n",
    "    \n",
    "    # Load all categorical variables from single file\n",
    "    cat_data = pd.read_csv(data_path+'combined_metadata.csv', sep=';')\n",
    "    print(\"Categorical data shape:\", cat_data.shape)\n",
    "    print(\"Available categorical variables:\", [col for col in cat_data.columns if col != 'cell_id'])\n",
    "    \n",
    "    # Determine which categories to use\n",
    "    if selected_categories is None:\n",
    "        # Use all columns except cell_id\n",
    "        categories_to_use = [col for col in cat_data.columns if col != 'cell_id']\n",
    "    else:\n",
    "        # Validate selected categories\n",
    "        invalid_categories = [cat for cat in selected_categories if cat not in cat_data.columns]\n",
    "        if invalid_categories:\n",
    "            raise ValueError(f\"Invalid categories: {invalid_categories}\")\n",
    "        categories_to_use = selected_categories\n",
    "    \n",
    "    print(f\"\\nUsing categorical variables: {categories_to_use}\")\n",
    "    \n",
    "    # Create dictionaries and inverse mappings for categorical variables\n",
    "    cat_dicts = []\n",
    "    encoded_covs = []\n",
    "    \n",
    "    # Process each selected column as a categorical variable\n",
    "    for column in categories_to_use:\n",
    "        # Get the column data\n",
    "        cat_vec = cat_data[column]\n",
    "        print(f\"\\nProcessing categorical variable: {column}\")\n",
    "        \n",
    "        # Create list of unique category names, sorted\n",
    "        dict_inv = np.array(list(sorted(set(cat_vec.values))))\n",
    "        dict_map = {t: i for i, t in enumerate(dict_inv)}\n",
    "        cat_dicts.append(dict_inv)\n",
    "        \n",
    "        # Convert categorical variables to integers\n",
    "        encoded = np.vectorize(lambda t: dict_map[t])(cat_vec)\n",
    "        encoded = encoded.reshape(-1, 1)  # Reshape to column vector\n",
    "        encoded_covs.append(encoded)\n",
    "        \n",
    "        print(f\"Categories in {column}:\", dict_inv)\n",
    "        print(f\"Number of categories:\", len(dict_inv))\n",
    "    \n",
    "    # Combine all categorical covariates\n",
    "    cat_covs = np.hstack(encoded_covs)\n",
    "    print(\"\\nCombined categorical covariates shape:\", cat_covs.shape)\n",
    "    \n",
    "    # Load numerical covariates (currently empty)\n",
    "    num_covs = np.zeros((x_train.shape[0], 0))\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to device\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)  # Keep on CPU for DataLoader\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(x_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    vocab_sizes = [len(c) for c in cat_dicts]\n",
    "    print(\"\\nVocabulary sizes for categorical variables:\", vocab_sizes)\n",
    "    nb_numeric = num_covs.shape[-1]\n",
    "    x_dim = x_train.shape[-1]\n",
    "    \n",
    "    generator = Generator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "        z_dim=CONFIG['latent_dim']).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers']).to(device)\n",
    "    \n",
    "    # Define save function\n",
    "    def save_models(generator, discriminator, epoch):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # create save directory\n",
    "        categories_str = \"+\".join(categories_to_use)\n",
    "        save_dir = os.path.join(data_path, \"saved_models\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Create run folder\n",
    "        run_dir = os.path.join(save_dir, f\"run_{timestamp}_{categories_str}\")\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "        # Save model initialization parameters\n",
    "        model_config = {\n",
    "            'x_dim': x_dim,\n",
    "            'vocab_sizes': vocab_sizes,\n",
    "            'nb_numeric': nb_numeric,\n",
    "            'h_dims': [CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "            'z_dim': CONFIG['latent_dim'],\n",
    "            'categories': categories_to_use,\n",
    "            'training_config': CONFIG}\n",
    "        config_path = os.path.join(run_dir, 'model_config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(model_config, f, indent=4)\n",
    "        \n",
    "        # Save generator\n",
    "        generator_path = os.path.join(run_dir, f\"generator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(generator.state_dict(), generator_path)\n",
    "        \n",
    "        \n",
    "        # Save discriminator\n",
    "        discriminator_path = os.path.join(run_dir, f\"discriminator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(discriminator.state_dict(), discriminator_path)\n",
    "        \n",
    "        print(f\"\\nModels saved at epoch {epoch + 1}:\")\n",
    "        print(f\"Generator: {generator_path}\")\n",
    "        print(f\"Discriminator: {discriminator_path}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        if wandb.run is not None:\n",
    "            wandb.save(generator_path)\n",
    "            wandb.save(discriminator_path)\n",
    "\n",
    "    # Initialize wandb with unique run name\n",
    "    run_name = f\"run_{int(time.time())}\"  # Uses timestamp for unique name\n",
    "    wandb.init(\n",
    "        project='adversarial_gene_expr',\n",
    "        config=CONFIG,\n",
    "        name=run_name,\n",
    "        reinit=True  # Ensures new run each time\n",
    "    )\n",
    "    \n",
    "    # Add selected categories to wandb config\n",
    "    wandb.config.update({'selected_categories': categories_to_use})\n",
    "    \n",
    "    # Train model\n",
    "    train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=train_loader,\n",
    "        cat_covs=cat_covs,\n",
    "        num_covs=num_covs,\n",
    "        config=CONFIG,\n",
    "        device=device,\n",
    "        save_fn=save_models\n",
    "        #save_fn=None\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    # Use specific categories:\n",
    "    main(selected_categories=['dataset','cell_type'])\n",
    "    \n",
    "    # Or use all available categories:\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for data gneration\n",
    "def inspect_generator_dims(generator):\n",
    "    \"\"\"\n",
    "    Inspect the generator's dimensions and architecture\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Generator model\n",
    "    \n",
    "    Returns:\n",
    "        dict containing dimension information\n",
    "    \"\"\"\n",
    "    # Get embedding dimensions\n",
    "    embedding_dims = [emb.embedding_dim for emb in generator.embeddings]\n",
    "    total_embedding_dim = sum(embedding_dims)\n",
    "    \n",
    "    # Get first layer dimension\n",
    "    first_layer_in_dim = generator.network[0].in_features\n",
    "    \n",
    "    return {\n",
    "        'embedding_dims': embedding_dims,\n",
    "        'total_embedding_dim': total_embedding_dim,\n",
    "        'first_layer_in_dim': first_layer_in_dim,\n",
    "        'recommended_latent_dim': first_layer_in_dim - total_embedding_dim\n",
    "    }\n",
    "\n",
    "def generate_expression_profiles(generator, n_samples, dataset_category, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate gene expression profiles using the trained cWGAN generator\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained Generator model\n",
    "        n_samples: Number of profiles to generate\n",
    "        dataset_category: Integer indicating which dataset category to generate (0-6 for dataset1-dataset7)\n",
    "        device: Device to run generation on ('cuda', 'mps', or 'cpu')\n",
    "        debug: If True, print debugging information\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of generated expression profiles with shape (n_samples, n_genes)\n",
    "    \"\"\"\n",
    "    # Set generator to eval mode\n",
    "    generator.eval()\n",
    "    \n",
    "    # Inspect dimensions\n",
    "    dims = inspect_generator_dims(generator)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Generator dimensions:\")\n",
    "        for k, v in dims.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    \n",
    "    # Create latent vectors\n",
    "    latent_dim = dims['recommended_latent_dim']\n",
    "    z = torch.randn(n_samples, latent_dim, device=device)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nLatent vector shape: {z.shape}\")\n",
    "    \n",
    "    # Create categorical condition tensor\n",
    "    cat_covs = torch.full((n_samples, 1), dataset_category, dtype=torch.long, device=device)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Categorical covariates shape: {cat_covs.shape}\")\n",
    "    \n",
    "    # Create empty numeric covariates tensor\n",
    "    num_covs = torch.zeros((n_samples, 0), device=device)\n",
    "    \n",
    "    # Generate samples\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings\n",
    "            embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(generator.embeddings)]\n",
    "            embedded = torch.cat(embeddings, dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Embedded shape: {embedded.shape}\")\n",
    "            \n",
    "            # Concatenate inputs\n",
    "            gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Generator input shape: {gen_input.shape}\")\n",
    "                print(f\"First layer input dim: {generator.network[0].in_features}\")\n",
    "                print(f\"First layer weight shape: {generator.network[0].weight.shape}\")\n",
    "            \n",
    "            # Generate samples\n",
    "            fake_samples = generator.network(gen_input)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(\"\\nError during generation:\")\n",
    "        print(e)\n",
    "        print(\"\\nGenerator architecture:\")\n",
    "        print(generator)\n",
    "        raise\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    return fake_samples.cpu().numpy()\n",
    "\n",
    "def generate_and_save_profiles(generator, n_samples_per_category, save_path, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate expression profiles for all dataset categories and save to file\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained Generator model\n",
    "        n_samples_per_category: Number of samples to generate per dataset category\n",
    "        save_path: Path to save the generated profiles\n",
    "        device: Device to run generation on ('cuda', 'mps', or 'cpu')\n",
    "        debug: If True, print debugging information\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "    all_categories = []\n",
    "    \n",
    "    # Generate samples for each dataset category\n",
    "    for category in range(7):  # 7 datasets (dataset1-dataset7)\n",
    "        if debug:\n",
    "            print(f\"\\nGenerating samples for dataset{category+1}\")\n",
    "        \n",
    "        samples = generate_expression_profiles(\n",
    "            generator, \n",
    "            n_samples_per_category, \n",
    "            category, \n",
    "            device,\n",
    "            debug=debug\n",
    "        )\n",
    "        all_samples.append(samples)\n",
    "        all_categories.extend([f'dataset{category+1}'] * n_samples_per_category)\n",
    "    # Print saved data path\n",
    "    print(\"Save location: \"+str(save_path))\n",
    "\n",
    "    # Combine all samples\n",
    "    all_samples = np.vstack(all_samples)\n",
    "    \n",
    "    # Save generated profiles\n",
    "    np.save(f'{save_path}_profiles.npy', all_samples)\n",
    "    \n",
    "    # Save category labels\n",
    "    with open(f'{save_path}_categories.txt', 'w') as f:\n",
    "        for category in all_categories:\n",
    "            f.write(f'{category}\\n')\n",
    "            \n",
    "    return all_samples, all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Save location: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/saved_models/run_20250113_135205_dataset/generated_data\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "\n",
    "# Set directories\n",
    "# 2 hidden layers\n",
    "#run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/saved_models/run_20250113_114232_dataset/\"\n",
    "#generator_model = \"generator_20250113_114232_dataset.pt\"\n",
    "# 3 hidden layers\n",
    "run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/saved_models/run_20250113_135205_dataset/\"\n",
    "generator_model = \"generator_20250113_135205_dataset_epoch_51.pt\"\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(run_dir, 'model_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "    \n",
    "# Initialize models with saved configuration\n",
    "generator = Generator(\n",
    "    x_dim=model_config['x_dim'],\n",
    "    vocab_sizes=model_config['vocab_sizes'],\n",
    "    nb_numeric=model_config['nb_numeric'],\n",
    "    h_dims=model_config['h_dims'],\n",
    "    z_dim=model_config['z_dim']).to(device)\n",
    "    \n",
    "discriminator = Discriminator(\n",
    "    x_dim=model_config['x_dim'],\n",
    "    vocab_sizes=model_config['vocab_sizes'],\n",
    "    nb_numeric=model_config['nb_numeric'],\n",
    "    h_dims=model_config['h_dims']).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#discriminator_path = os.path.join(run_dir, \"discriminator.pt\")\n",
    "#discriminator.load_state_dict(torch.load(discriminator_path, map_location=device, weights_only=True))\n",
    "generator_path = os.path.join(run_dir, generator_model)\n",
    "generator.load_state_dict(torch.load(generator_path, map_location=device, weights_only=True))\n",
    "\n",
    "\n",
    "all_samples, categories = generate_and_save_profiles(\n",
    "    generator,\n",
    "    n_samples_per_category=1000,\n",
    "    save_path=run_dir+'generated_data',\n",
    "    debug=False  # Enable debug output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.320732</td>\n",
       "      <td>0.105634</td>\n",
       "      <td>0.059878</td>\n",
       "      <td>-0.033626</td>\n",
       "      <td>0.315196</td>\n",
       "      <td>0.355873</td>\n",
       "      <td>0.098052</td>\n",
       "      <td>1.415885</td>\n",
       "      <td>0.138555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038606</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>0.049724</td>\n",
       "      <td>0.033215</td>\n",
       "      <td>0.174404</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.013922</td>\n",
       "      <td>0.012524</td>\n",
       "      <td>-0.024358</td>\n",
       "      <td>dataset1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.795014</td>\n",
       "      <td>11.064900</td>\n",
       "      <td>7.698052</td>\n",
       "      <td>6.852327</td>\n",
       "      <td>0.702508</td>\n",
       "      <td>1.186466</td>\n",
       "      <td>0.304192</td>\n",
       "      <td>6.257954</td>\n",
       "      <td>3.651256</td>\n",
       "      <td>5.293654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029625</td>\n",
       "      <td>-0.164511</td>\n",
       "      <td>-0.066902</td>\n",
       "      <td>-0.005725</td>\n",
       "      <td>-0.053271</td>\n",
       "      <td>0.016862</td>\n",
       "      <td>0.056550</td>\n",
       "      <td>-0.123670</td>\n",
       "      <td>-0.149712</td>\n",
       "      <td>dataset1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100273</td>\n",
       "      <td>2.477551</td>\n",
       "      <td>1.751069</td>\n",
       "      <td>1.583150</td>\n",
       "      <td>-0.010378</td>\n",
       "      <td>0.883565</td>\n",
       "      <td>0.253218</td>\n",
       "      <td>1.477944</td>\n",
       "      <td>1.379792</td>\n",
       "      <td>1.206260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014636</td>\n",
       "      <td>-0.010550</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>-0.004031</td>\n",
       "      <td>-0.040558</td>\n",
       "      <td>0.042852</td>\n",
       "      <td>0.056843</td>\n",
       "      <td>-0.005354</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>dataset1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.034639</td>\n",
       "      <td>1.667600</td>\n",
       "      <td>1.251640</td>\n",
       "      <td>1.145788</td>\n",
       "      <td>-0.147350</td>\n",
       "      <td>-0.009043</td>\n",
       "      <td>0.021922</td>\n",
       "      <td>1.158313</td>\n",
       "      <td>0.585592</td>\n",
       "      <td>0.775481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008856</td>\n",
       "      <td>-0.005037</td>\n",
       "      <td>-0.006993</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.022809</td>\n",
       "      <td>0.020079</td>\n",
       "      <td>0.043271</td>\n",
       "      <td>-0.002714</td>\n",
       "      <td>0.044753</td>\n",
       "      <td>dataset1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.099730</td>\n",
       "      <td>2.358865</td>\n",
       "      <td>1.671180</td>\n",
       "      <td>1.510806</td>\n",
       "      <td>-0.010824</td>\n",
       "      <td>0.802133</td>\n",
       "      <td>0.232048</td>\n",
       "      <td>1.419516</td>\n",
       "      <td>1.297776</td>\n",
       "      <td>1.144986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013183</td>\n",
       "      <td>-0.012915</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>-0.003016</td>\n",
       "      <td>-0.038905</td>\n",
       "      <td>0.038836</td>\n",
       "      <td>0.054853</td>\n",
       "      <td>-0.007304</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>dataset1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>0.168555</td>\n",
       "      <td>3.913768</td>\n",
       "      <td>2.765483</td>\n",
       "      <td>2.494067</td>\n",
       "      <td>-0.015362</td>\n",
       "      <td>0.981787</td>\n",
       "      <td>0.322876</td>\n",
       "      <td>2.348282</td>\n",
       "      <td>1.871573</td>\n",
       "      <td>1.913221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>-0.019285</td>\n",
       "      <td>-0.025493</td>\n",
       "      <td>-0.022838</td>\n",
       "      <td>-0.026965</td>\n",
       "      <td>0.032246</td>\n",
       "      <td>0.085965</td>\n",
       "      <td>0.015422</td>\n",
       "      <td>0.032537</td>\n",
       "      <td>dataset7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>2.949266</td>\n",
       "      <td>6.560752</td>\n",
       "      <td>4.519499</td>\n",
       "      <td>4.007252</td>\n",
       "      <td>0.760103</td>\n",
       "      <td>-5.030645</td>\n",
       "      <td>0.517229</td>\n",
       "      <td>4.105344</td>\n",
       "      <td>0.180469</td>\n",
       "      <td>3.276859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082215</td>\n",
       "      <td>-0.041214</td>\n",
       "      <td>0.152816</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>0.466038</td>\n",
       "      <td>-0.133744</td>\n",
       "      <td>-0.015720</td>\n",
       "      <td>-0.038499</td>\n",
       "      <td>0.360135</td>\n",
       "      <td>dataset7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>0.381713</td>\n",
       "      <td>5.291831</td>\n",
       "      <td>3.660735</td>\n",
       "      <td>3.238459</td>\n",
       "      <td>0.432806</td>\n",
       "      <td>1.683521</td>\n",
       "      <td>0.287335</td>\n",
       "      <td>2.875701</td>\n",
       "      <td>2.568585</td>\n",
       "      <td>2.648107</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003065</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>-0.005536</td>\n",
       "      <td>-0.044282</td>\n",
       "      <td>0.051976</td>\n",
       "      <td>0.068840</td>\n",
       "      <td>0.014409</td>\n",
       "      <td>-0.000424</td>\n",
       "      <td>-0.000549</td>\n",
       "      <td>dataset7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>-1.652966</td>\n",
       "      <td>-0.480020</td>\n",
       "      <td>-0.284708</td>\n",
       "      <td>-0.158320</td>\n",
       "      <td>-0.800277</td>\n",
       "      <td>0.109397</td>\n",
       "      <td>-0.567243</td>\n",
       "      <td>-0.265171</td>\n",
       "      <td>-0.451420</td>\n",
       "      <td>0.389981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055769</td>\n",
       "      <td>0.094848</td>\n",
       "      <td>0.044443</td>\n",
       "      <td>-0.125318</td>\n",
       "      <td>-0.165968</td>\n",
       "      <td>0.050426</td>\n",
       "      <td>0.132208</td>\n",
       "      <td>0.117812</td>\n",
       "      <td>-0.112642</td>\n",
       "      <td>dataset7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>-0.334864</td>\n",
       "      <td>0.917253</td>\n",
       "      <td>0.526947</td>\n",
       "      <td>0.508436</td>\n",
       "      <td>-0.073659</td>\n",
       "      <td>0.692343</td>\n",
       "      <td>-0.012822</td>\n",
       "      <td>0.410956</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>0.455444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>-0.006103</td>\n",
       "      <td>-0.023581</td>\n",
       "      <td>-0.063090</td>\n",
       "      <td>0.033996</td>\n",
       "      <td>0.107796</td>\n",
       "      <td>-0.004826</td>\n",
       "      <td>-0.040344</td>\n",
       "      <td>dataset7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows  1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2         3         4         5         6  \\\n",
       "0     0.029197   0.320732  0.105634  0.059878 -0.033626  0.315196  0.355873   \n",
       "1     0.795014  11.064900  7.698052  6.852327  0.702508  1.186466  0.304192   \n",
       "2     0.100273   2.477551  1.751069  1.583150 -0.010378  0.883565  0.253218   \n",
       "3    -0.034639   1.667600  1.251640  1.145788 -0.147350 -0.009043  0.021922   \n",
       "4     0.099730   2.358865  1.671180  1.510806 -0.010824  0.802133  0.232048   \n",
       "...        ...        ...       ...       ...       ...       ...       ...   \n",
       "6995  0.168555   3.913768  2.765483  2.494067 -0.015362  0.981787  0.322876   \n",
       "6996  2.949266   6.560752  4.519499  4.007252  0.760103 -5.030645  0.517229   \n",
       "6997  0.381713   5.291831  3.660735  3.238459  0.432806  1.683521  0.287335   \n",
       "6998 -1.652966  -0.480020 -0.284708 -0.158320 -0.800277  0.109397 -0.567243   \n",
       "6999 -0.334864   0.917253  0.526947  0.508436 -0.073659  0.692343 -0.012822   \n",
       "\n",
       "             7         8         9  ...       991       992       993  \\\n",
       "0     0.098052  1.415885  0.138555  ...  0.038606  0.017874  0.049724   \n",
       "1     6.257954  3.651256  5.293654  ... -0.029625 -0.164511 -0.066902   \n",
       "2     1.477944  1.379792  1.206260  ...  0.014636 -0.010550  0.019563   \n",
       "3     1.158313  0.585592  0.775481  ... -0.008856 -0.005037 -0.006993   \n",
       "4     1.419516  1.297776  1.144986  ...  0.013183 -0.012915  0.016879   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6995  2.348282  1.871573  1.913221  ...  0.005220 -0.019285 -0.025493   \n",
       "6996  4.105344  0.180469  3.276859  ... -0.082215 -0.041214  0.152816   \n",
       "6997  2.875701  2.568585  2.648107  ... -0.003065  0.001313 -0.005536   \n",
       "6998 -0.265171 -0.451420  0.389981  ... -0.055769  0.094848  0.044443   \n",
       "6999  0.410956  0.590551  0.455444  ...  0.000125  0.006495 -0.006103   \n",
       "\n",
       "           994       995       996       997       998       999   dataset  \n",
       "0     0.033215  0.174404  0.011404  0.013922  0.012524 -0.024358  dataset1  \n",
       "1    -0.005725 -0.053271  0.016862  0.056550 -0.123670 -0.149712  dataset1  \n",
       "2    -0.004031 -0.040558  0.042852  0.056843 -0.005354  0.013900  dataset1  \n",
       "3     0.000234  0.022809  0.020079  0.043271 -0.002714  0.044753  dataset1  \n",
       "4    -0.003016 -0.038905  0.038836  0.054853 -0.007304  0.010891  dataset1  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6995 -0.022838 -0.026965  0.032246  0.085965  0.015422  0.032537  dataset7  \n",
       "6996  0.310174  0.466038 -0.133744 -0.015720 -0.038499  0.360135  dataset7  \n",
       "6997 -0.044282  0.051976  0.068840  0.014409 -0.000424 -0.000549  dataset7  \n",
       "6998 -0.125318 -0.165968  0.050426  0.132208  0.117812 -0.112642  dataset7  \n",
       "6999 -0.023581 -0.063090  0.033996  0.107796 -0.004826 -0.040344  dataset7  \n",
       "\n",
       "[7000 rows x 1001 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load generated data\n",
    "# Load the generated profiles\n",
    "profiles = np.load(run_dir + 'generated_data_profiles.npy')\n",
    "\n",
    "# Load categories\n",
    "with open(run_dir + 'generated_data_categories.txt', 'r') as f:\n",
    "    categories = [line.strip() for line in f]\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(profiles)\n",
    "\n",
    "# Add categories as a column\n",
    "df['dataset'] = categories\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
