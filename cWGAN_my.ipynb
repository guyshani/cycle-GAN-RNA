{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "# Load expression matrix (csv file with rows as cells and columns as genes)\n",
    "data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/\"\n",
    "# Load expression matrix\n",
    "with h5py.File(data_path+'train_data_1dataset.h5', 'r') as f:\n",
    "    matrix = f['matrix'][:]\n",
    "\n",
    "# each row is a cell\n",
    "# matrix[:,0]\n",
    "\n",
    "x_train = matrix\n",
    "\n",
    "\n",
    "# Load cluster info\n",
    "cluster_vec = pd.read_csv(data_path+'train_data_1dataset_cluster.csv').T\n",
    "cluster_vec\n",
    "# Load numerical covariates\n",
    "num_covs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(cluster_vec.values.flatten())\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example raw data\n",
    "# tissues = np.array(['liver', 'brain', 'liver', 'kidney', 'brain'])\n",
    "\n",
    "# Create dictionaries and inverse mappings\n",
    "cat_dicts = []\n",
    "\n",
    "## Cluster covariate\n",
    "# Create list of unique cluster names, sorted\n",
    "cluster_dict_inv = np.array(list(sorted(set(cluster_vec.values.flatten()))))  # ['brain', 'kidney', 'liver']\n",
    "cluster_dict = {t: i for i, t in enumerate(cluster_dict_inv)}  # {'brain': 0, 'kidney': 1, 'liver': 2}\n",
    "cat_dicts.append(cluster_dict_inv)\n",
    "\n",
    "# Convert categorical variables to integers\n",
    "clusters_encoded = np.vectorize(lambda t: cluster_dict[t])(cluster_vec)\n",
    "\n",
    "print(\"Original clusters:\", cluster_vec)\n",
    "print(\"Encoded clusters:\", clusters_encoded)\n",
    "print(\"Cluster mapping:\", cluster_dict)\n",
    "\n",
    "print(\"\\ncat_dicts:\", cat_dicts)\n",
    "\n",
    "# This gives us vocab_sizes for model initialization\n",
    "vocab_sizes = [len(c) for c in cat_dicts]  # [3, 2] (3 tissue types, 2 dataset types)\n",
    "print(\"\\nvocab_sizes:\", vocab_sizes)\n",
    "\n",
    "# assign categorical covariates\n",
    "cat_covs = clusters_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims, z_dim):\n",
    "        \"\"\"\n",
    "        Generator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of output data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "            z_dim: Dimension of latent noise vector\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size)) \n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is latent dim + embedding dim + numeric covariates\n",
    "        input_dim = z_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build generator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, x_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims):\n",
    "        \"\"\"\n",
    "        Discriminator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of input data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size))\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is data dim + embedding dim + numeric covariates\n",
    "        input_dim = x_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build discriminator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        disc_input = torch.cat([x, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(disc_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, dataloader, cat_covs, num_covs, \n",
    "              config, device, score_fn=None, save_fn=None):\n",
    "    \"\"\"\n",
    "    Train the conditional GAN with progress tracking and proper device handling\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    g_optimizer = optim.RMSprop(generator.parameters(), lr=config['lr'])\n",
    "    d_optimizer = optim.RMSprop(discriminator.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Convert covariates to tensors and move to device\n",
    "    cat_covs = torch.tensor(cat_covs, dtype=torch.long).to(device)\n",
    "    num_covs = torch.tensor(num_covs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "    print(f\"Total batches per epoch: {total_batches}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        print(f\"\\nEpoch [{epoch+1}/{config['epochs']}]\")\n",
    "        \n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Move real data to device\n",
    "            real_data = real_data.to(device)\n",
    "            \n",
    "            # Get random batch of categorical and numerical covariates\n",
    "            batch_indices = torch.randint(0, cat_covs.size(0), (batch_size,))\n",
    "            batch_cat_covs = cat_covs[batch_indices]\n",
    "            batch_num_covs = num_covs[batch_indices]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(config['nb_critic']):\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "                fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate discriminator loss\n",
    "                real_validity = discriminator(real_data, batch_cat_covs, batch_num_covs)\n",
    "                fake_validity = discriminator(fake_data.detach(), batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                d_loss = -(torch.mean(real_validity) - torch.mean(fake_validity))\n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                # Clip discriminator weights (Wasserstein GAN)\n",
    "                for p in discriminator.parameters():\n",
    "                    p.data.clamp_(-0.01, 0.01)\n",
    "                    \n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "            fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "            \n",
    "            # Calculate generator loss\n",
    "            fake_validity = discriminator(fake_data, batch_cat_covs, batch_num_covs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            g_losses.append(g_loss.item())\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Batch [{batch_idx}/{total_batches}] \" \\\n",
    "                      f\"D_loss: {d_loss.item():.4f}, \" \\\n",
    "                      f\"G_loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average D_loss: {avg_d_loss:.4f}\")\n",
    "        print(f\"  Average G_loss: {avg_g_loss:.4f}\")\n",
    "        \n",
    "        # Log metrics\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'd_loss': np.mean(d_losses),\n",
    "                'g_loss': np.mean(g_losses)\n",
    "            })\n",
    "        \n",
    "        # Evaluate and save model if needed\n",
    "        if score_fn is not None and epoch % 10 == 0:\n",
    "            score = score_fn(generator)\n",
    "            print(f'Epoch {epoch}: Score = {score:.4f}')\n",
    "            \n",
    "        if save_fn is not None and epoch % 100 == 0:\n",
    "            save_fn(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Starting training for 500 epochs...\n",
      "Total batches per epoch: 31\n",
      "Using device: mps\n",
      "\n",
      "Epoch [1/500]\n",
      "  Batch [0/31] D_loss: -12.9184, G_loss: 6.4762\n",
      "  Batch [10/31] D_loss: -1.8696, G_loss: -0.0597\n",
      "  Batch [20/31] D_loss: -2.3705, G_loss: 0.5551\n",
      "  Batch [30/31] D_loss: -0.9042, G_loss: -0.8179\n",
      "\n",
      "Epoch 1 Summary:\n",
      "  Average D_loss: -1.5992\n",
      "  Average G_loss: 1.6461\n",
      "\n",
      "Epoch [2/500]\n",
      "  Batch [0/31] D_loss: -5.1307, G_loss: 3.7898\n",
      "  Batch [10/31] D_loss: -6.8352, G_loss: 4.9523\n",
      "  Batch [20/31] D_loss: -5.4994, G_loss: 1.7115\n",
      "  Batch [30/31] D_loss: -4.7128, G_loss: 0.1949\n",
      "\n",
      "Epoch 2 Summary:\n",
      "  Average D_loss: -2.9820\n",
      "  Average G_loss: 0.8077\n",
      "\n",
      "Epoch [3/500]\n",
      "  Batch [0/31] D_loss: -4.5063, G_loss: 0.3721\n",
      "  Batch [10/31] D_loss: -3.5614, G_loss: 1.9659\n",
      "  Batch [20/31] D_loss: -4.4366, G_loss: 0.8575\n",
      "  Batch [30/31] D_loss: -3.4399, G_loss: -0.2558\n",
      "\n",
      "Epoch 3 Summary:\n",
      "  Average D_loss: -2.8837\n",
      "  Average G_loss: 0.7646\n",
      "\n",
      "Epoch [4/500]\n",
      "  Batch [0/31] D_loss: -5.1230, G_loss: -0.6822\n",
      "  Batch [10/31] D_loss: -5.8349, G_loss: 2.1743\n",
      "  Batch [20/31] D_loss: -4.0830, G_loss: 0.0940\n",
      "  Batch [30/31] D_loss: -4.3871, G_loss: 2.0156\n",
      "\n",
      "Epoch 4 Summary:\n",
      "  Average D_loss: -2.9307\n",
      "  Average G_loss: 0.2303\n",
      "\n",
      "Epoch [5/500]\n",
      "  Batch [0/31] D_loss: -5.3729, G_loss: 2.8858\n",
      "  Batch [10/31] D_loss: -4.0573, G_loss: -1.7687\n",
      "  Batch [20/31] D_loss: -3.5627, G_loss: -0.9938\n",
      "  Batch [30/31] D_loss: -3.9436, G_loss: -1.4294\n",
      "\n",
      "Epoch 5 Summary:\n",
      "  Average D_loss: -2.3973\n",
      "  Average G_loss: 0.1133\n",
      "\n",
      "Epoch [6/500]\n",
      "  Batch [0/31] D_loss: -3.1398, G_loss: -1.6849\n",
      "  Batch [10/31] D_loss: -3.2200, G_loss: 1.0041\n",
      "  Batch [20/31] D_loss: -3.3879, G_loss: 0.9694\n",
      "  Batch [30/31] D_loss: -4.0572, G_loss: 2.0510\n",
      "\n",
      "Epoch 6 Summary:\n",
      "  Average D_loss: -2.3417\n",
      "  Average G_loss: 1.2276\n",
      "\n",
      "Epoch [7/500]\n",
      "  Batch [0/31] D_loss: -3.9980, G_loss: 0.1201\n",
      "  Batch [10/31] D_loss: -4.6685, G_loss: 3.5028\n",
      "  Batch [20/31] D_loss: -4.6039, G_loss: -1.5720\n",
      "  Batch [30/31] D_loss: -3.7595, G_loss: 2.5505\n",
      "\n",
      "Epoch 7 Summary:\n",
      "  Average D_loss: -2.6187\n",
      "  Average G_loss: 0.7477\n",
      "\n",
      "Epoch [8/500]\n",
      "  Batch [0/31] D_loss: -4.7697, G_loss: 2.3030\n",
      "  Batch [10/31] D_loss: -4.9826, G_loss: 2.4852\n",
      "  Batch [20/31] D_loss: -3.6094, G_loss: -0.4765\n",
      "  Batch [30/31] D_loss: -4.3163, G_loss: 2.4330\n",
      "\n",
      "Epoch 8 Summary:\n",
      "  Average D_loss: -2.4450\n",
      "  Average G_loss: 0.5905\n",
      "\n",
      "Epoch [9/500]\n",
      "  Batch [0/31] D_loss: -3.5185, G_loss: 0.0490\n",
      "  Batch [10/31] D_loss: -4.5903, G_loss: -3.3699\n",
      "  Batch [20/31] D_loss: -3.6925, G_loss: 1.3728\n",
      "  Batch [30/31] D_loss: -3.4328, G_loss: -0.5673\n",
      "\n",
      "Epoch 9 Summary:\n",
      "  Average D_loss: -2.2774\n",
      "  Average G_loss: 0.5301\n",
      "\n",
      "Epoch [10/500]\n",
      "  Batch [0/31] D_loss: -3.9047, G_loss: 0.2432\n",
      "  Batch [10/31] D_loss: -3.9063, G_loss: 2.7413\n",
      "  Batch [20/31] D_loss: -3.6755, G_loss: 0.8478\n",
      "  Batch [30/31] D_loss: -4.2146, G_loss: 0.4697\n",
      "\n",
      "Epoch 10 Summary:\n",
      "  Average D_loss: -2.2771\n",
      "  Average G_loss: 0.8861\n",
      "\n",
      "Epoch [11/500]\n",
      "  Batch [0/31] D_loss: -3.5760, G_loss: -0.2002\n",
      "  Batch [10/31] D_loss: -3.4008, G_loss: 1.5071\n",
      "  Batch [20/31] D_loss: -3.8118, G_loss: -2.4851\n",
      "  Batch [30/31] D_loss: -2.8268, G_loss: -0.2208\n",
      "\n",
      "Epoch 11 Summary:\n",
      "  Average D_loss: -1.9920\n",
      "  Average G_loss: 0.1789\n",
      "\n",
      "Epoch [12/500]\n",
      "  Batch [0/31] D_loss: -4.4153, G_loss: 1.7188\n",
      "  Batch [10/31] D_loss: -3.8572, G_loss: 0.2224\n",
      "  Batch [20/31] D_loss: -3.0329, G_loss: -1.6842\n",
      "  Batch [30/31] D_loss: -3.0180, G_loss: 2.8937\n",
      "\n",
      "Epoch 12 Summary:\n",
      "  Average D_loss: -1.7857\n",
      "  Average G_loss: 0.4136\n",
      "\n",
      "Epoch [13/500]\n",
      "  Batch [0/31] D_loss: -3.4710, G_loss: 2.6933\n",
      "  Batch [10/31] D_loss: -3.5670, G_loss: 0.2059\n",
      "  Batch [20/31] D_loss: -4.3711, G_loss: 4.4262\n",
      "  Batch [30/31] D_loss: -4.4628, G_loss: -1.3360\n",
      "\n",
      "Epoch 13 Summary:\n",
      "  Average D_loss: -1.8915\n",
      "  Average G_loss: 0.5905\n",
      "\n",
      "Epoch [14/500]\n",
      "  Batch [0/31] D_loss: -3.0100, G_loss: -0.2427\n",
      "  Batch [10/31] D_loss: -3.6004, G_loss: 1.3506\n",
      "  Batch [20/31] D_loss: -3.2771, G_loss: -0.3557\n",
      "  Batch [30/31] D_loss: -3.3587, G_loss: -0.1098\n",
      "\n",
      "Epoch 14 Summary:\n",
      "  Average D_loss: -1.9301\n",
      "  Average G_loss: 0.8391\n",
      "\n",
      "Epoch [15/500]\n",
      "  Batch [0/31] D_loss: -3.1681, G_loss: -0.2590\n",
      "  Batch [10/31] D_loss: -2.9926, G_loss: 0.3868\n",
      "  Batch [20/31] D_loss: -3.6978, G_loss: -1.7043\n",
      "  Batch [30/31] D_loss: -3.6690, G_loss: 0.5766\n",
      "\n",
      "Epoch 15 Summary:\n",
      "  Average D_loss: -1.9649\n",
      "  Average G_loss: 0.8999\n",
      "\n",
      "Epoch [16/500]\n",
      "  Batch [0/31] D_loss: -2.7312, G_loss: 0.8761\n",
      "  Batch [10/31] D_loss: -4.3338, G_loss: 2.5304\n",
      "  Batch [20/31] D_loss: -2.9705, G_loss: 0.0927\n",
      "  Batch [30/31] D_loss: -2.8351, G_loss: -1.1793\n",
      "\n",
      "Epoch 16 Summary:\n",
      "  Average D_loss: -1.7330\n",
      "  Average G_loss: 0.7121\n",
      "\n",
      "Epoch [17/500]\n",
      "  Batch [0/31] D_loss: -2.8961, G_loss: -0.1543\n",
      "  Batch [10/31] D_loss: -3.7751, G_loss: 3.7862\n",
      "  Batch [20/31] D_loss: -2.9533, G_loss: 0.1300\n",
      "  Batch [30/31] D_loss: -4.9205, G_loss: 4.6775\n",
      "\n",
      "Epoch 17 Summary:\n",
      "  Average D_loss: -1.5948\n",
      "  Average G_loss: 0.6810\n",
      "\n",
      "Epoch [18/500]\n",
      "  Batch [0/31] D_loss: -1.1158, G_loss: 1.7581\n",
      "  Batch [10/31] D_loss: -2.3460, G_loss: 0.0502\n",
      "  Batch [20/31] D_loss: -2.6361, G_loss: -2.3211\n",
      "  Batch [30/31] D_loss: -3.2956, G_loss: 1.5608\n",
      "\n",
      "Epoch 18 Summary:\n",
      "  Average D_loss: -1.5295\n",
      "  Average G_loss: 0.1925\n",
      "\n",
      "Epoch [19/500]\n",
      "  Batch [0/31] D_loss: -3.2530, G_loss: -1.9865\n",
      "  Batch [10/31] D_loss: -2.9289, G_loss: 0.0329\n",
      "  Batch [20/31] D_loss: -4.3919, G_loss: -1.6392\n",
      "  Batch [30/31] D_loss: -3.9925, G_loss: 2.4451\n",
      "\n",
      "Epoch 19 Summary:\n",
      "  Average D_loss: -1.6678\n",
      "  Average G_loss: 0.7438\n",
      "\n",
      "Epoch [20/500]\n",
      "  Batch [0/31] D_loss: -3.3535, G_loss: 1.5088\n",
      "  Batch [10/31] D_loss: -3.5861, G_loss: -0.8509\n",
      "  Batch [20/31] D_loss: -1.4248, G_loss: -3.3946\n",
      "  Batch [30/31] D_loss: -2.0655, G_loss: 3.4306\n",
      "\n",
      "Epoch 20 Summary:\n",
      "  Average D_loss: -1.7483\n",
      "  Average G_loss: 0.6640\n",
      "\n",
      "Epoch [21/500]\n",
      "  Batch [0/31] D_loss: -2.0181, G_loss: -0.3248\n",
      "  Batch [10/31] D_loss: -3.2570, G_loss: 0.3781\n",
      "  Batch [20/31] D_loss: -3.9192, G_loss: 2.2367\n",
      "  Batch [30/31] D_loss: -3.1042, G_loss: -0.9577\n",
      "\n",
      "Epoch 21 Summary:\n",
      "  Average D_loss: -1.6455\n",
      "  Average G_loss: 0.2006\n",
      "\n",
      "Epoch [22/500]\n",
      "  Batch [0/31] D_loss: -3.5412, G_loss: 3.1305\n",
      "  Batch [10/31] D_loss: -4.2832, G_loss: -1.6110\n",
      "  Batch [20/31] D_loss: -3.3653, G_loss: 1.2831\n",
      "  Batch [30/31] D_loss: -2.5031, G_loss: 0.2746\n",
      "\n",
      "Epoch 22 Summary:\n",
      "  Average D_loss: -1.6015\n",
      "  Average G_loss: 0.5169\n",
      "\n",
      "Epoch [23/500]\n",
      "  Batch [0/31] D_loss: -3.0897, G_loss: -1.5363\n",
      "  Batch [10/31] D_loss: -4.0489, G_loss: -4.9134\n",
      "  Batch [20/31] D_loss: -4.2644, G_loss: 5.8194\n",
      "  Batch [30/31] D_loss: -3.8123, G_loss: 2.0779\n",
      "\n",
      "Epoch 23 Summary:\n",
      "  Average D_loss: -1.7239\n",
      "  Average G_loss: 1.2771\n",
      "\n",
      "Epoch [24/500]\n",
      "  Batch [0/31] D_loss: -3.5440, G_loss: -0.4156\n",
      "  Batch [10/31] D_loss: -1.5308, G_loss: -1.0317\n",
      "  Batch [20/31] D_loss: -2.4656, G_loss: 3.1246\n",
      "  Batch [30/31] D_loss: -2.5854, G_loss: 0.6998\n",
      "\n",
      "Epoch 24 Summary:\n",
      "  Average D_loss: -1.2450\n",
      "  Average G_loss: 0.0011\n",
      "\n",
      "Epoch [25/500]\n",
      "  Batch [0/31] D_loss: -3.3597, G_loss: 1.5951\n",
      "  Batch [10/31] D_loss: -3.2720, G_loss: 2.9730\n",
      "  Batch [20/31] D_loss: -3.2742, G_loss: -0.9298\n",
      "  Batch [30/31] D_loss: -3.4363, G_loss: 4.9304\n",
      "\n",
      "Epoch 25 Summary:\n",
      "  Average D_loss: -1.7802\n",
      "  Average G_loss: 0.5658\n",
      "\n",
      "Epoch [26/500]\n",
      "  Batch [0/31] D_loss: -3.5393, G_loss: 4.7971\n",
      "  Batch [10/31] D_loss: -2.0391, G_loss: -2.9717\n",
      "  Batch [20/31] D_loss: -2.8880, G_loss: 3.2095\n",
      "  Batch [30/31] D_loss: -3.9362, G_loss: -1.4655\n",
      "\n",
      "Epoch 26 Summary:\n",
      "  Average D_loss: -1.6781\n",
      "  Average G_loss: 0.2585\n",
      "\n",
      "Epoch [27/500]\n",
      "  Batch [0/31] D_loss: -2.9669, G_loss: -1.4168\n",
      "  Batch [10/31] D_loss: -3.2245, G_loss: 3.9693\n",
      "  Batch [20/31] D_loss: -3.1094, G_loss: -2.9894\n",
      "  Batch [30/31] D_loss: -3.4194, G_loss: 2.3597\n",
      "\n",
      "Epoch 27 Summary:\n",
      "  Average D_loss: -1.6116\n",
      "  Average G_loss: 0.6360\n",
      "\n",
      "Epoch [28/500]\n",
      "  Batch [0/31] D_loss: -2.8528, G_loss: 2.5535\n",
      "  Batch [10/31] D_loss: -3.6621, G_loss: -0.2670\n",
      "  Batch [20/31] D_loss: -2.5546, G_loss: 0.3702\n",
      "  Batch [30/31] D_loss: -3.3515, G_loss: 2.8443\n",
      "\n",
      "Epoch 28 Summary:\n",
      "  Average D_loss: -1.5351\n",
      "  Average G_loss: 0.2674\n",
      "\n",
      "Epoch [29/500]\n",
      "  Batch [0/31] D_loss: -3.0119, G_loss: 3.8557\n",
      "  Batch [10/31] D_loss: -3.3500, G_loss: -1.9719\n",
      "  Batch [20/31] D_loss: -3.0539, G_loss: -0.7665\n",
      "  Batch [30/31] D_loss: -3.2989, G_loss: 1.1015\n",
      "\n",
      "Epoch 29 Summary:\n",
      "  Average D_loss: -1.6669\n",
      "  Average G_loss: 1.1620\n",
      "\n",
      "Epoch [30/500]\n",
      "  Batch [0/31] D_loss: -3.7100, G_loss: 0.5650\n",
      "  Batch [10/31] D_loss: -3.7785, G_loss: 0.2513\n",
      "  Batch [20/31] D_loss: -3.9672, G_loss: 3.8896\n",
      "  Batch [30/31] D_loss: -2.6498, G_loss: -4.8975\n",
      "\n",
      "Epoch 30 Summary:\n",
      "  Average D_loss: -1.5629\n",
      "  Average G_loss: 0.1677\n",
      "\n",
      "Epoch [31/500]\n",
      "  Batch [0/31] D_loss: -2.6268, G_loss: -4.7230\n",
      "  Batch [10/31] D_loss: -4.1339, G_loss: 6.2816\n",
      "  Batch [20/31] D_loss: -2.9517, G_loss: -2.2225\n",
      "  Batch [30/31] D_loss: -2.7085, G_loss: 4.3856\n",
      "\n",
      "Epoch 31 Summary:\n",
      "  Average D_loss: -1.2537\n",
      "  Average G_loss: 0.5095\n",
      "\n",
      "Epoch [32/500]\n",
      "  Batch [0/31] D_loss: -2.4378, G_loss: 4.0993\n",
      "  Batch [10/31] D_loss: -2.0189, G_loss: -0.4340\n",
      "  Batch [20/31] D_loss: -2.3687, G_loss: 0.3880\n",
      "  Batch [30/31] D_loss: -2.3530, G_loss: 3.8460\n",
      "\n",
      "Epoch 32 Summary:\n",
      "  Average D_loss: -1.1530\n",
      "  Average G_loss: 0.5072\n",
      "\n",
      "Epoch [33/500]\n",
      "  Batch [0/31] D_loss: -2.0727, G_loss: 4.0927\n",
      "  Batch [10/31] D_loss: -3.2626, G_loss: -2.2074\n",
      "  Batch [20/31] D_loss: -3.2048, G_loss: 3.9609\n",
      "  Batch [30/31] D_loss: -1.9318, G_loss: -2.9133\n",
      "\n",
      "Epoch 33 Summary:\n",
      "  Average D_loss: -1.1836\n",
      "  Average G_loss: 0.5394\n",
      "\n",
      "Epoch [34/500]\n",
      "  Batch [0/31] D_loss: -2.1310, G_loss: -3.5033\n",
      "  Batch [10/31] D_loss: -2.0548, G_loss: 2.1276\n",
      "  Batch [20/31] D_loss: -3.5591, G_loss: -2.7767\n",
      "  Batch [30/31] D_loss: -2.1840, G_loss: 6.0194\n",
      "\n",
      "Epoch 34 Summary:\n",
      "  Average D_loss: -1.1882\n",
      "  Average G_loss: 0.8074\n",
      "\n",
      "Epoch [35/500]\n",
      "  Batch [0/31] D_loss: -0.2999, G_loss: 3.1511\n",
      "  Batch [10/31] D_loss: -1.7785, G_loss: -2.3361\n",
      "  Batch [20/31] D_loss: -1.6018, G_loss: 3.0862\n",
      "  Batch [30/31] D_loss: -3.0733, G_loss: -2.9936\n",
      "\n",
      "Epoch 35 Summary:\n",
      "  Average D_loss: -0.9409\n",
      "  Average G_loss: 0.4582\n",
      "\n",
      "Epoch [36/500]\n",
      "  Batch [0/31] D_loss: -1.2007, G_loss: -2.5984\n",
      "  Batch [10/31] D_loss: -1.2065, G_loss: 2.6578\n",
      "  Batch [20/31] D_loss: -2.5775, G_loss: -1.2784\n",
      "  Batch [30/31] D_loss: -2.5705, G_loss: 4.5383\n",
      "\n",
      "Epoch 36 Summary:\n",
      "  Average D_loss: -0.9982\n",
      "  Average G_loss: 0.4922\n",
      "\n",
      "Epoch [37/500]\n",
      "  Batch [0/31] D_loss: -2.6801, G_loss: 3.7326\n",
      "  Batch [10/31] D_loss: -2.7731, G_loss: -0.8631\n",
      "  Batch [20/31] D_loss: -2.1246, G_loss: -0.5392\n",
      "  Batch [30/31] D_loss: -3.0266, G_loss: 2.2365\n",
      "\n",
      "Epoch 37 Summary:\n",
      "  Average D_loss: -1.1141\n",
      "  Average G_loss: 0.6533\n",
      "\n",
      "Epoch [38/500]\n",
      "  Batch [0/31] D_loss: -2.8298, G_loss: -1.5474\n",
      "  Batch [10/31] D_loss: -3.0024, G_loss: 3.4318\n",
      "  Batch [20/31] D_loss: -2.9215, G_loss: -1.0962\n",
      "  Batch [30/31] D_loss: -2.1343, G_loss: 1.8586\n",
      "\n",
      "Epoch 38 Summary:\n",
      "  Average D_loss: -1.0468\n",
      "  Average G_loss: 0.5485\n",
      "\n",
      "Epoch [39/500]\n",
      "  Batch [0/31] D_loss: -1.8422, G_loss: 1.3517\n",
      "  Batch [10/31] D_loss: -2.6037, G_loss: -0.4896\n",
      "  Batch [20/31] D_loss: -2.8257, G_loss: -2.2628\n",
      "  Batch [30/31] D_loss: -2.7385, G_loss: 1.5148\n",
      "\n",
      "Epoch 39 Summary:\n",
      "  Average D_loss: -1.0294\n",
      "  Average G_loss: 0.0672\n",
      "\n",
      "Epoch [40/500]\n",
      "  Batch [0/31] D_loss: -2.4466, G_loss: 3.4189\n",
      "  Batch [10/31] D_loss: -2.5388, G_loss: -0.5718\n",
      "  Batch [20/31] D_loss: -2.7508, G_loss: 1.7374\n",
      "  Batch [30/31] D_loss: -3.7222, G_loss: 0.5079\n",
      "\n",
      "Epoch 40 Summary:\n",
      "  Average D_loss: -1.0624\n",
      "  Average G_loss: 0.8090\n",
      "\n",
      "Epoch [41/500]\n",
      "  Batch [0/31] D_loss: -2.5314, G_loss: 0.8555\n",
      "  Batch [10/31] D_loss: -1.9602, G_loss: -0.8303\n",
      "  Batch [20/31] D_loss: -1.9016, G_loss: 4.3423\n",
      "  Batch [30/31] D_loss: -2.5357, G_loss: 2.0083\n",
      "\n",
      "Epoch 41 Summary:\n",
      "  Average D_loss: -1.0951\n",
      "  Average G_loss: 0.2083\n",
      "\n",
      "Epoch [42/500]\n",
      "  Batch [0/31] D_loss: -2.4464, G_loss: 1.0976\n",
      "  Batch [10/31] D_loss: -2.5279, G_loss: 3.0657\n",
      "  Batch [20/31] D_loss: -3.1747, G_loss: -2.7489\n",
      "  Batch [30/31] D_loss: -2.8425, G_loss: 3.3945\n",
      "\n",
      "Epoch 42 Summary:\n",
      "  Average D_loss: -1.0573\n",
      "  Average G_loss: 1.1895\n",
      "\n",
      "Epoch [43/500]\n",
      "  Batch [0/31] D_loss: -1.8676, G_loss: 3.8509\n",
      "  Batch [10/31] D_loss: -1.2191, G_loss: -3.1846\n",
      "  Batch [20/31] D_loss: -2.1434, G_loss: -0.9910\n",
      "  Batch [30/31] D_loss: -2.7405, G_loss: 0.4602\n",
      "\n",
      "Epoch 43 Summary:\n",
      "  Average D_loss: -0.9695\n",
      "  Average G_loss: 0.4382\n",
      "\n",
      "Epoch [44/500]\n",
      "  Batch [0/31] D_loss: -2.5141, G_loss: 2.3419\n",
      "  Batch [10/31] D_loss: -1.9370, G_loss: 0.0387\n",
      "  Batch [20/31] D_loss: -2.2579, G_loss: 0.8563\n",
      "  Batch [30/31] D_loss: -1.8743, G_loss: 1.6255\n",
      "\n",
      "Epoch 44 Summary:\n",
      "  Average D_loss: -1.0090\n",
      "  Average G_loss: 0.8755\n",
      "\n",
      "Epoch [45/500]\n",
      "  Batch [0/31] D_loss: -2.5609, G_loss: 0.9551\n",
      "  Batch [10/31] D_loss: -2.5774, G_loss: 0.5522\n",
      "  Batch [20/31] D_loss: -2.3535, G_loss: 0.4452\n",
      "  Batch [30/31] D_loss: -3.8523, G_loss: 3.4829\n",
      "\n",
      "Epoch 45 Summary:\n",
      "  Average D_loss: -1.2034\n",
      "  Average G_loss: 0.0206\n",
      "\n",
      "Epoch [46/500]\n",
      "  Batch [0/31] D_loss: -1.9419, G_loss: 3.6547\n",
      "  Batch [10/31] D_loss: -2.6209, G_loss: -2.3951\n",
      "  Batch [20/31] D_loss: -1.7232, G_loss: 0.8991\n",
      "  Batch [30/31] D_loss: -2.3244, G_loss: 0.2882\n",
      "\n",
      "Epoch 46 Summary:\n",
      "  Average D_loss: -0.8857\n",
      "  Average G_loss: 0.3176\n",
      "\n",
      "Epoch [47/500]\n",
      "  Batch [0/31] D_loss: -2.2625, G_loss: 1.2181\n",
      "  Batch [10/31] D_loss: -2.0881, G_loss: -2.2158\n",
      "  Batch [20/31] D_loss: -2.6806, G_loss: 3.1817\n",
      "  Batch [30/31] D_loss: -2.6320, G_loss: -3.1074\n",
      "\n",
      "Epoch 47 Summary:\n",
      "  Average D_loss: -1.1048\n",
      "  Average G_loss: 0.5500\n",
      "\n",
      "Epoch [48/500]\n",
      "  Batch [0/31] D_loss: -2.0843, G_loss: -3.1180\n",
      "  Batch [10/31] D_loss: -2.6079, G_loss: -1.5998\n",
      "  Batch [20/31] D_loss: -2.8209, G_loss: 0.2808\n",
      "  Batch [30/31] D_loss: -2.7231, G_loss: -1.6673\n",
      "\n",
      "Epoch 48 Summary:\n",
      "  Average D_loss: -0.9510\n",
      "  Average G_loss: 0.6767\n",
      "\n",
      "Epoch [49/500]\n",
      "  Batch [0/31] D_loss: -2.8033, G_loss: -1.5051\n",
      "  Batch [10/31] D_loss: -1.3484, G_loss: 3.9222\n",
      "  Batch [20/31] D_loss: -1.9415, G_loss: -1.2426\n",
      "  Batch [30/31] D_loss: -2.4900, G_loss: -0.7880\n",
      "\n",
      "Epoch 49 Summary:\n",
      "  Average D_loss: -1.1525\n",
      "  Average G_loss: 0.7734\n",
      "\n",
      "Epoch [50/500]\n",
      "  Batch [0/31] D_loss: -2.6093, G_loss: -1.5492\n",
      "  Batch [10/31] D_loss: -2.1692, G_loss: 1.1882\n",
      "  Batch [20/31] D_loss: -2.9168, G_loss: 0.0690\n",
      "  Batch [30/31] D_loss: -3.2339, G_loss: 2.5521\n",
      "\n",
      "Epoch 50 Summary:\n",
      "  Average D_loss: -1.0829\n",
      "  Average G_loss: 0.7019\n",
      "\n",
      "Epoch [51/500]\n",
      "  Batch [0/31] D_loss: -2.2520, G_loss: -0.3576\n",
      "  Batch [10/31] D_loss: -2.1980, G_loss: -4.0654\n",
      "  Batch [20/31] D_loss: -4.1023, G_loss: 5.2346\n",
      "  Batch [30/31] D_loss: -2.5732, G_loss: -3.4880\n",
      "\n",
      "Epoch 51 Summary:\n",
      "  Average D_loss: -1.0516\n",
      "  Average G_loss: 0.4086\n",
      "\n",
      "Epoch [52/500]\n",
      "  Batch [0/31] D_loss: -2.4397, G_loss: -4.4271\n",
      "  Batch [10/31] D_loss: -2.0885, G_loss: 3.4118\n",
      "  Batch [20/31] D_loss: -2.3073, G_loss: -2.3789\n",
      "  Batch [30/31] D_loss: -3.0408, G_loss: -1.8252\n",
      "\n",
      "Epoch 52 Summary:\n",
      "  Average D_loss: -0.9348\n",
      "  Average G_loss: 0.0340\n",
      "\n",
      "Epoch [53/500]\n",
      "  Batch [0/31] D_loss: -2.6006, G_loss: 2.5289\n",
      "  Batch [10/31] D_loss: -2.4704, G_loss: -3.2684\n",
      "  Batch [20/31] D_loss: -1.6258, G_loss: 0.6202\n",
      "  Batch [30/31] D_loss: -2.4017, G_loss: -0.2631\n",
      "\n",
      "Epoch 53 Summary:\n",
      "  Average D_loss: -1.0606\n",
      "  Average G_loss: 0.7631\n",
      "\n",
      "Epoch [54/500]\n",
      "  Batch [0/31] D_loss: -3.3076, G_loss: 2.5327\n",
      "  Batch [10/31] D_loss: -2.7346, G_loss: -1.6178\n",
      "  Batch [20/31] D_loss: -3.0975, G_loss: 3.7680\n",
      "  Batch [30/31] D_loss: -3.6965, G_loss: 1.3246\n",
      "\n",
      "Epoch 54 Summary:\n",
      "  Average D_loss: -1.2420\n",
      "  Average G_loss: 0.4881\n",
      "\n",
      "Epoch [55/500]\n",
      "  Batch [0/31] D_loss: -3.8182, G_loss: 3.0783\n",
      "  Batch [10/31] D_loss: -2.6487, G_loss: -0.7664\n",
      "  Batch [20/31] D_loss: -2.4582, G_loss: -0.4922\n",
      "  Batch [30/31] D_loss: -2.6406, G_loss: -1.3757\n",
      "\n",
      "Epoch 55 Summary:\n",
      "  Average D_loss: -1.1130\n",
      "  Average G_loss: 0.3292\n",
      "\n",
      "Epoch [56/500]\n",
      "  Batch [0/31] D_loss: -2.2685, G_loss: -1.0641\n",
      "  Batch [10/31] D_loss: -4.7120, G_loss: -4.7647\n",
      "  Batch [20/31] D_loss: -2.3366, G_loss: 3.3385\n",
      "  Batch [30/31] D_loss: -2.2427, G_loss: -3.6819\n",
      "\n",
      "Epoch 56 Summary:\n",
      "  Average D_loss: -1.1484\n",
      "  Average G_loss: 0.8125\n",
      "\n",
      "Epoch [57/500]\n",
      "  Batch [0/31] D_loss: -1.7572, G_loss: -3.2160\n",
      "  Batch [10/31] D_loss: -1.3857, G_loss: 2.4592\n",
      "  Batch [20/31] D_loss: -1.5895, G_loss: 3.4817\n",
      "  Batch [30/31] D_loss: -2.8791, G_loss: -1.3832\n",
      "\n",
      "Epoch 57 Summary:\n",
      "  Average D_loss: -0.9892\n",
      "  Average G_loss: 0.2724\n",
      "\n",
      "Epoch [58/500]\n",
      "  Batch [0/31] D_loss: -4.7971, G_loss: -3.0734\n",
      "  Batch [10/31] D_loss: -1.2752, G_loss: 3.3249\n",
      "  Batch [20/31] D_loss: -3.1183, G_loss: -1.6722\n",
      "  Batch [30/31] D_loss: -2.4061, G_loss: 1.9938\n",
      "\n",
      "Epoch 58 Summary:\n",
      "  Average D_loss: -1.2211\n",
      "  Average G_loss: 0.1354\n",
      "\n",
      "Epoch [59/500]\n",
      "  Batch [0/31] D_loss: -2.2851, G_loss: 2.7418\n",
      "  Batch [10/31] D_loss: -1.8768, G_loss: 0.4500\n",
      "  Batch [20/31] D_loss: -2.4739, G_loss: -0.4688\n",
      "  Batch [30/31] D_loss: -1.8051, G_loss: 0.9229\n",
      "\n",
      "Epoch 59 Summary:\n",
      "  Average D_loss: -1.1203\n",
      "  Average G_loss: 1.1300\n",
      "\n",
      "Epoch [60/500]\n",
      "  Batch [0/31] D_loss: -1.8562, G_loss: 1.2767\n",
      "  Batch [10/31] D_loss: -2.5780, G_loss: -1.2687\n",
      "  Batch [20/31] D_loss: -2.6877, G_loss: 3.0144\n",
      "  Batch [30/31] D_loss: -2.9267, G_loss: -1.4561\n",
      "\n",
      "Epoch 60 Summary:\n",
      "  Average D_loss: -1.1294\n",
      "  Average G_loss: 0.6112\n",
      "\n",
      "Epoch [61/500]\n",
      "  Batch [0/31] D_loss: -3.5202, G_loss: -2.1950\n",
      "  Batch [10/31] D_loss: -2.0672, G_loss: 1.8698\n",
      "  Batch [20/31] D_loss: -2.6305, G_loss: 1.9969\n",
      "  Batch [30/31] D_loss: -3.0834, G_loss: 4.1466\n",
      "\n",
      "Epoch 61 Summary:\n",
      "  Average D_loss: -1.1382\n",
      "  Average G_loss: 0.5805\n",
      "\n",
      "Epoch [62/500]\n",
      "  Batch [0/31] D_loss: -3.3849, G_loss: 4.1800\n",
      "  Batch [10/31] D_loss: -1.5584, G_loss: -2.0860\n",
      "  Batch [20/31] D_loss: -2.0589, G_loss: -0.3975\n",
      "  Batch [30/31] D_loss: -2.8437, G_loss: 4.1640\n",
      "\n",
      "Epoch 62 Summary:\n",
      "  Average D_loss: -1.1115\n",
      "  Average G_loss: 0.5498\n",
      "\n",
      "Epoch [63/500]\n",
      "  Batch [0/31] D_loss: -2.2746, G_loss: 3.2223\n",
      "  Batch [10/31] D_loss: -2.0876, G_loss: -2.1251\n",
      "  Batch [20/31] D_loss: -1.7382, G_loss: -0.9812\n",
      "  Batch [30/31] D_loss: -3.2296, G_loss: 2.3411\n",
      "\n",
      "Epoch 63 Summary:\n",
      "  Average D_loss: -1.1123\n",
      "  Average G_loss: 0.6537\n",
      "\n",
      "Epoch [64/500]\n",
      "  Batch [0/31] D_loss: -3.4195, G_loss: 3.1053\n",
      "  Batch [10/31] D_loss: -2.4414, G_loss: 1.4905\n",
      "  Batch [20/31] D_loss: -3.1276, G_loss: 3.4652\n",
      "  Batch [30/31] D_loss: -3.1174, G_loss: -1.5841\n",
      "\n",
      "Epoch 64 Summary:\n",
      "  Average D_loss: -1.2005\n",
      "  Average G_loss: -0.0849\n",
      "\n",
      "Epoch [65/500]\n",
      "  Batch [0/31] D_loss: -2.8353, G_loss: -2.0138\n",
      "  Batch [10/31] D_loss: -2.9766, G_loss: -0.4993\n",
      "  Batch [20/31] D_loss: -2.3700, G_loss: 0.2373\n",
      "  Batch [30/31] D_loss: -2.3996, G_loss: 0.0059\n",
      "\n",
      "Epoch 65 Summary:\n",
      "  Average D_loss: -1.1146\n",
      "  Average G_loss: 0.2209\n",
      "\n",
      "Epoch [66/500]\n",
      "  Batch [0/31] D_loss: -2.3578, G_loss: 0.8030\n",
      "  Batch [10/31] D_loss: -2.2406, G_loss: -1.0761\n",
      "  Batch [20/31] D_loss: -4.4529, G_loss: -5.6454\n",
      "  Batch [30/31] D_loss: -2.3406, G_loss: 2.7315\n",
      "\n",
      "Epoch 66 Summary:\n",
      "  Average D_loss: -1.2997\n",
      "  Average G_loss: 0.4755\n",
      "\n",
      "Epoch [67/500]\n",
      "  Batch [0/31] D_loss: -2.2737, G_loss: 4.2787\n",
      "  Batch [10/31] D_loss: -2.6526, G_loss: 3.3781\n",
      "  Batch [20/31] D_loss: -4.1081, G_loss: -4.1879\n",
      "  Batch [30/31] D_loss: -2.9962, G_loss: 6.3333\n",
      "\n",
      "Epoch 67 Summary:\n",
      "  Average D_loss: -1.2346\n",
      "  Average G_loss: 0.5286\n",
      "\n",
      "Epoch [68/500]\n",
      "  Batch [0/31] D_loss: -2.9466, G_loss: 6.5517\n",
      "  Batch [10/31] D_loss: -2.0082, G_loss: 1.1372\n",
      "  Batch [20/31] D_loss: -2.9560, G_loss: -3.0282\n",
      "  Batch [30/31] D_loss: -2.7115, G_loss: -2.5882\n",
      "\n",
      "Epoch 68 Summary:\n",
      "  Average D_loss: -1.0380\n",
      "  Average G_loss: 0.7725\n",
      "\n",
      "Epoch [69/500]\n",
      "  Batch [0/31] D_loss: -2.1464, G_loss: -0.9990\n",
      "  Batch [10/31] D_loss: -3.2999, G_loss: 2.5519\n",
      "  Batch [20/31] D_loss: -3.6287, G_loss: -3.2785\n",
      "  Batch [30/31] D_loss: -2.4943, G_loss: 3.7664\n",
      "\n",
      "Epoch 69 Summary:\n",
      "  Average D_loss: -1.2495\n",
      "  Average G_loss: 0.4886\n",
      "\n",
      "Epoch [70/500]\n",
      "  Batch [0/31] D_loss: -1.9288, G_loss: 4.7356\n",
      "  Batch [10/31] D_loss: -1.8961, G_loss: -2.1082\n",
      "  Batch [20/31] D_loss: -2.0452, G_loss: 0.7264\n",
      "  Batch [30/31] D_loss: -2.0016, G_loss: 1.5292\n",
      "\n",
      "Epoch 70 Summary:\n",
      "  Average D_loss: -1.0102\n",
      "  Average G_loss: 0.9233\n",
      "\n",
      "Epoch [71/500]\n",
      "  Batch [0/31] D_loss: -2.1131, G_loss: 1.8391\n",
      "  Batch [10/31] D_loss: -2.6898, G_loss: -1.0003\n",
      "  Batch [20/31] D_loss: -1.7499, G_loss: 1.4920\n",
      "  Batch [30/31] D_loss: -2.1349, G_loss: -3.2761\n",
      "\n",
      "Epoch 71 Summary:\n",
      "  Average D_loss: -1.0108\n",
      "  Average G_loss: 0.4399\n",
      "\n",
      "Epoch [72/500]\n",
      "  Batch [0/31] D_loss: -2.2055, G_loss: -3.6260\n",
      "  Batch [10/31] D_loss: -0.3613, G_loss: 1.8584\n",
      "  Batch [20/31] D_loss: -2.2688, G_loss: -0.6186\n",
      "  Batch [30/31] D_loss: -0.6935, G_loss: 1.2648\n",
      "\n",
      "Epoch 72 Summary:\n",
      "  Average D_loss: -1.0366\n",
      "  Average G_loss: 0.6896\n",
      "\n",
      "Epoch [73/500]\n",
      "  Batch [0/31] D_loss: -1.8088, G_loss: 0.8904\n",
      "  Batch [10/31] D_loss: -3.1388, G_loss: 0.7697\n",
      "  Batch [20/31] D_loss: -2.6519, G_loss: 3.0836\n",
      "  Batch [30/31] D_loss: -2.5877, G_loss: 0.1734\n",
      "\n",
      "Epoch 73 Summary:\n",
      "  Average D_loss: -1.1922\n",
      "  Average G_loss: -0.2881\n",
      "\n",
      "Epoch [74/500]\n",
      "  Batch [0/31] D_loss: -3.5357, G_loss: 4.8729\n",
      "  Batch [10/31] D_loss: -3.1702, G_loss: -1.0752\n",
      "  Batch [20/31] D_loss: -2.0032, G_loss: 1.4135\n",
      "  Batch [30/31] D_loss: -1.2098, G_loss: -0.4027\n",
      "\n",
      "Epoch 74 Summary:\n",
      "  Average D_loss: -1.0807\n",
      "  Average G_loss: 0.9870\n",
      "\n",
      "Epoch [75/500]\n",
      "  Batch [0/31] D_loss: -2.4729, G_loss: -0.3421\n",
      "  Batch [10/31] D_loss: -2.8003, G_loss: -1.1032\n",
      "  Batch [20/31] D_loss: -2.7383, G_loss: 1.0132\n",
      "  Batch [30/31] D_loss: -0.4042, G_loss: -3.2887\n",
      "\n",
      "Epoch 75 Summary:\n",
      "  Average D_loss: -1.1608\n",
      "  Average G_loss: -0.0548\n",
      "\n",
      "Epoch [76/500]\n",
      "  Batch [0/31] D_loss: -0.4948, G_loss: -1.8239\n",
      "  Batch [10/31] D_loss: -2.2395, G_loss: 1.2333\n",
      "  Batch [20/31] D_loss: -2.1155, G_loss: -1.3965\n",
      "  Batch [30/31] D_loss: -2.5389, G_loss: 0.2275\n",
      "\n",
      "Epoch 76 Summary:\n",
      "  Average D_loss: -0.9488\n",
      "  Average G_loss: 0.7306\n",
      "\n",
      "Epoch [77/500]\n",
      "  Batch [0/31] D_loss: -2.5368, G_loss: -1.3314\n",
      "  Batch [10/31] D_loss: -3.2663, G_loss: 2.6428\n",
      "  Batch [20/31] D_loss: -2.0198, G_loss: 0.3268\n",
      "  Batch [30/31] D_loss: -2.7215, G_loss: 0.2285\n",
      "\n",
      "Epoch 77 Summary:\n",
      "  Average D_loss: -1.1345\n",
      "  Average G_loss: 0.5897\n",
      "\n",
      "Epoch [78/500]\n",
      "  Batch [0/31] D_loss: -2.8829, G_loss: -0.8420\n",
      "  Batch [10/31] D_loss: -2.4546, G_loss: 2.6604\n",
      "  Batch [20/31] D_loss: -2.4198, G_loss: 1.8152\n",
      "  Batch [30/31] D_loss: -2.6089, G_loss: 0.7692\n",
      "\n",
      "Epoch 78 Summary:\n",
      "  Average D_loss: -1.2059\n",
      "  Average G_loss: 0.3422\n",
      "\n",
      "Epoch [79/500]\n",
      "  Batch [0/31] D_loss: -2.6628, G_loss: 4.1061\n",
      "  Batch [10/31] D_loss: -2.9604, G_loss: -4.7784\n",
      "  Batch [20/31] D_loss: -2.7564, G_loss: 2.8159\n",
      "  Batch [30/31] D_loss: -2.4886, G_loss: -2.4714\n",
      "\n",
      "Epoch 79 Summary:\n",
      "  Average D_loss: -1.1438\n",
      "  Average G_loss: 0.7671\n",
      "\n",
      "Epoch [80/500]\n",
      "  Batch [0/31] D_loss: -1.5855, G_loss: -0.6303\n",
      "  Batch [10/31] D_loss: -4.0593, G_loss: 6.5865\n",
      "  Batch [20/31] D_loss: -2.7731, G_loss: 0.4341\n",
      "  Batch [30/31] D_loss: -2.8558, G_loss: 0.3278\n",
      "\n",
      "Epoch 80 Summary:\n",
      "  Average D_loss: -1.1158\n",
      "  Average G_loss: 0.1557\n",
      "\n",
      "Epoch [81/500]\n",
      "  Batch [0/31] D_loss: -2.5157, G_loss: 0.0349\n",
      "  Batch [10/31] D_loss: -3.2161, G_loss: -1.5349\n",
      "  Batch [20/31] D_loss: -3.5264, G_loss: -2.4638\n",
      "  Batch [30/31] D_loss: -3.0255, G_loss: 0.4919\n",
      "\n",
      "Epoch 81 Summary:\n",
      "  Average D_loss: -1.2583\n",
      "  Average G_loss: 0.7030\n",
      "\n",
      "Epoch [82/500]\n",
      "  Batch [0/31] D_loss: -3.1503, G_loss: 0.9715\n",
      "  Batch [10/31] D_loss: -2.3193, G_loss: 0.8485\n",
      "  Batch [20/31] D_loss: -2.4588, G_loss: 2.4569\n",
      "  Batch [30/31] D_loss: -1.4348, G_loss: 0.5940\n",
      "\n",
      "Epoch 82 Summary:\n",
      "  Average D_loss: -1.1600\n",
      "  Average G_loss: 0.0610\n",
      "\n",
      "Epoch [83/500]\n",
      "  Batch [0/31] D_loss: -2.2583, G_loss: -0.0342\n",
      "  Batch [10/31] D_loss: -3.3002, G_loss: 3.8659\n",
      "  Batch [20/31] D_loss: -3.1075, G_loss: 2.1105\n",
      "  Batch [30/31] D_loss: -2.1082, G_loss: -6.3748\n",
      "\n",
      "Epoch 83 Summary:\n",
      "  Average D_loss: -1.2672\n",
      "  Average G_loss: -0.1020\n",
      "\n",
      "Epoch [84/500]\n",
      "  Batch [0/31] D_loss: -1.5507, G_loss: -5.4770\n",
      "  Batch [10/31] D_loss: -2.7419, G_loss: 3.8730\n",
      "  Batch [20/31] D_loss: -2.1215, G_loss: 1.8789\n",
      "  Batch [30/31] D_loss: -1.2357, G_loss: -0.0557\n",
      "\n",
      "Epoch 84 Summary:\n",
      "  Average D_loss: -0.8767\n",
      "  Average G_loss: 0.6056\n",
      "\n",
      "Epoch [85/500]\n",
      "  Batch [0/31] D_loss: -2.4047, G_loss: 2.2751\n",
      "  Batch [10/31] D_loss: -2.7508, G_loss: -1.5162\n",
      "  Batch [20/31] D_loss: -3.3846, G_loss: 4.6428\n",
      "  Batch [30/31] D_loss: -2.8482, G_loss: -0.0487\n",
      "\n",
      "Epoch 85 Summary:\n",
      "  Average D_loss: -1.2623\n",
      "  Average G_loss: 0.8466\n",
      "\n",
      "Epoch [86/500]\n",
      "  Batch [0/31] D_loss: -3.6473, G_loss: -2.3847\n",
      "  Batch [10/31] D_loss: -2.5639, G_loss: 1.8396\n",
      "  Batch [20/31] D_loss: -2.7353, G_loss: -1.9435\n",
      "  Batch [30/31] D_loss: -1.3313, G_loss: 2.4131\n",
      "\n",
      "Epoch 86 Summary:\n",
      "  Average D_loss: -1.0879\n",
      "  Average G_loss: 0.6131\n",
      "\n",
      "Epoch [87/500]\n",
      "  Batch [0/31] D_loss: -2.1382, G_loss: -0.6739\n",
      "  Batch [10/31] D_loss: -1.9724, G_loss: -0.1035\n",
      "  Batch [20/31] D_loss: -3.1026, G_loss: 5.0335\n",
      "  Batch [30/31] D_loss: -2.2642, G_loss: -4.4258\n",
      "\n",
      "Epoch 87 Summary:\n",
      "  Average D_loss: -1.1348\n",
      "  Average G_loss: 0.3331\n",
      "\n",
      "Epoch [88/500]\n",
      "  Batch [0/31] D_loss: -2.7011, G_loss: -4.9389\n",
      "  Batch [10/31] D_loss: -2.2748, G_loss: 1.9853\n",
      "  Batch [20/31] D_loss: -2.2991, G_loss: -3.5851\n",
      "  Batch [30/31] D_loss: -2.2202, G_loss: -1.2187\n",
      "\n",
      "Epoch 88 Summary:\n",
      "  Average D_loss: -1.0945\n",
      "  Average G_loss: 0.4513\n",
      "\n",
      "Epoch [89/500]\n",
      "  Batch [0/31] D_loss: -2.4980, G_loss: -3.3337\n",
      "  Batch [10/31] D_loss: -1.1123, G_loss: 2.9490\n",
      "  Batch [20/31] D_loss: -3.1813, G_loss: -4.0351\n",
      "  Batch [30/31] D_loss: -1.8462, G_loss: 2.8054\n",
      "\n",
      "Epoch 89 Summary:\n",
      "  Average D_loss: -1.0797\n",
      "  Average G_loss: 0.6408\n",
      "\n",
      "Epoch [90/500]\n",
      "  Batch [0/31] D_loss: -2.1990, G_loss: 3.6239\n",
      "  Batch [10/31] D_loss: -1.4084, G_loss: -3.7390\n",
      "  Batch [20/31] D_loss: -1.5529, G_loss: 4.6784\n",
      "  Batch [30/31] D_loss: -1.9521, G_loss: 3.4155\n",
      "\n",
      "Epoch 90 Summary:\n",
      "  Average D_loss: -1.0135\n",
      "  Average G_loss: 0.5950\n",
      "\n",
      "Epoch [91/500]\n",
      "  Batch [0/31] D_loss: -3.6301, G_loss: 3.3705\n",
      "  Batch [10/31] D_loss: -1.6380, G_loss: 0.1578\n",
      "  Batch [20/31] D_loss: -2.0431, G_loss: 2.0596\n",
      "  Batch [30/31] D_loss: -1.1772, G_loss: -1.2600\n",
      "\n",
      "Epoch 91 Summary:\n",
      "  Average D_loss: -0.9547\n",
      "  Average G_loss: -0.8154\n",
      "\n",
      "Epoch [92/500]\n",
      "  Batch [0/31] D_loss: -2.5027, G_loss: 1.3667\n",
      "  Batch [10/31] D_loss: -1.8761, G_loss: 1.9420\n",
      "  Batch [20/31] D_loss: -1.8200, G_loss: -2.9119\n",
      "  Batch [30/31] D_loss: -4.3365, G_loss: 5.1976\n",
      "\n",
      "Epoch 92 Summary:\n",
      "  Average D_loss: -1.0985\n",
      "  Average G_loss: 1.0418\n",
      "\n",
      "Epoch [93/500]\n",
      "  Batch [0/31] D_loss: -3.3682, G_loss: 3.7568\n",
      "  Batch [10/31] D_loss: -1.6283, G_loss: 0.4954\n",
      "  Batch [20/31] D_loss: -1.4451, G_loss: 0.5726\n",
      "  Batch [30/31] D_loss: -2.1841, G_loss: -1.8259\n",
      "\n",
      "Epoch 93 Summary:\n",
      "  Average D_loss: -0.9070\n",
      "  Average G_loss: -0.2216\n",
      "\n",
      "Epoch [94/500]\n",
      "  Batch [0/31] D_loss: -1.3275, G_loss: -1.6890\n",
      "  Batch [10/31] D_loss: -3.2341, G_loss: 3.5089\n",
      "  Batch [20/31] D_loss: -2.4316, G_loss: 0.9506\n",
      "  Batch [30/31] D_loss: -2.4145, G_loss: 0.8070\n",
      "\n",
      "Epoch 94 Summary:\n",
      "  Average D_loss: -1.1305\n",
      "  Average G_loss: 0.8238\n",
      "\n",
      "Epoch [95/500]\n",
      "  Batch [0/31] D_loss: -3.2501, G_loss: -2.6567\n",
      "  Batch [10/31] D_loss: -2.5880, G_loss: 3.5275\n",
      "  Batch [20/31] D_loss: -2.5956, G_loss: 0.3998\n",
      "  Batch [30/31] D_loss: -3.2145, G_loss: 2.2196\n",
      "\n",
      "Epoch 95 Summary:\n",
      "  Average D_loss: -1.1716\n",
      "  Average G_loss: 0.1885\n",
      "\n",
      "Epoch [96/500]\n",
      "  Batch [0/31] D_loss: -2.5487, G_loss: -3.7373\n",
      "  Batch [10/31] D_loss: -2.2627, G_loss: -2.2142\n",
      "  Batch [20/31] D_loss: -3.2420, G_loss: 3.7785\n",
      "  Batch [30/31] D_loss: -2.5411, G_loss: 3.4471\n",
      "\n",
      "Epoch 96 Summary:\n",
      "  Average D_loss: -1.0481\n",
      "  Average G_loss: 0.7155\n",
      "\n",
      "Epoch [97/500]\n",
      "  Batch [0/31] D_loss: -2.7002, G_loss: -0.9418\n",
      "  Batch [10/31] D_loss: -2.4758, G_loss: 1.7631\n",
      "  Batch [20/31] D_loss: -4.2674, G_loss: 4.9918\n",
      "  Batch [30/31] D_loss: -3.7632, G_loss: -3.5730\n",
      "\n",
      "Epoch 97 Summary:\n",
      "  Average D_loss: -1.1196\n",
      "  Average G_loss: -0.2024\n",
      "\n",
      "Epoch [98/500]\n",
      "  Batch [0/31] D_loss: -1.8752, G_loss: -0.8741\n",
      "  Batch [10/31] D_loss: -1.5418, G_loss: 4.6764\n",
      "  Batch [20/31] D_loss: -3.0782, G_loss: 1.1415\n",
      "  Batch [30/31] D_loss: -3.1164, G_loss: 4.5315\n",
      "\n",
      "Epoch 98 Summary:\n",
      "  Average D_loss: -1.1053\n",
      "  Average G_loss: 0.6750\n",
      "\n",
      "Epoch [99/500]\n",
      "  Batch [0/31] D_loss: -2.7351, G_loss: 3.0093\n",
      "  Batch [10/31] D_loss: -3.2249, G_loss: -2.1591\n",
      "  Batch [20/31] D_loss: -1.9361, G_loss: 1.9358\n",
      "  Batch [30/31] D_loss: -2.0480, G_loss: 1.4060\n",
      "\n",
      "Epoch 99 Summary:\n",
      "  Average D_loss: -1.0963\n",
      "  Average G_loss: 0.2369\n",
      "\n",
      "Epoch [100/500]\n",
      "  Batch [0/31] D_loss: -1.8423, G_loss: 1.8796\n",
      "  Batch [10/31] D_loss: -1.5234, G_loss: 1.3593\n",
      "  Batch [20/31] D_loss: -3.3581, G_loss: 0.6775\n",
      "  Batch [30/31] D_loss: -3.6511, G_loss: -1.3767\n",
      "\n",
      "Epoch 100 Summary:\n",
      "  Average D_loss: -1.0809\n",
      "  Average G_loss: 1.2795\n",
      "\n",
      "Epoch [101/500]\n",
      "  Batch [0/31] D_loss: -1.5151, G_loss: -1.2968\n",
      "  Batch [10/31] D_loss: -2.6375, G_loss: 2.8162\n",
      "  Batch [20/31] D_loss: -1.2433, G_loss: 2.2778\n",
      "  Batch [30/31] D_loss: -2.1953, G_loss: 1.6928\n",
      "\n",
      "Epoch 101 Summary:\n",
      "  Average D_loss: -0.9183\n",
      "  Average G_loss: 0.3546\n",
      "\n",
      "Epoch [102/500]\n",
      "  Batch [0/31] D_loss: -2.7789, G_loss: -2.6341\n",
      "  Batch [10/31] D_loss: -1.3319, G_loss: 0.5712\n",
      "  Batch [20/31] D_loss: 0.0363, G_loss: 1.9302\n",
      "  Batch [30/31] D_loss: -2.0118, G_loss: -6.0155\n",
      "\n",
      "Epoch 102 Summary:\n",
      "  Average D_loss: -1.1273\n",
      "  Average G_loss: -0.4562\n",
      "\n",
      "Epoch [103/500]\n",
      "  Batch [0/31] D_loss: -4.7817, G_loss: -8.2586\n",
      "  Batch [10/31] D_loss: -2.3325, G_loss: -0.0117\n",
      "  Batch [20/31] D_loss: -2.8320, G_loss: 4.5082\n",
      "  Batch [30/31] D_loss: -1.0792, G_loss: -3.0425\n",
      "\n",
      "Epoch 103 Summary:\n",
      "  Average D_loss: -1.0130\n",
      "  Average G_loss: -0.0392\n",
      "\n",
      "Epoch [104/500]\n",
      "  Batch [0/31] D_loss: -3.5875, G_loss: -5.7844\n",
      "  Batch [10/31] D_loss: -0.1970, G_loss: 3.0867\n",
      "  Batch [20/31] D_loss: -1.2654, G_loss: -1.9227\n",
      "  Batch [30/31] D_loss: -3.5630, G_loss: 4.6203\n",
      "\n",
      "Epoch 104 Summary:\n",
      "  Average D_loss: -0.8810\n",
      "  Average G_loss: 0.5150\n",
      "\n",
      "Epoch [105/500]\n",
      "  Batch [0/31] D_loss: -1.6157, G_loss: 4.6322\n",
      "  Batch [10/31] D_loss: -2.1439, G_loss: -0.4722\n",
      "  Batch [20/31] D_loss: -2.4704, G_loss: -1.0471\n",
      "  Batch [30/31] D_loss: -2.5706, G_loss: -2.5914\n",
      "\n",
      "Epoch 105 Summary:\n",
      "  Average D_loss: -0.9230\n",
      "  Average G_loss: 0.7137\n",
      "\n",
      "Epoch [106/500]\n",
      "  Batch [0/31] D_loss: -2.0096, G_loss: 0.7890\n",
      "  Batch [10/31] D_loss: -2.6919, G_loss: -1.7854\n",
      "  Batch [20/31] D_loss: -2.4738, G_loss: 2.8367\n",
      "  Batch [30/31] D_loss: -3.2063, G_loss: 3.2620\n",
      "\n",
      "Epoch 106 Summary:\n",
      "  Average D_loss: -1.1736\n",
      "  Average G_loss: 0.8856\n",
      "\n",
      "Epoch [107/500]\n",
      "  Batch [0/31] D_loss: -2.9585, G_loss: 1.6674\n",
      "  Batch [10/31] D_loss: -2.3861, G_loss: 2.9987\n",
      "  Batch [20/31] D_loss: -1.5299, G_loss: 0.1470\n",
      "  Batch [30/31] D_loss: -1.5933, G_loss: -5.1160\n",
      "\n",
      "Epoch 107 Summary:\n",
      "  Average D_loss: -0.9407\n",
      "  Average G_loss: -0.2635\n",
      "\n",
      "Epoch [108/500]\n",
      "  Batch [0/31] D_loss: -0.9492, G_loss: -2.4761\n",
      "  Batch [10/31] D_loss: -2.2931, G_loss: 1.5210\n",
      "  Batch [20/31] D_loss: -2.7029, G_loss: -1.1078\n",
      "  Batch [30/31] D_loss: -1.4051, G_loss: -2.5380\n",
      "\n",
      "Epoch 108 Summary:\n",
      "  Average D_loss: -1.1103\n",
      "  Average G_loss: -0.3721\n",
      "\n",
      "Epoch [109/500]\n",
      "  Batch [0/31] D_loss: -1.6358, G_loss: 4.0377\n",
      "  Batch [10/31] D_loss: -0.1496, G_loss: 1.9655\n",
      "  Batch [20/31] D_loss: -0.4392, G_loss: -3.7612\n",
      "  Batch [30/31] D_loss: -2.7697, G_loss: 5.4418\n",
      "\n",
      "Epoch 109 Summary:\n",
      "  Average D_loss: -0.9128\n",
      "  Average G_loss: 1.7030\n",
      "\n",
      "Epoch [110/500]\n",
      "  Batch [0/31] D_loss: -1.0775, G_loss: 2.5974\n",
      "  Batch [10/31] D_loss: -2.7670, G_loss: -2.7415\n",
      "  Batch [20/31] D_loss: -2.7502, G_loss: 3.6895\n",
      "  Batch [30/31] D_loss: -2.4518, G_loss: -2.0650\n",
      "\n",
      "Epoch 110 Summary:\n",
      "  Average D_loss: -1.0039\n",
      "  Average G_loss: 0.7200\n",
      "\n",
      "Epoch [111/500]\n",
      "  Batch [0/31] D_loss: -2.8129, G_loss: -3.7902\n",
      "  Batch [10/31] D_loss: -2.9731, G_loss: 4.9856\n",
      "  Batch [20/31] D_loss: -1.8283, G_loss: -4.2929\n",
      "  Batch [30/31] D_loss: -1.9592, G_loss: -1.0757\n",
      "\n",
      "Epoch 111 Summary:\n",
      "  Average D_loss: -0.9862\n",
      "  Average G_loss: -0.8978\n",
      "\n",
      "Epoch [112/500]\n",
      "  Batch [0/31] D_loss: -1.9119, G_loss: 1.4584\n",
      "  Batch [10/31] D_loss: -1.7905, G_loss: 1.2575\n",
      "  Batch [20/31] D_loss: -3.4373, G_loss: 3.4307\n",
      "  Batch [30/31] D_loss: -1.9492, G_loss: -4.1196\n",
      "\n",
      "Epoch 112 Summary:\n",
      "  Average D_loss: -0.8862\n",
      "  Average G_loss: 0.4683\n",
      "\n",
      "Epoch [113/500]\n",
      "  Batch [0/31] D_loss: -1.6905, G_loss: -3.5877\n",
      "  Batch [10/31] D_loss: -2.6873, G_loss: 4.8419\n",
      "  Batch [20/31] D_loss: -2.9364, G_loss: 3.4996\n",
      "  Batch [30/31] D_loss: -1.6625, G_loss: -3.0556\n",
      "\n",
      "Epoch 113 Summary:\n",
      "  Average D_loss: -1.2236\n",
      "  Average G_loss: 0.1862\n",
      "\n",
      "Epoch [114/500]\n",
      "  Batch [0/31] D_loss: -2.8930, G_loss: -3.1086\n",
      "  Batch [10/31] D_loss: -1.2489, G_loss: 1.8832\n",
      "  Batch [20/31] D_loss: -2.6917, G_loss: -2.0408\n",
      "  Batch [30/31] D_loss: -3.5190, G_loss: 4.0969\n",
      "\n",
      "Epoch 114 Summary:\n",
      "  Average D_loss: -0.8187\n",
      "  Average G_loss: 0.5529\n",
      "\n",
      "Epoch [115/500]\n",
      "  Batch [0/31] D_loss: -3.1669, G_loss: 5.3562\n",
      "  Batch [10/31] D_loss: -2.7228, G_loss: 2.2166\n",
      "  Batch [20/31] D_loss: -1.0225, G_loss: -3.8743\n",
      "  Batch [30/31] D_loss: -2.6946, G_loss: 6.7431\n",
      "\n",
      "Epoch 115 Summary:\n",
      "  Average D_loss: -1.0504\n",
      "  Average G_loss: 0.6817\n",
      "\n",
      "Epoch [116/500]\n",
      "  Batch [0/31] D_loss: -1.0471, G_loss: 2.9654\n",
      "  Batch [10/31] D_loss: -2.3498, G_loss: -0.3205\n",
      "  Batch [20/31] D_loss: -2.5228, G_loss: -1.6309\n",
      "  Batch [30/31] D_loss: -2.3525, G_loss: 4.2292\n",
      "\n",
      "Epoch 116 Summary:\n",
      "  Average D_loss: -1.0582\n",
      "  Average G_loss: 0.6322\n",
      "\n",
      "Epoch [117/500]\n",
      "  Batch [0/31] D_loss: -2.1934, G_loss: 2.1104\n",
      "  Batch [10/31] D_loss: -3.5679, G_loss: 1.1597\n",
      "  Batch [20/31] D_loss: -2.3238, G_loss: 0.9138\n",
      "  Batch [30/31] D_loss: -4.7510, G_loss: -5.1838\n",
      "\n",
      "Epoch 117 Summary:\n",
      "  Average D_loss: -1.2882\n",
      "  Average G_loss: -0.8250\n",
      "\n",
      "Epoch [118/500]\n",
      "  Batch [0/31] D_loss: -1.2529, G_loss: 2.6697\n",
      "  Batch [10/31] D_loss: -0.7692, G_loss: 4.7140\n",
      "  Batch [20/31] D_loss: -1.5691, G_loss: -4.9114\n",
      "  Batch [30/31] D_loss: -1.6987, G_loss: 2.3984\n",
      "\n",
      "Epoch 118 Summary:\n",
      "  Average D_loss: -0.6991\n",
      "  Average G_loss: 0.8141\n",
      "\n",
      "Epoch [119/500]\n",
      "  Batch [0/31] D_loss: -2.0951, G_loss: 3.5768\n",
      "  Batch [10/31] D_loss: -0.5774, G_loss: 0.3257\n",
      "  Batch [20/31] D_loss: -2.2897, G_loss: -5.0492\n",
      "  Batch [30/31] D_loss: -1.1145, G_loss: -0.7185\n",
      "\n",
      "Epoch 119 Summary:\n",
      "  Average D_loss: -0.8048\n",
      "  Average G_loss: 0.6184\n",
      "\n",
      "Epoch [120/500]\n",
      "  Batch [0/31] D_loss: -3.5920, G_loss: 1.0355\n",
      "  Batch [10/31] D_loss: -4.3735, G_loss: -5.7817\n",
      "  Batch [20/31] D_loss: -1.6564, G_loss: 1.0655\n",
      "  Batch [30/31] D_loss: -2.6709, G_loss: 2.9467\n",
      "\n",
      "Epoch 120 Summary:\n",
      "  Average D_loss: -1.1678\n",
      "  Average G_loss: 0.6457\n",
      "\n",
      "Epoch [121/500]\n",
      "  Batch [0/31] D_loss: -2.2667, G_loss: 2.0715\n",
      "  Batch [10/31] D_loss: -2.3833, G_loss: -0.7952\n",
      "  Batch [20/31] D_loss: -1.1170, G_loss: 3.8701\n",
      "  Batch [30/31] D_loss: -1.9398, G_loss: -0.7930\n",
      "\n",
      "Epoch 121 Summary:\n",
      "  Average D_loss: -0.9350\n",
      "  Average G_loss: 0.1278\n",
      "\n",
      "Epoch [122/500]\n",
      "  Batch [0/31] D_loss: -3.2471, G_loss: -0.9382\n",
      "  Batch [10/31] D_loss: -1.9037, G_loss: 1.3416\n",
      "  Batch [20/31] D_loss: -2.1844, G_loss: 3.0786\n",
      "  Batch [30/31] D_loss: -2.3038, G_loss: -2.5635\n",
      "\n",
      "Epoch 122 Summary:\n",
      "  Average D_loss: -1.0505\n",
      "  Average G_loss: 0.5717\n",
      "\n",
      "Epoch [123/500]\n",
      "  Batch [0/31] D_loss: -4.0361, G_loss: -3.8510\n",
      "  Batch [10/31] D_loss: -2.0690, G_loss: 0.8072\n",
      "  Batch [20/31] D_loss: -2.0382, G_loss: 0.0981\n",
      "  Batch [30/31] D_loss: -1.5132, G_loss: 0.6994\n",
      "\n",
      "Epoch 123 Summary:\n",
      "  Average D_loss: -1.0617\n",
      "  Average G_loss: 0.2528\n",
      "\n",
      "Epoch [124/500]\n",
      "  Batch [0/31] D_loss: -2.3303, G_loss: -0.1117\n",
      "  Batch [10/31] D_loss: -2.5338, G_loss: 0.2230\n",
      "  Batch [20/31] D_loss: -2.2621, G_loss: -2.2762\n",
      "  Batch [30/31] D_loss: -2.6352, G_loss: -0.8229\n",
      "\n",
      "Epoch 124 Summary:\n",
      "  Average D_loss: -1.0033\n",
      "  Average G_loss: 0.6823\n",
      "\n",
      "Epoch [125/500]\n",
      "  Batch [0/31] D_loss: -2.2404, G_loss: -1.6313\n",
      "  Batch [10/31] D_loss: -2.7960, G_loss: -0.3182\n",
      "  Batch [20/31] D_loss: -2.3235, G_loss: 0.1930\n",
      "  Batch [30/31] D_loss: -2.2400, G_loss: -0.2493\n",
      "\n",
      "Epoch 125 Summary:\n",
      "  Average D_loss: -1.0863\n",
      "  Average G_loss: 0.4491\n",
      "\n",
      "Epoch [126/500]\n",
      "  Batch [0/31] D_loss: -2.7888, G_loss: -1.7810\n",
      "  Batch [10/31] D_loss: -2.3875, G_loss: 1.7929\n",
      "  Batch [20/31] D_loss: -1.2155, G_loss: -2.4964\n",
      "  Batch [30/31] D_loss: -3.1753, G_loss: -0.4232\n",
      "\n",
      "Epoch 126 Summary:\n",
      "  Average D_loss: -1.0957\n",
      "  Average G_loss: -0.2486\n",
      "\n",
      "Epoch [127/500]\n",
      "  Batch [0/31] D_loss: -2.1912, G_loss: 0.8766\n",
      "  Batch [10/31] D_loss: -2.5776, G_loss: -1.8158\n",
      "  Batch [20/31] D_loss: -3.0895, G_loss: 4.5118\n",
      "  Batch [30/31] D_loss: -2.8802, G_loss: 1.0142\n",
      "\n",
      "Epoch 127 Summary:\n",
      "  Average D_loss: -1.0612\n",
      "  Average G_loss: 1.3597\n",
      "\n",
      "Epoch [128/500]\n",
      "  Batch [0/31] D_loss: -3.2582, G_loss: 1.7983\n",
      "  Batch [10/31] D_loss: -2.5181, G_loss: -7.7390\n",
      "  Batch [20/31] D_loss: -2.7806, G_loss: 3.5768\n",
      "  Batch [30/31] D_loss: -1.2537, G_loss: 0.8157\n",
      "\n",
      "Epoch 128 Summary:\n",
      "  Average D_loss: -0.9980\n",
      "  Average G_loss: -0.1735\n",
      "\n",
      "Epoch [129/500]\n",
      "  Batch [0/31] D_loss: -2.0337, G_loss: 0.9785\n",
      "  Batch [10/31] D_loss: -0.5455, G_loss: -2.2209\n",
      "  Batch [20/31] D_loss: -1.3921, G_loss: 0.7557\n",
      "  Batch [30/31] D_loss: -2.0270, G_loss: 0.3927\n",
      "\n",
      "Epoch 129 Summary:\n",
      "  Average D_loss: -0.8902\n",
      "  Average G_loss: 0.2494\n",
      "\n",
      "Epoch [130/500]\n",
      "  Batch [0/31] D_loss: -2.7642, G_loss: 2.4101\n",
      "  Batch [10/31] D_loss: -2.1443, G_loss: -3.5934\n",
      "  Batch [20/31] D_loss: -3.3540, G_loss: 3.6447\n",
      "  Batch [30/31] D_loss: -2.0912, G_loss: -1.9706\n",
      "\n",
      "Epoch 130 Summary:\n",
      "  Average D_loss: -1.1269\n",
      "  Average G_loss: 0.0020\n",
      "\n",
      "Epoch [131/500]\n",
      "  Batch [0/31] D_loss: -3.8923, G_loss: -6.2318\n",
      "  Batch [10/31] D_loss: -1.2313, G_loss: 1.9436\n",
      "  Batch [20/31] D_loss: -2.2167, G_loss: -2.8924\n",
      "  Batch [30/31] D_loss: -1.3087, G_loss: -3.3333\n",
      "\n",
      "Epoch 131 Summary:\n",
      "  Average D_loss: -1.0857\n",
      "  Average G_loss: -0.1743\n",
      "\n",
      "Epoch [132/500]\n",
      "  Batch [0/31] D_loss: -2.1554, G_loss: 3.2942\n",
      "  Batch [10/31] D_loss: -1.4521, G_loss: 4.8564\n",
      "  Batch [20/31] D_loss: -1.6646, G_loss: -4.9490\n",
      "  Batch [30/31] D_loss: -2.2130, G_loss: 4.7029\n",
      "\n",
      "Epoch 132 Summary:\n",
      "  Average D_loss: -0.8685\n",
      "  Average G_loss: 0.3379\n",
      "\n",
      "Epoch [133/500]\n",
      "  Batch [0/31] D_loss: -0.2535, G_loss: 1.8183\n",
      "  Batch [10/31] D_loss: -2.2928, G_loss: 1.8257\n",
      "  Batch [20/31] D_loss: -2.5997, G_loss: 1.9026\n",
      "  Batch [30/31] D_loss: -1.1832, G_loss: -3.7928\n",
      "\n",
      "Epoch 133 Summary:\n",
      "  Average D_loss: -0.9342\n",
      "  Average G_loss: 0.0968\n",
      "\n",
      "Epoch [134/500]\n",
      "  Batch [0/31] D_loss: -3.7960, G_loss: -5.5996\n",
      "  Batch [10/31] D_loss: -1.3367, G_loss: 5.3259\n",
      "  Batch [20/31] D_loss: -2.0226, G_loss: -3.6613\n",
      "  Batch [30/31] D_loss: -1.3061, G_loss: 1.7009\n",
      "\n",
      "Epoch 134 Summary:\n",
      "  Average D_loss: -0.9145\n",
      "  Average G_loss: -0.1524\n",
      "\n",
      "Epoch [135/500]\n",
      "  Batch [0/31] D_loss: -1.9703, G_loss: 1.9606\n",
      "  Batch [10/31] D_loss: -1.9891, G_loss: 1.8947\n",
      "  Batch [20/31] D_loss: -2.9949, G_loss: 2.4905\n",
      "  Batch [30/31] D_loss: -4.4923, G_loss: -4.8666\n",
      "\n",
      "Epoch 135 Summary:\n",
      "  Average D_loss: -0.9901\n",
      "  Average G_loss: 2.0416\n",
      "\n",
      "Epoch [136/500]\n",
      "  Batch [0/31] D_loss: -2.0704, G_loss: -5.0605\n",
      "  Batch [10/31] D_loss: -1.4581, G_loss: 2.2025\n",
      "  Batch [20/31] D_loss: -1.6906, G_loss: -1.0627\n",
      "  Batch [30/31] D_loss: -3.0948, G_loss: 0.3151\n",
      "\n",
      "Epoch 136 Summary:\n",
      "  Average D_loss: -0.8918\n",
      "  Average G_loss: -0.9477\n",
      "\n",
      "Epoch [137/500]\n",
      "  Batch [0/31] D_loss: -2.7531, G_loss: 2.8042\n",
      "  Batch [10/31] D_loss: -3.1933, G_loss: 2.1519\n",
      "  Batch [20/31] D_loss: -3.0410, G_loss: -5.8278\n",
      "  Batch [30/31] D_loss: -2.5583, G_loss: 7.2545\n",
      "\n",
      "Epoch 137 Summary:\n",
      "  Average D_loss: -1.1035\n",
      "  Average G_loss: 0.3844\n",
      "\n",
      "Epoch [138/500]\n",
      "  Batch [0/31] D_loss: -1.3353, G_loss: 5.9402\n",
      "  Batch [10/31] D_loss: -0.9536, G_loss: -0.9628\n",
      "  Batch [20/31] D_loss: -1.0875, G_loss: 1.6790\n",
      "  Batch [30/31] D_loss: -1.7367, G_loss: -1.9136\n",
      "\n",
      "Epoch 138 Summary:\n",
      "  Average D_loss: -0.6728\n",
      "  Average G_loss: 0.3950\n",
      "\n",
      "Epoch [139/500]\n",
      "  Batch [0/31] D_loss: -2.0616, G_loss: -1.4746\n",
      "  Batch [10/31] D_loss: -1.3468, G_loss: 1.5548\n",
      "  Batch [20/31] D_loss: -3.5021, G_loss: -2.2445\n",
      "  Batch [30/31] D_loss: -3.0348, G_loss: 0.4981\n",
      "\n",
      "Epoch 139 Summary:\n",
      "  Average D_loss: -0.9604\n",
      "  Average G_loss: 0.3713\n",
      "\n",
      "Epoch [140/500]\n",
      "  Batch [0/31] D_loss: -2.9225, G_loss: 3.8230\n",
      "  Batch [10/31] D_loss: -1.5778, G_loss: -0.3147\n",
      "  Batch [20/31] D_loss: -0.3469, G_loss: -4.2075\n",
      "  Batch [30/31] D_loss: -2.0390, G_loss: 4.6273\n",
      "\n",
      "Epoch 140 Summary:\n",
      "  Average D_loss: -0.8269\n",
      "  Average G_loss: 0.1846\n",
      "\n",
      "Epoch [141/500]\n",
      "  Batch [0/31] D_loss: -2.1106, G_loss: 4.4185\n",
      "  Batch [10/31] D_loss: -2.6955, G_loss: -2.3371\n",
      "  Batch [20/31] D_loss: -2.6679, G_loss: 3.5902\n",
      "  Batch [30/31] D_loss: -3.1081, G_loss: -1.0706\n",
      "\n",
      "Epoch 141 Summary:\n",
      "  Average D_loss: -0.9170\n",
      "  Average G_loss: 0.8935\n",
      "\n",
      "Epoch [142/500]\n",
      "  Batch [0/31] D_loss: -3.4247, G_loss: -3.1925\n",
      "  Batch [10/31] D_loss: -3.3627, G_loss: 2.6895\n",
      "  Batch [20/31] D_loss: -2.3113, G_loss: 1.8041\n",
      "  Batch [30/31] D_loss: -2.7751, G_loss: 2.5790\n",
      "\n",
      "Epoch 142 Summary:\n",
      "  Average D_loss: -0.9735\n",
      "  Average G_loss: -0.5556\n",
      "\n",
      "Epoch [143/500]\n",
      "  Batch [0/31] D_loss: -1.4070, G_loss: -0.8570\n",
      "  Batch [10/31] D_loss: -2.8506, G_loss: -0.1804\n",
      "  Batch [20/31] D_loss: -1.6467, G_loss: 1.4570\n",
      "  Batch [30/31] D_loss: -4.6193, G_loss: 1.5876\n",
      "\n",
      "Epoch 143 Summary:\n",
      "  Average D_loss: -0.8901\n",
      "  Average G_loss: 1.2335\n",
      "\n",
      "Epoch [144/500]\n",
      "  Batch [0/31] D_loss: -5.3343, G_loss: 4.6168\n",
      "  Batch [10/31] D_loss: -2.3978, G_loss: -6.1520\n",
      "  Batch [20/31] D_loss: -2.5505, G_loss: 6.4455\n",
      "  Batch [30/31] D_loss: -0.8320, G_loss: 2.4128\n",
      "\n",
      "Epoch 144 Summary:\n",
      "  Average D_loss: -1.0918\n",
      "  Average G_loss: 0.4358\n",
      "\n",
      "Epoch [145/500]\n",
      "  Batch [0/31] D_loss: -1.9800, G_loss: 2.2740\n",
      "  Batch [10/31] D_loss: -1.5232, G_loss: -3.2083\n",
      "  Batch [20/31] D_loss: -1.7918, G_loss: 2.4230\n",
      "  Batch [30/31] D_loss: -4.1352, G_loss: 4.4073\n",
      "\n",
      "Epoch 145 Summary:\n",
      "  Average D_loss: -1.0005\n",
      "  Average G_loss: -0.1908\n",
      "\n",
      "Epoch [146/500]\n",
      "  Batch [0/31] D_loss: -2.5150, G_loss: 0.4528\n",
      "  Batch [10/31] D_loss: -3.1416, G_loss: -1.8618\n",
      "  Batch [20/31] D_loss: -4.6527, G_loss: 6.4179\n",
      "  Batch [30/31] D_loss: -3.2666, G_loss: 1.3356\n",
      "\n",
      "Epoch 146 Summary:\n",
      "  Average D_loss: -1.0572\n",
      "  Average G_loss: 0.6566\n",
      "\n",
      "Epoch [147/500]\n",
      "  Batch [0/31] D_loss: -2.4286, G_loss: 2.3925\n",
      "  Batch [10/31] D_loss: -2.2545, G_loss: -1.6725\n",
      "  Batch [20/31] D_loss: -1.6135, G_loss: 3.7232\n",
      "  Batch [30/31] D_loss: -2.8790, G_loss: -3.9335\n",
      "\n",
      "Epoch 147 Summary:\n",
      "  Average D_loss: -0.9936\n",
      "  Average G_loss: 0.0278\n",
      "\n",
      "Epoch [148/500]\n",
      "  Batch [0/31] D_loss: -2.1552, G_loss: -4.7147\n",
      "  Batch [10/31] D_loss: -2.3145, G_loss: 2.3847\n",
      "  Batch [20/31] D_loss: -1.2478, G_loss: -1.3944\n",
      "  Batch [30/31] D_loss: -1.9884, G_loss: 2.2236\n",
      "\n",
      "Epoch 148 Summary:\n",
      "  Average D_loss: -0.8876\n",
      "  Average G_loss: -0.0817\n",
      "\n",
      "Epoch [149/500]\n",
      "  Batch [0/31] D_loss: -3.5887, G_loss: 3.2651\n",
      "  Batch [10/31] D_loss: -2.4498, G_loss: 1.9617\n",
      "  Batch [20/31] D_loss: -1.0890, G_loss: -0.7803\n",
      "  Batch [30/31] D_loss: -3.0016, G_loss: 6.1421\n",
      "\n",
      "Epoch 149 Summary:\n",
      "  Average D_loss: -1.0370\n",
      "  Average G_loss: 0.1009\n",
      "\n",
      "Epoch [150/500]\n",
      "  Batch [0/31] D_loss: -2.6433, G_loss: 5.8972\n",
      "  Batch [10/31] D_loss: -2.0538, G_loss: -0.8091\n",
      "  Batch [20/31] D_loss: 0.6919, G_loss: -4.0002\n",
      "  Batch [30/31] D_loss: -2.8021, G_loss: 5.5365\n",
      "\n",
      "Epoch 150 Summary:\n",
      "  Average D_loss: -1.0228\n",
      "  Average G_loss: 0.3142\n",
      "\n",
      "Epoch [151/500]\n",
      "  Batch [0/31] D_loss: -2.4742, G_loss: 5.1301\n",
      "  Batch [10/31] D_loss: -3.4416, G_loss: -6.4625\n",
      "  Batch [20/31] D_loss: -2.0070, G_loss: 0.7864\n",
      "  Batch [30/31] D_loss: -2.6480, G_loss: -3.3200\n",
      "\n",
      "Epoch 151 Summary:\n",
      "  Average D_loss: -0.9661\n",
      "  Average G_loss: 0.4051\n",
      "\n",
      "Epoch [152/500]\n",
      "  Batch [0/31] D_loss: -3.3475, G_loss: -5.3394\n",
      "  Batch [10/31] D_loss: -3.1914, G_loss: 2.9633\n",
      "  Batch [20/31] D_loss: -1.7270, G_loss: -0.0831\n",
      "  Batch [30/31] D_loss: -1.8619, G_loss: -3.1900\n",
      "\n",
      "Epoch 152 Summary:\n",
      "  Average D_loss: -0.9093\n",
      "  Average G_loss: 0.1859\n",
      "\n",
      "Epoch [153/500]\n",
      "  Batch [0/31] D_loss: -2.4208, G_loss: -0.4389\n",
      "  Batch [10/31] D_loss: -2.3523, G_loss: -3.7917\n",
      "  Batch [20/31] D_loss: -2.3254, G_loss: 4.0800\n",
      "  Batch [30/31] D_loss: -1.4788, G_loss: -0.3481\n",
      "\n",
      "Epoch 153 Summary:\n",
      "  Average D_loss: -1.0545\n",
      "  Average G_loss: 0.1060\n",
      "\n",
      "Epoch [154/500]\n",
      "  Batch [0/31] D_loss: -1.9986, G_loss: 1.7623\n",
      "  Batch [10/31] D_loss: -2.2412, G_loss: 1.8547\n",
      "  Batch [20/31] D_loss: -2.9444, G_loss: 2.0274\n",
      "  Batch [30/31] D_loss: -4.5165, G_loss: 4.1003\n",
      "\n",
      "Epoch 154 Summary:\n",
      "  Average D_loss: -0.9855\n",
      "  Average G_loss: 0.8809\n",
      "\n",
      "Epoch [155/500]\n",
      "  Batch [0/31] D_loss: -3.8210, G_loss: 5.7408\n",
      "  Batch [10/31] D_loss: -3.3030, G_loss: -1.7336\n",
      "  Batch [20/31] D_loss: -3.3771, G_loss: 4.9198\n",
      "  Batch [30/31] D_loss: -2.8581, G_loss: 0.4070\n",
      "\n",
      "Epoch 155 Summary:\n",
      "  Average D_loss: -1.0801\n",
      "  Average G_loss: 0.2022\n",
      "\n",
      "Epoch [156/500]\n",
      "  Batch [0/31] D_loss: -2.2278, G_loss: 0.5814\n",
      "  Batch [10/31] D_loss: -2.1528, G_loss: -0.6823\n",
      "  Batch [20/31] D_loss: -0.8369, G_loss: -3.1646\n",
      "  Batch [30/31] D_loss: -1.8035, G_loss: 1.2690\n",
      "\n",
      "Epoch 156 Summary:\n",
      "  Average D_loss: -1.0578\n",
      "  Average G_loss: 0.2901\n",
      "\n",
      "Epoch [157/500]\n",
      "  Batch [0/31] D_loss: -3.0304, G_loss: 2.3133\n",
      "  Batch [10/31] D_loss: -2.5956, G_loss: -1.0291\n",
      "  Batch [20/31] D_loss: -1.8977, G_loss: -2.3617\n",
      "  Batch [30/31] D_loss: -2.5423, G_loss: -2.1471\n",
      "\n",
      "Epoch 157 Summary:\n",
      "  Average D_loss: -1.0792\n",
      "  Average G_loss: 0.0549\n",
      "\n",
      "Epoch [158/500]\n",
      "  Batch [0/31] D_loss: -2.8684, G_loss: 1.8488\n",
      "  Batch [10/31] D_loss: -2.2286, G_loss: 2.6882\n",
      "  Batch [20/31] D_loss: -2.7955, G_loss: 1.8788\n",
      "  Batch [30/31] D_loss: -2.7270, G_loss: -2.0021\n",
      "\n",
      "Epoch 158 Summary:\n",
      "  Average D_loss: -1.0537\n",
      "  Average G_loss: 0.3422\n",
      "\n",
      "Epoch [159/500]\n",
      "  Batch [0/31] D_loss: -2.3820, G_loss: -1.2206\n",
      "  Batch [10/31] D_loss: -2.3422, G_loss: 1.6951\n",
      "  Batch [20/31] D_loss: -2.9250, G_loss: -0.4562\n",
      "  Batch [30/31] D_loss: -3.0443, G_loss: 3.4890\n",
      "\n",
      "Epoch 159 Summary:\n",
      "  Average D_loss: -1.0320\n",
      "  Average G_loss: -0.1264\n",
      "\n",
      "Epoch [160/500]\n",
      "  Batch [0/31] D_loss: -2.3860, G_loss: 3.5382\n",
      "  Batch [10/31] D_loss: -1.8600, G_loss: -2.1980\n",
      "  Batch [20/31] D_loss: -3.7225, G_loss: -1.5222\n",
      "  Batch [30/31] D_loss: -3.2972, G_loss: -4.9964\n",
      "\n",
      "Epoch 160 Summary:\n",
      "  Average D_loss: -1.2276\n",
      "  Average G_loss: 0.2514\n",
      "\n",
      "Epoch [161/500]\n",
      "  Batch [0/31] D_loss: -1.1906, G_loss: -4.1287\n",
      "  Batch [10/31] D_loss: -1.7267, G_loss: 2.5300\n",
      "  Batch [20/31] D_loss: -2.4214, G_loss: -4.3297\n",
      "  Batch [30/31] D_loss: -2.6642, G_loss: 3.0421\n",
      "\n",
      "Epoch 161 Summary:\n",
      "  Average D_loss: -0.8348\n",
      "  Average G_loss: 1.0104\n",
      "\n",
      "Epoch [162/500]\n",
      "  Batch [0/31] D_loss: -1.8392, G_loss: 2.2780\n",
      "  Batch [10/31] D_loss: -2.3512, G_loss: -0.6650\n",
      "  Batch [20/31] D_loss: -1.8952, G_loss: -1.7383\n",
      "  Batch [30/31] D_loss: -2.2840, G_loss: -0.6308\n",
      "\n",
      "Epoch 162 Summary:\n",
      "  Average D_loss: -1.1094\n",
      "  Average G_loss: -0.1803\n",
      "\n",
      "Epoch [163/500]\n",
      "  Batch [0/31] D_loss: -1.4478, G_loss: 0.0293\n",
      "  Batch [10/31] D_loss: -2.6262, G_loss: 3.0920\n",
      "  Batch [20/31] D_loss: -1.8106, G_loss: -1.2893\n",
      "  Batch [30/31] D_loss: -4.3854, G_loss: 4.8812\n",
      "\n",
      "Epoch 163 Summary:\n",
      "  Average D_loss: -1.0074\n",
      "  Average G_loss: 0.8005\n",
      "\n",
      "Epoch [164/500]\n",
      "  Batch [0/31] D_loss: -1.9992, G_loss: 0.1230\n",
      "  Batch [10/31] D_loss: -2.3822, G_loss: -0.4226\n",
      "  Batch [20/31] D_loss: -2.5625, G_loss: 1.2577\n",
      "  Batch [30/31] D_loss: -2.2624, G_loss: -0.5568\n",
      "\n",
      "Epoch 164 Summary:\n",
      "  Average D_loss: -0.9709\n",
      "  Average G_loss: 0.3837\n",
      "\n",
      "Epoch [165/500]\n",
      "  Batch [0/31] D_loss: -2.3304, G_loss: -1.5527\n",
      "  Batch [10/31] D_loss: -2.6760, G_loss: -1.1471\n",
      "  Batch [20/31] D_loss: -2.5625, G_loss: -1.1680\n",
      "  Batch [30/31] D_loss: -2.0815, G_loss: 0.1960\n",
      "\n",
      "Epoch 165 Summary:\n",
      "  Average D_loss: -1.1512\n",
      "  Average G_loss: -0.1153\n",
      "\n",
      "Epoch [166/500]\n",
      "  Batch [0/31] D_loss: -2.7755, G_loss: -0.5318\n",
      "  Batch [10/31] D_loss: -2.6993, G_loss: -3.5656\n",
      "  Batch [20/31] D_loss: -2.3736, G_loss: -0.0080\n",
      "  Batch [30/31] D_loss: -2.2051, G_loss: 2.8797\n",
      "\n",
      "Epoch 166 Summary:\n",
      "  Average D_loss: -0.8647\n",
      "  Average G_loss: 0.2022\n",
      "\n",
      "Epoch [167/500]\n",
      "  Batch [0/31] D_loss: -1.3444, G_loss: 1.3982\n",
      "  Batch [10/31] D_loss: -2.5332, G_loss: 2.1483\n",
      "  Batch [20/31] D_loss: -3.6356, G_loss: 3.5957\n",
      "  Batch [30/31] D_loss: -2.2975, G_loss: 1.5997\n",
      "\n",
      "Epoch 167 Summary:\n",
      "  Average D_loss: -1.0339\n",
      "  Average G_loss: 0.4943\n",
      "\n",
      "Epoch [168/500]\n",
      "  Batch [0/31] D_loss: -1.9241, G_loss: -0.5724\n",
      "  Batch [10/31] D_loss: -2.0252, G_loss: -2.2921\n",
      "  Batch [20/31] D_loss: -3.2279, G_loss: 3.0228\n",
      "  Batch [30/31] D_loss: -2.7917, G_loss: -0.8970\n",
      "\n",
      "Epoch 168 Summary:\n",
      "  Average D_loss: -1.0441\n",
      "  Average G_loss: 0.6283\n",
      "\n",
      "Epoch [169/500]\n",
      "  Batch [0/31] D_loss: -2.0614, G_loss: 0.7445\n",
      "  Batch [10/31] D_loss: -2.9060, G_loss: -3.3773\n",
      "  Batch [20/31] D_loss: -2.2443, G_loss: -0.3513\n",
      "  Batch [30/31] D_loss: -2.3644, G_loss: 1.5491\n",
      "\n",
      "Epoch 169 Summary:\n",
      "  Average D_loss: -1.0651\n",
      "  Average G_loss: -0.2313\n",
      "\n",
      "Epoch [170/500]\n",
      "  Batch [0/31] D_loss: -2.7376, G_loss: 2.3746\n",
      "  Batch [10/31] D_loss: -1.5100, G_loss: -0.5773\n",
      "  Batch [20/31] D_loss: -1.7696, G_loss: 1.3400\n",
      "  Batch [30/31] D_loss: -2.6438, G_loss: 0.5772\n",
      "\n",
      "Epoch 170 Summary:\n",
      "  Average D_loss: -1.1760\n",
      "  Average G_loss: 0.3690\n",
      "\n",
      "Epoch [171/500]\n",
      "  Batch [0/31] D_loss: -2.6235, G_loss: 1.9484\n",
      "  Batch [10/31] D_loss: -2.3385, G_loss: -0.6570\n",
      "  Batch [20/31] D_loss: -3.3182, G_loss: -3.2897\n",
      "  Batch [30/31] D_loss: -2.7978, G_loss: 3.2930\n",
      "\n",
      "Epoch 171 Summary:\n",
      "  Average D_loss: -1.1235\n",
      "  Average G_loss: -0.2412\n",
      "\n",
      "Epoch [172/500]\n",
      "  Batch [0/31] D_loss: -2.0316, G_loss: 3.9308\n",
      "  Batch [10/31] D_loss: -3.2122, G_loss: -6.7700\n",
      "  Batch [20/31] D_loss: -2.2794, G_loss: 2.3294\n",
      "  Batch [30/31] D_loss: -2.1620, G_loss: 0.3487\n",
      "\n",
      "Epoch 172 Summary:\n",
      "  Average D_loss: -1.0511\n",
      "  Average G_loss: 0.4322\n",
      "\n",
      "Epoch [173/500]\n",
      "  Batch [0/31] D_loss: -3.6223, G_loss: 1.5206\n",
      "  Batch [10/31] D_loss: -1.0696, G_loss: -4.2214\n",
      "  Batch [20/31] D_loss: -1.4139, G_loss: -0.3491\n",
      "  Batch [30/31] D_loss: -1.7488, G_loss: -1.0883\n",
      "\n",
      "Epoch 173 Summary:\n",
      "  Average D_loss: -1.0764\n",
      "  Average G_loss: -0.4842\n",
      "\n",
      "Epoch [174/500]\n",
      "  Batch [0/31] D_loss: -2.5717, G_loss: 1.5911\n",
      "  Batch [10/31] D_loss: -1.7376, G_loss: 1.2285\n",
      "  Batch [20/31] D_loss: -1.5905, G_loss: -0.8194\n",
      "  Batch [30/31] D_loss: -2.8382, G_loss: -0.8522\n",
      "\n",
      "Epoch 174 Summary:\n",
      "  Average D_loss: -0.9590\n",
      "  Average G_loss: 0.5379\n",
      "\n",
      "Epoch [175/500]\n",
      "  Batch [0/31] D_loss: -1.8535, G_loss: 0.1563\n",
      "  Batch [10/31] D_loss: -2.4592, G_loss: -1.9340\n",
      "  Batch [20/31] D_loss: -1.8281, G_loss: 3.3206\n",
      "  Batch [30/31] D_loss: -3.4197, G_loss: -3.3245\n",
      "\n",
      "Epoch 175 Summary:\n",
      "  Average D_loss: -1.0989\n",
      "  Average G_loss: 0.5429\n",
      "\n",
      "Epoch [176/500]\n",
      "  Batch [0/31] D_loss: -2.9071, G_loss: -4.5614\n",
      "  Batch [10/31] D_loss: -6.0098, G_loss: 8.8009\n",
      "  Batch [20/31] D_loss: -2.2001, G_loss: 2.9999\n",
      "  Batch [30/31] D_loss: -2.5701, G_loss: -7.2477\n",
      "\n",
      "Epoch 176 Summary:\n",
      "  Average D_loss: -1.4331\n",
      "  Average G_loss: 0.1326\n",
      "\n",
      "Epoch [177/500]\n",
      "  Batch [0/31] D_loss: -0.0382, G_loss: -2.6616\n",
      "  Batch [10/31] D_loss: -2.1616, G_loss: 3.2184\n",
      "  Batch [20/31] D_loss: -2.4043, G_loss: 3.4658\n",
      "  Batch [30/31] D_loss: -0.4253, G_loss: -1.0483\n",
      "\n",
      "Epoch 177 Summary:\n",
      "  Average D_loss: -1.2445\n",
      "  Average G_loss: -0.3233\n",
      "\n",
      "Epoch [178/500]\n",
      "  Batch [0/31] D_loss: -1.7306, G_loss: -0.2124\n",
      "  Batch [10/31] D_loss: -1.2572, G_loss: 3.9247\n",
      "  Batch [20/31] D_loss: -2.8818, G_loss: -4.8113\n",
      "  Batch [30/31] D_loss: -1.9556, G_loss: 0.3890\n",
      "\n",
      "Epoch 178 Summary:\n",
      "  Average D_loss: -0.8084\n",
      "  Average G_loss: -0.3571\n",
      "\n",
      "Epoch [179/500]\n",
      "  Batch [0/31] D_loss: -1.6932, G_loss: 0.6504\n",
      "  Batch [10/31] D_loss: -1.9058, G_loss: 3.7725\n",
      "  Batch [20/31] D_loss: -2.2200, G_loss: 0.0447\n",
      "  Batch [30/31] D_loss: -3.1818, G_loss: 3.6256\n",
      "\n",
      "Epoch 179 Summary:\n",
      "  Average D_loss: -0.9385\n",
      "  Average G_loss: 0.7132\n",
      "\n",
      "Epoch [180/500]\n",
      "  Batch [0/31] D_loss: -3.5228, G_loss: 4.5220\n",
      "  Batch [10/31] D_loss: -1.9315, G_loss: 0.5752\n",
      "  Batch [20/31] D_loss: -2.8387, G_loss: -5.5089\n",
      "  Batch [30/31] D_loss: -2.3287, G_loss: 3.0952\n",
      "\n",
      "Epoch 180 Summary:\n",
      "  Average D_loss: -1.1172\n",
      "  Average G_loss: -0.1633\n",
      "\n",
      "Epoch [181/500]\n",
      "  Batch [0/31] D_loss: -2.4267, G_loss: 3.5168\n",
      "  Batch [10/31] D_loss: -2.2983, G_loss: 5.2720\n",
      "  Batch [20/31] D_loss: -3.7035, G_loss: -3.6851\n",
      "  Batch [30/31] D_loss: -3.5131, G_loss: -0.0317\n",
      "\n",
      "Epoch 181 Summary:\n",
      "  Average D_loss: -1.1187\n",
      "  Average G_loss: 0.2413\n",
      "\n",
      "Epoch [182/500]\n",
      "  Batch [0/31] D_loss: -4.2167, G_loss: 6.0193\n",
      "  Batch [10/31] D_loss: -1.6690, G_loss: 0.4326\n",
      "  Batch [20/31] D_loss: -1.5896, G_loss: -4.1028\n",
      "  Batch [30/31] D_loss: -2.4723, G_loss: 4.9533\n",
      "\n",
      "Epoch 182 Summary:\n",
      "  Average D_loss: -0.9962\n",
      "  Average G_loss: 0.8699\n",
      "\n",
      "Epoch [183/500]\n",
      "  Batch [0/31] D_loss: -3.9495, G_loss: 5.6396\n",
      "  Batch [10/31] D_loss: -1.6284, G_loss: -1.5490\n",
      "  Batch [20/31] D_loss: -1.9964, G_loss: -0.5607\n",
      "  Batch [30/31] D_loss: -2.2144, G_loss: -0.8881\n",
      "\n",
      "Epoch 183 Summary:\n",
      "  Average D_loss: -1.0922\n",
      "  Average G_loss: 0.5700\n",
      "\n",
      "Epoch [184/500]\n",
      "  Batch [0/31] D_loss: -2.2517, G_loss: -1.9620\n",
      "  Batch [10/31] D_loss: -2.1166, G_loss: -2.0835\n",
      "  Batch [20/31] D_loss: -1.0574, G_loss: 0.8256\n",
      "  Batch [30/31] D_loss: -1.4462, G_loss: -1.7938\n",
      "\n",
      "Epoch 184 Summary:\n",
      "  Average D_loss: -1.0257\n",
      "  Average G_loss: 0.2997\n",
      "\n",
      "Epoch [185/500]\n",
      "  Batch [0/31] D_loss: -0.9802, G_loss: -1.2584\n",
      "  Batch [10/31] D_loss: -1.9446, G_loss: 2.7536\n",
      "  Batch [20/31] D_loss: -2.6827, G_loss: -5.4035\n",
      "  Batch [30/31] D_loss: -3.5585, G_loss: 7.9794\n",
      "\n",
      "Epoch 185 Summary:\n",
      "  Average D_loss: -1.1839\n",
      "  Average G_loss: 0.0199\n",
      "\n",
      "Epoch [186/500]\n",
      "  Batch [0/31] D_loss: -3.8823, G_loss: 8.3818\n",
      "  Batch [10/31] D_loss: -1.6378, G_loss: -1.9714\n",
      "  Batch [20/31] D_loss: -1.7867, G_loss: -1.4930\n",
      "  Batch [30/31] D_loss: -0.8680, G_loss: 0.9820\n",
      "\n",
      "Epoch 186 Summary:\n",
      "  Average D_loss: -0.7808\n",
      "  Average G_loss: 1.2901\n",
      "\n",
      "Epoch [187/500]\n",
      "  Batch [0/31] D_loss: -1.6176, G_loss: 1.5297\n",
      "  Batch [10/31] D_loss: -2.0619, G_loss: -5.2508\n",
      "  Batch [20/31] D_loss: -2.0380, G_loss: 1.4003\n",
      "  Batch [30/31] D_loss: -1.9630, G_loss: 2.4400\n",
      "\n",
      "Epoch 187 Summary:\n",
      "  Average D_loss: -0.8919\n",
      "  Average G_loss: -0.3733\n",
      "\n",
      "Epoch [188/500]\n",
      "  Batch [0/31] D_loss: -2.0004, G_loss: 0.3499\n",
      "  Batch [10/31] D_loss: -2.7842, G_loss: 2.6876\n",
      "  Batch [20/31] D_loss: -1.1768, G_loss: 1.0786\n",
      "  Batch [30/31] D_loss: -3.0027, G_loss: 3.0102\n",
      "\n",
      "Epoch 188 Summary:\n",
      "  Average D_loss: -0.9307\n",
      "  Average G_loss: 0.2168\n",
      "\n",
      "Epoch [189/500]\n",
      "  Batch [0/31] D_loss: -2.8418, G_loss: 0.4061\n",
      "  Batch [10/31] D_loss: -2.6407, G_loss: 0.4493\n",
      "  Batch [20/31] D_loss: -2.5549, G_loss: -2.8832\n",
      "  Batch [30/31] D_loss: -2.3363, G_loss: 3.4711\n",
      "\n",
      "Epoch 189 Summary:\n",
      "  Average D_loss: -1.1071\n",
      "  Average G_loss: 0.3211\n",
      "\n",
      "Epoch [190/500]\n",
      "  Batch [0/31] D_loss: -2.1567, G_loss: -1.6821\n",
      "  Batch [10/31] D_loss: -3.5256, G_loss: 3.3684\n",
      "  Batch [20/31] D_loss: -2.8345, G_loss: -5.1130\n",
      "  Batch [30/31] D_loss: -5.4402, G_loss: 9.4559\n",
      "\n",
      "Epoch 190 Summary:\n",
      "  Average D_loss: -1.3064\n",
      "  Average G_loss: -0.2610\n",
      "\n",
      "Epoch [191/500]\n",
      "  Batch [0/31] D_loss: -4.3800, G_loss: 9.0822\n",
      "  Batch [10/31] D_loss: -4.1124, G_loss: -8.3160\n",
      "  Batch [20/31] D_loss: -2.2596, G_loss: 1.5441\n",
      "  Batch [30/31] D_loss: -3.0045, G_loss: -2.9240\n",
      "\n",
      "Epoch 191 Summary:\n",
      "  Average D_loss: -0.9341\n",
      "  Average G_loss: 0.5263\n",
      "\n",
      "Epoch [192/500]\n",
      "  Batch [0/31] D_loss: -1.4328, G_loss: -2.9468\n",
      "  Batch [10/31] D_loss: -3.0816, G_loss: -3.7828\n",
      "  Batch [20/31] D_loss: -1.7748, G_loss: 0.4744\n",
      "  Batch [30/31] D_loss: -2.2407, G_loss: 2.4488\n",
      "\n",
      "Epoch 192 Summary:\n",
      "  Average D_loss: -0.9303\n",
      "  Average G_loss: 0.4122\n",
      "\n",
      "Epoch [193/500]\n",
      "  Batch [0/31] D_loss: -1.8900, G_loss: 1.7661\n",
      "  Batch [10/31] D_loss: -0.9732, G_loss: -4.7818\n",
      "  Batch [20/31] D_loss: -2.4443, G_loss: 3.6424\n",
      "  Batch [30/31] D_loss: -2.4005, G_loss: 1.8070\n",
      "\n",
      "Epoch 193 Summary:\n",
      "  Average D_loss: -0.9886\n",
      "  Average G_loss: 0.1991\n",
      "\n",
      "Epoch [194/500]\n",
      "  Batch [0/31] D_loss: -2.0118, G_loss: 1.4437\n",
      "  Batch [10/31] D_loss: -1.2847, G_loss: -3.0851\n",
      "  Batch [20/31] D_loss: -0.1141, G_loss: -1.0118\n",
      "  Batch [30/31] D_loss: -3.4534, G_loss: 6.5287\n",
      "\n",
      "Epoch 194 Summary:\n",
      "  Average D_loss: -1.0669\n",
      "  Average G_loss: -0.4586\n",
      "\n",
      "Epoch [195/500]\n",
      "  Batch [0/31] D_loss: -3.0015, G_loss: 7.6602\n",
      "  Batch [10/31] D_loss: -2.1918, G_loss: -1.3217\n",
      "  Batch [20/31] D_loss: -2.4786, G_loss: -1.7290\n",
      "  Batch [30/31] D_loss: -3.5558, G_loss: 5.6912\n",
      "\n",
      "Epoch 195 Summary:\n",
      "  Average D_loss: -1.0682\n",
      "  Average G_loss: -0.0745\n",
      "\n",
      "Epoch [196/500]\n",
      "  Batch [0/31] D_loss: -4.0232, G_loss: 7.1836\n",
      "  Batch [10/31] D_loss: -1.8832, G_loss: 2.6768\n",
      "  Batch [20/31] D_loss: -2.6571, G_loss: -2.2135\n",
      "  Batch [30/31] D_loss: -3.3722, G_loss: 0.6477\n",
      "\n",
      "Epoch 196 Summary:\n",
      "  Average D_loss: -1.2510\n",
      "  Average G_loss: 0.1898\n",
      "\n",
      "Epoch [197/500]\n",
      "  Batch [0/31] D_loss: -3.8475, G_loss: 1.5658\n",
      "  Batch [10/31] D_loss: -3.9173, G_loss: -3.4575\n",
      "  Batch [20/31] D_loss: -3.9597, G_loss: 4.7154\n",
      "  Batch [30/31] D_loss: -1.9629, G_loss: -2.6821\n",
      "\n",
      "Epoch 197 Summary:\n",
      "  Average D_loss: -1.2479\n",
      "  Average G_loss: 0.5924\n",
      "\n",
      "Epoch [198/500]\n",
      "  Batch [0/31] D_loss: -1.4401, G_loss: -1.4560\n",
      "  Batch [10/31] D_loss: -2.5687, G_loss: -2.2914\n",
      "  Batch [20/31] D_loss: -1.1170, G_loss: 0.7847\n",
      "  Batch [30/31] D_loss: -3.1921, G_loss: -2.7217\n",
      "\n",
      "Epoch 198 Summary:\n",
      "  Average D_loss: -0.9863\n",
      "  Average G_loss: 0.6265\n",
      "\n",
      "Epoch [199/500]\n",
      "  Batch [0/31] D_loss: -3.1097, G_loss: -1.8757\n",
      "  Batch [10/31] D_loss: -2.8355, G_loss: -2.7363\n",
      "  Batch [20/31] D_loss: -2.0996, G_loss: -2.9873\n",
      "  Batch [30/31] D_loss: -2.2022, G_loss: 1.4157\n",
      "\n",
      "Epoch 199 Summary:\n",
      "  Average D_loss: -1.3396\n",
      "  Average G_loss: -1.3409\n",
      "\n",
      "Epoch [200/500]\n",
      "  Batch [0/31] D_loss: -3.2791, G_loss: -0.0884\n",
      "  Batch [10/31] D_loss: -1.1094, G_loss: 3.2675\n",
      "  Batch [20/31] D_loss: -3.4862, G_loss: -7.5522\n",
      "  Batch [30/31] D_loss: -0.8659, G_loss: 2.6218\n",
      "\n",
      "Epoch 200 Summary:\n",
      "  Average D_loss: -1.1275\n",
      "  Average G_loss: 0.2785\n",
      "\n",
      "Epoch [201/500]\n",
      "  Batch [0/31] D_loss: -3.4655, G_loss: 5.7039\n",
      "  Batch [10/31] D_loss: -1.9781, G_loss: 2.0169\n",
      "  Batch [20/31] D_loss: -1.9408, G_loss: -3.2499\n",
      "  Batch [30/31] D_loss: -3.3558, G_loss: 2.7254\n",
      "\n",
      "Epoch 201 Summary:\n",
      "  Average D_loss: -0.9921\n",
      "  Average G_loss: -0.0280\n",
      "\n",
      "Epoch [202/500]\n",
      "  Batch [0/31] D_loss: -2.8381, G_loss: -0.7147\n",
      "  Batch [10/31] D_loss: -2.2964, G_loss: 2.5839\n",
      "  Batch [20/31] D_loss: -3.1623, G_loss: -5.0914\n",
      "  Batch [30/31] D_loss: -3.8717, G_loss: 4.7935\n",
      "\n",
      "Epoch 202 Summary:\n",
      "  Average D_loss: -1.2482\n",
      "  Average G_loss: 0.9400\n",
      "\n",
      "Epoch [203/500]\n",
      "  Batch [0/31] D_loss: -3.6132, G_loss: 4.5091\n",
      "  Batch [10/31] D_loss: -2.5279, G_loss: -0.9630\n",
      "  Batch [20/31] D_loss: -1.7180, G_loss: -0.5590\n",
      "  Batch [30/31] D_loss: -3.3074, G_loss: 4.2240\n",
      "\n",
      "Epoch 203 Summary:\n",
      "  Average D_loss: -1.1671\n",
      "  Average G_loss: 0.3823\n",
      "\n",
      "Epoch [204/500]\n",
      "  Batch [0/31] D_loss: -1.9086, G_loss: 3.9283\n",
      "  Batch [10/31] D_loss: -3.3637, G_loss: 3.7899\n",
      "  Batch [20/31] D_loss: -2.3234, G_loss: -0.0583\n",
      "  Batch [30/31] D_loss: -1.9158, G_loss: 0.5070\n",
      "\n",
      "Epoch 204 Summary:\n",
      "  Average D_loss: -0.9668\n",
      "  Average G_loss: 0.6874\n",
      "\n",
      "Epoch [205/500]\n",
      "  Batch [0/31] D_loss: -2.9688, G_loss: 4.0310\n",
      "  Batch [10/31] D_loss: -2.3362, G_loss: 0.0349\n",
      "  Batch [20/31] D_loss: -3.5366, G_loss: 2.9788\n",
      "  Batch [30/31] D_loss: -2.7293, G_loss: -2.8055\n",
      "\n",
      "Epoch 205 Summary:\n",
      "  Average D_loss: -1.2194\n",
      "  Average G_loss: 0.1046\n",
      "\n",
      "Epoch [206/500]\n",
      "  Batch [0/31] D_loss: -2.6936, G_loss: -1.9447\n",
      "  Batch [10/31] D_loss: -1.8313, G_loss: -0.6104\n",
      "  Batch [20/31] D_loss: -1.6146, G_loss: -4.0682\n",
      "  Batch [30/31] D_loss: -0.9990, G_loss: -0.8317\n",
      "\n",
      "Epoch 206 Summary:\n",
      "  Average D_loss: -0.7992\n",
      "  Average G_loss: -0.8952\n",
      "\n",
      "Epoch [207/500]\n",
      "  Batch [0/31] D_loss: -1.8553, G_loss: 0.2644\n",
      "  Batch [10/31] D_loss: -2.1141, G_loss: 3.5925\n",
      "  Batch [20/31] D_loss: -2.0608, G_loss: -2.4849\n",
      "  Batch [30/31] D_loss: -3.7052, G_loss: 4.8274\n",
      "\n",
      "Epoch 207 Summary:\n",
      "  Average D_loss: -1.0607\n",
      "  Average G_loss: 1.4166\n",
      "\n",
      "Epoch [208/500]\n",
      "  Batch [0/31] D_loss: -2.3264, G_loss: 0.0575\n",
      "  Batch [10/31] D_loss: -3.2582, G_loss: -0.8213\n",
      "  Batch [20/31] D_loss: -1.9768, G_loss: 0.1850\n",
      "  Batch [30/31] D_loss: -3.4714, G_loss: 2.4019\n",
      "\n",
      "Epoch 208 Summary:\n",
      "  Average D_loss: -1.1230\n",
      "  Average G_loss: 0.0855\n",
      "\n",
      "Epoch [209/500]\n",
      "  Batch [0/31] D_loss: -2.7492, G_loss: 3.7782\n",
      "  Batch [10/31] D_loss: -5.2543, G_loss: -0.7479\n",
      "  Batch [20/31] D_loss: -2.6453, G_loss: -2.2599\n",
      "  Batch [30/31] D_loss: -1.9026, G_loss: 0.9588\n",
      "\n",
      "Epoch 209 Summary:\n",
      "  Average D_loss: -1.1693\n",
      "  Average G_loss: -0.0915\n",
      "\n",
      "Epoch [210/500]\n",
      "  Batch [0/31] D_loss: -3.2447, G_loss: 0.7691\n",
      "  Batch [10/31] D_loss: -3.0384, G_loss: -2.5271\n",
      "  Batch [20/31] D_loss: -2.2858, G_loss: 0.0591\n",
      "  Batch [30/31] D_loss: -1.5994, G_loss: -3.0882\n",
      "\n",
      "Epoch 210 Summary:\n",
      "  Average D_loss: -1.1540\n",
      "  Average G_loss: -1.0994\n",
      "\n",
      "Epoch [211/500]\n",
      "  Batch [0/31] D_loss: -1.7309, G_loss: 0.1883\n",
      "  Batch [10/31] D_loss: -2.2496, G_loss: 5.1189\n",
      "  Batch [20/31] D_loss: -3.6305, G_loss: 4.8233\n",
      "  Batch [30/31] D_loss: -2.5892, G_loss: -0.1721\n",
      "\n",
      "Epoch 211 Summary:\n",
      "  Average D_loss: -0.8992\n",
      "  Average G_loss: 1.2109\n",
      "\n",
      "Epoch [212/500]\n",
      "  Batch [0/31] D_loss: -3.1628, G_loss: 1.2053\n",
      "  Batch [10/31] D_loss: -2.9203, G_loss: -1.4239\n",
      "  Batch [20/31] D_loss: -1.2948, G_loss: 2.9572\n",
      "  Batch [30/31] D_loss: -1.7493, G_loss: -3.0593\n",
      "\n",
      "Epoch 212 Summary:\n",
      "  Average D_loss: -1.1439\n",
      "  Average G_loss: 0.5117\n",
      "\n",
      "Epoch [213/500]\n",
      "  Batch [0/31] D_loss: -1.0309, G_loss: -2.7908\n",
      "  Batch [10/31] D_loss: -3.0652, G_loss: -6.5602\n",
      "  Batch [20/31] D_loss: -4.0419, G_loss: 4.4817\n",
      "  Batch [30/31] D_loss: -2.4481, G_loss: -2.2304\n",
      "\n",
      "Epoch 213 Summary:\n",
      "  Average D_loss: -1.4260\n",
      "  Average G_loss: 0.3332\n",
      "\n",
      "Epoch [214/500]\n",
      "  Batch [0/31] D_loss: -2.1754, G_loss: -2.8169\n",
      "  Batch [10/31] D_loss: -2.3665, G_loss: 1.8776\n",
      "  Batch [20/31] D_loss: -1.8338, G_loss: -2.3738\n",
      "  Batch [30/31] D_loss: -1.6016, G_loss: 3.2309\n",
      "\n",
      "Epoch 214 Summary:\n",
      "  Average D_loss: -1.1401\n",
      "  Average G_loss: -0.3443\n",
      "\n",
      "Epoch [215/500]\n",
      "  Batch [0/31] D_loss: -1.5985, G_loss: 2.8373\n",
      "  Batch [10/31] D_loss: -2.2295, G_loss: 1.8638\n",
      "  Batch [20/31] D_loss: -2.5830, G_loss: -1.2417\n",
      "  Batch [30/31] D_loss: -0.7793, G_loss: -3.7227\n",
      "\n",
      "Epoch 215 Summary:\n",
      "  Average D_loss: -1.0795\n",
      "  Average G_loss: 0.2201\n",
      "\n",
      "Epoch [216/500]\n",
      "  Batch [0/31] D_loss: -1.8051, G_loss: -4.4762\n",
      "  Batch [10/31] D_loss: -2.2054, G_loss: 2.7821\n",
      "  Batch [20/31] D_loss: -2.7975, G_loss: 1.9284\n",
      "  Batch [30/31] D_loss: -3.8664, G_loss: -5.9941\n",
      "\n",
      "Epoch 216 Summary:\n",
      "  Average D_loss: -1.3076\n",
      "  Average G_loss: 0.7100\n",
      "\n",
      "Epoch [217/500]\n",
      "  Batch [0/31] D_loss: -0.7209, G_loss: -3.6886\n",
      "  Batch [10/31] D_loss: -2.1469, G_loss: 3.0720\n",
      "  Batch [20/31] D_loss: -2.1452, G_loss: 5.6506\n",
      "  Batch [30/31] D_loss: -2.0253, G_loss: -0.0194\n",
      "\n",
      "Epoch 217 Summary:\n",
      "  Average D_loss: -1.0429\n",
      "  Average G_loss: 0.9904\n",
      "\n",
      "Epoch [218/500]\n",
      "  Batch [0/31] D_loss: -2.1552, G_loss: -4.0605\n",
      "  Batch [10/31] D_loss: -2.1124, G_loss: -3.9399\n",
      "  Batch [20/31] D_loss: -2.7080, G_loss: 2.9843\n",
      "  Batch [30/31] D_loss: -4.1296, G_loss: 6.0616\n",
      "\n",
      "Epoch 218 Summary:\n",
      "  Average D_loss: -1.0394\n",
      "  Average G_loss: 0.3252\n",
      "\n",
      "Epoch [219/500]\n",
      "  Batch [0/31] D_loss: -0.0110, G_loss: 3.4302\n",
      "  Batch [10/31] D_loss: -0.9101, G_loss: -5.3579\n",
      "  Batch [20/31] D_loss: -2.5381, G_loss: 4.4082\n",
      "  Batch [30/31] D_loss: -2.4002, G_loss: 0.0553\n",
      "\n",
      "Epoch 219 Summary:\n",
      "  Average D_loss: -0.9705\n",
      "  Average G_loss: -0.6220\n",
      "\n",
      "Epoch [220/500]\n",
      "  Batch [0/31] D_loss: -2.0900, G_loss: -0.6109\n",
      "  Batch [10/31] D_loss: -3.5959, G_loss: -2.0926\n",
      "  Batch [20/31] D_loss: -2.2025, G_loss: 2.1080\n",
      "  Batch [30/31] D_loss: -1.9318, G_loss: 0.0994\n",
      "\n",
      "Epoch 220 Summary:\n",
      "  Average D_loss: -1.0524\n",
      "  Average G_loss: 0.4181\n",
      "\n",
      "Epoch [221/500]\n",
      "  Batch [0/31] D_loss: -2.8115, G_loss: -0.8125\n",
      "  Batch [10/31] D_loss: -2.3333, G_loss: 0.1615\n",
      "  Batch [20/31] D_loss: -2.4631, G_loss: -0.9628\n",
      "  Batch [30/31] D_loss: -2.6116, G_loss: -1.6533\n",
      "\n",
      "Epoch 221 Summary:\n",
      "  Average D_loss: -0.9701\n",
      "  Average G_loss: 0.4205\n",
      "\n",
      "Epoch [222/500]\n",
      "  Batch [0/31] D_loss: -2.4106, G_loss: -2.5880\n",
      "  Batch [10/31] D_loss: -2.5938, G_loss: -1.9554\n",
      "  Batch [20/31] D_loss: -2.8459, G_loss: 1.3802\n",
      "  Batch [30/31] D_loss: -2.2281, G_loss: -0.3286\n",
      "\n",
      "Epoch 222 Summary:\n",
      "  Average D_loss: -1.0556\n",
      "  Average G_loss: -0.2248\n",
      "\n",
      "Epoch [223/500]\n",
      "  Batch [0/31] D_loss: -2.1813, G_loss: 0.8860\n",
      "  Batch [10/31] D_loss: -2.4153, G_loss: -1.6208\n",
      "  Batch [20/31] D_loss: -2.7754, G_loss: -5.9965\n",
      "  Batch [30/31] D_loss: -3.4723, G_loss: 6.8024\n",
      "\n",
      "Epoch 223 Summary:\n",
      "  Average D_loss: -1.4250\n",
      "  Average G_loss: -0.3872\n",
      "\n",
      "Epoch [224/500]\n",
      "  Batch [0/31] D_loss: -0.1393, G_loss: 4.2160\n",
      "  Batch [10/31] D_loss: -3.3993, G_loss: -4.2754\n",
      "  Batch [20/31] D_loss: -1.8602, G_loss: -3.9580\n",
      "  Batch [30/31] D_loss: -2.5633, G_loss: 2.9366\n",
      "\n",
      "Epoch 224 Summary:\n",
      "  Average D_loss: -1.0426\n",
      "  Average G_loss: 0.3716\n",
      "\n",
      "Epoch [225/500]\n",
      "  Batch [0/31] D_loss: -3.2243, G_loss: 1.4421\n",
      "  Batch [10/31] D_loss: -2.0553, G_loss: 1.3757\n",
      "  Batch [20/31] D_loss: -2.7615, G_loss: -0.4888\n",
      "  Batch [30/31] D_loss: -1.7891, G_loss: -2.7282\n",
      "\n",
      "Epoch 225 Summary:\n",
      "  Average D_loss: -0.9852\n",
      "  Average G_loss: 0.5096\n",
      "\n",
      "Epoch [226/500]\n",
      "  Batch [0/31] D_loss: -3.2224, G_loss: -4.8673\n",
      "  Batch [10/31] D_loss: -1.9972, G_loss: -2.6723\n",
      "  Batch [20/31] D_loss: -1.7478, G_loss: -1.1649\n",
      "  Batch [30/31] D_loss: -3.7691, G_loss: -6.3757\n",
      "\n",
      "Epoch 226 Summary:\n",
      "  Average D_loss: -1.1710\n",
      "  Average G_loss: 0.0701\n",
      "\n",
      "Epoch [227/500]\n",
      "  Batch [0/31] D_loss: -1.8861, G_loss: -6.2354\n",
      "  Batch [10/31] D_loss: -1.8490, G_loss: 2.2773\n",
      "  Batch [20/31] D_loss: -2.4766, G_loss: 1.0952\n",
      "  Batch [30/31] D_loss: -2.0827, G_loss: 1.1630\n",
      "\n",
      "Epoch 227 Summary:\n",
      "  Average D_loss: -1.0089\n",
      "  Average G_loss: 0.9523\n",
      "\n",
      "Epoch [228/500]\n",
      "  Batch [0/31] D_loss: -2.7894, G_loss: -2.6025\n",
      "  Batch [10/31] D_loss: -3.2527, G_loss: 4.8296\n",
      "  Batch [20/31] D_loss: -2.0381, G_loss: 2.1017\n",
      "  Batch [30/31] D_loss: -1.3434, G_loss: -0.3948\n",
      "\n",
      "Epoch 228 Summary:\n",
      "  Average D_loss: -1.2233\n",
      "  Average G_loss: 0.4690\n",
      "\n",
      "Epoch [229/500]\n",
      "  Batch [0/31] D_loss: -1.3982, G_loss: -0.8901\n",
      "  Batch [10/31] D_loss: -3.2811, G_loss: -8.0757\n",
      "  Batch [20/31] D_loss: -2.4824, G_loss: 2.4359\n",
      "  Batch [30/31] D_loss: -1.9252, G_loss: 0.0302\n",
      "\n",
      "Epoch 229 Summary:\n",
      "  Average D_loss: -1.2094\n",
      "  Average G_loss: -0.4075\n",
      "\n",
      "Epoch [230/500]\n",
      "  Batch [0/31] D_loss: -2.7113, G_loss: 0.8230\n",
      "  Batch [10/31] D_loss: -2.3733, G_loss: -5.2229\n",
      "  Batch [20/31] D_loss: -5.0457, G_loss: 6.6922\n",
      "  Batch [30/31] D_loss: -2.1780, G_loss: -3.1237\n",
      "\n",
      "Epoch 230 Summary:\n",
      "  Average D_loss: -1.3270\n",
      "  Average G_loss: 0.6470\n",
      "\n",
      "Epoch [231/500]\n",
      "  Batch [0/31] D_loss: -2.0097, G_loss: -2.7090\n",
      "  Batch [10/31] D_loss: -3.4991, G_loss: -3.3128\n",
      "  Batch [20/31] D_loss: -3.3427, G_loss: 7.2861\n",
      "  Batch [30/31] D_loss: -2.3882, G_loss: -2.2859\n",
      "\n",
      "Epoch 231 Summary:\n",
      "  Average D_loss: -1.1575\n",
      "  Average G_loss: -0.2939\n",
      "\n",
      "Epoch [232/500]\n",
      "  Batch [0/31] D_loss: -3.2900, G_loss: -5.3999\n",
      "  Batch [10/31] D_loss: -1.8056, G_loss: 0.6389\n",
      "  Batch [20/31] D_loss: -2.4292, G_loss: 5.1280\n",
      "  Batch [30/31] D_loss: -2.7891, G_loss: -5.3653\n",
      "\n",
      "Epoch 232 Summary:\n",
      "  Average D_loss: -1.1402\n",
      "  Average G_loss: 0.1326\n",
      "\n",
      "Epoch [233/500]\n",
      "  Batch [0/31] D_loss: -2.5971, G_loss: -5.6825\n",
      "  Batch [10/31] D_loss: -2.2397, G_loss: 3.6206\n",
      "  Batch [20/31] D_loss: -3.0953, G_loss: -2.8483\n",
      "  Batch [30/31] D_loss: -1.8270, G_loss: 0.4881\n",
      "\n",
      "Epoch 233 Summary:\n",
      "  Average D_loss: -1.0876\n",
      "  Average G_loss: -0.6785\n",
      "\n",
      "Epoch [234/500]\n",
      "  Batch [0/31] D_loss: -2.8744, G_loss: -2.8071\n",
      "  Batch [10/31] D_loss: -2.6165, G_loss: 2.9660\n",
      "  Batch [20/31] D_loss: -2.9082, G_loss: 3.3795\n",
      "  Batch [30/31] D_loss: -2.0635, G_loss: -1.2069\n",
      "\n",
      "Epoch 234 Summary:\n",
      "  Average D_loss: -1.1047\n",
      "  Average G_loss: 1.3449\n",
      "\n",
      "Epoch [235/500]\n",
      "  Batch [0/31] D_loss: -2.4275, G_loss: -3.6660\n",
      "  Batch [10/31] D_loss: -2.6649, G_loss: 0.1855\n",
      "  Batch [20/31] D_loss: -2.0132, G_loss: 3.1896\n",
      "  Batch [30/31] D_loss: -0.3848, G_loss: -3.3635\n",
      "\n",
      "Epoch 235 Summary:\n",
      "  Average D_loss: -1.2411\n",
      "  Average G_loss: -0.9441\n",
      "\n",
      "Epoch [236/500]\n",
      "  Batch [0/31] D_loss: -2.9552, G_loss: -5.5309\n",
      "  Batch [10/31] D_loss: -2.8675, G_loss: 7.1459\n",
      "  Batch [20/31] D_loss: -2.7246, G_loss: 1.9292\n",
      "  Batch [30/31] D_loss: -2.3276, G_loss: -1.5728\n",
      "\n",
      "Epoch 236 Summary:\n",
      "  Average D_loss: -0.8847\n",
      "  Average G_loss: 0.3987\n",
      "\n",
      "Epoch [237/500]\n",
      "  Batch [0/31] D_loss: -1.8761, G_loss: -2.9707\n",
      "  Batch [10/31] D_loss: -3.1593, G_loss: -3.0604\n",
      "  Batch [20/31] D_loss: -3.9152, G_loss: -3.4844\n",
      "  Batch [30/31] D_loss: -4.1886, G_loss: 6.7234\n",
      "\n",
      "Epoch 237 Summary:\n",
      "  Average D_loss: -1.1746\n",
      "  Average G_loss: 0.3621\n",
      "\n",
      "Epoch [238/500]\n",
      "  Batch [0/31] D_loss: -2.2329, G_loss: 4.5346\n",
      "  Batch [10/31] D_loss: -1.0869, G_loss: -4.7320\n",
      "  Batch [20/31] D_loss: -2.6131, G_loss: -0.6462\n",
      "  Batch [30/31] D_loss: -1.3785, G_loss: 3.1437\n",
      "\n",
      "Epoch 238 Summary:\n",
      "  Average D_loss: -1.0501\n",
      "  Average G_loss: 0.1393\n",
      "\n",
      "Epoch [239/500]\n",
      "  Batch [0/31] D_loss: -1.8086, G_loss: 2.4001\n",
      "  Batch [10/31] D_loss: -1.5652, G_loss: 1.8853\n",
      "  Batch [20/31] D_loss: -2.3083, G_loss: 3.2571\n",
      "  Batch [30/31] D_loss: -6.6713, G_loss: -5.8735\n",
      "\n",
      "Epoch 239 Summary:\n",
      "  Average D_loss: -1.1835\n",
      "  Average G_loss: 0.6500\n",
      "\n",
      "Epoch [240/500]\n",
      "  Batch [0/31] D_loss: -2.7286, G_loss: -5.8960\n",
      "  Batch [10/31] D_loss: -2.3758, G_loss: 1.4639\n",
      "  Batch [20/31] D_loss: -2.4178, G_loss: -1.6257\n",
      "  Batch [30/31] D_loss: -2.2794, G_loss: -3.4662\n",
      "\n",
      "Epoch 240 Summary:\n",
      "  Average D_loss: -1.1011\n",
      "  Average G_loss: 0.0426\n",
      "\n",
      "Epoch [241/500]\n",
      "  Batch [0/31] D_loss: -3.1014, G_loss: -2.3921\n",
      "  Batch [10/31] D_loss: -2.2929, G_loss: 3.1426\n",
      "  Batch [20/31] D_loss: -1.2989, G_loss: -4.9458\n",
      "  Batch [30/31] D_loss: -2.1065, G_loss: 3.1505\n",
      "\n",
      "Epoch 241 Summary:\n",
      "  Average D_loss: -0.9444\n",
      "  Average G_loss: -0.7415\n",
      "\n",
      "Epoch [242/500]\n",
      "  Batch [0/31] D_loss: -3.8812, G_loss: 4.2621\n",
      "  Batch [10/31] D_loss: -0.9847, G_loss: 3.1574\n",
      "  Batch [20/31] D_loss: -2.1548, G_loss: -3.7173\n",
      "  Batch [30/31] D_loss: -2.7461, G_loss: 0.5124\n",
      "\n",
      "Epoch 242 Summary:\n",
      "  Average D_loss: -0.9989\n",
      "  Average G_loss: -0.1535\n",
      "\n",
      "Epoch [243/500]\n",
      "  Batch [0/31] D_loss: -2.9724, G_loss: 1.9110\n",
      "  Batch [10/31] D_loss: -1.2742, G_loss: 3.5826\n",
      "  Batch [20/31] D_loss: -3.4834, G_loss: -5.2785\n",
      "  Batch [30/31] D_loss: -2.0850, G_loss: 3.0383\n",
      "\n",
      "Epoch 243 Summary:\n",
      "  Average D_loss: -1.1708\n",
      "  Average G_loss: 0.7188\n",
      "\n",
      "Epoch [244/500]\n",
      "  Batch [0/31] D_loss: -2.0775, G_loss: 1.4055\n",
      "  Batch [10/31] D_loss: -2.4810, G_loss: 1.6508\n",
      "  Batch [20/31] D_loss: -1.2455, G_loss: -4.4969\n",
      "  Batch [30/31] D_loss: -3.6208, G_loss: 3.5596\n",
      "\n",
      "Epoch 244 Summary:\n",
      "  Average D_loss: -1.1167\n",
      "  Average G_loss: -0.2449\n",
      "\n",
      "Epoch [245/500]\n",
      "  Batch [0/31] D_loss: -3.1950, G_loss: 3.6293\n",
      "  Batch [10/31] D_loss: -2.1356, G_loss: 4.5834\n",
      "  Batch [20/31] D_loss: -3.0369, G_loss: -2.5446\n",
      "  Batch [30/31] D_loss: -2.3475, G_loss: 0.4900\n",
      "\n",
      "Epoch 245 Summary:\n",
      "  Average D_loss: -1.2512\n",
      "  Average G_loss: 0.1643\n",
      "\n",
      "Epoch [246/500]\n",
      "  Batch [0/31] D_loss: -2.8535, G_loss: 2.6103\n",
      "  Batch [10/31] D_loss: -3.6873, G_loss: -0.2819\n",
      "  Batch [20/31] D_loss: -3.3494, G_loss: 3.1852\n",
      "  Batch [30/31] D_loss: -2.7886, G_loss: -2.7611\n",
      "\n",
      "Epoch 246 Summary:\n",
      "  Average D_loss: -1.1398\n",
      "  Average G_loss: 0.9482\n",
      "\n",
      "Epoch [247/500]\n",
      "  Batch [0/31] D_loss: -3.0041, G_loss: -2.7417\n",
      "  Batch [10/31] D_loss: -3.3075, G_loss: 3.4385\n",
      "  Batch [20/31] D_loss: -1.8093, G_loss: -2.7740\n",
      "  Batch [30/31] D_loss: -2.6950, G_loss: 1.6782\n",
      "\n",
      "Epoch 247 Summary:\n",
      "  Average D_loss: -1.3127\n",
      "  Average G_loss: -0.2544\n",
      "\n",
      "Epoch [248/500]\n",
      "  Batch [0/31] D_loss: -2.4165, G_loss: -1.0683\n",
      "  Batch [10/31] D_loss: -2.0051, G_loss: -0.6092\n",
      "  Batch [20/31] D_loss: -2.7026, G_loss: -6.9658\n",
      "  Batch [30/31] D_loss: -1.7562, G_loss: 0.4490\n",
      "\n",
      "Epoch 248 Summary:\n",
      "  Average D_loss: -1.1042\n",
      "  Average G_loss: -0.7194\n",
      "\n",
      "Epoch [249/500]\n",
      "  Batch [0/31] D_loss: -3.0151, G_loss: 2.1599\n",
      "  Batch [10/31] D_loss: -2.7500, G_loss: 4.5064\n",
      "  Batch [20/31] D_loss: -1.9566, G_loss: -2.4664\n",
      "  Batch [30/31] D_loss: -2.4145, G_loss: -1.0915\n",
      "\n",
      "Epoch 249 Summary:\n",
      "  Average D_loss: -1.1113\n",
      "  Average G_loss: 1.2314\n",
      "\n",
      "Epoch [250/500]\n",
      "  Batch [0/31] D_loss: -2.8181, G_loss: -4.4580\n",
      "  Batch [10/31] D_loss: -2.5783, G_loss: -1.5226\n",
      "  Batch [20/31] D_loss: -2.6838, G_loss: -1.6906\n",
      "  Batch [30/31] D_loss: -2.0904, G_loss: -1.3915\n",
      "\n",
      "Epoch 250 Summary:\n",
      "  Average D_loss: -1.3880\n",
      "  Average G_loss: 0.2997\n",
      "\n",
      "Epoch [251/500]\n",
      "  Batch [0/31] D_loss: -2.7193, G_loss: -3.2622\n",
      "  Batch [10/31] D_loss: -1.7213, G_loss: -2.1892\n",
      "  Batch [20/31] D_loss: -2.4645, G_loss: -0.6149\n",
      "  Batch [30/31] D_loss: -2.5790, G_loss: -3.3603\n",
      "\n",
      "Epoch 251 Summary:\n",
      "  Average D_loss: -1.2127\n",
      "  Average G_loss: -0.1441\n",
      "\n",
      "Epoch [252/500]\n",
      "  Batch [0/31] D_loss: -2.9719, G_loss: -0.8801\n",
      "  Batch [10/31] D_loss: -1.9198, G_loss: 0.3238\n",
      "  Batch [20/31] D_loss: -2.1011, G_loss: -2.5430\n",
      "  Batch [30/31] D_loss: -2.0747, G_loss: 1.7722\n",
      "\n",
      "Epoch 252 Summary:\n",
      "  Average D_loss: -1.0904\n",
      "  Average G_loss: 0.5396\n",
      "\n",
      "Epoch [253/500]\n",
      "  Batch [0/31] D_loss: -2.9050, G_loss: 2.4175\n",
      "  Batch [10/31] D_loss: -2.0178, G_loss: -4.8686\n",
      "  Batch [20/31] D_loss: -2.4053, G_loss: -0.0707\n",
      "  Batch [30/31] D_loss: -2.4464, G_loss: -1.4657\n",
      "\n",
      "Epoch 253 Summary:\n",
      "  Average D_loss: -1.2252\n",
      "  Average G_loss: 0.0898\n",
      "\n",
      "Epoch [254/500]\n",
      "  Batch [0/31] D_loss: -2.5332, G_loss: -0.9302\n",
      "  Batch [10/31] D_loss: -3.1702, G_loss: -1.8063\n",
      "  Batch [20/31] D_loss: -2.8298, G_loss: -1.4453\n",
      "  Batch [30/31] D_loss: -2.9787, G_loss: 2.2984\n",
      "\n",
      "Epoch 254 Summary:\n",
      "  Average D_loss: -1.2715\n",
      "  Average G_loss: -0.2368\n",
      "\n",
      "Epoch [255/500]\n",
      "  Batch [0/31] D_loss: -1.9391, G_loss: 0.4718\n",
      "  Batch [10/31] D_loss: -3.7781, G_loss: 4.3761\n",
      "  Batch [20/31] D_loss: -2.2490, G_loss: -0.2640\n",
      "  Batch [30/31] D_loss: -2.8273, G_loss: 0.4557\n",
      "\n",
      "Epoch 255 Summary:\n",
      "  Average D_loss: -1.1968\n",
      "  Average G_loss: 0.6329\n",
      "\n",
      "Epoch [256/500]\n",
      "  Batch [0/31] D_loss: -3.7533, G_loss: 4.1865\n",
      "  Batch [10/31] D_loss: -3.0379, G_loss: -0.7657\n",
      "  Batch [20/31] D_loss: -1.4576, G_loss: -3.5606\n",
      "  Batch [30/31] D_loss: -2.6085, G_loss: -0.9445\n",
      "\n",
      "Epoch 256 Summary:\n",
      "  Average D_loss: -1.3031\n",
      "  Average G_loss: -0.7328\n",
      "\n",
      "Epoch [257/500]\n",
      "  Batch [0/31] D_loss: -3.5240, G_loss: 2.8999\n",
      "  Batch [10/31] D_loss: -3.2277, G_loss: 3.2460\n",
      "  Batch [20/31] D_loss: -2.9441, G_loss: -3.1012\n",
      "  Batch [30/31] D_loss: -2.6294, G_loss: -1.2364\n",
      "\n",
      "Epoch 257 Summary:\n",
      "  Average D_loss: -1.1487\n",
      "  Average G_loss: 0.9619\n",
      "\n",
      "Epoch [258/500]\n",
      "  Batch [0/31] D_loss: -2.0995, G_loss: -0.4841\n",
      "  Batch [10/31] D_loss: -3.2105, G_loss: -0.8572\n",
      "  Batch [20/31] D_loss: -2.8823, G_loss: -6.2507\n",
      "  Batch [30/31] D_loss: -4.8674, G_loss: 6.0135\n",
      "\n",
      "Epoch 258 Summary:\n",
      "  Average D_loss: -1.2089\n",
      "  Average G_loss: -0.0213\n",
      "\n",
      "Epoch [259/500]\n",
      "  Batch [0/31] D_loss: -2.6584, G_loss: 5.0405\n",
      "  Batch [10/31] D_loss: -3.0676, G_loss: -7.5699\n",
      "  Batch [20/31] D_loss: -4.3157, G_loss: 6.9309\n",
      "  Batch [30/31] D_loss: -3.2074, G_loss: -1.1686\n",
      "\n",
      "Epoch 259 Summary:\n",
      "  Average D_loss: -1.4111\n",
      "  Average G_loss: -0.0666\n",
      "\n",
      "Epoch [260/500]\n",
      "  Batch [0/31] D_loss: -2.9599, G_loss: 0.6429\n",
      "  Batch [10/31] D_loss: -3.6641, G_loss: 1.7041\n",
      "  Batch [20/31] D_loss: -1.8686, G_loss: -0.8954\n",
      "  Batch [30/31] D_loss: -2.1372, G_loss: 3.4650\n",
      "\n",
      "Epoch 260 Summary:\n",
      "  Average D_loss: -1.1330\n",
      "  Average G_loss: 0.5803\n",
      "\n",
      "Epoch [261/500]\n",
      "  Batch [0/31] D_loss: -2.4732, G_loss: 4.0699\n",
      "  Batch [10/31] D_loss: -2.4508, G_loss: -7.1588\n",
      "  Batch [20/31] D_loss: -2.7406, G_loss: 1.9878\n",
      "  Batch [30/31] D_loss: -2.3164, G_loss: 0.1705\n",
      "\n",
      "Epoch 261 Summary:\n",
      "  Average D_loss: -1.0925\n",
      "  Average G_loss: -0.6955\n",
      "\n",
      "Epoch [262/500]\n",
      "  Batch [0/31] D_loss: -3.0841, G_loss: -3.6519\n",
      "  Batch [10/31] D_loss: -3.3018, G_loss: 6.2571\n",
      "  Batch [20/31] D_loss: -2.9216, G_loss: -1.3258\n",
      "  Batch [30/31] D_loss: -2.3142, G_loss: -5.3803\n",
      "\n",
      "Epoch 262 Summary:\n",
      "  Average D_loss: -1.1887\n",
      "  Average G_loss: 0.8340\n",
      "\n",
      "Epoch [263/500]\n",
      "  Batch [0/31] D_loss: -2.8759, G_loss: -6.4750\n",
      "  Batch [10/31] D_loss: -2.5127, G_loss: 2.1088\n",
      "  Batch [20/31] D_loss: -1.7139, G_loss: 4.2390\n",
      "  Batch [30/31] D_loss: -1.9418, G_loss: -1.5795\n",
      "\n",
      "Epoch 263 Summary:\n",
      "  Average D_loss: -0.8712\n",
      "  Average G_loss: 0.5239\n",
      "\n",
      "Epoch [264/500]\n",
      "  Batch [0/31] D_loss: -2.1070, G_loss: -3.2118\n",
      "  Batch [10/31] D_loss: -1.3654, G_loss: -0.6273\n",
      "  Batch [20/31] D_loss: -2.6144, G_loss: 1.5093\n",
      "  Batch [30/31] D_loss: -2.5895, G_loss: 1.7656\n",
      "\n",
      "Epoch 264 Summary:\n",
      "  Average D_loss: -0.8676\n",
      "  Average G_loss: -0.0810\n",
      "\n",
      "Epoch [265/500]\n",
      "  Batch [0/31] D_loss: -2.7573, G_loss: 0.7728\n",
      "  Batch [10/31] D_loss: -2.8378, G_loss: -2.4950\n",
      "  Batch [20/31] D_loss: -3.4391, G_loss: 3.9592\n",
      "  Batch [30/31] D_loss: -3.5321, G_loss: 2.8937\n",
      "\n",
      "Epoch 265 Summary:\n",
      "  Average D_loss: -1.1810\n",
      "  Average G_loss: 0.1178\n",
      "\n",
      "Epoch [266/500]\n",
      "  Batch [0/31] D_loss: -3.2336, G_loss: 3.7887\n",
      "  Batch [10/31] D_loss: -1.9051, G_loss: -2.8266\n",
      "  Batch [20/31] D_loss: -2.4981, G_loss: 1.8849\n",
      "  Batch [30/31] D_loss: -1.8366, G_loss: -1.5424\n",
      "\n",
      "Epoch 266 Summary:\n",
      "  Average D_loss: -1.0781\n",
      "  Average G_loss: -0.1878\n",
      "\n",
      "Epoch [267/500]\n",
      "  Batch [0/31] D_loss: -1.8841, G_loss: -1.5226\n",
      "  Batch [10/31] D_loss: -2.4305, G_loss: -0.8691\n",
      "  Batch [20/31] D_loss: -2.2277, G_loss: -0.4913\n",
      "  Batch [30/31] D_loss: -2.4012, G_loss: -3.1900\n",
      "\n",
      "Epoch 267 Summary:\n",
      "  Average D_loss: -1.2042\n",
      "  Average G_loss: -0.9371\n",
      "\n",
      "Epoch [268/500]\n",
      "  Batch [0/31] D_loss: -2.1487, G_loss: 3.8293\n",
      "  Batch [10/31] D_loss: -2.6406, G_loss: 4.2045\n",
      "  Batch [20/31] D_loss: -2.4430, G_loss: -0.3488\n",
      "  Batch [30/31] D_loss: -2.8944, G_loss: -1.1184\n",
      "\n",
      "Epoch 268 Summary:\n",
      "  Average D_loss: -0.9377\n",
      "  Average G_loss: 0.8977\n",
      "\n",
      "Epoch [269/500]\n",
      "  Batch [0/31] D_loss: -2.4116, G_loss: -1.3691\n",
      "  Batch [10/31] D_loss: -2.1012, G_loss: 0.7108\n",
      "  Batch [20/31] D_loss: -2.0430, G_loss: -0.0929\n",
      "  Batch [30/31] D_loss: -2.0719, G_loss: 1.1507\n",
      "\n",
      "Epoch 269 Summary:\n",
      "  Average D_loss: -1.0951\n",
      "  Average G_loss: 1.2253\n",
      "\n",
      "Epoch [270/500]\n",
      "  Batch [0/31] D_loss: -2.2409, G_loss: 0.2920\n",
      "  Batch [10/31] D_loss: 0.1819, G_loss: -3.1252\n",
      "  Batch [20/31] D_loss: -2.5227, G_loss: 3.2286\n",
      "  Batch [30/31] D_loss: -2.6885, G_loss: 2.8623\n",
      "\n",
      "Epoch 270 Summary:\n",
      "  Average D_loss: -1.1277\n",
      "  Average G_loss: 0.2840\n",
      "\n",
      "Epoch [271/500]\n",
      "  Batch [0/31] D_loss: -2.1134, G_loss: 2.0337\n",
      "  Batch [10/31] D_loss: -3.2867, G_loss: -4.7596\n",
      "  Batch [20/31] D_loss: -2.2851, G_loss: 6.5445\n",
      "  Batch [30/31] D_loss: -2.8467, G_loss: -1.1532\n",
      "\n",
      "Epoch 271 Summary:\n",
      "  Average D_loss: -1.1582\n",
      "  Average G_loss: -0.0978\n",
      "\n",
      "Epoch [272/500]\n",
      "  Batch [0/31] D_loss: -2.0967, G_loss: 2.6958\n",
      "  Batch [10/31] D_loss: -2.2857, G_loss: -5.6530\n",
      "  Batch [20/31] D_loss: -3.2376, G_loss: 2.6861\n",
      "  Batch [30/31] D_loss: -4.3163, G_loss: 5.5333\n",
      "\n",
      "Epoch 272 Summary:\n",
      "  Average D_loss: -1.1599\n",
      "  Average G_loss: -0.1730\n",
      "\n",
      "Epoch [273/500]\n",
      "  Batch [0/31] D_loss: -2.1385, G_loss: 4.2240\n",
      "  Batch [10/31] D_loss: -2.4327, G_loss: 1.3570\n",
      "  Batch [20/31] D_loss: -1.5766, G_loss: -1.7113\n",
      "  Batch [30/31] D_loss: -1.4772, G_loss: -4.1554\n",
      "\n",
      "Epoch 273 Summary:\n",
      "  Average D_loss: -1.0029\n",
      "  Average G_loss: -0.2175\n",
      "\n",
      "Epoch [274/500]\n",
      "  Batch [0/31] D_loss: -3.3785, G_loss: -6.0285\n",
      "  Batch [10/31] D_loss: -2.5482, G_loss: 2.2511\n",
      "  Batch [20/31] D_loss: -2.0745, G_loss: 0.5858\n",
      "  Batch [30/31] D_loss: -3.6348, G_loss: -0.5845\n",
      "\n",
      "Epoch 274 Summary:\n",
      "  Average D_loss: -1.1518\n",
      "  Average G_loss: 0.0631\n",
      "\n",
      "Epoch [275/500]\n",
      "  Batch [0/31] D_loss: -1.0024, G_loss: 0.7820\n",
      "  Batch [10/31] D_loss: -3.0957, G_loss: 5.2733\n",
      "  Batch [20/31] D_loss: -2.8186, G_loss: 0.9541\n",
      "  Batch [30/31] D_loss: -2.7874, G_loss: 4.8399\n",
      "\n",
      "Epoch 275 Summary:\n",
      "  Average D_loss: -1.0391\n",
      "  Average G_loss: 0.2886\n",
      "\n",
      "Epoch [276/500]\n",
      "  Batch [0/31] D_loss: -1.5609, G_loss: 4.7082\n",
      "  Batch [10/31] D_loss: -1.4301, G_loss: 1.8612\n",
      "  Batch [20/31] D_loss: -2.5338, G_loss: -1.9363\n",
      "  Batch [30/31] D_loss: -2.1758, G_loss: -0.4281\n",
      "\n",
      "Epoch 276 Summary:\n",
      "  Average D_loss: -0.9583\n",
      "  Average G_loss: 0.6243\n",
      "\n",
      "Epoch [277/500]\n",
      "  Batch [0/31] D_loss: -2.3640, G_loss: 2.3062\n",
      "  Batch [10/31] D_loss: -2.5552, G_loss: -2.9283\n",
      "  Batch [20/31] D_loss: -0.9118, G_loss: 1.4186\n",
      "  Batch [30/31] D_loss: -2.0444, G_loss: 0.8322\n",
      "\n",
      "Epoch 277 Summary:\n",
      "  Average D_loss: -1.1057\n",
      "  Average G_loss: 0.1645\n",
      "\n",
      "Epoch [278/500]\n",
      "  Batch [0/31] D_loss: -2.4869, G_loss: -2.1835\n",
      "  Batch [10/31] D_loss: -3.0907, G_loss: -0.2531\n",
      "  Batch [20/31] D_loss: -2.7184, G_loss: 3.8592\n",
      "  Batch [30/31] D_loss: -2.3539, G_loss: -0.1246\n",
      "\n",
      "Epoch 278 Summary:\n",
      "  Average D_loss: -1.0460\n",
      "  Average G_loss: 0.5252\n",
      "\n",
      "Epoch [279/500]\n",
      "  Batch [0/31] D_loss: -3.2967, G_loss: -2.3187\n",
      "  Batch [10/31] D_loss: -1.7189, G_loss: -3.7784\n",
      "  Batch [20/31] D_loss: -2.2866, G_loss: 5.2097\n",
      "  Batch [30/31] D_loss: -1.8330, G_loss: 1.9216\n",
      "\n",
      "Epoch 279 Summary:\n",
      "  Average D_loss: -1.1830\n",
      "  Average G_loss: 0.3730\n",
      "\n",
      "Epoch [280/500]\n",
      "  Batch [0/31] D_loss: -2.3996, G_loss: 1.9445\n",
      "  Batch [10/31] D_loss: -2.3609, G_loss: -3.7460\n",
      "  Batch [20/31] D_loss: -2.7009, G_loss: 2.4392\n",
      "  Batch [30/31] D_loss: -2.3912, G_loss: 1.6521\n",
      "\n",
      "Epoch 280 Summary:\n",
      "  Average D_loss: -1.0742\n",
      "  Average G_loss: -0.5402\n",
      "\n",
      "Epoch [281/500]\n",
      "  Batch [0/31] D_loss: -1.6878, G_loss: 3.2132\n",
      "  Batch [10/31] D_loss: -3.0174, G_loss: -1.6463\n",
      "  Batch [20/31] D_loss: -2.2752, G_loss: -5.6732\n",
      "  Batch [30/31] D_loss: -3.0621, G_loss: 3.5805\n",
      "\n",
      "Epoch 281 Summary:\n",
      "  Average D_loss: -1.1444\n",
      "  Average G_loss: -1.2038\n",
      "\n",
      "Epoch [282/500]\n",
      "  Batch [0/31] D_loss: -2.8115, G_loss: 5.0490\n",
      "  Batch [10/31] D_loss: -1.4724, G_loss: 2.4301\n",
      "  Batch [20/31] D_loss: -0.5611, G_loss: -3.2661\n",
      "  Batch [30/31] D_loss: -2.1059, G_loss: 1.8088\n",
      "\n",
      "Epoch 282 Summary:\n",
      "  Average D_loss: -0.9675\n",
      "  Average G_loss: 0.3068\n",
      "\n",
      "Epoch [283/500]\n",
      "  Batch [0/31] D_loss: -2.9728, G_loss: 3.9653\n",
      "  Batch [10/31] D_loss: -3.1580, G_loss: -1.0025\n",
      "  Batch [20/31] D_loss: -2.1902, G_loss: -0.6693\n",
      "  Batch [30/31] D_loss: -2.4922, G_loss: 2.9780\n",
      "\n",
      "Epoch 283 Summary:\n",
      "  Average D_loss: -1.1919\n",
      "  Average G_loss: 0.6402\n",
      "\n",
      "Epoch [284/500]\n",
      "  Batch [0/31] D_loss: -1.9957, G_loss: 2.1137\n",
      "  Batch [10/31] D_loss: -2.5732, G_loss: 0.2941\n",
      "  Batch [20/31] D_loss: -2.3267, G_loss: -2.8020\n",
      "  Batch [30/31] D_loss: -3.5208, G_loss: 5.4723\n",
      "\n",
      "Epoch 284 Summary:\n",
      "  Average D_loss: -1.2508\n",
      "  Average G_loss: 0.0429\n",
      "\n",
      "Epoch [285/500]\n",
      "  Batch [0/31] D_loss: -3.2267, G_loss: 5.6880\n",
      "  Batch [10/31] D_loss: -2.8304, G_loss: -3.0138\n",
      "  Batch [20/31] D_loss: -2.7716, G_loss: 0.0758\n",
      "  Batch [30/31] D_loss: -2.7568, G_loss: 1.1116\n",
      "\n",
      "Epoch 285 Summary:\n",
      "  Average D_loss: -1.2074\n",
      "  Average G_loss: 0.1429\n",
      "\n",
      "Epoch [286/500]\n",
      "  Batch [0/31] D_loss: -2.3811, G_loss: 0.6081\n",
      "  Batch [10/31] D_loss: -3.3619, G_loss: 3.3849\n",
      "  Batch [20/31] D_loss: -3.1641, G_loss: 1.3879\n",
      "  Batch [30/31] D_loss: -2.1956, G_loss: -0.1173\n",
      "\n",
      "Epoch 286 Summary:\n",
      "  Average D_loss: -1.2078\n",
      "  Average G_loss: 0.3373\n",
      "\n",
      "Epoch [287/500]\n",
      "  Batch [0/31] D_loss: -3.7139, G_loss: 3.5898\n",
      "  Batch [10/31] D_loss: -2.4586, G_loss: -1.2942\n",
      "  Batch [20/31] D_loss: -2.2453, G_loss: 0.8227\n",
      "  Batch [30/31] D_loss: -2.5586, G_loss: -0.3416\n",
      "\n",
      "Epoch 287 Summary:\n",
      "  Average D_loss: -1.1703\n",
      "  Average G_loss: 0.5294\n",
      "\n",
      "Epoch [288/500]\n",
      "  Batch [0/31] D_loss: -2.4089, G_loss: 0.8886\n",
      "  Batch [10/31] D_loss: -2.1053, G_loss: 2.3406\n",
      "  Batch [20/31] D_loss: -2.3110, G_loss: -1.8306\n",
      "  Batch [30/31] D_loss: -1.7004, G_loss: 1.7016\n",
      "\n",
      "Epoch 288 Summary:\n",
      "  Average D_loss: -1.2495\n",
      "  Average G_loss: 0.3900\n",
      "\n",
      "Epoch [289/500]\n",
      "  Batch [0/31] D_loss: -3.4309, G_loss: 3.9611\n",
      "  Batch [10/31] D_loss: -2.7599, G_loss: -0.2403\n",
      "  Batch [20/31] D_loss: -2.1348, G_loss: -0.9093\n",
      "  Batch [30/31] D_loss: -1.4488, G_loss: -0.2026\n",
      "\n",
      "Epoch 289 Summary:\n",
      "  Average D_loss: -1.2705\n",
      "  Average G_loss: -0.4494\n",
      "\n",
      "Epoch [290/500]\n",
      "  Batch [0/31] D_loss: -1.8067, G_loss: -2.0434\n",
      "  Batch [10/31] D_loss: -3.4182, G_loss: 4.4431\n",
      "  Batch [20/31] D_loss: -2.0203, G_loss: -1.3624\n",
      "  Batch [30/31] D_loss: -3.4479, G_loss: 5.5313\n",
      "\n",
      "Epoch 290 Summary:\n",
      "  Average D_loss: -1.2970\n",
      "  Average G_loss: 0.5031\n",
      "\n",
      "Epoch [291/500]\n",
      "  Batch [0/31] D_loss: -1.3597, G_loss: 3.7552\n",
      "  Batch [10/31] D_loss: -1.5722, G_loss: -1.3296\n",
      "  Batch [20/31] D_loss: -3.0430, G_loss: 3.6214\n",
      "  Batch [30/31] D_loss: -2.6183, G_loss: -0.9483\n",
      "\n",
      "Epoch 291 Summary:\n",
      "  Average D_loss: -1.0403\n",
      "  Average G_loss: 0.2977\n",
      "\n",
      "Epoch [292/500]\n",
      "  Batch [0/31] D_loss: -2.3638, G_loss: -2.2183\n",
      "  Batch [10/31] D_loss: -2.5954, G_loss: -3.9871\n",
      "  Batch [20/31] D_loss: -3.0144, G_loss: 3.0353\n",
      "  Batch [30/31] D_loss: -1.8003, G_loss: -2.4113\n",
      "\n",
      "Epoch 292 Summary:\n",
      "  Average D_loss: -1.2053\n",
      "  Average G_loss: -0.6120\n",
      "\n",
      "Epoch [293/500]\n",
      "  Batch [0/31] D_loss: -1.9954, G_loss: 0.3110\n",
      "  Batch [10/31] D_loss: -1.9376, G_loss: 0.9253\n",
      "  Batch [20/31] D_loss: -1.7943, G_loss: -6.1588\n",
      "  Batch [30/31] D_loss: -2.1475, G_loss: 3.8256\n",
      "\n",
      "Epoch 293 Summary:\n",
      "  Average D_loss: -1.1586\n",
      "  Average G_loss: -0.2777\n",
      "\n",
      "Epoch [294/500]\n",
      "  Batch [0/31] D_loss: -2.6769, G_loss: 4.6564\n",
      "  Batch [10/31] D_loss: -1.9865, G_loss: -0.1621\n",
      "  Batch [20/31] D_loss: -3.1960, G_loss: -4.4747\n",
      "  Batch [30/31] D_loss: -0.8150, G_loss: -1.5294\n",
      "\n",
      "Epoch 294 Summary:\n",
      "  Average D_loss: -0.8452\n",
      "  Average G_loss: 0.0247\n",
      "\n",
      "Epoch [295/500]\n",
      "  Batch [0/31] D_loss: -1.8621, G_loss: -2.0300\n",
      "  Batch [10/31] D_loss: -0.8539, G_loss: 1.3602\n",
      "  Batch [20/31] D_loss: -2.7603, G_loss: 1.6991\n",
      "  Batch [30/31] D_loss: -2.8360, G_loss: -0.5940\n",
      "\n",
      "Epoch 295 Summary:\n",
      "  Average D_loss: -0.9954\n",
      "  Average G_loss: 1.6312\n",
      "\n",
      "Epoch [296/500]\n",
      "  Batch [0/31] D_loss: -3.5059, G_loss: -2.5152\n",
      "  Batch [10/31] D_loss: -2.4188, G_loss: 3.1378\n",
      "  Batch [20/31] D_loss: -3.6309, G_loss: -2.3668\n",
      "  Batch [30/31] D_loss: -2.4330, G_loss: 1.6647\n",
      "\n",
      "Epoch 296 Summary:\n",
      "  Average D_loss: -1.3509\n",
      "  Average G_loss: -0.8436\n",
      "\n",
      "Epoch [297/500]\n",
      "  Batch [0/31] D_loss: -4.1881, G_loss: -0.9991\n",
      "  Batch [10/31] D_loss: -2.4250, G_loss: -2.0929\n",
      "  Batch [20/31] D_loss: -1.6251, G_loss: 2.1496\n",
      "  Batch [30/31] D_loss: -2.8872, G_loss: 3.7527\n",
      "\n",
      "Epoch 297 Summary:\n",
      "  Average D_loss: -1.1924\n",
      "  Average G_loss: 0.5800\n",
      "\n",
      "Epoch [298/500]\n",
      "  Batch [0/31] D_loss: -2.4127, G_loss: 3.0891\n",
      "  Batch [10/31] D_loss: -2.5169, G_loss: -2.6167\n",
      "  Batch [20/31] D_loss: -2.3849, G_loss: 0.0085\n",
      "  Batch [30/31] D_loss: -2.7287, G_loss: 2.5751\n",
      "\n",
      "Epoch 298 Summary:\n",
      "  Average D_loss: -1.2230\n",
      "  Average G_loss: 0.1735\n",
      "\n",
      "Epoch [299/500]\n",
      "  Batch [0/31] D_loss: -1.9693, G_loss: 0.3812\n",
      "  Batch [10/31] D_loss: -4.3650, G_loss: -5.6116\n",
      "  Batch [20/31] D_loss: -1.4075, G_loss: 4.3661\n",
      "  Batch [30/31] D_loss: -2.1040, G_loss: -3.4314\n",
      "\n",
      "Epoch 299 Summary:\n",
      "  Average D_loss: -1.4212\n",
      "  Average G_loss: -0.3456\n",
      "\n",
      "Epoch [300/500]\n",
      "  Batch [0/31] D_loss: -2.6736, G_loss: -2.4981\n",
      "  Batch [10/31] D_loss: -3.0933, G_loss: 3.1338\n",
      "  Batch [20/31] D_loss: -3.2697, G_loss: -2.2623\n",
      "  Batch [30/31] D_loss: -3.1141, G_loss: -1.4083\n",
      "\n",
      "Epoch 300 Summary:\n",
      "  Average D_loss: -1.1250\n",
      "  Average G_loss: 0.2892\n",
      "\n",
      "Epoch [301/500]\n",
      "  Batch [0/31] D_loss: -3.7270, G_loss: -0.6333\n",
      "  Batch [10/31] D_loss: -3.8969, G_loss: 2.1025\n",
      "  Batch [20/31] D_loss: -2.4008, G_loss: -0.6472\n",
      "  Batch [30/31] D_loss: -0.0498, G_loss: -2.9473\n",
      "\n",
      "Epoch 301 Summary:\n",
      "  Average D_loss: -1.2199\n",
      "  Average G_loss: -0.3174\n",
      "\n",
      "Epoch [302/500]\n",
      "  Batch [0/31] D_loss: -1.9149, G_loss: -3.2560\n",
      "  Batch [10/31] D_loss: -2.5214, G_loss: 3.1121\n",
      "  Batch [20/31] D_loss: -2.0263, G_loss: 2.1240\n",
      "  Batch [30/31] D_loss: -2.8823, G_loss: 0.7899\n",
      "\n",
      "Epoch 302 Summary:\n",
      "  Average D_loss: -1.0991\n",
      "  Average G_loss: 0.4961\n",
      "\n",
      "Epoch [303/500]\n",
      "  Batch [0/31] D_loss: -3.7002, G_loss: -4.0396\n",
      "  Batch [10/31] D_loss: -3.0003, G_loss: 2.6500\n",
      "  Batch [20/31] D_loss: -4.3491, G_loss: -4.0746\n",
      "  Batch [30/31] D_loss: -3.1063, G_loss: -0.5863\n",
      "\n",
      "Epoch 303 Summary:\n",
      "  Average D_loss: -1.3196\n",
      "  Average G_loss: -0.0311\n",
      "\n",
      "Epoch [304/500]\n",
      "  Batch [0/31] D_loss: -2.4121, G_loss: 2.1475\n",
      "  Batch [10/31] D_loss: -2.3730, G_loss: 0.5648\n",
      "  Batch [20/31] D_loss: -2.7625, G_loss: 0.1893\n",
      "  Batch [30/31] D_loss: -3.0301, G_loss: -1.1026\n",
      "\n",
      "Epoch 304 Summary:\n",
      "  Average D_loss: -1.1387\n",
      "  Average G_loss: 0.1998\n",
      "\n",
      "Epoch [305/500]\n",
      "  Batch [0/31] D_loss: -2.7152, G_loss: 1.4219\n",
      "  Batch [10/31] D_loss: -3.0373, G_loss: 2.0551\n",
      "  Batch [20/31] D_loss: -2.2741, G_loss: 2.0719\n",
      "  Batch [30/31] D_loss: -2.3754, G_loss: 1.1210\n",
      "\n",
      "Epoch 305 Summary:\n",
      "  Average D_loss: -1.1882\n",
      "  Average G_loss: 0.3824\n",
      "\n",
      "Epoch [306/500]\n",
      "  Batch [0/31] D_loss: -3.2617, G_loss: -0.9035\n",
      "  Batch [10/31] D_loss: -2.2332, G_loss: 0.0333\n",
      "  Batch [20/31] D_loss: -3.1108, G_loss: -0.8577\n",
      "  Batch [30/31] D_loss: -2.0308, G_loss: 2.7284\n",
      "\n",
      "Epoch 306 Summary:\n",
      "  Average D_loss: -1.3053\n",
      "  Average G_loss: 0.4942\n",
      "\n",
      "Epoch [307/500]\n",
      "  Batch [0/31] D_loss: -2.2257, G_loss: -1.6980\n",
      "  Batch [10/31] D_loss: -2.0199, G_loss: -2.4914\n",
      "  Batch [20/31] D_loss: -1.7683, G_loss: 1.8802\n",
      "  Batch [30/31] D_loss: -2.7160, G_loss: 0.7260\n",
      "\n",
      "Epoch 307 Summary:\n",
      "  Average D_loss: -1.2997\n",
      "  Average G_loss: -0.5467\n",
      "\n",
      "Epoch [308/500]\n",
      "  Batch [0/31] D_loss: -2.4969, G_loss: 2.2065\n",
      "  Batch [10/31] D_loss: -2.1191, G_loss: -1.4315\n",
      "  Batch [20/31] D_loss: -2.7802, G_loss: 3.3440\n",
      "  Batch [30/31] D_loss: -1.6166, G_loss: 1.9659\n",
      "\n",
      "Epoch 308 Summary:\n",
      "  Average D_loss: -1.2610\n",
      "  Average G_loss: 0.2071\n",
      "\n",
      "Epoch [309/500]\n",
      "  Batch [0/31] D_loss: -1.8582, G_loss: 0.5514\n",
      "  Batch [10/31] D_loss: -4.1756, G_loss: 5.8501\n",
      "  Batch [20/31] D_loss: -2.4715, G_loss: -0.2639\n",
      "  Batch [30/31] D_loss: -2.8053, G_loss: 4.0959\n",
      "\n",
      "Epoch 309 Summary:\n",
      "  Average D_loss: -1.2477\n",
      "  Average G_loss: 0.6895\n",
      "\n",
      "Epoch [310/500]\n",
      "  Batch [0/31] D_loss: -1.2726, G_loss: 0.0024\n",
      "  Batch [10/31] D_loss: -3.6628, G_loss: 2.8474\n",
      "  Batch [20/31] D_loss: -2.7701, G_loss: -6.5013\n",
      "  Batch [30/31] D_loss: -1.7887, G_loss: 0.1740\n",
      "\n",
      "Epoch 310 Summary:\n",
      "  Average D_loss: -1.1095\n",
      "  Average G_loss: -1.2073\n",
      "\n",
      "Epoch [311/500]\n",
      "  Batch [0/31] D_loss: -1.5345, G_loss: 0.1536\n",
      "  Batch [10/31] D_loss: -2.8196, G_loss: 2.0164\n",
      "  Batch [20/31] D_loss: -2.3005, G_loss: 0.1949\n",
      "  Batch [30/31] D_loss: -2.6554, G_loss: -3.2397\n",
      "\n",
      "Epoch 311 Summary:\n",
      "  Average D_loss: -1.1956\n",
      "  Average G_loss: 1.1127\n",
      "\n",
      "Epoch [312/500]\n",
      "  Batch [0/31] D_loss: -2.7465, G_loss: -0.1741\n",
      "  Batch [10/31] D_loss: -3.0280, G_loss: -2.5573\n",
      "  Batch [20/31] D_loss: -2.0445, G_loss: 0.8325\n",
      "  Batch [30/31] D_loss: -3.8143, G_loss: 2.8976\n",
      "\n",
      "Epoch 312 Summary:\n",
      "  Average D_loss: -1.1526\n",
      "  Average G_loss: -0.0473\n",
      "\n",
      "Epoch [313/500]\n",
      "  Batch [0/31] D_loss: -3.0049, G_loss: 1.6994\n",
      "  Batch [10/31] D_loss: -3.4999, G_loss: 3.3706\n",
      "  Batch [20/31] D_loss: -2.6694, G_loss: 0.8787\n",
      "  Batch [30/31] D_loss: -2.6453, G_loss: -2.0080\n",
      "\n",
      "Epoch 313 Summary:\n",
      "  Average D_loss: -1.1704\n",
      "  Average G_loss: -0.0820\n",
      "\n",
      "Epoch [314/500]\n",
      "  Batch [0/31] D_loss: -3.2351, G_loss: 1.6941\n",
      "  Batch [10/31] D_loss: -3.3323, G_loss: 0.7166\n",
      "  Batch [20/31] D_loss: -5.3637, G_loss: -7.2009\n",
      "  Batch [30/31] D_loss: -2.6128, G_loss: 4.9173\n",
      "\n",
      "Epoch 314 Summary:\n",
      "  Average D_loss: -1.2487\n",
      "  Average G_loss: 0.6422\n",
      "\n",
      "Epoch [315/500]\n",
      "  Batch [0/31] D_loss: -1.8851, G_loss: 3.7381\n",
      "  Batch [10/31] D_loss: -2.2239, G_loss: -2.8515\n",
      "  Batch [20/31] D_loss: -2.0296, G_loss: -0.8171\n",
      "  Batch [30/31] D_loss: -1.9413, G_loss: 3.7209\n",
      "\n",
      "Epoch 315 Summary:\n",
      "  Average D_loss: -1.1489\n",
      "  Average G_loss: 0.0114\n",
      "\n",
      "Epoch [316/500]\n",
      "  Batch [0/31] D_loss: -3.2019, G_loss: 3.9089\n",
      "  Batch [10/31] D_loss: -2.4007, G_loss: -1.8432\n",
      "  Batch [20/31] D_loss: -2.3472, G_loss: 4.8512\n",
      "  Batch [30/31] D_loss: -1.6651, G_loss: 0.3250\n",
      "\n",
      "Epoch 316 Summary:\n",
      "  Average D_loss: -1.0814\n",
      "  Average G_loss: 0.4547\n",
      "\n",
      "Epoch [317/500]\n",
      "  Batch [0/31] D_loss: -2.5197, G_loss: 1.0256\n",
      "  Batch [10/31] D_loss: -2.3464, G_loss: -1.4191\n",
      "  Batch [20/31] D_loss: -3.5919, G_loss: 0.9169\n",
      "  Batch [30/31] D_loss: -1.9198, G_loss: 2.5274\n",
      "\n",
      "Epoch 317 Summary:\n",
      "  Average D_loss: -1.2364\n",
      "  Average G_loss: -0.7037\n",
      "\n",
      "Epoch [318/500]\n",
      "  Batch [0/31] D_loss: -4.0073, G_loss: 6.7264\n",
      "  Batch [10/31] D_loss: -2.4741, G_loss: -0.7895\n",
      "  Batch [20/31] D_loss: -2.1003, G_loss: -2.1243\n",
      "  Batch [30/31] D_loss: -1.5498, G_loss: -1.1141\n",
      "\n",
      "Epoch 318 Summary:\n",
      "  Average D_loss: -1.1815\n",
      "  Average G_loss: 0.1273\n",
      "\n",
      "Epoch [319/500]\n",
      "  Batch [0/31] D_loss: -2.6579, G_loss: 0.6132\n",
      "  Batch [10/31] D_loss: -3.4121, G_loss: 2.2933\n",
      "  Batch [20/31] D_loss: -2.5462, G_loss: 0.9626\n",
      "  Batch [30/31] D_loss: -2.0466, G_loss: -2.5498\n",
      "\n",
      "Epoch 319 Summary:\n",
      "  Average D_loss: -1.2272\n",
      "  Average G_loss: 0.7005\n",
      "\n",
      "Epoch [320/500]\n",
      "  Batch [0/31] D_loss: -1.6963, G_loss: -1.9812\n",
      "  Batch [10/31] D_loss: -2.5822, G_loss: 0.0976\n",
      "  Batch [20/31] D_loss: -2.0370, G_loss: 0.6439\n",
      "  Batch [30/31] D_loss: -1.7480, G_loss: -2.7616\n",
      "\n",
      "Epoch 320 Summary:\n",
      "  Average D_loss: -1.1390\n",
      "  Average G_loss: 0.2953\n",
      "\n",
      "Epoch [321/500]\n",
      "  Batch [0/31] D_loss: -2.5467, G_loss: -1.3618\n",
      "  Batch [10/31] D_loss: -1.7873, G_loss: 2.4030\n",
      "  Batch [20/31] D_loss: -2.4846, G_loss: 1.4899\n",
      "  Batch [30/31] D_loss: -4.2242, G_loss: -5.4256\n",
      "\n",
      "Epoch 321 Summary:\n",
      "  Average D_loss: -1.3441\n",
      "  Average G_loss: -0.1377\n",
      "\n",
      "Epoch [322/500]\n",
      "  Batch [0/31] D_loss: -0.7122, G_loss: -3.0439\n",
      "  Batch [10/31] D_loss: -3.1689, G_loss: -4.2279\n",
      "  Batch [20/31] D_loss: -2.8696, G_loss: 3.4479\n",
      "  Batch [30/31] D_loss: -0.9861, G_loss: -3.2956\n",
      "\n",
      "Epoch 322 Summary:\n",
      "  Average D_loss: -1.1960\n",
      "  Average G_loss: -0.1123\n",
      "\n",
      "Epoch [323/500]\n",
      "  Batch [0/31] D_loss: -1.8479, G_loss: 0.4244\n",
      "  Batch [10/31] D_loss: -1.8021, G_loss: 3.9280\n",
      "  Batch [20/31] D_loss: -3.3845, G_loss: -3.5068\n",
      "  Batch [30/31] D_loss: -2.9559, G_loss: -0.2222\n",
      "\n",
      "Epoch 323 Summary:\n",
      "  Average D_loss: -1.1838\n",
      "  Average G_loss: 0.3832\n",
      "\n",
      "Epoch [324/500]\n",
      "  Batch [0/31] D_loss: -2.0218, G_loss: -1.3467\n",
      "  Batch [10/31] D_loss: -2.9979, G_loss: 3.2456\n",
      "  Batch [20/31] D_loss: -3.4131, G_loss: -1.6752\n",
      "  Batch [30/31] D_loss: -2.6268, G_loss: 3.8633\n",
      "\n",
      "Epoch 324 Summary:\n",
      "  Average D_loss: -1.1770\n",
      "  Average G_loss: 0.0805\n",
      "\n",
      "Epoch [325/500]\n",
      "  Batch [0/31] D_loss: -3.9166, G_loss: 2.1825\n",
      "  Batch [10/31] D_loss: -3.0596, G_loss: 3.4531\n",
      "  Batch [20/31] D_loss: -3.2771, G_loss: -4.9543\n",
      "  Batch [30/31] D_loss: -1.6106, G_loss: 1.1042\n",
      "\n",
      "Epoch 325 Summary:\n",
      "  Average D_loss: -1.1656\n",
      "  Average G_loss: 0.0231\n",
      "\n",
      "Epoch [326/500]\n",
      "  Batch [0/31] D_loss: -1.9318, G_loss: -0.9768\n",
      "  Batch [10/31] D_loss: -3.9637, G_loss: 1.9207\n",
      "  Batch [20/31] D_loss: -1.8065, G_loss: 0.2379\n",
      "  Batch [30/31] D_loss: -3.0186, G_loss: -1.2678\n",
      "\n",
      "Epoch 326 Summary:\n",
      "  Average D_loss: -1.3238\n",
      "  Average G_loss: -0.0052\n",
      "\n",
      "Epoch [327/500]\n",
      "  Batch [0/31] D_loss: -1.8623, G_loss: 1.9256\n",
      "  Batch [10/31] D_loss: -2.5281, G_loss: 1.3371\n",
      "  Batch [20/31] D_loss: -2.0979, G_loss: -0.3809\n",
      "  Batch [30/31] D_loss: -2.0071, G_loss: -0.6048\n",
      "\n",
      "Epoch 327 Summary:\n",
      "  Average D_loss: -1.1439\n",
      "  Average G_loss: 0.5210\n",
      "\n",
      "Epoch [328/500]\n",
      "  Batch [0/31] D_loss: -2.7627, G_loss: 1.5947\n",
      "  Batch [10/31] D_loss: -1.5345, G_loss: 2.3546\n",
      "  Batch [20/31] D_loss: -2.8322, G_loss: -4.1374\n",
      "  Batch [30/31] D_loss: -1.7303, G_loss: -2.8908\n",
      "\n",
      "Epoch 328 Summary:\n",
      "  Average D_loss: -1.4091\n",
      "  Average G_loss: -1.1045\n",
      "\n",
      "Epoch [329/500]\n",
      "  Batch [0/31] D_loss: -1.5498, G_loss: 0.0699\n",
      "  Batch [10/31] D_loss: -2.8905, G_loss: 1.6243\n",
      "  Batch [20/31] D_loss: -3.1320, G_loss: 2.9100\n",
      "  Batch [30/31] D_loss: -2.2157, G_loss: -1.5129\n",
      "\n",
      "Epoch 329 Summary:\n",
      "  Average D_loss: -1.0542\n",
      "  Average G_loss: 0.8379\n",
      "\n",
      "Epoch [330/500]\n",
      "  Batch [0/31] D_loss: -2.8300, G_loss: -1.3501\n",
      "  Batch [10/31] D_loss: -1.5342, G_loss: 1.7315\n",
      "  Batch [20/31] D_loss: -1.9836, G_loss: -1.8752\n",
      "  Batch [30/31] D_loss: -3.2856, G_loss: 3.2612\n",
      "\n",
      "Epoch 330 Summary:\n",
      "  Average D_loss: -1.0745\n",
      "  Average G_loss: 0.4999\n",
      "\n",
      "Epoch [331/500]\n",
      "  Batch [0/31] D_loss: -2.5753, G_loss: 0.5480\n",
      "  Batch [10/31] D_loss: -1.9156, G_loss: -1.0818\n",
      "  Batch [20/31] D_loss: -1.7239, G_loss: 2.5998\n",
      "  Batch [30/31] D_loss: -2.0367, G_loss: 1.3885\n",
      "\n",
      "Epoch 331 Summary:\n",
      "  Average D_loss: -1.0958\n",
      "  Average G_loss: 0.3810\n",
      "\n",
      "Epoch [332/500]\n",
      "  Batch [0/31] D_loss: -2.4058, G_loss: 4.3335\n",
      "  Batch [10/31] D_loss: -2.4082, G_loss: -6.8143\n",
      "  Batch [20/31] D_loss: -3.6453, G_loss: 5.7388\n",
      "  Batch [30/31] D_loss: -2.1738, G_loss: 1.3978\n",
      "\n",
      "Epoch 332 Summary:\n",
      "  Average D_loss: -1.1175\n",
      "  Average G_loss: -0.0836\n",
      "\n",
      "Epoch [333/500]\n",
      "  Batch [0/31] D_loss: -2.0960, G_loss: -0.9364\n",
      "  Batch [10/31] D_loss: -2.8042, G_loss: -1.0707\n",
      "  Batch [20/31] D_loss: -4.8245, G_loss: 5.3075\n",
      "  Batch [30/31] D_loss: -1.7244, G_loss: -1.2843\n",
      "\n",
      "Epoch 333 Summary:\n",
      "  Average D_loss: -1.1399\n",
      "  Average G_loss: 0.2279\n",
      "\n",
      "Epoch [334/500]\n",
      "  Batch [0/31] D_loss: -2.7623, G_loss: -1.2247\n",
      "  Batch [10/31] D_loss: -2.2698, G_loss: 3.2331\n",
      "  Batch [20/31] D_loss: -0.8472, G_loss: 0.6377\n",
      "  Batch [30/31] D_loss: -2.1542, G_loss: -1.8816\n",
      "\n",
      "Epoch 334 Summary:\n",
      "  Average D_loss: -1.1684\n",
      "  Average G_loss: -0.1566\n",
      "\n",
      "Epoch [335/500]\n",
      "  Batch [0/31] D_loss: -2.9558, G_loss: -1.1129\n",
      "  Batch [10/31] D_loss: -1.8486, G_loss: -3.3095\n",
      "  Batch [20/31] D_loss: -2.1023, G_loss: 0.9999\n",
      "  Batch [30/31] D_loss: -2.7490, G_loss: 0.7412\n",
      "\n",
      "Epoch 335 Summary:\n",
      "  Average D_loss: -1.2006\n",
      "  Average G_loss: 0.1566\n",
      "\n",
      "Epoch [336/500]\n",
      "  Batch [0/31] D_loss: -2.9746, G_loss: -1.8017\n",
      "  Batch [10/31] D_loss: -3.0174, G_loss: 1.4050\n",
      "  Batch [20/31] D_loss: -1.7611, G_loss: 2.6695\n",
      "  Batch [30/31] D_loss: -4.9078, G_loss: 6.0630\n",
      "\n",
      "Epoch 336 Summary:\n",
      "  Average D_loss: -1.2619\n",
      "  Average G_loss: 0.0108\n",
      "\n",
      "Epoch [337/500]\n",
      "  Batch [0/31] D_loss: -2.1711, G_loss: 0.9051\n",
      "  Batch [10/31] D_loss: -2.2394, G_loss: -1.6904\n",
      "  Batch [20/31] D_loss: -1.0500, G_loss: 0.9561\n",
      "  Batch [30/31] D_loss: -1.8878, G_loss: -1.0885\n",
      "\n",
      "Epoch 337 Summary:\n",
      "  Average D_loss: -1.0437\n",
      "  Average G_loss: 0.0723\n",
      "\n",
      "Epoch [338/500]\n",
      "  Batch [0/31] D_loss: -2.4605, G_loss: -1.8140\n",
      "  Batch [10/31] D_loss: -2.6081, G_loss: -3.7960\n",
      "  Batch [20/31] D_loss: -2.3274, G_loss: 1.5496\n",
      "  Batch [30/31] D_loss: -3.2534, G_loss: -0.3591\n",
      "\n",
      "Epoch 338 Summary:\n",
      "  Average D_loss: -1.2112\n",
      "  Average G_loss: 0.0354\n",
      "\n",
      "Epoch [339/500]\n",
      "  Batch [0/31] D_loss: -3.5602, G_loss: 3.4333\n",
      "  Batch [10/31] D_loss: -2.2747, G_loss: 1.0392\n",
      "  Batch [20/31] D_loss: -2.7820, G_loss: -3.4454\n",
      "  Batch [30/31] D_loss: -2.2596, G_loss: 0.6665\n",
      "\n",
      "Epoch 339 Summary:\n",
      "  Average D_loss: -1.2374\n",
      "  Average G_loss: -0.4065\n",
      "\n",
      "Epoch [340/500]\n",
      "  Batch [0/31] D_loss: -2.6294, G_loss: -1.9576\n",
      "  Batch [10/31] D_loss: -1.0999, G_loss: 1.0054\n",
      "  Batch [20/31] D_loss: -2.5478, G_loss: -1.8160\n",
      "  Batch [30/31] D_loss: -2.7686, G_loss: 4.4651\n",
      "\n",
      "Epoch 340 Summary:\n",
      "  Average D_loss: -1.1058\n",
      "  Average G_loss: 0.3660\n",
      "\n",
      "Epoch [341/500]\n",
      "  Batch [0/31] D_loss: -1.6475, G_loss: 0.2546\n",
      "  Batch [10/31] D_loss: -2.7856, G_loss: -3.3703\n",
      "  Batch [20/31] D_loss: -3.7066, G_loss: 5.2529\n",
      "  Batch [30/31] D_loss: -2.0404, G_loss: 1.2193\n",
      "\n",
      "Epoch 341 Summary:\n",
      "  Average D_loss: -1.2312\n",
      "  Average G_loss: 0.2840\n",
      "\n",
      "Epoch [342/500]\n",
      "  Batch [0/31] D_loss: -2.3711, G_loss: -1.2190\n",
      "  Batch [10/31] D_loss: -2.0488, G_loss: 3.0545\n",
      "  Batch [20/31] D_loss: -1.8364, G_loss: -4.3211\n",
      "  Batch [30/31] D_loss: -2.2992, G_loss: 1.4571\n",
      "\n",
      "Epoch 342 Summary:\n",
      "  Average D_loss: -1.1974\n",
      "  Average G_loss: -0.3344\n",
      "\n",
      "Epoch [343/500]\n",
      "  Batch [0/31] D_loss: -3.0765, G_loss: 2.6178\n",
      "  Batch [10/31] D_loss: -3.9019, G_loss: -0.7758\n",
      "  Batch [20/31] D_loss: -1.7418, G_loss: 0.6932\n",
      "  Batch [30/31] D_loss: -2.8642, G_loss: -6.2809\n",
      "\n",
      "Epoch 343 Summary:\n",
      "  Average D_loss: -1.2293\n",
      "  Average G_loss: -0.2834\n",
      "\n",
      "Epoch [344/500]\n",
      "  Batch [0/31] D_loss: -2.0947, G_loss: -6.5014\n",
      "  Batch [10/31] D_loss: -2.4706, G_loss: 6.5666\n",
      "  Batch [20/31] D_loss: -2.2797, G_loss: 0.7054\n",
      "  Batch [30/31] D_loss: -2.1513, G_loss: -6.2582\n",
      "\n",
      "Epoch 344 Summary:\n",
      "  Average D_loss: -0.9306\n",
      "  Average G_loss: 0.6383\n",
      "\n",
      "Epoch [345/500]\n",
      "  Batch [0/31] D_loss: -0.5918, G_loss: -3.1653\n",
      "  Batch [10/31] D_loss: -2.5556, G_loss: 0.4812\n",
      "  Batch [20/31] D_loss: -3.5022, G_loss: -0.9886\n",
      "  Batch [30/31] D_loss: -3.1978, G_loss: -2.4889\n",
      "\n",
      "Epoch 345 Summary:\n",
      "  Average D_loss: -1.1559\n",
      "  Average G_loss: 0.7251\n",
      "\n",
      "Epoch [346/500]\n",
      "  Batch [0/31] D_loss: -2.9147, G_loss: 0.2215\n",
      "  Batch [10/31] D_loss: -2.3506, G_loss: -1.5109\n",
      "  Batch [20/31] D_loss: -1.9959, G_loss: -0.5409\n",
      "  Batch [30/31] D_loss: -2.7279, G_loss: 4.0852\n",
      "\n",
      "Epoch 346 Summary:\n",
      "  Average D_loss: -1.2729\n",
      "  Average G_loss: 0.3264\n",
      "\n",
      "Epoch [347/500]\n",
      "  Batch [0/31] D_loss: -2.8554, G_loss: 0.6390\n",
      "  Batch [10/31] D_loss: -2.8405, G_loss: 0.5440\n",
      "  Batch [20/31] D_loss: -1.2360, G_loss: -1.2329\n",
      "  Batch [30/31] D_loss: -2.1859, G_loss: -0.5278\n",
      "\n",
      "Epoch 347 Summary:\n",
      "  Average D_loss: -1.0038\n",
      "  Average G_loss: -0.7669\n",
      "\n",
      "Epoch [348/500]\n",
      "  Batch [0/31] D_loss: -3.2362, G_loss: 2.6838\n",
      "  Batch [10/31] D_loss: -2.5278, G_loss: -2.8660\n",
      "  Batch [20/31] D_loss: -3.1847, G_loss: -1.5418\n",
      "  Batch [30/31] D_loss: -2.2846, G_loss: -2.9842\n",
      "\n",
      "Epoch 348 Summary:\n",
      "  Average D_loss: -1.3163\n",
      "  Average G_loss: 0.3264\n",
      "\n",
      "Epoch [349/500]\n",
      "  Batch [0/31] D_loss: -1.9855, G_loss: -0.3829\n",
      "  Batch [10/31] D_loss: -2.4128, G_loss: 1.9816\n",
      "  Batch [20/31] D_loss: -2.9467, G_loss: -0.7560\n",
      "  Batch [30/31] D_loss: -2.8866, G_loss: -3.4868\n",
      "\n",
      "Epoch 349 Summary:\n",
      "  Average D_loss: -1.2472\n",
      "  Average G_loss: 0.3095\n",
      "\n",
      "Epoch [350/500]\n",
      "  Batch [0/31] D_loss: -1.5207, G_loss: -2.4495\n",
      "  Batch [10/31] D_loss: -2.3049, G_loss: 0.9453\n",
      "  Batch [20/31] D_loss: -2.4068, G_loss: -1.3062\n",
      "  Batch [30/31] D_loss: -2.5161, G_loss: -0.7101\n",
      "\n",
      "Epoch 350 Summary:\n",
      "  Average D_loss: -1.3477\n",
      "  Average G_loss: -0.3612\n",
      "\n",
      "Epoch [351/500]\n",
      "  Batch [0/31] D_loss: -2.9009, G_loss: 3.6390\n",
      "  Batch [10/31] D_loss: -2.1002, G_loss: -1.7788\n",
      "  Batch [20/31] D_loss: -2.7291, G_loss: 2.5948\n",
      "  Batch [30/31] D_loss: -2.2729, G_loss: -0.3224\n",
      "\n",
      "Epoch 351 Summary:\n",
      "  Average D_loss: -1.2196\n",
      "  Average G_loss: -0.0421\n",
      "\n",
      "Epoch [352/500]\n",
      "  Batch [0/31] D_loss: -2.4307, G_loss: -1.7563\n",
      "  Batch [10/31] D_loss: -3.1121, G_loss: 1.0730\n",
      "  Batch [20/31] D_loss: -2.7976, G_loss: 3.3293\n",
      "  Batch [30/31] D_loss: -1.3991, G_loss: -3.6003\n",
      "\n",
      "Epoch 352 Summary:\n",
      "  Average D_loss: -1.0954\n",
      "  Average G_loss: 0.4755\n",
      "\n",
      "Epoch [353/500]\n",
      "  Batch [0/31] D_loss: -1.2756, G_loss: -3.3269\n",
      "  Batch [10/31] D_loss: -1.6390, G_loss: 0.7559\n",
      "  Batch [20/31] D_loss: -3.4911, G_loss: 4.7491\n",
      "  Batch [30/31] D_loss: -2.9185, G_loss: -2.6633\n",
      "\n",
      "Epoch 353 Summary:\n",
      "  Average D_loss: -1.1026\n",
      "  Average G_loss: -0.1752\n",
      "\n",
      "Epoch [354/500]\n",
      "  Batch [0/31] D_loss: -2.7573, G_loss: 0.3035\n",
      "  Batch [10/31] D_loss: -2.8664, G_loss: 4.7672\n",
      "  Batch [20/31] D_loss: -2.4647, G_loss: 4.0125\n",
      "  Batch [30/31] D_loss: -2.1983, G_loss: 0.4820\n",
      "\n",
      "Epoch 354 Summary:\n",
      "  Average D_loss: -1.1155\n",
      "  Average G_loss: 0.5994\n",
      "\n",
      "Epoch [355/500]\n",
      "  Batch [0/31] D_loss: -2.5484, G_loss: -1.7498\n",
      "  Batch [10/31] D_loss: -2.3997, G_loss: -1.3019\n",
      "  Batch [20/31] D_loss: -2.0309, G_loss: 0.9207\n",
      "  Batch [30/31] D_loss: -3.3531, G_loss: 1.8055\n",
      "\n",
      "Epoch 355 Summary:\n",
      "  Average D_loss: -1.3240\n",
      "  Average G_loss: 0.3058\n",
      "\n",
      "Epoch [356/500]\n",
      "  Batch [0/31] D_loss: -2.5800, G_loss: -2.2670\n",
      "  Batch [10/31] D_loss: -1.4573, G_loss: -1.9265\n",
      "  Batch [20/31] D_loss: -1.1960, G_loss: 2.5190\n",
      "  Batch [30/31] D_loss: -2.6740, G_loss: -1.0483\n",
      "\n",
      "Epoch 356 Summary:\n",
      "  Average D_loss: -1.2203\n",
      "  Average G_loss: 0.1055\n",
      "\n",
      "Epoch [357/500]\n",
      "  Batch [0/31] D_loss: -2.3397, G_loss: -1.7649\n",
      "  Batch [10/31] D_loss: -2.3361, G_loss: -0.7570\n",
      "  Batch [20/31] D_loss: -2.8692, G_loss: 4.9233\n",
      "  Batch [30/31] D_loss: -3.9463, G_loss: 2.1483\n",
      "\n",
      "Epoch 357 Summary:\n",
      "  Average D_loss: -1.3122\n",
      "  Average G_loss: -0.7193\n",
      "\n",
      "Epoch [358/500]\n",
      "  Batch [0/31] D_loss: -3.0344, G_loss: 3.1879\n",
      "  Batch [10/31] D_loss: -2.2163, G_loss: -1.2912\n",
      "  Batch [20/31] D_loss: -3.2453, G_loss: 0.6306\n",
      "  Batch [30/31] D_loss: -2.4603, G_loss: -3.7371\n",
      "\n",
      "Epoch 358 Summary:\n",
      "  Average D_loss: -1.0132\n",
      "  Average G_loss: 0.1271\n",
      "\n",
      "Epoch [359/500]\n",
      "  Batch [0/31] D_loss: -2.4476, G_loss: -3.8936\n",
      "  Batch [10/31] D_loss: -3.3124, G_loss: -5.1309\n",
      "  Batch [20/31] D_loss: -3.8762, G_loss: 3.8008\n",
      "  Batch [30/31] D_loss: -2.2656, G_loss: -0.1583\n",
      "\n",
      "Epoch 359 Summary:\n",
      "  Average D_loss: -1.2724\n",
      "  Average G_loss: 0.2130\n",
      "\n",
      "Epoch [360/500]\n",
      "  Batch [0/31] D_loss: -2.3620, G_loss: -2.6488\n",
      "  Batch [10/31] D_loss: -4.6839, G_loss: 6.3717\n",
      "  Batch [20/31] D_loss: -1.3989, G_loss: 3.1347\n",
      "  Batch [30/31] D_loss: -2.1736, G_loss: 0.6224\n",
      "\n",
      "Epoch 360 Summary:\n",
      "  Average D_loss: -1.1782\n",
      "  Average G_loss: 0.7419\n",
      "\n",
      "Epoch [361/500]\n",
      "  Batch [0/31] D_loss: -3.8515, G_loss: -2.7289\n",
      "  Batch [10/31] D_loss: -2.2482, G_loss: 2.7907\n",
      "  Batch [20/31] D_loss: -2.9826, G_loss: -5.4211\n",
      "  Batch [30/31] D_loss: -2.5589, G_loss: 1.6967\n",
      "\n",
      "Epoch 361 Summary:\n",
      "  Average D_loss: -1.2351\n",
      "  Average G_loss: -0.6087\n",
      "\n",
      "Epoch [362/500]\n",
      "  Batch [0/31] D_loss: -2.6503, G_loss: -0.2633\n",
      "  Batch [10/31] D_loss: -2.3276, G_loss: 0.5658\n",
      "  Batch [20/31] D_loss: -2.8676, G_loss: 0.9332\n",
      "  Batch [30/31] D_loss: -2.0322, G_loss: -0.7022\n",
      "\n",
      "Epoch 362 Summary:\n",
      "  Average D_loss: -1.2262\n",
      "  Average G_loss: 0.8679\n",
      "\n",
      "Epoch [363/500]\n",
      "  Batch [0/31] D_loss: -2.4497, G_loss: -1.6610\n",
      "  Batch [10/31] D_loss: -2.4097, G_loss: -1.1168\n",
      "  Batch [20/31] D_loss: -4.1432, G_loss: -4.1799\n",
      "  Batch [30/31] D_loss: -2.6427, G_loss: 3.2790\n",
      "\n",
      "Epoch 363 Summary:\n",
      "  Average D_loss: -1.3655\n",
      "  Average G_loss: -0.2891\n",
      "\n",
      "Epoch [364/500]\n",
      "  Batch [0/31] D_loss: -1.5116, G_loss: 1.8428\n",
      "  Batch [10/31] D_loss: -3.0526, G_loss: -4.1212\n",
      "  Batch [20/31] D_loss: -2.3445, G_loss: 0.1091\n",
      "  Batch [30/31] D_loss: -3.1814, G_loss: -3.4189\n",
      "\n",
      "Epoch 364 Summary:\n",
      "  Average D_loss: -1.1640\n",
      "  Average G_loss: -0.1384\n",
      "\n",
      "Epoch [365/500]\n",
      "  Batch [0/31] D_loss: -2.2752, G_loss: -0.1213\n",
      "  Batch [10/31] D_loss: -2.6199, G_loss: -0.4276\n",
      "  Batch [20/31] D_loss: -2.1148, G_loss: 2.4638\n",
      "  Batch [30/31] D_loss: -2.3677, G_loss: 1.6801\n",
      "\n",
      "Epoch 365 Summary:\n",
      "  Average D_loss: -1.2533\n",
      "  Average G_loss: 0.9645\n",
      "\n",
      "Epoch [366/500]\n",
      "  Batch [0/31] D_loss: -2.3642, G_loss: 1.2392\n",
      "  Batch [10/31] D_loss: -3.4883, G_loss: 0.7795\n",
      "  Batch [20/31] D_loss: -1.6332, G_loss: 3.3548\n",
      "  Batch [30/31] D_loss: -2.7284, G_loss: -5.3200\n",
      "\n",
      "Epoch 366 Summary:\n",
      "  Average D_loss: -1.0806\n",
      "  Average G_loss: -0.1354\n",
      "\n",
      "Epoch [367/500]\n",
      "  Batch [0/31] D_loss: -0.6733, G_loss: -4.2056\n",
      "  Batch [10/31] D_loss: -3.0263, G_loss: -3.6870\n",
      "  Batch [20/31] D_loss: -2.5891, G_loss: 4.5392\n",
      "  Batch [30/31] D_loss: -2.5931, G_loss: 0.6591\n",
      "\n",
      "Epoch 367 Summary:\n",
      "  Average D_loss: -1.1048\n",
      "  Average G_loss: 0.7970\n",
      "\n",
      "Epoch [368/500]\n",
      "  Batch [0/31] D_loss: -2.7770, G_loss: 2.5527\n",
      "  Batch [10/31] D_loss: -2.6472, G_loss: -5.2351\n",
      "  Batch [20/31] D_loss: -1.0051, G_loss: -3.1084\n",
      "  Batch [30/31] D_loss: -2.5618, G_loss: 2.6368\n",
      "\n",
      "Epoch 368 Summary:\n",
      "  Average D_loss: -1.0699\n",
      "  Average G_loss: -0.9669\n",
      "\n",
      "Epoch [369/500]\n",
      "  Batch [0/31] D_loss: -2.1621, G_loss: 0.0859\n",
      "  Batch [10/31] D_loss: -2.5917, G_loss: 3.2536\n",
      "  Batch [20/31] D_loss: -1.2275, G_loss: -4.5638\n",
      "  Batch [30/31] D_loss: -3.0184, G_loss: 2.3355\n",
      "\n",
      "Epoch 369 Summary:\n",
      "  Average D_loss: -0.9837\n",
      "  Average G_loss: -0.6999\n",
      "\n",
      "Epoch [370/500]\n",
      "  Batch [0/31] D_loss: -1.6312, G_loss: 1.5736\n",
      "  Batch [10/31] D_loss: -1.8599, G_loss: 3.7496\n",
      "  Batch [20/31] D_loss: -2.5778, G_loss: 5.4716\n",
      "  Batch [30/31] D_loss: -2.9044, G_loss: -4.6214\n",
      "\n",
      "Epoch 370 Summary:\n",
      "  Average D_loss: -1.0710\n",
      "  Average G_loss: 1.4148\n",
      "\n",
      "Epoch [371/500]\n",
      "  Batch [0/31] D_loss: -2.5894, G_loss: -1.0616\n",
      "  Batch [10/31] D_loss: -2.7747, G_loss: -3.9265\n",
      "  Batch [20/31] D_loss: -3.6893, G_loss: 5.0111\n",
      "  Batch [30/31] D_loss: -3.2291, G_loss: -2.6799\n",
      "\n",
      "Epoch 371 Summary:\n",
      "  Average D_loss: -1.2653\n",
      "  Average G_loss: -0.1654\n",
      "\n",
      "Epoch [372/500]\n",
      "  Batch [0/31] D_loss: -1.7099, G_loss: -1.3890\n",
      "  Batch [10/31] D_loss: -2.8331, G_loss: -4.8025\n",
      "  Batch [20/31] D_loss: -2.0787, G_loss: 2.5638\n",
      "  Batch [30/31] D_loss: -1.6527, G_loss: -2.8303\n",
      "\n",
      "Epoch 372 Summary:\n",
      "  Average D_loss: -1.1578\n",
      "  Average G_loss: -0.0984\n",
      "\n",
      "Epoch [373/500]\n",
      "  Batch [0/31] D_loss: -1.5001, G_loss: -2.8171\n",
      "  Batch [10/31] D_loss: -2.6168, G_loss: 1.8479\n",
      "  Batch [20/31] D_loss: -3.5509, G_loss: -7.2406\n",
      "  Batch [30/31] D_loss: -1.8031, G_loss: 0.6676\n",
      "\n",
      "Epoch 373 Summary:\n",
      "  Average D_loss: -1.1274\n",
      "  Average G_loss: -1.3364\n",
      "\n",
      "Epoch [374/500]\n",
      "  Batch [0/31] D_loss: -2.0224, G_loss: 0.8364\n",
      "  Batch [10/31] D_loss: -1.0113, G_loss: 3.4927\n",
      "  Batch [20/31] D_loss: -3.1293, G_loss: 2.5487\n",
      "  Batch [30/31] D_loss: -1.9252, G_loss: -1.5334\n",
      "\n",
      "Epoch 374 Summary:\n",
      "  Average D_loss: -0.8566\n",
      "  Average G_loss: 1.7730\n",
      "\n",
      "Epoch [375/500]\n",
      "  Batch [0/31] D_loss: -2.4553, G_loss: -3.4729\n",
      "  Batch [10/31] D_loss: -2.4096, G_loss: -2.1241\n",
      "  Batch [20/31] D_loss: -3.6624, G_loss: -2.1817\n",
      "  Batch [30/31] D_loss: -3.2567, G_loss: 4.2680\n",
      "\n",
      "Epoch 375 Summary:\n",
      "  Average D_loss: -1.2615\n",
      "  Average G_loss: 0.4223\n",
      "\n",
      "Epoch [376/500]\n",
      "  Batch [0/31] D_loss: -2.2420, G_loss: 0.7868\n",
      "  Batch [10/31] D_loss: -3.8314, G_loss: -3.3323\n",
      "  Batch [20/31] D_loss: -3.7249, G_loss: -7.7186\n",
      "  Batch [30/31] D_loss: -2.7123, G_loss: 1.1927\n",
      "\n",
      "Epoch 376 Summary:\n",
      "  Average D_loss: -1.2223\n",
      "  Average G_loss: -0.0170\n",
      "\n",
      "Epoch [377/500]\n",
      "  Batch [0/31] D_loss: -2.4236, G_loss: -2.4409\n",
      "  Batch [10/31] D_loss: -0.2071, G_loss: -1.4183\n",
      "  Batch [20/31] D_loss: -2.8618, G_loss: -0.0047\n",
      "  Batch [30/31] D_loss: -3.3026, G_loss: 6.8228\n",
      "\n",
      "Epoch 377 Summary:\n",
      "  Average D_loss: -1.0643\n",
      "  Average G_loss: 0.3994\n",
      "\n",
      "Epoch [378/500]\n",
      "  Batch [0/31] D_loss: -0.2518, G_loss: 3.1551\n",
      "  Batch [10/31] D_loss: -2.0448, G_loss: -3.0913\n",
      "  Batch [20/31] D_loss: -1.5811, G_loss: -1.6185\n",
      "  Batch [30/31] D_loss: -3.1464, G_loss: 3.3239\n",
      "\n",
      "Epoch 378 Summary:\n",
      "  Average D_loss: -0.9457\n",
      "  Average G_loss: -0.8970\n",
      "\n",
      "Epoch [379/500]\n",
      "  Batch [0/31] D_loss: -2.1745, G_loss: 0.7090\n",
      "  Batch [10/31] D_loss: -2.4419, G_loss: 0.5152\n",
      "  Batch [20/31] D_loss: -1.9839, G_loss: -2.5614\n",
      "  Batch [30/31] D_loss: -2.4576, G_loss: 0.6498\n",
      "\n",
      "Epoch 379 Summary:\n",
      "  Average D_loss: -1.0746\n",
      "  Average G_loss: 0.4247\n",
      "\n",
      "Epoch [380/500]\n",
      "  Batch [0/31] D_loss: -3.1766, G_loss: -1.1689\n",
      "  Batch [10/31] D_loss: -2.3246, G_loss: 2.8846\n",
      "  Batch [20/31] D_loss: -2.3934, G_loss: -2.1209\n",
      "  Batch [30/31] D_loss: -3.1218, G_loss: -2.8302\n",
      "\n",
      "Epoch 380 Summary:\n",
      "  Average D_loss: -1.1823\n",
      "  Average G_loss: 0.5566\n",
      "\n",
      "Epoch [381/500]\n",
      "  Batch [0/31] D_loss: -2.3187, G_loss: -3.1979\n",
      "  Batch [10/31] D_loss: -3.4888, G_loss: -3.5169\n",
      "  Batch [20/31] D_loss: -5.1661, G_loss: 6.1718\n",
      "  Batch [30/31] D_loss: -2.7374, G_loss: 3.1114\n",
      "\n",
      "Epoch 381 Summary:\n",
      "  Average D_loss: -1.2475\n",
      "  Average G_loss: 0.1753\n",
      "\n",
      "Epoch [382/500]\n",
      "  Batch [0/31] D_loss: -2.4099, G_loss: 3.1621\n",
      "  Batch [10/31] D_loss: -2.5843, G_loss: -5.7526\n",
      "  Batch [20/31] D_loss: -1.6480, G_loss: -4.2995\n",
      "  Batch [30/31] D_loss: -1.5477, G_loss: 2.2670\n",
      "\n",
      "Epoch 382 Summary:\n",
      "  Average D_loss: -0.9721\n",
      "  Average G_loss: -1.6516\n",
      "\n",
      "Epoch [383/500]\n",
      "  Batch [0/31] D_loss: -1.4602, G_loss: 2.6277\n",
      "  Batch [10/31] D_loss: -2.3578, G_loss: 4.5583\n",
      "  Batch [20/31] D_loss: -2.7428, G_loss: -4.9095\n",
      "  Batch [30/31] D_loss: -1.3818, G_loss: -2.7919\n",
      "\n",
      "Epoch 383 Summary:\n",
      "  Average D_loss: -1.3027\n",
      "  Average G_loss: -1.1035\n",
      "\n",
      "Epoch [384/500]\n",
      "  Batch [0/31] D_loss: -0.7283, G_loss: -3.4261\n",
      "  Batch [10/31] D_loss: -3.5882, G_loss: 9.2868\n",
      "  Batch [20/31] D_loss: -1.6769, G_loss: -4.2493\n",
      "  Batch [30/31] D_loss: -1.8959, G_loss: -0.3069\n",
      "\n",
      "Epoch 384 Summary:\n",
      "  Average D_loss: -1.0091\n",
      "  Average G_loss: 0.1171\n",
      "\n",
      "Epoch [385/500]\n",
      "  Batch [0/31] D_loss: -0.9219, G_loss: -0.0084\n",
      "  Batch [10/31] D_loss: -2.3565, G_loss: 7.5824\n",
      "  Batch [20/31] D_loss: -2.5410, G_loss: -5.4999\n",
      "  Batch [30/31] D_loss: -2.2184, G_loss: -2.2451\n",
      "\n",
      "Epoch 385 Summary:\n",
      "  Average D_loss: -0.8372\n",
      "  Average G_loss: 0.6918\n",
      "\n",
      "Epoch [386/500]\n",
      "  Batch [0/31] D_loss: -1.3031, G_loss: -1.1934\n",
      "  Batch [10/31] D_loss: -2.1366, G_loss: 0.8001\n",
      "  Batch [20/31] D_loss: -2.2936, G_loss: 0.7067\n",
      "  Batch [30/31] D_loss: -2.4231, G_loss: 1.1720\n",
      "\n",
      "Epoch 386 Summary:\n",
      "  Average D_loss: -1.0661\n",
      "  Average G_loss: 1.9248\n",
      "\n",
      "Epoch [387/500]\n",
      "  Batch [0/31] D_loss: -1.9798, G_loss: -0.8056\n",
      "  Batch [10/31] D_loss: -3.8755, G_loss: -3.4704\n",
      "  Batch [20/31] D_loss: -2.5128, G_loss: 4.1930\n",
      "  Batch [30/31] D_loss: -1.0277, G_loss: 2.8468\n",
      "\n",
      "Epoch 387 Summary:\n",
      "  Average D_loss: -1.1822\n",
      "  Average G_loss: 0.0164\n",
      "\n",
      "Epoch [388/500]\n",
      "  Batch [0/31] D_loss: -1.8596, G_loss: 0.7178\n",
      "  Batch [10/31] D_loss: -2.7529, G_loss: -1.3414\n",
      "  Batch [20/31] D_loss: -2.5137, G_loss: 2.6226\n",
      "  Batch [30/31] D_loss: -3.8280, G_loss: 2.6094\n",
      "\n",
      "Epoch 388 Summary:\n",
      "  Average D_loss: -1.1717\n",
      "  Average G_loss: -0.6883\n",
      "\n",
      "Epoch [389/500]\n",
      "  Batch [0/31] D_loss: -2.9815, G_loss: 4.6382\n",
      "  Batch [10/31] D_loss: -2.5645, G_loss: 0.8155\n",
      "  Batch [20/31] D_loss: -1.8202, G_loss: -0.4827\n",
      "  Batch [30/31] D_loss: -4.0535, G_loss: 4.1023\n",
      "\n",
      "Epoch 389 Summary:\n",
      "  Average D_loss: -1.1281\n",
      "  Average G_loss: 1.2180\n",
      "\n",
      "Epoch [390/500]\n",
      "  Batch [0/31] D_loss: -2.2284, G_loss: 3.9714\n",
      "  Batch [10/31] D_loss: -2.9814, G_loss: -5.0129\n",
      "  Batch [20/31] D_loss: -4.2493, G_loss: 3.9310\n",
      "  Batch [30/31] D_loss: -3.0365, G_loss: -0.7199\n",
      "\n",
      "Epoch 390 Summary:\n",
      "  Average D_loss: -1.2339\n",
      "  Average G_loss: -0.5460\n",
      "\n",
      "Epoch [391/500]\n",
      "  Batch [0/31] D_loss: -1.4780, G_loss: 0.1962\n",
      "  Batch [10/31] D_loss: -2.5939, G_loss: 0.4023\n",
      "  Batch [20/31] D_loss: -2.1133, G_loss: 1.2262\n",
      "  Batch [30/31] D_loss: -1.7930, G_loss: 1.7647\n",
      "\n",
      "Epoch 391 Summary:\n",
      "  Average D_loss: -1.0962\n",
      "  Average G_loss: 0.3856\n",
      "\n",
      "Epoch [392/500]\n",
      "  Batch [0/31] D_loss: -2.7198, G_loss: 2.2220\n",
      "  Batch [10/31] D_loss: -3.5613, G_loss: -1.6101\n",
      "  Batch [20/31] D_loss: -2.7291, G_loss: 2.6438\n",
      "  Batch [30/31] D_loss: -3.6635, G_loss: 2.0162\n",
      "\n",
      "Epoch 392 Summary:\n",
      "  Average D_loss: -1.2544\n",
      "  Average G_loss: 0.1089\n",
      "\n",
      "Epoch [393/500]\n",
      "  Batch [0/31] D_loss: -3.0190, G_loss: 3.8168\n",
      "  Batch [10/31] D_loss: -2.0642, G_loss: -1.7985\n",
      "  Batch [20/31] D_loss: -1.6271, G_loss: 3.0447\n",
      "  Batch [30/31] D_loss: -2.1773, G_loss: 2.1552\n",
      "\n",
      "Epoch 393 Summary:\n",
      "  Average D_loss: -1.1900\n",
      "  Average G_loss: -0.0825\n",
      "\n",
      "Epoch [394/500]\n",
      "  Batch [0/31] D_loss: -2.8688, G_loss: 3.1776\n",
      "  Batch [10/31] D_loss: -4.4324, G_loss: -8.5110\n",
      "  Batch [20/31] D_loss: -2.4658, G_loss: 3.2654\n",
      "  Batch [30/31] D_loss: -2.9036, G_loss: 0.3852\n",
      "\n",
      "Epoch 394 Summary:\n",
      "  Average D_loss: -1.3953\n",
      "  Average G_loss: 0.2002\n",
      "\n",
      "Epoch [395/500]\n",
      "  Batch [0/31] D_loss: -2.1456, G_loss: 1.9265\n",
      "  Batch [10/31] D_loss: -2.0400, G_loss: -3.1781\n",
      "  Batch [20/31] D_loss: -3.8174, G_loss: -4.1984\n",
      "  Batch [30/31] D_loss: -2.5503, G_loss: -2.5357\n",
      "\n",
      "Epoch 395 Summary:\n",
      "  Average D_loss: -1.2777\n",
      "  Average G_loss: -0.2168\n",
      "\n",
      "Epoch [396/500]\n",
      "  Batch [0/31] D_loss: -1.7838, G_loss: -1.8332\n",
      "  Batch [10/31] D_loss: -3.7340, G_loss: 3.2820\n",
      "  Batch [20/31] D_loss: -3.0051, G_loss: -2.2098\n",
      "  Batch [30/31] D_loss: -1.7393, G_loss: -0.2198\n",
      "\n",
      "Epoch 396 Summary:\n",
      "  Average D_loss: -1.2545\n",
      "  Average G_loss: 0.3343\n",
      "\n",
      "Epoch [397/500]\n",
      "  Batch [0/31] D_loss: -2.5643, G_loss: -2.1065\n",
      "  Batch [10/31] D_loss: -1.9759, G_loss: -0.8964\n",
      "  Batch [20/31] D_loss: -3.5676, G_loss: -2.7920\n",
      "  Batch [30/31] D_loss: -2.1835, G_loss: -0.3339\n",
      "\n",
      "Epoch 397 Summary:\n",
      "  Average D_loss: -1.1836\n",
      "  Average G_loss: -0.3754\n",
      "\n",
      "Epoch [398/500]\n",
      "  Batch [0/31] D_loss: -2.1967, G_loss: 1.8375\n",
      "  Batch [10/31] D_loss: -2.8690, G_loss: -0.7736\n",
      "  Batch [20/31] D_loss: -1.7261, G_loss: 0.8380\n",
      "  Batch [30/31] D_loss: -3.3908, G_loss: 4.6319\n",
      "\n",
      "Epoch 398 Summary:\n",
      "  Average D_loss: -1.1950\n",
      "  Average G_loss: 0.6950\n",
      "\n",
      "Epoch [399/500]\n",
      "  Batch [0/31] D_loss: -3.4953, G_loss: 4.1664\n",
      "  Batch [10/31] D_loss: -3.8317, G_loss: 1.7607\n",
      "  Batch [20/31] D_loss: -1.8613, G_loss: -1.8903\n",
      "  Batch [30/31] D_loss: -2.8416, G_loss: 2.9680\n",
      "\n",
      "Epoch 399 Summary:\n",
      "  Average D_loss: -1.2868\n",
      "  Average G_loss: -0.4218\n",
      "\n",
      "Epoch [400/500]\n",
      "  Batch [0/31] D_loss: -3.1739, G_loss: 6.3332\n",
      "  Batch [10/31] D_loss: -2.4513, G_loss: 4.9391\n",
      "  Batch [20/31] D_loss: -4.4628, G_loss: -4.7347\n",
      "  Batch [30/31] D_loss: -2.1329, G_loss: 3.4355\n",
      "\n",
      "Epoch 400 Summary:\n",
      "  Average D_loss: -1.0349\n",
      "  Average G_loss: -0.0089\n",
      "\n",
      "Epoch [401/500]\n",
      "  Batch [0/31] D_loss: -2.0150, G_loss: -2.5459\n",
      "  Batch [10/31] D_loss: -1.5062, G_loss: 4.0836\n",
      "  Batch [20/31] D_loss: -2.5328, G_loss: -3.1757\n",
      "  Batch [30/31] D_loss: -2.7876, G_loss: -1.5469\n",
      "\n",
      "Epoch 401 Summary:\n",
      "  Average D_loss: -1.2028\n",
      "  Average G_loss: 0.0285\n",
      "\n",
      "Epoch [402/500]\n",
      "  Batch [0/31] D_loss: -2.1058, G_loss: -2.8494\n",
      "  Batch [10/31] D_loss: -1.3137, G_loss: 3.7642\n",
      "  Batch [20/31] D_loss: -1.1825, G_loss: 1.0031\n",
      "  Batch [30/31] D_loss: -4.6398, G_loss: -6.9626\n",
      "\n",
      "Epoch 402 Summary:\n",
      "  Average D_loss: -1.2330\n",
      "  Average G_loss: -0.1498\n",
      "\n",
      "Epoch [403/500]\n",
      "  Batch [0/31] D_loss: -1.8626, G_loss: -7.0730\n",
      "  Batch [10/31] D_loss: -1.8585, G_loss: 2.2837\n",
      "  Batch [20/31] D_loss: -2.1998, G_loss: -2.4305\n",
      "  Batch [30/31] D_loss: -2.5177, G_loss: 2.7264\n",
      "\n",
      "Epoch 403 Summary:\n",
      "  Average D_loss: -0.9093\n",
      "  Average G_loss: 1.2474\n",
      "\n",
      "Epoch [404/500]\n",
      "  Batch [0/31] D_loss: -2.3281, G_loss: -0.2479\n",
      "  Batch [10/31] D_loss: -0.8860, G_loss: 0.2491\n",
      "  Batch [20/31] D_loss: -3.0189, G_loss: -1.8427\n",
      "  Batch [30/31] D_loss: -1.6392, G_loss: -0.7027\n",
      "\n",
      "Epoch 404 Summary:\n",
      "  Average D_loss: -1.1646\n",
      "  Average G_loss: -0.5696\n",
      "\n",
      "Epoch [405/500]\n",
      "  Batch [0/31] D_loss: -3.4725, G_loss: 2.2361\n",
      "  Batch [10/31] D_loss: -2.0176, G_loss: -2.6323\n",
      "  Batch [20/31] D_loss: -3.5999, G_loss: 3.6765\n",
      "  Batch [30/31] D_loss: -1.9954, G_loss: -0.1320\n",
      "\n",
      "Epoch 405 Summary:\n",
      "  Average D_loss: -1.2455\n",
      "  Average G_loss: 0.1403\n",
      "\n",
      "Epoch [406/500]\n",
      "  Batch [0/31] D_loss: -2.8283, G_loss: -0.9644\n",
      "  Batch [10/31] D_loss: -2.2128, G_loss: -1.5930\n",
      "  Batch [20/31] D_loss: -3.9578, G_loss: 5.3248\n",
      "  Batch [30/31] D_loss: -2.7182, G_loss: -1.9089\n",
      "\n",
      "Epoch 406 Summary:\n",
      "  Average D_loss: -1.0353\n",
      "  Average G_loss: 0.1513\n",
      "\n",
      "Epoch [407/500]\n",
      "  Batch [0/31] D_loss: -2.2725, G_loss: -2.2765\n",
      "  Batch [10/31] D_loss: -3.1740, G_loss: 2.6859\n",
      "  Batch [20/31] D_loss: -2.6678, G_loss: 0.6195\n",
      "  Batch [30/31] D_loss: -2.9279, G_loss: -0.4255\n",
      "\n",
      "Epoch 407 Summary:\n",
      "  Average D_loss: -1.2733\n",
      "  Average G_loss: -0.3750\n",
      "\n",
      "Epoch [408/500]\n",
      "  Batch [0/31] D_loss: -3.6384, G_loss: -1.5627\n",
      "  Batch [10/31] D_loss: -2.3300, G_loss: 1.9334\n",
      "  Batch [20/31] D_loss: -1.9044, G_loss: -2.8775\n",
      "  Batch [30/31] D_loss: -3.1064, G_loss: 3.5085\n",
      "\n",
      "Epoch 408 Summary:\n",
      "  Average D_loss: -1.2628\n",
      "  Average G_loss: 0.1965\n",
      "\n",
      "Epoch [409/500]\n",
      "  Batch [0/31] D_loss: -1.3059, G_loss: 2.9364\n",
      "  Batch [10/31] D_loss: -2.0698, G_loss: 2.4980\n",
      "  Batch [20/31] D_loss: -4.2176, G_loss: -3.5694\n",
      "  Batch [30/31] D_loss: -3.0322, G_loss: 3.9128\n",
      "\n",
      "Epoch 409 Summary:\n",
      "  Average D_loss: -1.2352\n",
      "  Average G_loss: 0.3040\n",
      "\n",
      "Epoch [410/500]\n",
      "  Batch [0/31] D_loss: -2.7751, G_loss: -0.0154\n",
      "  Batch [10/31] D_loss: -2.6924, G_loss: -3.7314\n",
      "  Batch [20/31] D_loss: -1.8939, G_loss: 0.3585\n",
      "  Batch [30/31] D_loss: -2.4704, G_loss: -5.4206\n",
      "\n",
      "Epoch 410 Summary:\n",
      "  Average D_loss: -1.2897\n",
      "  Average G_loss: 0.3698\n",
      "\n",
      "Epoch [411/500]\n",
      "  Batch [0/31] D_loss: -0.7511, G_loss: -4.1775\n",
      "  Batch [10/31] D_loss: -1.9998, G_loss: 0.3613\n",
      "  Batch [20/31] D_loss: -2.9933, G_loss: 0.5483\n",
      "  Batch [30/31] D_loss: -2.1682, G_loss: 2.0064\n",
      "\n",
      "Epoch 411 Summary:\n",
      "  Average D_loss: -1.1532\n",
      "  Average G_loss: -0.0460\n",
      "\n",
      "Epoch [412/500]\n",
      "  Batch [0/31] D_loss: -3.8587, G_loss: -0.4749\n",
      "  Batch [10/31] D_loss: -2.1498, G_loss: -3.2598\n",
      "  Batch [20/31] D_loss: -2.9130, G_loss: 1.5711\n",
      "  Batch [30/31] D_loss: -1.0769, G_loss: -6.1678\n",
      "\n",
      "Epoch 412 Summary:\n",
      "  Average D_loss: -1.3056\n",
      "  Average G_loss: -0.1952\n",
      "\n",
      "Epoch [413/500]\n",
      "  Batch [0/31] D_loss: -0.9094, G_loss: -3.3098\n",
      "  Batch [10/31] D_loss: -3.2667, G_loss: -1.4688\n",
      "  Batch [20/31] D_loss: -2.4530, G_loss: -2.6749\n",
      "  Batch [30/31] D_loss: -1.7731, G_loss: -5.1861\n",
      "\n",
      "Epoch 413 Summary:\n",
      "  Average D_loss: -0.9827\n",
      "  Average G_loss: -1.2489\n",
      "\n",
      "Epoch [414/500]\n",
      "  Batch [0/31] D_loss: -1.7760, G_loss: -5.1465\n",
      "  Batch [10/31] D_loss: -2.9613, G_loss: 5.0733\n",
      "  Batch [20/31] D_loss: -1.4981, G_loss: 4.4428\n",
      "  Batch [30/31] D_loss: -2.3490, G_loss: -2.2526\n",
      "\n",
      "Epoch 414 Summary:\n",
      "  Average D_loss: -0.9338\n",
      "  Average G_loss: 2.0321\n",
      "\n",
      "Epoch [415/500]\n",
      "  Batch [0/31] D_loss: -2.0590, G_loss: -2.8713\n",
      "  Batch [10/31] D_loss: -2.8121, G_loss: -4.8441\n",
      "  Batch [20/31] D_loss: -1.5367, G_loss: 0.6012\n",
      "  Batch [30/31] D_loss: -5.0226, G_loss: 8.2161\n",
      "\n",
      "Epoch 415 Summary:\n",
      "  Average D_loss: -0.9765\n",
      "  Average G_loss: -0.8882\n",
      "\n",
      "Epoch [416/500]\n",
      "  Batch [0/31] D_loss: -2.8864, G_loss: 6.4285\n",
      "  Batch [10/31] D_loss: -2.5211, G_loss: 1.9072\n",
      "  Batch [20/31] D_loss: -1.4504, G_loss: -0.9355\n",
      "  Batch [30/31] D_loss: -3.4668, G_loss: 3.1329\n",
      "\n",
      "Epoch 416 Summary:\n",
      "  Average D_loss: -0.9498\n",
      "  Average G_loss: 0.3081\n",
      "\n",
      "Epoch [417/500]\n",
      "  Batch [0/31] D_loss: -2.9173, G_loss: 3.6760\n",
      "  Batch [10/31] D_loss: -2.6231, G_loss: -0.8898\n",
      "  Batch [20/31] D_loss: -2.5295, G_loss: -4.7166\n",
      "  Batch [30/31] D_loss: -2.1300, G_loss: 1.0702\n",
      "\n",
      "Epoch 417 Summary:\n",
      "  Average D_loss: -1.2469\n",
      "  Average G_loss: 0.1601\n",
      "\n",
      "Epoch [418/500]\n",
      "  Batch [0/31] D_loss: -1.9859, G_loss: -1.0515\n",
      "  Batch [10/31] D_loss: -2.2177, G_loss: -1.9714\n",
      "  Batch [20/31] D_loss: -2.0755, G_loss: 3.4711\n",
      "  Batch [30/31] D_loss: -2.8251, G_loss: 7.4267\n",
      "\n",
      "Epoch 418 Summary:\n",
      "  Average D_loss: -1.1878\n",
      "  Average G_loss: 0.1369\n",
      "\n",
      "Epoch [419/500]\n",
      "  Batch [0/31] D_loss: -0.2371, G_loss: 3.2424\n",
      "  Batch [10/31] D_loss: -2.1877, G_loss: -1.2115\n",
      "  Batch [20/31] D_loss: -1.9973, G_loss: -0.6505\n",
      "  Batch [30/31] D_loss: -2.7822, G_loss: 2.6291\n",
      "\n",
      "Epoch 419 Summary:\n",
      "  Average D_loss: -1.0797\n",
      "  Average G_loss: -0.2062\n",
      "\n",
      "Epoch [420/500]\n",
      "  Batch [0/31] D_loss: -2.6440, G_loss: 5.1681\n",
      "  Batch [10/31] D_loss: -2.3276, G_loss: -1.4536\n",
      "  Batch [20/31] D_loss: -2.3921, G_loss: -2.2924\n",
      "  Batch [30/31] D_loss: -2.1297, G_loss: 1.5657\n",
      "\n",
      "Epoch 420 Summary:\n",
      "  Average D_loss: -1.1268\n",
      "  Average G_loss: 0.6183\n",
      "\n",
      "Epoch [421/500]\n",
      "  Batch [0/31] D_loss: -4.1274, G_loss: 4.2413\n",
      "  Batch [10/31] D_loss: -2.6781, G_loss: -2.8272\n",
      "  Batch [20/31] D_loss: -1.5628, G_loss: -3.1008\n",
      "  Batch [30/31] D_loss: -3.6188, G_loss: -5.1415\n",
      "\n",
      "Epoch 421 Summary:\n",
      "  Average D_loss: -1.2389\n",
      "  Average G_loss: -0.2707\n",
      "\n",
      "Epoch [422/500]\n",
      "  Batch [0/31] D_loss: -2.3448, G_loss: -4.3642\n",
      "  Batch [10/31] D_loss: -2.9816, G_loss: 2.6381\n",
      "  Batch [20/31] D_loss: -1.8420, G_loss: -0.9630\n",
      "  Batch [30/31] D_loss: -1.6726, G_loss: 0.2165\n",
      "\n",
      "Epoch 422 Summary:\n",
      "  Average D_loss: -1.2376\n",
      "  Average G_loss: 0.0999\n",
      "\n",
      "Epoch [423/500]\n",
      "  Batch [0/31] D_loss: -2.7423, G_loss: 1.6955\n",
      "  Batch [10/31] D_loss: -1.9615, G_loss: 1.9556\n",
      "  Batch [20/31] D_loss: -1.9462, G_loss: -0.3589\n",
      "  Batch [30/31] D_loss: -2.6962, G_loss: -3.3022\n",
      "\n",
      "Epoch 423 Summary:\n",
      "  Average D_loss: -1.1057\n",
      "  Average G_loss: 0.3948\n",
      "\n",
      "Epoch [424/500]\n",
      "  Batch [0/31] D_loss: -3.1122, G_loss: -4.1294\n",
      "  Batch [10/31] D_loss: -2.3973, G_loss: -1.9708\n",
      "  Batch [20/31] D_loss: -2.6808, G_loss: 6.7809\n",
      "  Batch [30/31] D_loss: -2.1043, G_loss: 2.1717\n",
      "\n",
      "Epoch 424 Summary:\n",
      "  Average D_loss: -1.3797\n",
      "  Average G_loss: 1.0041\n",
      "\n",
      "Epoch [425/500]\n",
      "  Batch [0/31] D_loss: -2.5432, G_loss: 5.3342\n",
      "  Batch [10/31] D_loss: -0.3202, G_loss: -4.3137\n",
      "  Batch [20/31] D_loss: -1.4339, G_loss: -1.5004\n",
      "  Batch [30/31] D_loss: -2.3048, G_loss: 4.0153\n",
      "\n",
      "Epoch 425 Summary:\n",
      "  Average D_loss: -0.8901\n",
      "  Average G_loss: -1.5616\n",
      "\n",
      "Epoch [426/500]\n",
      "  Batch [0/31] D_loss: -2.6593, G_loss: 5.2972\n",
      "  Batch [10/31] D_loss: -0.8089, G_loss: -1.3578\n",
      "  Batch [20/31] D_loss: -1.5857, G_loss: -0.2572\n",
      "  Batch [30/31] D_loss: -3.1566, G_loss: 3.4402\n",
      "\n",
      "Epoch 426 Summary:\n",
      "  Average D_loss: -0.9502\n",
      "  Average G_loss: 0.7314\n",
      "\n",
      "Epoch [427/500]\n",
      "  Batch [0/31] D_loss: -2.4799, G_loss: -0.4558\n",
      "  Batch [10/31] D_loss: -2.2605, G_loss: 2.7610\n",
      "  Batch [20/31] D_loss: -2.8887, G_loss: -2.3477\n",
      "  Batch [30/31] D_loss: -2.9873, G_loss: 1.0557\n",
      "\n",
      "Epoch 427 Summary:\n",
      "  Average D_loss: -1.1733\n",
      "  Average G_loss: 0.0149\n",
      "\n",
      "Epoch [428/500]\n",
      "  Batch [0/31] D_loss: -4.3266, G_loss: 5.7913\n",
      "  Batch [10/31] D_loss: -3.2831, G_loss: -1.3919\n",
      "  Batch [20/31] D_loss: -2.1712, G_loss: -0.2982\n",
      "  Batch [30/31] D_loss: -2.7175, G_loss: -3.7408\n",
      "\n",
      "Epoch 428 Summary:\n",
      "  Average D_loss: -1.2150\n",
      "  Average G_loss: 0.3351\n",
      "\n",
      "Epoch [429/500]\n",
      "  Batch [0/31] D_loss: -2.2672, G_loss: -0.5066\n",
      "  Batch [10/31] D_loss: -2.5843, G_loss: -1.2539\n",
      "  Batch [20/31] D_loss: -1.8307, G_loss: -0.6449\n",
      "  Batch [30/31] D_loss: -1.8627, G_loss: -0.1700\n",
      "\n",
      "Epoch 429 Summary:\n",
      "  Average D_loss: -1.0450\n",
      "  Average G_loss: -0.9448\n",
      "\n",
      "Epoch [430/500]\n",
      "  Batch [0/31] D_loss: -2.4494, G_loss: 1.7067\n",
      "  Batch [10/31] D_loss: -2.3791, G_loss: 3.8328\n",
      "  Batch [20/31] D_loss: -2.8485, G_loss: -2.9841\n",
      "  Batch [30/31] D_loss: -1.6002, G_loss: 3.5928\n",
      "\n",
      "Epoch 430 Summary:\n",
      "  Average D_loss: -1.1311\n",
      "  Average G_loss: 0.7486\n",
      "\n",
      "Epoch [431/500]\n",
      "  Batch [0/31] D_loss: -2.0804, G_loss: 2.8440\n",
      "  Batch [10/31] D_loss: -1.9538, G_loss: 1.8544\n",
      "  Batch [20/31] D_loss: -4.0939, G_loss: 3.9800\n",
      "  Batch [30/31] D_loss: -2.1179, G_loss: 1.3694\n",
      "\n",
      "Epoch 431 Summary:\n",
      "  Average D_loss: -1.0544\n",
      "  Average G_loss: 0.2320\n",
      "\n",
      "Epoch [432/500]\n",
      "  Batch [0/31] D_loss: -2.8141, G_loss: -0.8761\n",
      "  Batch [10/31] D_loss: -3.1161, G_loss: 0.7193\n",
      "  Batch [20/31] D_loss: -2.0243, G_loss: 2.9901\n",
      "  Batch [30/31] D_loss: -2.7107, G_loss: 2.7631\n",
      "\n",
      "Epoch 432 Summary:\n",
      "  Average D_loss: -1.1578\n",
      "  Average G_loss: 0.2144\n",
      "\n",
      "Epoch [433/500]\n",
      "  Batch [0/31] D_loss: -1.6655, G_loss: -0.1430\n",
      "  Batch [10/31] D_loss: -1.7930, G_loss: -0.3740\n",
      "  Batch [20/31] D_loss: -1.9914, G_loss: 0.6999\n",
      "  Batch [30/31] D_loss: -1.9404, G_loss: 4.5226\n",
      "\n",
      "Epoch 433 Summary:\n",
      "  Average D_loss: -1.2603\n",
      "  Average G_loss: -0.1983\n",
      "\n",
      "Epoch [434/500]\n",
      "  Batch [0/31] D_loss: -2.4548, G_loss: 5.2006\n",
      "  Batch [10/31] D_loss: -1.1356, G_loss: -3.6118\n",
      "  Batch [20/31] D_loss: -2.2889, G_loss: 2.3917\n",
      "  Batch [30/31] D_loss: -3.3516, G_loss: -2.4424\n",
      "\n",
      "Epoch 434 Summary:\n",
      "  Average D_loss: -0.9342\n",
      "  Average G_loss: 0.7477\n",
      "\n",
      "Epoch [435/500]\n",
      "  Batch [0/31] D_loss: -2.6187, G_loss: 0.9833\n",
      "  Batch [10/31] D_loss: -1.9091, G_loss: -2.2254\n",
      "  Batch [20/31] D_loss: -1.6909, G_loss: -0.0601\n",
      "  Batch [30/31] D_loss: -2.6041, G_loss: 2.0837\n",
      "\n",
      "Epoch 435 Summary:\n",
      "  Average D_loss: -1.1874\n",
      "  Average G_loss: -0.3081\n",
      "\n",
      "Epoch [436/500]\n",
      "  Batch [0/31] D_loss: -5.0059, G_loss: 4.3935\n",
      "  Batch [10/31] D_loss: -2.9092, G_loss: 1.5042\n",
      "  Batch [20/31] D_loss: -2.5476, G_loss: -5.0109\n",
      "  Batch [30/31] D_loss: -1.7494, G_loss: 0.2441\n",
      "\n",
      "Epoch 436 Summary:\n",
      "  Average D_loss: -1.2201\n",
      "  Average G_loss: -1.0524\n",
      "\n",
      "Epoch [437/500]\n",
      "  Batch [0/31] D_loss: -2.0423, G_loss: 1.2806\n",
      "  Batch [10/31] D_loss: -0.7436, G_loss: 5.4049\n",
      "  Batch [20/31] D_loss: -1.2616, G_loss: -0.3020\n",
      "  Batch [30/31] D_loss: -3.8635, G_loss: -2.8054\n",
      "\n",
      "Epoch 437 Summary:\n",
      "  Average D_loss: -1.1730\n",
      "  Average G_loss: 1.5460\n",
      "\n",
      "Epoch [438/500]\n",
      "  Batch [0/31] D_loss: -1.7287, G_loss: -3.6243\n",
      "  Batch [10/31] D_loss: -1.8392, G_loss: -0.0556\n",
      "  Batch [20/31] D_loss: -2.0531, G_loss: 3.0096\n",
      "  Batch [30/31] D_loss: -2.4329, G_loss: -2.2864\n",
      "\n",
      "Epoch 438 Summary:\n",
      "  Average D_loss: -0.9516\n",
      "  Average G_loss: 0.6181\n",
      "\n",
      "Epoch [439/500]\n",
      "  Batch [0/31] D_loss: -1.3378, G_loss: -1.3986\n",
      "  Batch [10/31] D_loss: -0.0667, G_loss: -3.6042\n",
      "  Batch [20/31] D_loss: -4.2554, G_loss: 6.5552\n",
      "  Batch [30/31] D_loss: -2.2892, G_loss: -0.9684\n",
      "\n",
      "Epoch 439 Summary:\n",
      "  Average D_loss: -1.0390\n",
      "  Average G_loss: -0.6183\n",
      "\n",
      "Epoch [440/500]\n",
      "  Batch [0/31] D_loss: -1.5463, G_loss: -1.0003\n",
      "  Batch [10/31] D_loss: -2.8889, G_loss: 0.1372\n",
      "  Batch [20/31] D_loss: -2.3114, G_loss: -6.6652\n",
      "  Batch [30/31] D_loss: -3.0956, G_loss: 1.2555\n",
      "\n",
      "Epoch 440 Summary:\n",
      "  Average D_loss: -1.0643\n",
      "  Average G_loss: -1.4599\n",
      "\n",
      "Epoch [441/500]\n",
      "  Batch [0/31] D_loss: -2.0657, G_loss: 3.3394\n",
      "  Batch [10/31] D_loss: -0.1494, G_loss: 1.9902\n",
      "  Batch [20/31] D_loss: -2.3052, G_loss: -5.1403\n",
      "  Batch [30/31] D_loss: -2.5284, G_loss: 1.8803\n",
      "\n",
      "Epoch 441 Summary:\n",
      "  Average D_loss: -0.9484\n",
      "  Average G_loss: 0.5052\n",
      "\n",
      "Epoch [442/500]\n",
      "  Batch [0/31] D_loss: -2.7027, G_loss: -0.8125\n",
      "  Batch [10/31] D_loss: -2.4147, G_loss: 3.7609\n",
      "  Batch [20/31] D_loss: -1.3757, G_loss: 4.0065\n",
      "  Batch [30/31] D_loss: -2.8203, G_loss: -4.6205\n",
      "\n",
      "Epoch 442 Summary:\n",
      "  Average D_loss: -1.1471\n",
      "  Average G_loss: 0.6758\n",
      "\n",
      "Epoch [443/500]\n",
      "  Batch [0/31] D_loss: -1.6856, G_loss: -2.3290\n",
      "  Batch [10/31] D_loss: -3.4540, G_loss: 5.6378\n",
      "  Batch [20/31] D_loss: -1.1095, G_loss: -0.6393\n",
      "  Batch [30/31] D_loss: -1.3095, G_loss: -7.0586\n",
      "\n",
      "Epoch 443 Summary:\n",
      "  Average D_loss: -1.3895\n",
      "  Average G_loss: -0.1920\n",
      "\n",
      "Epoch [444/500]\n",
      "  Batch [0/31] D_loss: -1.1959, G_loss: -5.6150\n",
      "  Batch [10/31] D_loss: -0.7496, G_loss: -0.4929\n",
      "  Batch [20/31] D_loss: -2.0434, G_loss: 4.8266\n",
      "  Batch [30/31] D_loss: -2.1860, G_loss: 1.9571\n",
      "\n",
      "Epoch 444 Summary:\n",
      "  Average D_loss: -0.7501\n",
      "  Average G_loss: 0.4098\n",
      "\n",
      "Epoch [445/500]\n",
      "  Batch [0/31] D_loss: -1.9053, G_loss: 3.7711\n",
      "  Batch [10/31] D_loss: -1.8919, G_loss: -2.2963\n",
      "  Batch [20/31] D_loss: -3.0605, G_loss: -0.9693\n",
      "  Batch [30/31] D_loss: -1.9297, G_loss: 3.8712\n",
      "\n",
      "Epoch 445 Summary:\n",
      "  Average D_loss: -1.1160\n",
      "  Average G_loss: 0.2786\n",
      "\n",
      "Epoch [446/500]\n",
      "  Batch [0/31] D_loss: -3.1760, G_loss: 3.5187\n",
      "  Batch [10/31] D_loss: -1.6698, G_loss: -2.2340\n",
      "  Batch [20/31] D_loss: -2.4141, G_loss: 1.8515\n",
      "  Batch [30/31] D_loss: -2.9378, G_loss: 2.6756\n",
      "\n",
      "Epoch 446 Summary:\n",
      "  Average D_loss: -1.0807\n",
      "  Average G_loss: 0.0450\n",
      "\n",
      "Epoch [447/500]\n",
      "  Batch [0/31] D_loss: -4.6679, G_loss: 6.1445\n",
      "  Batch [10/31] D_loss: -1.9572, G_loss: -3.2291\n",
      "  Batch [20/31] D_loss: -2.6349, G_loss: 4.3963\n",
      "  Batch [30/31] D_loss: -2.1151, G_loss: -1.9695\n",
      "\n",
      "Epoch 447 Summary:\n",
      "  Average D_loss: -1.1398\n",
      "  Average G_loss: 0.1116\n",
      "\n",
      "Epoch [448/500]\n",
      "  Batch [0/31] D_loss: -2.3663, G_loss: -0.1311\n",
      "  Batch [10/31] D_loss: -2.6130, G_loss: -7.6611\n",
      "  Batch [20/31] D_loss: -2.4088, G_loss: 1.7191\n",
      "  Batch [30/31] D_loss: -2.3588, G_loss: 5.5551\n",
      "\n",
      "Epoch 448 Summary:\n",
      "  Average D_loss: -1.1678\n",
      "  Average G_loss: -0.2391\n",
      "\n",
      "Epoch [449/500]\n",
      "  Batch [0/31] D_loss: -2.2358, G_loss: 3.7658\n",
      "  Batch [10/31] D_loss: -2.4413, G_loss: 3.3682\n",
      "  Batch [20/31] D_loss: -2.1549, G_loss: -4.5543\n",
      "  Batch [30/31] D_loss: -3.6135, G_loss: 4.6046\n",
      "\n",
      "Epoch 449 Summary:\n",
      "  Average D_loss: -1.0590\n",
      "  Average G_loss: -0.1934\n",
      "\n",
      "Epoch [450/500]\n",
      "  Batch [0/31] D_loss: -1.6751, G_loss: 0.7350\n",
      "  Batch [10/31] D_loss: -2.5152, G_loss: 2.4475\n",
      "  Batch [20/31] D_loss: -3.0420, G_loss: 4.7587\n",
      "  Batch [30/31] D_loss: -3.1748, G_loss: 4.8685\n",
      "\n",
      "Epoch 450 Summary:\n",
      "  Average D_loss: -1.0197\n",
      "  Average G_loss: 1.0925\n",
      "\n",
      "Epoch [451/500]\n",
      "  Batch [0/31] D_loss: -1.3046, G_loss: 1.8120\n",
      "  Batch [10/31] D_loss: -2.3003, G_loss: -4.6129\n",
      "  Batch [20/31] D_loss: -1.9297, G_loss: 1.5754\n",
      "  Batch [30/31] D_loss: -2.3389, G_loss: 5.0755\n",
      "\n",
      "Epoch 451 Summary:\n",
      "  Average D_loss: -1.1376\n",
      "  Average G_loss: -0.8081\n",
      "\n",
      "Epoch [452/500]\n",
      "  Batch [0/31] D_loss: -1.8927, G_loss: 4.3715\n",
      "  Batch [10/31] D_loss: -1.1883, G_loss: -2.7341\n",
      "  Batch [20/31] D_loss: -3.1506, G_loss: 2.9462\n",
      "  Batch [30/31] D_loss: -2.1265, G_loss: 2.2860\n",
      "\n",
      "Epoch 452 Summary:\n",
      "  Average D_loss: -1.0196\n",
      "  Average G_loss: 0.6189\n",
      "\n",
      "Epoch [453/500]\n",
      "  Batch [0/31] D_loss: -2.0930, G_loss: 1.8424\n",
      "  Batch [10/31] D_loss: -2.3722, G_loss: -0.7534\n",
      "  Batch [20/31] D_loss: -1.0940, G_loss: -5.5946\n",
      "  Batch [30/31] D_loss: -2.5316, G_loss: -0.8685\n",
      "\n",
      "Epoch 453 Summary:\n",
      "  Average D_loss: -1.2958\n",
      "  Average G_loss: -1.7336\n",
      "\n",
      "Epoch [454/500]\n",
      "  Batch [0/31] D_loss: -2.5536, G_loss: 4.3476\n",
      "  Batch [10/31] D_loss: -1.4754, G_loss: 2.1426\n",
      "  Batch [20/31] D_loss: -2.3354, G_loss: 3.2325\n",
      "  Batch [30/31] D_loss: -2.1420, G_loss: 1.0187\n",
      "\n",
      "Epoch 454 Summary:\n",
      "  Average D_loss: -0.8230\n",
      "  Average G_loss: 1.9445\n",
      "\n",
      "Epoch [455/500]\n",
      "  Batch [0/31] D_loss: -3.0694, G_loss: 4.9802\n",
      "  Batch [10/31] D_loss: -3.5608, G_loss: -5.8498\n",
      "  Batch [20/31] D_loss: -1.0858, G_loss: -2.9848\n",
      "  Batch [30/31] D_loss: -1.4915, G_loss: 6.1770\n",
      "\n",
      "Epoch 455 Summary:\n",
      "  Average D_loss: -1.4449\n",
      "  Average G_loss: -0.4038\n",
      "\n",
      "Epoch [456/500]\n",
      "  Batch [0/31] D_loss: -0.1466, G_loss: 3.2758\n",
      "  Batch [10/31] D_loss: -3.4130, G_loss: 4.5139\n",
      "  Batch [20/31] D_loss: -2.8651, G_loss: -1.0990\n",
      "  Batch [30/31] D_loss: -2.9945, G_loss: -2.4922\n",
      "\n",
      "Epoch 456 Summary:\n",
      "  Average D_loss: -1.0632\n",
      "  Average G_loss: -0.6565\n",
      "\n",
      "Epoch [457/500]\n",
      "  Batch [0/31] D_loss: -2.5855, G_loss: -4.5303\n",
      "  Batch [10/31] D_loss: -2.1099, G_loss: 4.5309\n",
      "  Batch [20/31] D_loss: -3.3299, G_loss: 4.6679\n",
      "  Batch [30/31] D_loss: -2.4204, G_loss: 1.8449\n",
      "\n",
      "Epoch 457 Summary:\n",
      "  Average D_loss: -1.1074\n",
      "  Average G_loss: 0.1042\n",
      "\n",
      "Epoch [458/500]\n",
      "  Batch [0/31] D_loss: -3.4566, G_loss: 5.1958\n",
      "  Batch [10/31] D_loss: -2.3397, G_loss: 0.7017\n",
      "  Batch [20/31] D_loss: -2.2443, G_loss: -2.1121\n",
      "  Batch [30/31] D_loss: -1.5709, G_loss: -8.8402\n",
      "\n",
      "Epoch 458 Summary:\n",
      "  Average D_loss: -1.4050\n",
      "  Average G_loss: -0.1537\n",
      "\n",
      "Epoch [459/500]\n",
      "  Batch [0/31] D_loss: -3.3879, G_loss: -11.5074\n",
      "  Batch [10/31] D_loss: -1.5781, G_loss: 1.9123\n",
      "  Batch [20/31] D_loss: -5.4289, G_loss: 8.2555\n",
      "  Batch [30/31] D_loss: -2.5316, G_loss: -4.3773\n",
      "\n",
      "Epoch 459 Summary:\n",
      "  Average D_loss: -0.7774\n",
      "  Average G_loss: 1.1674\n",
      "\n",
      "Epoch [460/500]\n",
      "  Batch [0/31] D_loss: -2.6487, G_loss: -3.8736\n",
      "  Batch [10/31] D_loss: -1.7389, G_loss: -3.0319\n",
      "  Batch [20/31] D_loss: -2.1754, G_loss: 4.6370\n",
      "  Batch [30/31] D_loss: -2.4962, G_loss: -3.9679\n",
      "\n",
      "Epoch 460 Summary:\n",
      "  Average D_loss: -0.9732\n",
      "  Average G_loss: -0.2552\n",
      "\n",
      "Epoch [461/500]\n",
      "  Batch [0/31] D_loss: -1.3294, G_loss: 0.0055\n",
      "  Batch [10/31] D_loss: -2.1927, G_loss: 1.6806\n",
      "  Batch [20/31] D_loss: -3.5320, G_loss: -8.7291\n",
      "  Batch [30/31] D_loss: -1.1437, G_loss: 3.6867\n",
      "\n",
      "Epoch 461 Summary:\n",
      "  Average D_loss: -1.0827\n",
      "  Average G_loss: -1.4802\n",
      "\n",
      "Epoch [462/500]\n",
      "  Batch [0/31] D_loss: -2.5690, G_loss: 5.0856\n",
      "  Batch [10/31] D_loss: -1.8735, G_loss: 3.3219\n",
      "  Batch [20/31] D_loss: -2.8700, G_loss: -0.1571\n",
      "  Batch [30/31] D_loss: -2.3488, G_loss: 2.5809\n",
      "\n",
      "Epoch 462 Summary:\n",
      "  Average D_loss: -1.1346\n",
      "  Average G_loss: 1.5006\n",
      "\n",
      "Epoch [463/500]\n",
      "  Batch [0/31] D_loss: -3.4037, G_loss: 2.5704\n",
      "  Batch [10/31] D_loss: -2.5324, G_loss: -1.3410\n",
      "  Batch [20/31] D_loss: -1.1618, G_loss: -0.1786\n",
      "  Batch [30/31] D_loss: -2.4012, G_loss: 0.8920\n",
      "\n",
      "Epoch 463 Summary:\n",
      "  Average D_loss: -1.1993\n",
      "  Average G_loss: 0.3298\n",
      "\n",
      "Epoch [464/500]\n",
      "  Batch [0/31] D_loss: -2.3516, G_loss: 0.6388\n",
      "  Batch [10/31] D_loss: -3.3340, G_loss: 0.2839\n",
      "  Batch [20/31] D_loss: -2.4404, G_loss: -0.4270\n",
      "  Batch [30/31] D_loss: -2.4191, G_loss: -3.2928\n",
      "\n",
      "Epoch 464 Summary:\n",
      "  Average D_loss: -0.9804\n",
      "  Average G_loss: 0.5447\n",
      "\n",
      "Epoch [465/500]\n",
      "  Batch [0/31] D_loss: -2.3775, G_loss: -0.7787\n",
      "  Batch [10/31] D_loss: -1.8633, G_loss: -3.2799\n",
      "  Batch [20/31] D_loss: -2.3935, G_loss: 3.5731\n",
      "  Batch [30/31] D_loss: -3.3402, G_loss: 0.5790\n",
      "\n",
      "Epoch 465 Summary:\n",
      "  Average D_loss: -1.0465\n",
      "  Average G_loss: -0.1255\n",
      "\n",
      "Epoch [466/500]\n",
      "  Batch [0/31] D_loss: -2.0684, G_loss: 0.5568\n",
      "  Batch [10/31] D_loss: -2.5762, G_loss: 0.6071\n",
      "  Batch [20/31] D_loss: -2.9146, G_loss: -0.8120\n",
      "  Batch [30/31] D_loss: -3.8041, G_loss: -4.3548\n",
      "\n",
      "Epoch 466 Summary:\n",
      "  Average D_loss: -1.1256\n",
      "  Average G_loss: -0.0684\n",
      "\n",
      "Epoch [467/500]\n",
      "  Batch [0/31] D_loss: -2.0191, G_loss: -2.7502\n",
      "  Batch [10/31] D_loss: -2.7990, G_loss: -5.2823\n",
      "  Batch [20/31] D_loss: -2.0485, G_loss: 5.1467\n",
      "  Batch [30/31] D_loss: -2.0181, G_loss: -0.1618\n",
      "\n",
      "Epoch 467 Summary:\n",
      "  Average D_loss: -1.2918\n",
      "  Average G_loss: 0.9532\n",
      "\n",
      "Epoch [468/500]\n",
      "  Batch [0/31] D_loss: -2.0102, G_loss: 1.9347\n",
      "  Batch [10/31] D_loss: -2.6549, G_loss: -7.0739\n",
      "  Batch [20/31] D_loss: -1.9185, G_loss: 0.1575\n",
      "  Batch [30/31] D_loss: -1.7131, G_loss: 2.9717\n",
      "\n",
      "Epoch 468 Summary:\n",
      "  Average D_loss: -1.1618\n",
      "  Average G_loss: -1.3488\n",
      "\n",
      "Epoch [469/500]\n",
      "  Batch [0/31] D_loss: -3.4553, G_loss: 3.3318\n",
      "  Batch [10/31] D_loss: -1.2949, G_loss: -0.5885\n",
      "  Batch [20/31] D_loss: -5.6765, G_loss: -9.4764\n",
      "  Batch [30/31] D_loss: -3.2568, G_loss: 1.0928\n",
      "\n",
      "Epoch 469 Summary:\n",
      "  Average D_loss: -1.0557\n",
      "  Average G_loss: -0.1511\n",
      "\n",
      "Epoch [470/500]\n",
      "  Batch [0/31] D_loss: -2.8296, G_loss: -1.5694\n",
      "  Batch [10/31] D_loss: -2.8661, G_loss: 2.2891\n",
      "  Batch [20/31] D_loss: -2.0980, G_loss: -2.5418\n",
      "  Batch [30/31] D_loss: -3.2683, G_loss: 5.3708\n",
      "\n",
      "Epoch 470 Summary:\n",
      "  Average D_loss: -1.2757\n",
      "  Average G_loss: 0.6277\n",
      "\n",
      "Epoch [471/500]\n",
      "  Batch [0/31] D_loss: -2.0454, G_loss: 2.8697\n",
      "  Batch [10/31] D_loss: -3.1722, G_loss: -0.5436\n",
      "  Batch [20/31] D_loss: -4.3924, G_loss: -6.1475\n",
      "  Batch [30/31] D_loss: -2.4313, G_loss: 3.3155\n",
      "\n",
      "Epoch 471 Summary:\n",
      "  Average D_loss: -1.2701\n",
      "  Average G_loss: 0.1671\n",
      "\n",
      "Epoch [472/500]\n",
      "  Batch [0/31] D_loss: -3.8149, G_loss: 5.5769\n",
      "  Batch [10/31] D_loss: -2.2900, G_loss: 1.0070\n",
      "  Batch [20/31] D_loss: -2.5461, G_loss: 3.4940\n",
      "  Batch [30/31] D_loss: -2.6346, G_loss: -3.0108\n",
      "\n",
      "Epoch 472 Summary:\n",
      "  Average D_loss: -1.0635\n",
      "  Average G_loss: -0.1101\n",
      "\n",
      "Epoch [473/500]\n",
      "  Batch [0/31] D_loss: -1.9562, G_loss: -3.3475\n",
      "  Batch [10/31] D_loss: -3.5321, G_loss: 5.1727\n",
      "  Batch [20/31] D_loss: -2.7488, G_loss: 3.7006\n",
      "  Batch [30/31] D_loss: -2.8266, G_loss: -1.6408\n",
      "\n",
      "Epoch 473 Summary:\n",
      "  Average D_loss: -1.1618\n",
      "  Average G_loss: 0.5000\n",
      "\n",
      "Epoch [474/500]\n",
      "  Batch [0/31] D_loss: -2.7037, G_loss: -0.5873\n",
      "  Batch [10/31] D_loss: -2.2956, G_loss: 0.0850\n",
      "  Batch [20/31] D_loss: -1.9006, G_loss: -0.6058\n",
      "  Batch [30/31] D_loss: -1.8168, G_loss: 4.2103\n",
      "\n",
      "Epoch 474 Summary:\n",
      "  Average D_loss: -1.4077\n",
      "  Average G_loss: 0.0409\n",
      "\n",
      "Epoch [475/500]\n",
      "  Batch [0/31] D_loss: -3.2869, G_loss: 6.5270\n",
      "  Batch [10/31] D_loss: -2.2329, G_loss: 1.9993\n",
      "  Batch [20/31] D_loss: -3.2321, G_loss: -3.6749\n",
      "  Batch [30/31] D_loss: -1.0135, G_loss: 3.4870\n",
      "\n",
      "Epoch 475 Summary:\n",
      "  Average D_loss: -1.1539\n",
      "  Average G_loss: -0.9145\n",
      "\n",
      "Epoch [476/500]\n",
      "  Batch [0/31] D_loss: -1.4189, G_loss: 2.5437\n",
      "  Batch [10/31] D_loss: -2.2195, G_loss: 1.8297\n",
      "  Batch [20/31] D_loss: -2.3999, G_loss: 2.2176\n",
      "  Batch [30/31] D_loss: -2.4239, G_loss: -0.1157\n",
      "\n",
      "Epoch 476 Summary:\n",
      "  Average D_loss: -0.9776\n",
      "  Average G_loss: 1.3200\n",
      "\n",
      "Epoch [477/500]\n",
      "  Batch [0/31] D_loss: -2.7098, G_loss: -2.3520\n",
      "  Batch [10/31] D_loss: -2.1687, G_loss: -1.7533\n",
      "  Batch [20/31] D_loss: -2.3682, G_loss: -0.9720\n",
      "  Batch [30/31] D_loss: -2.0182, G_loss: 1.4866\n",
      "\n",
      "Epoch 477 Summary:\n",
      "  Average D_loss: -1.0956\n",
      "  Average G_loss: 0.2644\n",
      "\n",
      "Epoch [478/500]\n",
      "  Batch [0/31] D_loss: -5.0359, G_loss: 4.8416\n",
      "  Batch [10/31] D_loss: -1.6565, G_loss: -5.4023\n",
      "  Batch [20/31] D_loss: -2.0830, G_loss: 0.5760\n",
      "  Batch [30/31] D_loss: -3.4816, G_loss: 2.0424\n",
      "\n",
      "Epoch 478 Summary:\n",
      "  Average D_loss: -1.1711\n",
      "  Average G_loss: -0.5883\n",
      "\n",
      "Epoch [479/500]\n",
      "  Batch [0/31] D_loss: -2.6730, G_loss: 4.0447\n",
      "  Batch [10/31] D_loss: -2.9562, G_loss: 0.3901\n",
      "  Batch [20/31] D_loss: -3.0180, G_loss: -2.9572\n",
      "  Batch [30/31] D_loss: -3.5214, G_loss: -3.2466\n",
      "\n",
      "Epoch 479 Summary:\n",
      "  Average D_loss: -1.2042\n",
      "  Average G_loss: 0.2833\n",
      "\n",
      "Epoch [480/500]\n",
      "  Batch [0/31] D_loss: -2.8462, G_loss: -3.4894\n",
      "  Batch [10/31] D_loss: -3.5397, G_loss: -1.6666\n",
      "  Batch [20/31] D_loss: -2.5255, G_loss: 1.4885\n",
      "  Batch [30/31] D_loss: -1.9611, G_loss: 0.1159\n",
      "\n",
      "Epoch 480 Summary:\n",
      "  Average D_loss: -1.1578\n",
      "  Average G_loss: -0.1760\n",
      "\n",
      "Epoch [481/500]\n",
      "  Batch [0/31] D_loss: -1.5500, G_loss: -0.5353\n",
      "  Batch [10/31] D_loss: -1.6535, G_loss: -2.9083\n",
      "  Batch [20/31] D_loss: -3.1983, G_loss: 0.6184\n",
      "  Batch [30/31] D_loss: -2.7056, G_loss: -1.6942\n",
      "\n",
      "Epoch 481 Summary:\n",
      "  Average D_loss: -1.2133\n",
      "  Average G_loss: -0.1668\n",
      "\n",
      "Epoch [482/500]\n",
      "  Batch [0/31] D_loss: -2.4866, G_loss: -1.9712\n",
      "  Batch [10/31] D_loss: -2.4791, G_loss: 3.8150\n",
      "  Batch [20/31] D_loss: -5.6208, G_loss: 6.7965\n",
      "  Batch [30/31] D_loss: -2.6774, G_loss: -1.2306\n",
      "\n",
      "Epoch 482 Summary:\n",
      "  Average D_loss: -1.2804\n",
      "  Average G_loss: 0.2902\n",
      "\n",
      "Epoch [483/500]\n",
      "  Batch [0/31] D_loss: -2.4453, G_loss: -0.5641\n",
      "  Batch [10/31] D_loss: -0.7034, G_loss: -3.6313\n",
      "  Batch [20/31] D_loss: -2.8481, G_loss: 0.4253\n",
      "  Batch [30/31] D_loss: -3.1570, G_loss: 3.5458\n",
      "\n",
      "Epoch 483 Summary:\n",
      "  Average D_loss: -1.1983\n",
      "  Average G_loss: 0.2014\n",
      "\n",
      "Epoch [484/500]\n",
      "  Batch [0/31] D_loss: -1.9587, G_loss: 1.3214\n",
      "  Batch [10/31] D_loss: -2.8250, G_loss: -3.4218\n",
      "  Batch [20/31] D_loss: -3.0921, G_loss: 1.6326\n",
      "  Batch [30/31] D_loss: -2.6580, G_loss: -0.7035\n",
      "\n",
      "Epoch 484 Summary:\n",
      "  Average D_loss: -1.2438\n",
      "  Average G_loss: -0.0790\n",
      "\n",
      "Epoch [485/500]\n",
      "  Batch [0/31] D_loss: -2.6569, G_loss: 1.0933\n",
      "  Batch [10/31] D_loss: -2.3717, G_loss: 1.1894\n",
      "  Batch [20/31] D_loss: -2.3260, G_loss: -2.8976\n",
      "  Batch [30/31] D_loss: -2.5715, G_loss: 3.8605\n",
      "\n",
      "Epoch 485 Summary:\n",
      "  Average D_loss: -1.2514\n",
      "  Average G_loss: -0.5823\n",
      "\n",
      "Epoch [486/500]\n",
      "  Batch [0/31] D_loss: -3.2493, G_loss: 5.9575\n",
      "  Batch [10/31] D_loss: -3.0575, G_loss: 3.6967\n",
      "  Batch [20/31] D_loss: -3.0401, G_loss: -1.0641\n",
      "  Batch [30/31] D_loss: -2.2055, G_loss: 4.9326\n",
      "\n",
      "Epoch 486 Summary:\n",
      "  Average D_loss: -1.2337\n",
      "  Average G_loss: 1.1416\n",
      "\n",
      "Epoch [487/500]\n",
      "  Batch [0/31] D_loss: -2.4435, G_loss: 3.4317\n",
      "  Batch [10/31] D_loss: -1.5841, G_loss: -2.2023\n",
      "  Batch [20/31] D_loss: -1.3477, G_loss: 1.1751\n",
      "  Batch [30/31] D_loss: -2.2254, G_loss: 0.9826\n",
      "\n",
      "Epoch 487 Summary:\n",
      "  Average D_loss: -1.0844\n",
      "  Average G_loss: 0.1302\n",
      "\n",
      "Epoch [488/500]\n",
      "  Batch [0/31] D_loss: -2.4935, G_loss: -1.4303\n",
      "  Batch [10/31] D_loss: -2.8118, G_loss: -0.2614\n",
      "  Batch [20/31] D_loss: -4.2127, G_loss: 1.4784\n",
      "  Batch [30/31] D_loss: -2.2700, G_loss: 3.4296\n",
      "\n",
      "Epoch 488 Summary:\n",
      "  Average D_loss: -1.2629\n",
      "  Average G_loss: -0.4324\n",
      "\n",
      "Epoch [489/500]\n",
      "  Batch [0/31] D_loss: -1.7938, G_loss: 2.7326\n",
      "  Batch [10/31] D_loss: -3.0235, G_loss: 0.8028\n",
      "  Batch [20/31] D_loss: -1.7393, G_loss: -4.6140\n",
      "  Batch [30/31] D_loss: -1.5490, G_loss: 7.2391\n",
      "\n",
      "Epoch 489 Summary:\n",
      "  Average D_loss: -1.4818\n",
      "  Average G_loss: -0.7305\n",
      "\n",
      "Epoch [490/500]\n",
      "  Batch [0/31] D_loss: -1.1665, G_loss: 5.3911\n",
      "  Batch [10/31] D_loss: -2.2576, G_loss: 4.2544\n",
      "  Batch [20/31] D_loss: -0.5508, G_loss: -1.8132\n",
      "  Batch [30/31] D_loss: -3.6111, G_loss: -4.0411\n",
      "\n",
      "Epoch 490 Summary:\n",
      "  Average D_loss: -1.0138\n",
      "  Average G_loss: 0.9731\n",
      "\n",
      "Epoch [491/500]\n",
      "  Batch [0/31] D_loss: -1.5390, G_loss: -3.2225\n",
      "  Batch [10/31] D_loss: -2.5953, G_loss: 6.5912\n",
      "  Batch [20/31] D_loss: -2.2738, G_loss: -2.7377\n",
      "  Batch [30/31] D_loss: -1.7092, G_loss: 3.6846\n",
      "\n",
      "Epoch 491 Summary:\n",
      "  Average D_loss: -0.9801\n",
      "  Average G_loss: 0.8847\n",
      "\n",
      "Epoch [492/500]\n",
      "  Batch [0/31] D_loss: -1.7535, G_loss: 4.9315\n",
      "  Batch [10/31] D_loss: -1.9624, G_loss: -6.0814\n",
      "  Batch [20/31] D_loss: -0.6873, G_loss: -2.1260\n",
      "  Batch [30/31] D_loss: -2.0472, G_loss: 2.1554\n",
      "\n",
      "Epoch 492 Summary:\n",
      "  Average D_loss: -0.6856\n",
      "  Average G_loss: -1.3786\n",
      "\n",
      "Epoch [493/500]\n",
      "  Batch [0/31] D_loss: -2.2946, G_loss: -0.8616\n",
      "  Batch [10/31] D_loss: -0.5823, G_loss: -0.2696\n",
      "  Batch [20/31] D_loss: -2.6350, G_loss: 2.0779\n",
      "  Batch [30/31] D_loss: -1.5343, G_loss: -3.9337\n",
      "\n",
      "Epoch 493 Summary:\n",
      "  Average D_loss: -0.9624\n",
      "  Average G_loss: -0.2748\n",
      "\n",
      "Epoch [494/500]\n",
      "  Batch [0/31] D_loss: -2.0925, G_loss: -4.3869\n",
      "  Batch [10/31] D_loss: -1.5825, G_loss: 4.6926\n",
      "  Batch [20/31] D_loss: -1.8542, G_loss: 2.1688\n",
      "  Batch [30/31] D_loss: -3.4165, G_loss: 4.3552\n",
      "\n",
      "Epoch 494 Summary:\n",
      "  Average D_loss: -0.9123\n",
      "  Average G_loss: 1.8057\n",
      "\n",
      "Epoch [495/500]\n",
      "  Batch [0/31] D_loss: -2.6667, G_loss: 1.0506\n",
      "  Batch [10/31] D_loss: -3.5954, G_loss: -8.1175\n",
      "  Batch [20/31] D_loss: -2.4044, G_loss: -1.2540\n",
      "  Batch [30/31] D_loss: -3.0718, G_loss: 3.5085\n",
      "\n",
      "Epoch 495 Summary:\n",
      "  Average D_loss: -1.2251\n",
      "  Average G_loss: -1.4217\n",
      "\n",
      "Epoch [496/500]\n",
      "  Batch [0/31] D_loss: -2.4314, G_loss: 3.2638\n",
      "  Batch [10/31] D_loss: -1.7116, G_loss: 0.8186\n",
      "  Batch [20/31] D_loss: -2.1918, G_loss: 0.9353\n",
      "  Batch [30/31] D_loss: -1.6685, G_loss: -5.4137\n",
      "\n",
      "Epoch 496 Summary:\n",
      "  Average D_loss: -1.1109\n",
      "  Average G_loss: 0.8742\n",
      "\n",
      "Epoch [497/500]\n",
      "  Batch [0/31] D_loss: -2.1789, G_loss: -5.9913\n",
      "  Batch [10/31] D_loss: -2.4671, G_loss: 4.2818\n",
      "  Batch [20/31] D_loss: -1.4549, G_loss: -2.4333\n",
      "  Batch [30/31] D_loss: -2.6687, G_loss: 2.0089\n",
      "\n",
      "Epoch 497 Summary:\n",
      "  Average D_loss: -0.9641\n",
      "  Average G_loss: -0.6068\n",
      "\n",
      "Epoch [498/500]\n",
      "  Batch [0/31] D_loss: -3.0011, G_loss: 3.2642\n",
      "  Batch [10/31] D_loss: -1.8316, G_loss: -0.8382\n",
      "  Batch [20/31] D_loss: -3.2690, G_loss: 4.8439\n",
      "  Batch [30/31] D_loss: -0.7825, G_loss: -0.8974\n",
      "\n",
      "Epoch 498 Summary:\n",
      "  Average D_loss: -1.0485\n",
      "  Average G_loss: 1.0432\n",
      "\n",
      "Epoch [499/500]\n",
      "  Batch [0/31] D_loss: -3.7620, G_loss: -4.8829\n",
      "  Batch [10/31] D_loss: -1.9307, G_loss: -0.1968\n",
      "  Batch [20/31] D_loss: -2.6073, G_loss: -0.1672\n",
      "  Batch [30/31] D_loss: -2.1703, G_loss: -4.2968\n",
      "\n",
      "Epoch 499 Summary:\n",
      "  Average D_loss: -1.1022\n",
      "  Average G_loss: -0.6568\n",
      "\n",
      "Epoch [500/500]\n",
      "  Batch [0/31] D_loss: -0.8896, G_loss: -2.6263\n",
      "  Batch [10/31] D_loss: -3.0469, G_loss: -0.3355\n",
      "  Batch [20/31] D_loss: -2.0082, G_loss: 0.2668\n",
      "  Batch [30/31] D_loss: -2.3321, G_loss: -1.7644\n",
      "\n",
      "Epoch 500 Summary:\n",
      "  Average D_loss: -1.0252\n",
      "  Average G_loss: 0.3427\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'epochs': 500,\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 32,\n",
    "        'nb_layers': 2,\n",
    "        'hdim': 256,\n",
    "        'lr': 5e-4,\n",
    "        'nb_critic': 5\n",
    "    }\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/\"\n",
    "    \n",
    "    # Load expression matrix\n",
    "    with h5py.File(data_path+'train_data_1dataset.h5', 'r') as f:\n",
    "        x_train = f['matrix'][:]\n",
    "    \n",
    "    # Load cluster info\n",
    "    cluster_vec = pd.read_csv(data_path+'train_data_1dataset_cluster.csv').T\n",
    "    \n",
    "    # Load numerical covariates (currently empty)\n",
    "    num_covs = np.zeros((x_train.shape[0], 0))  # Changed to create empty numpy array\n",
    "    \n",
    "    # Create dictionaries and inverse mappings for categorical variables\n",
    "    cat_dicts = []\n",
    "    \n",
    "    # Create list of unique cluster names, sorted\n",
    "    cluster_dict_inv = np.array(list(sorted(set(cluster_vec.values.flatten()))))\n",
    "    cluster_dict = {t: i for i, t in enumerate(cluster_dict_inv)}\n",
    "    cat_dicts.append(cluster_dict_inv)\n",
    "    \n",
    "    # Convert categorical variables to integers\n",
    "    clusters_encoded = np.vectorize(lambda t: cluster_dict[t])(cluster_vec)\n",
    "    \n",
    "    # Assign categorical covariates\n",
    "    cat_covs = clusters_encoded\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to device\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)  # Keep on CPU for DataLoader\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(x_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    vocab_sizes = [len(c) for c in cat_dicts]\n",
    "    nb_numeric = num_covs.shape[-1]\n",
    "    x_dim = x_train.shape[-1]\n",
    "    \n",
    "    generator = Generator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "        z_dim=CONFIG['latent_dim']\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(project='adversarial_gene_expr', config=CONFIG)\n",
    "    \n",
    "    # Train model\n",
    "    train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=train_loader,\n",
    "        cat_covs=cat_covs,\n",
    "        num_covs=num_covs,\n",
    "        config=CONFIG,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
