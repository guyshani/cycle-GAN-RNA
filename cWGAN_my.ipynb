{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims, z_dim):\n",
    "        \"\"\"\n",
    "        Generator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of output data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "            z_dim: Dimension of latent noise vector\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size)) \n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is latent dim + embedding dim + numeric covariates\n",
    "        input_dim = z_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build generator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, x_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims):\n",
    "        \"\"\"\n",
    "        Discriminator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of input data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size))\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is data dim + embedding dim + numeric covariates\n",
    "        input_dim = x_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build discriminator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        disc_input = torch.cat([x, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(disc_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, cat_covs, num_covs, device):\n",
    "    \"\"\"\n",
    "    Calculate gradient penalty for WGAN-GP\n",
    "    \"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand((real_samples.size(0), 1), device=device)\n",
    "    \n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    \n",
    "    # Calculate discriminator output for interpolated samples\n",
    "    d_interpolates = discriminator(interpolates, cat_covs, num_covs)\n",
    "    \n",
    "    # Get gradients w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # Calculate gradient penalty\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, cat_covs, num_covs, \n",
    "              config, device, score_fn=None, save_fn=None):\n",
    "    \"\"\"\n",
    "    Train the conditional GAN with progress tracking and proper device handling\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optimizer = optim.RMSprop(generator.parameters(), lr=config['lr'])\n",
    "    d_optimizer = optim.RMSprop(discriminator.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Lambda for gradient penalty\n",
    "    lambda_gp = 10\n",
    "    \n",
    "    # Convert covariates to tensors and move to device\n",
    "    cat_covs = torch.tensor(cat_covs, dtype=torch.long).to(device)\n",
    "    num_covs = torch.tensor(num_covs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "    print(f\"Total batches per epoch: {total_batches}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        print(f\"\\nEpoch [{epoch+1}/{config['epochs']}]\")\n",
    "        \n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Move real data to device\n",
    "            real_data = real_data.to(device)\n",
    "            \n",
    "            # Get random batch of categorical and numerical covariates\n",
    "            batch_indices = torch.randint(0, cat_covs.size(0), (batch_size,))\n",
    "            batch_cat_covs = cat_covs[batch_indices]\n",
    "            batch_num_covs = num_covs[batch_indices]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(config['nb_critic']):\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "                fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate discriminator output for real and fake data\n",
    "                real_validity = discriminator(real_data, batch_cat_covs, batch_num_covs)\n",
    "                fake_validity = discriminator(fake_data.detach(), batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate gradient penalty\n",
    "                gp = compute_gradient_penalty(\n",
    "                    discriminator,\n",
    "                    real_data,\n",
    "                    fake_data.detach(),\n",
    "                    batch_cat_covs,\n",
    "                    batch_num_covs,\n",
    "                    device)\n",
    "                \n",
    "                # Calculate discriminator loss with gradient penalty\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
    "                \n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "            fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "            \n",
    "            # Calculate generator loss\n",
    "            fake_validity = discriminator(fake_data, batch_cat_covs, batch_num_covs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            g_losses.append(g_loss.item())\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Batch [{batch_idx}/{total_batches}] \" \\\n",
    "                      f\"D_loss: {d_loss.item():.4f}, \" \\\n",
    "                      f\"G_loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average D_loss: {avg_d_loss:.4f}\")\n",
    "        print(f\"  Average G_loss: {avg_g_loss:.4f}\")\n",
    "        \n",
    "        # Log metrics\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'd_loss': np.mean(d_losses),\n",
    "                'g_loss': np.mean(g_losses)\n",
    "            })\n",
    "        \n",
    "        # Evaluate and save model if needed\n",
    "        if score_fn is not None and epoch % 10 == 0:\n",
    "            score = score_fn(generator)\n",
    "            print(f'Epoch {epoch}: Score = {score:.4f}')\n",
    "        \n",
    "        if save_fn is not None and epoch % 20 == 0:\n",
    "            save_fn(generator, discriminator, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(selected_categories=None):\n",
    "    \"\"\"\n",
    "    Train the GAN with selected categorical variables\n",
    "    Args:\n",
    "        selected_categories: List of column names to use as categorical variables.\n",
    "                           If None, uses all columns except 'cell_id'\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'epochs': 100,\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 32,\n",
    "        'nb_layers': 3,\n",
    "        'hdim': 256,\n",
    "        'lr': 1e-4,\n",
    "        'nb_critic': 5,\n",
    "        'lambda_gp': 10  # Gradient penalty coefficient\n",
    "    }\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/\"\n",
    "    \n",
    "    # Load expression matrix\n",
    "    # matrix with cells as columns and genes as rows\n",
    "    with h5py.File(data_path+'combined_normalized_data.h5', 'r') as f:\n",
    "        x_train = f['matrix'][:]\n",
    "    \n",
    "    # Load all categorical variables from single file\n",
    "    cat_data = pd.read_csv(data_path+'combined_metadata.csv', sep=';')\n",
    "    print(\"Categorical data shape:\", cat_data.shape)\n",
    "    print(\"Available categorical variables:\", [col for col in cat_data.columns if col != 'cell_id'])\n",
    "    \n",
    "    # Determine which categories to use\n",
    "    if selected_categories is None:\n",
    "        # Use all columns except cell_id\n",
    "        categories_to_use = [col for col in cat_data.columns if col != 'cell_id']\n",
    "    else:\n",
    "        # Validate selected categories\n",
    "        invalid_categories = [cat for cat in selected_categories if cat not in cat_data.columns]\n",
    "        if invalid_categories:\n",
    "            raise ValueError(f\"Invalid categories: {invalid_categories}\")\n",
    "        categories_to_use = selected_categories\n",
    "    \n",
    "    print(f\"\\nUsing categorical variables: {categories_to_use}\")\n",
    "    \n",
    "    # Create dictionaries and inverse mappings for categorical variables\n",
    "    cat_dicts = []\n",
    "    encoded_covs = []\n",
    "    \n",
    "    # Process each selected column as a categorical variable\n",
    "    for column in categories_to_use:\n",
    "        # Get the column data\n",
    "        cat_vec = cat_data[column]\n",
    "        print(f\"\\nProcessing categorical variable: {column}\")\n",
    "        \n",
    "        # Create list of unique category names, sorted\n",
    "        dict_inv = np.array(list(sorted(set(cat_vec.values))))\n",
    "        dict_map = {t: i for i, t in enumerate(dict_inv)}\n",
    "        cat_dicts.append(dict_inv)\n",
    "        \n",
    "        # Convert categorical variables to integers\n",
    "        encoded = np.vectorize(lambda t: dict_map[t])(cat_vec)\n",
    "        encoded = encoded.reshape(-1, 1)  # Reshape to column vector\n",
    "        encoded_covs.append(encoded)\n",
    "        \n",
    "        print(f\"Categories in {column}:\", dict_inv)\n",
    "        print(f\"Number of categories:\", len(dict_inv))\n",
    "    \n",
    "    # Combine all categorical covariates\n",
    "    cat_covs = np.hstack(encoded_covs)\n",
    "    print(\"\\nCombined categorical covariates shape:\", cat_covs.shape)\n",
    "    \n",
    "    # Load numerical covariates (currently empty)\n",
    "    num_covs = np.zeros((x_train.shape[0], 0))\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to device\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)  # Keep on CPU for DataLoader\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(x_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    vocab_sizes = [len(c) for c in cat_dicts]\n",
    "    print(\"\\nVocabulary sizes for categorical variables:\", vocab_sizes)\n",
    "    nb_numeric = num_covs.shape[-1]\n",
    "    x_dim = x_train.shape[-1]\n",
    "    \n",
    "    generator = Generator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "        z_dim=CONFIG['latent_dim']).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers']).to(device)\n",
    "    \n",
    "    # Define save function\n",
    "    def save_models(generator, discriminator, epoch):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # create save directory\n",
    "        categories_str = \"+\".join(categories_to_use)\n",
    "        save_dir = os.path.join(data_path, \"saved_models\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Create run folder\n",
    "        run_dir = os.path.join(save_dir, f\"run_{timestamp}_{categories_str}\")\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "        # Save model initialization parameters\n",
    "        model_config = {\n",
    "            'x_dim': x_dim,\n",
    "            'vocab_sizes': vocab_sizes,\n",
    "            'nb_numeric': nb_numeric,\n",
    "            'h_dims': [CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "            'z_dim': CONFIG['latent_dim'],\n",
    "            'categories': categories_to_use,\n",
    "            'training_config': CONFIG}\n",
    "        config_path = os.path.join(run_dir, 'model_config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(model_config, f, indent=4)\n",
    "        \n",
    "        # Save generator\n",
    "        generator_path = os.path.join(run_dir, f\"generator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(generator.state_dict(), generator_path)\n",
    "        \n",
    "        \n",
    "        # Save discriminator\n",
    "        discriminator_path = os.path.join(run_dir, f\"discriminator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(discriminator.state_dict(), discriminator_path)\n",
    "        \n",
    "        print(f\"\\nModels saved at epoch {epoch + 1}:\")\n",
    "        print(f\"Generator: {generator_path}\")\n",
    "        print(f\"Discriminator: {discriminator_path}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        if wandb.run is not None:\n",
    "            wandb.save(generator_path)\n",
    "            wandb.save(discriminator_path)\n",
    "\n",
    "    # Initialize wandb with unique run name\n",
    "    run_name = f\"run_{int(time.time())}\"  # Uses timestamp for unique name\n",
    "    wandb.init(\n",
    "        project='adversarial_gene_expr',\n",
    "        config=CONFIG,\n",
    "        name=run_name,\n",
    "        reinit=True  # Ensures new run each time\n",
    "    )\n",
    "    \n",
    "    # Add selected categories to wandb config\n",
    "    wandb.config.update({'selected_categories': categories_to_use})\n",
    "    \n",
    "    # Train model\n",
    "    train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=train_loader,\n",
    "        cat_covs=cat_covs,\n",
    "        num_covs=num_covs,\n",
    "        config=CONFIG,\n",
    "        device=device,\n",
    "        save_fn=save_models\n",
    "        #save_fn=None\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    # Use specific categories:\n",
    "    main(selected_categories=['dataset','cell_type'])\n",
    "    \n",
    "    # Or use all available categories:\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for data gneration\n",
    "def inspect_generator_dims(generator):\n",
    "    \"\"\"\n",
    "    Inspect the generator's dimensions and architecture\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Generator model\n",
    "    \n",
    "    Returns:\n",
    "        dict containing dimension information\n",
    "    \"\"\"\n",
    "    # Get embedding dimensions\n",
    "    embedding_dims = [emb.embedding_dim for emb in generator.embeddings]\n",
    "    total_embedding_dim = sum(embedding_dims)\n",
    "    \n",
    "    # Get first layer dimension\n",
    "    first_layer_in_dim = generator.network[0].in_features\n",
    "    \n",
    "    return {\n",
    "        'embedding_dims': embedding_dims,\n",
    "        'total_embedding_dim': total_embedding_dim,\n",
    "        'first_layer_in_dim': first_layer_in_dim,\n",
    "        'recommended_latent_dim': first_layer_in_dim - total_embedding_dim\n",
    "    }\n",
    "\n",
    "def generate_expression_profiles(generator, n_samples, dataset_category, cell_type_category, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate gene expression profiles using the trained cWGAN generator\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained Generator model\n",
    "        n_samples: Number of profiles to generate\n",
    "        dataset_category: Integer indicating which dataset category to generate\n",
    "        cell_type_category: Integer indicating which cell type to generate\n",
    "        device: Device to run generation on ('cuda', 'mps', or 'cpu')\n",
    "        debug: If True, print debugging information\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of generated expression profiles with shape (n_samples, n_genes)\n",
    "    \"\"\"\n",
    "    # Set generator to eval mode\n",
    "    generator.eval()\n",
    "    \n",
    "    # Inspect dimensions\n",
    "    dims = inspect_generator_dims(generator)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Generator dimensions:\")\n",
    "        for k, v in dims.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    \n",
    "    # Create latent vectors\n",
    "    latent_dim = dims['recommended_latent_dim']\n",
    "    z = torch.randn(n_samples, latent_dim, device=device)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nLatent vector shape: {z.shape}\")\n",
    "    \n",
    "    # Create categorical condition tensor with dataset and cell type\n",
    "    num_embeddings = len(generator.embeddings)\n",
    "    cat_covs = torch.zeros((n_samples, num_embeddings), dtype=torch.long, device=device)\n",
    "    cat_covs[:, 0] = dataset_category  # Set dataset category\n",
    "    cat_covs[:, 1] = cell_type_category  # Set cell type category\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Categorical covariates shape: {cat_covs.shape}\")\n",
    "        print(f\"Number of embedding layers: {num_embeddings}\")\n",
    "    \n",
    "    # Create empty numeric covariates tensor\n",
    "    num_covs = torch.zeros((n_samples, 0), device=device)\n",
    "    \n",
    "    # Generate samples\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings\n",
    "            embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(generator.embeddings)]\n",
    "            embedded = torch.cat(embeddings, dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Embedded shape: {embedded.shape}\")\n",
    "            \n",
    "            # Concatenate inputs\n",
    "            gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Generator input shape: {gen_input.shape}\")\n",
    "                print(f\"First layer input dim: {generator.network[0].in_features}\")\n",
    "                print(f\"First layer weight shape: {generator.network[0].weight.shape}\")\n",
    "            \n",
    "            # Generate samples\n",
    "            fake_samples = generator.network(gen_input)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(\"\\nError during generation:\")\n",
    "        print(e)\n",
    "        print(\"\\nGenerator architecture:\")\n",
    "        print(generator)\n",
    "        raise\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    return fake_samples.cpu().numpy()\n",
    "\n",
    "def generate_and_save_profiles(generator, samples_per_combination, save_path, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate expression profiles with custom sample counts for each dataset and cell type combination\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained Generator model\n",
    "        samples_per_combination: Dictionary with tuples as keys (dataset_num, cell_type_num) \n",
    "                               and number of samples as values\n",
    "        save_path: Path to save the generated profiles\n",
    "        device: Device to run generation on ('cuda', 'mps', or 'cpu')\n",
    "        debug: If True, print debugging information\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "    all_categories = []\n",
    "    \n",
    "    # Cell type mapping\n",
    "    cell_type_names = {\n",
    "        0: 'B cells',\n",
    "        1: 'Dendritic cells',\n",
    "        2: 'Endothelial cells',\n",
    "        3: 'Erythrocytes',\n",
    "        4: 'Fibroblasts',\n",
    "        5: 'Granulocytes',\n",
    "        6: 'Macrophages',\n",
    "        7: 'Monocytes',\n",
    "        8: 'NK cells',\n",
    "        9: 'T cells'\n",
    "    }\n",
    "    \n",
    "    # Generate samples for specified combinations\n",
    "    for (dataset_num, cell_type_num), n_samples in samples_per_combination.items():\n",
    "        dataset_category = dataset_num - 1  # Convert dataset number (1-7) to category index (0-6)\n",
    "        cell_type_category = cell_type_num\n",
    "        cell_type_name = cell_type_names[cell_type_num]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nGenerating {n_samples} samples for dataset{dataset_num}, {cell_type_name}\")\n",
    "        \n",
    "        samples = generate_expression_profiles(\n",
    "            generator, \n",
    "            n_samples, \n",
    "            dataset_category,\n",
    "            cell_type_category,\n",
    "            device,\n",
    "            debug=debug\n",
    "        )\n",
    "        all_samples.append(samples)\n",
    "        all_categories.extend([f'dataset{dataset_num}_{cell_type_name}'] * n_samples)\n",
    "\n",
    "    print(\"Save location: \"+str(save_path))\n",
    "\n",
    "    # Combine all samples\n",
    "    all_samples = np.vstack(all_samples)\n",
    "    \n",
    "    # Save generated profiles\n",
    "    np.save(f'{save_path}_profiles.npy', all_samples)\n",
    "    \n",
    "    # Save category labels\n",
    "    with open(f'{save_path}_categories.txt', 'w') as f:\n",
    "        for category in all_categories:\n",
    "            f.write(f'{category}\\n')\n",
    "\n",
    "    \n",
    "            \n",
    "    return all_samples, all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "\n",
    "# Set directories\n",
    "# 3 hidden layers\n",
    "data_path = \"~/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/\"\n",
    "run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_150654_dataset_cell_type\"\n",
    "generator_model = \"generator_20250121_150654_dataset_cell_type_epoch_61.pt\"\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(run_dir, 'model_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "    \n",
    "# Initialize models with saved configuration\n",
    "generator = Generator(\n",
    "    x_dim=model_config['x_dim'],\n",
    "    vocab_sizes=model_config['vocab_sizes'],\n",
    "    nb_numeric=model_config['nb_numeric'],\n",
    "    h_dims=model_config['h_dims'],\n",
    "    z_dim=model_config['z_dim']).to(device)\n",
    "    \n",
    "discriminator = Discriminator(\n",
    "    x_dim=model_config['x_dim'],\n",
    "    vocab_sizes=model_config['vocab_sizes'],\n",
    "    nb_numeric=model_config['nb_numeric'],\n",
    "    h_dims=model_config['h_dims']).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#discriminator_path = os.path.join(run_dir, \"discriminator.pt\")\n",
    "#discriminator.load_state_dict(torch.load(discriminator_path, map_location=device, weights_only=True))\n",
    "generator_path = os.path.join(run_dir, generator_model)\n",
    "generator.load_state_dict(torch.load(generator_path, map_location=device, weights_only=True))\n",
    "\n",
    "meta = pd.read_csv(data_path+'combined_metadata.csv', sep=';')\n",
    "# create a dict listing datasets and celltypes\n",
    "counts = meta.groupby(['dataset', 'cell_type']).size().to_dict()\n",
    "\n",
    "# Convert the dict keys from tuple of strings to tuple of numbers\n",
    "samples_dict = {}\n",
    "for (dataset, cell_type), count in counts.items():\n",
    "    # Extract dataset number (assuming format 'dataset1', 'dataset2', etc.)\n",
    "    dataset_num = int(dataset.replace('dataset', ''))\n",
    "    \n",
    "    # Get cell type number (assuming you have a mapping for cell types to numbers)\n",
    "    cell_type_map = {\n",
    "        'B cells': 0,\n",
    "        'Dendritic cells': 1,\n",
    "        'Endothelial cells': 2,\n",
    "        'Erythrocytes': 3,\n",
    "        'Fibroblasts': 4,\n",
    "        'Granulocytes': 5,\n",
    "        'Macrophages': 6,\n",
    "        'Monocytes': 7,\n",
    "        'NK cells': 8,\n",
    "        'T cells': 9\n",
    "    }\n",
    "    cell_type_num = cell_type_map[cell_type]\n",
    "    \n",
    "    # Add to new dictionary with numerical tuple as key\n",
    "    samples_dict[(dataset_num, cell_type_num)] = count\n",
    "\n",
    "\n",
    "#samples_dict = {\n",
    "#    (1, 0): 1000,  # 1000 samples from dataset1, cell type 0\n",
    "#    (1, 1): 500,   # 500 samples from dataset1, cell type 1\n",
    "#    (3, 2): 750    # 750 samples from dataset3, cell type 2\n",
    "#}\n",
    "\n",
    "## this function will generate data based on the relative quantities in samples_dict\n",
    "## and will generate a profiles.npy (containing expression profile for each sample)\n",
    "## and a categories.txt (containing labels for each samples) files\n",
    "all_samples, categories = generate_and_save_profiles(\n",
    "    generator,\n",
    "    samples_per_combination=samples_dict,\n",
    "    save_path=run_dir+'_generated_data',\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and analyze generated data\n",
    "# Load the generated profiles\n",
    "profiles = np.load(run_dir + '_generated_data_profiles.npy')\n",
    "\n",
    "# Load categories\n",
    "with open(run_dir + '_generated_data_categories.txt', 'r') as f:\n",
    "    categories = [line.strip() for line in f]\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(profiles)\n",
    "\n",
    "# Add categories as a column\n",
    "df['labels'] = categories\n",
    "\n",
    "## \n",
    "# Add separate dataset and cell_type columns\n",
    "df['dataset'] = df['labels'].apply(lambda x: x.split('_')[0])\n",
    "df['cell_type'] = df['labels'].apply(lambda x: x.split('_')[1])\n",
    "# drop the combined labels colum\n",
    "df=df.drop(['labels'], axis=1)\n",
    "\n",
    "# Save a csv file\n",
    "df.to_csv(f'{run_dir}_generated_data.csv', index=False)\n",
    "#\n",
    "with h5py.File(f'{run_dir}_generated_data.h5', 'w') as f:\n",
    "    # Save as matrix - select only numerical values\n",
    "    f.create_dataset('matrix', data=df.select_dtypes(include=[np.number]).values)  # or df.to_numpy()\n",
    "\n",
    "# If you need to save the categorical information separately:\n",
    "df_labels = df.select_dtypes(exclude=[np.number])\n",
    "df_labels.to_csv(f'{run_dir}_generated_labels.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
