{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims, z_dim):\n",
    "        \"\"\"\n",
    "        Generator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of output data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "            z_dim: Dimension of latent noise vector\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size)) \n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is latent dim + embedding dim + numeric covariates\n",
    "        input_dim = z_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build generator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, x_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims):\n",
    "        \"\"\"\n",
    "        Discriminator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of input data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size))\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is data dim + embedding dim + numeric covariates\n",
    "        input_dim = x_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build discriminator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        disc_input = torch.cat([x, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(disc_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, cat_covs, num_covs, device):\n",
    "    \"\"\"\n",
    "    Calculate gradient penalty for WGAN-GP\n",
    "    \"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand((real_samples.size(0), 1), device=device)\n",
    "    \n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    \n",
    "    # Calculate discriminator output for interpolated samples\n",
    "    d_interpolates = discriminator(interpolates, cat_covs, num_covs)\n",
    "    \n",
    "    # Get gradients w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # Calculate gradient penalty\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, cat_covs, num_covs, \n",
    "              config, device, score_fn=None, save_fn=None):\n",
    "    \"\"\"\n",
    "    Train the conditional GAN with progress tracking and proper device handling\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optimizer = optim.RMSprop(generator.parameters(), lr=config['lr'])\n",
    "    d_optimizer = optim.RMSprop(discriminator.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Lambda for gradient penalty\n",
    "    lambda_gp = 10\n",
    "    \n",
    "    # Convert covariates to tensors and move to device\n",
    "    cat_covs = torch.tensor(cat_covs, dtype=torch.long).to(device)\n",
    "    num_covs = torch.tensor(num_covs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "    print(f\"Total batches per epoch: {total_batches}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        print(f\"\\nEpoch [{epoch+1}/{config['epochs']}]\")\n",
    "        \n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Move real data to device\n",
    "            real_data = real_data.to(device)\n",
    "            \n",
    "            # Get random batch of categorical and numerical covariates\n",
    "            batch_indices = torch.randint(0, cat_covs.size(0), (batch_size,))\n",
    "            batch_cat_covs = cat_covs[batch_indices]\n",
    "            batch_num_covs = num_covs[batch_indices]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(config['nb_critic']):\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "                fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate discriminator output for real and fake data\n",
    "                real_validity = discriminator(real_data, batch_cat_covs, batch_num_covs)\n",
    "                fake_validity = discriminator(fake_data.detach(), batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate gradient penalty\n",
    "                gp = compute_gradient_penalty(\n",
    "                    discriminator,\n",
    "                    real_data,\n",
    "                    fake_data.detach(),\n",
    "                    batch_cat_covs,\n",
    "                    batch_num_covs,\n",
    "                    device)\n",
    "                \n",
    "                # Calculate discriminator loss with gradient penalty\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
    "                \n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "            fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "            \n",
    "            # Calculate generator loss\n",
    "            fake_validity = discriminator(fake_data, batch_cat_covs, batch_num_covs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            g_losses.append(g_loss.item())\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Batch [{batch_idx}/{total_batches}] \" \\\n",
    "                      f\"D_loss: {d_loss.item():.4f}, \" \\\n",
    "                      f\"G_loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average D_loss: {avg_d_loss:.4f}\")\n",
    "        print(f\"  Average G_loss: {avg_g_loss:.4f}\")\n",
    "        \n",
    "        # Log metrics\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'd_loss': np.mean(d_losses),\n",
    "                'g_loss': np.mean(g_losses)\n",
    "            })\n",
    "        \n",
    "        # Evaluate and save model if needed\n",
    "        if score_fn is not None and epoch % 10 == 0:\n",
    "            score = score_fn(generator)\n",
    "            print(f'Epoch {epoch}: Score = {score:.4f}')\n",
    "        \n",
    "        if save_fn is not None and epoch % 20 == 0:\n",
    "            save_fn(generator, discriminator, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Categorical data shape: (41588, 4)\n",
      "Available categorical variables: ['dataset', 'cluster', 'cell_type']\n",
      "\n",
      "Using categorical variables: ['dataset', 'cell_type']\n",
      "\n",
      "Processing categorical variable: dataset\n",
      "Categories in dataset: ['dataset1' 'dataset2' 'dataset3' 'dataset4' 'dataset5' 'dataset6'\n",
      " 'dataset7']\n",
      "Number of categories: 7\n",
      "\n",
      "Processing categorical variable: cell_type\n",
      "Categories in cell_type: ['B cells' 'Dendritic cells' 'Endothelial cells' 'Erythrocytes'\n",
      " 'Fibroblasts' 'Granulocytes' 'Macrophages' 'Monocytes' 'NK cells'\n",
      " 'T cells']\n",
      "Number of categories: 10\n",
      "\n",
      "Combined categorical covariates shape: (41588, 2)\n",
      "\n",
      "Vocabulary sizes for categorical variables: [7, 10]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_1737454544</strong> at: <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/pfz5x2di' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/pfz5x2di</a><br> View project at: <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250121_121545-pfz5x2di/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/guyshani/Documents/PHD/Aim_2/cycle_GAN/wandb/run-20250121_121822-5yuc3c16</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/5yuc3c16' target=\"_blank\">run_1737454702</a></strong> to <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/5yuc3c16' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/5yuc3c16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 100 epochs...\n",
      "Total batches per epoch: 1299\n",
      "Using device: mps\n",
      "\n",
      "Epoch [1/100]\n",
      "  Batch [0/1299] D_loss: -0.4650, G_loss: 0.9766\n",
      "  Batch [10/1299] D_loss: -11.8101, G_loss: -5.3853\n",
      "  Batch [20/1299] D_loss: -9.4431, G_loss: -7.2299\n",
      "  Batch [30/1299] D_loss: -9.3688, G_loss: -8.2170\n",
      "  Batch [40/1299] D_loss: -5.9184, G_loss: -5.2426\n",
      "  Batch [50/1299] D_loss: -6.5075, G_loss: -9.0455\n",
      "  Batch [60/1299] D_loss: -6.4068, G_loss: -7.1682\n",
      "  Batch [70/1299] D_loss: -4.7488, G_loss: -5.1376\n",
      "  Batch [80/1299] D_loss: -3.2610, G_loss: -6.4150\n",
      "  Batch [90/1299] D_loss: -4.3258, G_loss: -4.0177\n",
      "  Batch [100/1299] D_loss: -4.1959, G_loss: -7.8522\n",
      "  Batch [110/1299] D_loss: -3.8843, G_loss: -6.2385\n",
      "  Batch [120/1299] D_loss: -3.6678, G_loss: -6.3344\n",
      "  Batch [130/1299] D_loss: -4.6632, G_loss: -3.0054\n",
      "  Batch [140/1299] D_loss: -3.2591, G_loss: -3.4850\n",
      "  Batch [150/1299] D_loss: -4.9756, G_loss: -4.9019\n",
      "  Batch [160/1299] D_loss: -3.5521, G_loss: -4.1244\n",
      "  Batch [170/1299] D_loss: -3.8147, G_loss: -3.0476\n",
      "  Batch [180/1299] D_loss: -3.0167, G_loss: -6.7670\n",
      "  Batch [190/1299] D_loss: -2.1377, G_loss: -8.2821\n",
      "  Batch [200/1299] D_loss: -0.5245, G_loss: -3.8372\n",
      "  Batch [210/1299] D_loss: -1.1425, G_loss: -5.6063\n",
      "  Batch [220/1299] D_loss: -1.0640, G_loss: -3.5872\n",
      "  Batch [230/1299] D_loss: -1.0142, G_loss: -7.2617\n",
      "  Batch [240/1299] D_loss: -1.1480, G_loss: -2.7203\n",
      "  Batch [250/1299] D_loss: -4.2732, G_loss: -6.4847\n",
      "  Batch [260/1299] D_loss: -4.2849, G_loss: 5.0627\n",
      "  Batch [270/1299] D_loss: -2.2162, G_loss: -0.6123\n",
      "  Batch [280/1299] D_loss: -3.3419, G_loss: 0.4963\n",
      "  Batch [290/1299] D_loss: -0.8412, G_loss: 0.8434\n",
      "  Batch [300/1299] D_loss: -1.2279, G_loss: -1.0662\n",
      "  Batch [310/1299] D_loss: -3.3969, G_loss: 1.1326\n",
      "  Batch [320/1299] D_loss: -3.1825, G_loss: 2.1376\n",
      "  Batch [330/1299] D_loss: -2.6353, G_loss: 1.5120\n",
      "  Batch [340/1299] D_loss: -2.1569, G_loss: -1.1605\n",
      "  Batch [350/1299] D_loss: -1.6449, G_loss: 2.4690\n",
      "  Batch [360/1299] D_loss: -1.6030, G_loss: -2.7269\n",
      "  Batch [370/1299] D_loss: -0.1098, G_loss: -1.4724\n",
      "  Batch [380/1299] D_loss: -3.5206, G_loss: 3.7806\n",
      "  Batch [390/1299] D_loss: -3.0054, G_loss: 1.3487\n",
      "  Batch [400/1299] D_loss: -2.3406, G_loss: -3.0373\n",
      "  Batch [410/1299] D_loss: -3.7911, G_loss: 1.9272\n",
      "  Batch [420/1299] D_loss: -2.4971, G_loss: 1.4999\n",
      "  Batch [430/1299] D_loss: -2.9161, G_loss: -0.1220\n",
      "  Batch [440/1299] D_loss: -2.5042, G_loss: 1.3410\n",
      "  Batch [450/1299] D_loss: -2.7289, G_loss: 0.2807\n",
      "  Batch [460/1299] D_loss: -3.8174, G_loss: 1.3504\n",
      "  Batch [470/1299] D_loss: -3.1710, G_loss: -1.7531\n",
      "  Batch [480/1299] D_loss: -3.9017, G_loss: 1.7063\n",
      "  Batch [490/1299] D_loss: -3.9994, G_loss: -1.7488\n",
      "  Batch [500/1299] D_loss: -4.5097, G_loss: -0.7771\n",
      "  Batch [510/1299] D_loss: -5.3551, G_loss: 2.7365\n",
      "  Batch [520/1299] D_loss: -3.5358, G_loss: 2.2454\n",
      "  Batch [530/1299] D_loss: -1.2509, G_loss: -1.9360\n",
      "  Batch [540/1299] D_loss: -3.9429, G_loss: -1.4926\n",
      "  Batch [550/1299] D_loss: -3.6416, G_loss: 3.4974\n",
      "  Batch [560/1299] D_loss: -1.9640, G_loss: 3.3821\n",
      "  Batch [570/1299] D_loss: -2.4300, G_loss: -1.8493\n",
      "  Batch [580/1299] D_loss: -3.9249, G_loss: -0.9702\n",
      "  Batch [590/1299] D_loss: -3.3194, G_loss: 0.8939\n",
      "  Batch [600/1299] D_loss: -3.0280, G_loss: 1.0858\n",
      "  Batch [610/1299] D_loss: -1.6316, G_loss: -1.9589\n",
      "  Batch [620/1299] D_loss: -3.6679, G_loss: 1.4273\n",
      "  Batch [630/1299] D_loss: -1.3939, G_loss: -2.1917\n",
      "  Batch [640/1299] D_loss: -2.2204, G_loss: -2.4183\n",
      "  Batch [650/1299] D_loss: -3.3980, G_loss: -1.1532\n",
      "  Batch [660/1299] D_loss: -1.0040, G_loss: -1.4564\n",
      "  Batch [670/1299] D_loss: -1.9817, G_loss: -0.0056\n",
      "  Batch [680/1299] D_loss: -1.6902, G_loss: -0.0349\n",
      "  Batch [690/1299] D_loss: -1.2950, G_loss: -2.5919\n",
      "  Batch [700/1299] D_loss: -1.5147, G_loss: -1.1987\n",
      "  Batch [710/1299] D_loss: -3.2774, G_loss: 0.4108\n",
      "  Batch [720/1299] D_loss: -3.1277, G_loss: 0.3541\n",
      "  Batch [730/1299] D_loss: -1.3562, G_loss: -0.2756\n",
      "  Batch [740/1299] D_loss: -2.7243, G_loss: 2.1029\n",
      "  Batch [750/1299] D_loss: -2.2019, G_loss: 0.7332\n",
      "  Batch [760/1299] D_loss: -2.1724, G_loss: -1.7974\n",
      "  Batch [770/1299] D_loss: -2.4008, G_loss: -0.3573\n",
      "  Batch [780/1299] D_loss: -3.8510, G_loss: -1.9625\n",
      "  Batch [790/1299] D_loss: -3.4020, G_loss: 0.5438\n",
      "  Batch [800/1299] D_loss: -4.1160, G_loss: -0.6736\n",
      "  Batch [810/1299] D_loss: -2.2485, G_loss: 0.4441\n",
      "  Batch [820/1299] D_loss: -2.3384, G_loss: 1.4854\n",
      "  Batch [830/1299] D_loss: -2.9489, G_loss: -3.4255\n",
      "  Batch [840/1299] D_loss: -1.4849, G_loss: -0.9666\n",
      "  Batch [850/1299] D_loss: -4.0127, G_loss: 2.5806\n",
      "  Batch [860/1299] D_loss: -3.4433, G_loss: -1.0620\n",
      "  Batch [870/1299] D_loss: -0.8844, G_loss: -0.4514\n",
      "  Batch [880/1299] D_loss: -3.2236, G_loss: -2.5465\n",
      "  Batch [890/1299] D_loss: -3.4279, G_loss: -1.8398\n",
      "  Batch [900/1299] D_loss: -3.9259, G_loss: 1.6268\n",
      "  Batch [910/1299] D_loss: -3.5706, G_loss: 0.2939\n",
      "  Batch [920/1299] D_loss: -2.8745, G_loss: 0.6541\n",
      "  Batch [930/1299] D_loss: -2.9705, G_loss: 1.6706\n",
      "  Batch [940/1299] D_loss: -4.2833, G_loss: 0.1664\n",
      "  Batch [950/1299] D_loss: -3.5573, G_loss: -0.1666\n",
      "  Batch [960/1299] D_loss: -4.2006, G_loss: 4.3129\n",
      "  Batch [970/1299] D_loss: -2.8800, G_loss: -2.0116\n",
      "  Batch [980/1299] D_loss: -2.4140, G_loss: 0.1261\n",
      "  Batch [990/1299] D_loss: -4.0729, G_loss: -4.9118\n",
      "  Batch [1000/1299] D_loss: -2.1163, G_loss: 2.2492\n",
      "  Batch [1010/1299] D_loss: -3.0530, G_loss: -0.0666\n",
      "  Batch [1020/1299] D_loss: -3.3753, G_loss: 2.9966\n",
      "  Batch [1030/1299] D_loss: -4.2039, G_loss: -0.8926\n",
      "  Batch [1040/1299] D_loss: -1.7152, G_loss: -1.5136\n",
      "  Batch [1050/1299] D_loss: -3.8314, G_loss: -1.6201\n",
      "  Batch [1060/1299] D_loss: -4.0828, G_loss: 3.8329\n",
      "  Batch [1070/1299] D_loss: -3.1613, G_loss: 3.2854\n",
      "  Batch [1080/1299] D_loss: -1.9366, G_loss: 2.4372\n",
      "  Batch [1090/1299] D_loss: -3.2451, G_loss: 2.3290\n",
      "  Batch [1100/1299] D_loss: -3.3269, G_loss: -4.1753\n",
      "  Batch [1110/1299] D_loss: -2.9625, G_loss: 2.2621\n",
      "  Batch [1120/1299] D_loss: -2.4629, G_loss: -0.4858\n",
      "  Batch [1130/1299] D_loss: -2.0639, G_loss: 0.3229\n",
      "  Batch [1140/1299] D_loss: -2.9327, G_loss: -1.9877\n",
      "  Batch [1150/1299] D_loss: -1.7217, G_loss: -0.0993\n",
      "  Batch [1160/1299] D_loss: -2.2478, G_loss: -0.0859\n",
      "  Batch [1170/1299] D_loss: -3.2546, G_loss: -0.2666\n",
      "  Batch [1180/1299] D_loss: -3.0475, G_loss: 0.4991\n",
      "  Batch [1190/1299] D_loss: -0.7456, G_loss: 2.0313\n",
      "  Batch [1200/1299] D_loss: -3.5421, G_loss: -1.4020\n",
      "  Batch [1210/1299] D_loss: -3.1609, G_loss: 0.5544\n",
      "  Batch [1220/1299] D_loss: -2.7013, G_loss: -0.8402\n",
      "  Batch [1230/1299] D_loss: -1.5653, G_loss: 1.3598\n",
      "  Batch [1240/1299] D_loss: -4.2148, G_loss: 0.1242\n",
      "  Batch [1250/1299] D_loss: -2.8473, G_loss: -1.7773\n",
      "  Batch [1260/1299] D_loss: -1.5314, G_loss: -1.9946\n",
      "  Batch [1270/1299] D_loss: -1.9531, G_loss: -0.1516\n",
      "  Batch [1280/1299] D_loss: -4.3022, G_loss: 2.2756\n",
      "  Batch [1290/1299] D_loss: -3.1607, G_loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Average D_loss: -2.9655\n",
      "  Average G_loss: -1.1891\n",
      "\n",
      "Models saved at epoch 1:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_121949_dataset+cell_type/generator_20250121_121949_dataset+cell_type_epoch_1.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_121949_dataset+cell_type/discriminator_20250121_121949_dataset+cell_type_epoch_1.pt\n",
      "\n",
      "Epoch [2/100]\n",
      "  Batch [0/1299] D_loss: -2.1918, G_loss: 0.3683\n",
      "  Batch [10/1299] D_loss: -3.2372, G_loss: -1.5384\n",
      "  Batch [20/1299] D_loss: -2.7977, G_loss: -0.4146\n",
      "  Batch [30/1299] D_loss: -4.1501, G_loss: -1.4355\n",
      "  Batch [40/1299] D_loss: -3.3868, G_loss: 1.7798\n",
      "  Batch [50/1299] D_loss: -4.0314, G_loss: -0.5715\n",
      "  Batch [60/1299] D_loss: -2.1887, G_loss: 0.4195\n",
      "  Batch [70/1299] D_loss: -3.9844, G_loss: 1.5717\n",
      "  Batch [80/1299] D_loss: -2.6740, G_loss: -1.4374\n",
      "  Batch [90/1299] D_loss: -3.8382, G_loss: -0.7443\n",
      "  Batch [100/1299] D_loss: -3.1587, G_loss: -2.2567\n",
      "  Batch [110/1299] D_loss: -4.0568, G_loss: 2.4820\n",
      "  Batch [120/1299] D_loss: -3.1248, G_loss: -1.4262\n",
      "  Batch [130/1299] D_loss: -4.4049, G_loss: -0.6459\n",
      "  Batch [140/1299] D_loss: -3.2565, G_loss: 0.9591\n",
      "  Batch [150/1299] D_loss: -3.7284, G_loss: 0.3848\n",
      "  Batch [160/1299] D_loss: -4.2837, G_loss: 0.3409\n",
      "  Batch [170/1299] D_loss: -3.8678, G_loss: -0.7627\n",
      "  Batch [180/1299] D_loss: -4.5067, G_loss: -2.6422\n",
      "  Batch [190/1299] D_loss: -1.6782, G_loss: -0.2298\n",
      "  Batch [200/1299] D_loss: -3.5982, G_loss: 1.6361\n",
      "  Batch [210/1299] D_loss: -2.3181, G_loss: -3.4237\n",
      "  Batch [220/1299] D_loss: -3.9540, G_loss: -0.2115\n",
      "  Batch [230/1299] D_loss: -4.4290, G_loss: -0.2743\n",
      "  Batch [240/1299] D_loss: -2.4312, G_loss: 2.2807\n",
      "  Batch [250/1299] D_loss: -3.3035, G_loss: 1.0000\n",
      "  Batch [260/1299] D_loss: -2.4617, G_loss: 0.4927\n",
      "  Batch [270/1299] D_loss: -3.8998, G_loss: 0.4380\n",
      "  Batch [280/1299] D_loss: -3.0366, G_loss: 0.2376\n",
      "  Batch [290/1299] D_loss: -2.2756, G_loss: 0.7484\n",
      "  Batch [300/1299] D_loss: -2.8639, G_loss: -0.0732\n",
      "  Batch [310/1299] D_loss: -2.8801, G_loss: -0.0368\n",
      "  Batch [320/1299] D_loss: -2.3451, G_loss: 1.3030\n",
      "  Batch [330/1299] D_loss: -3.1672, G_loss: -1.5118\n",
      "  Batch [340/1299] D_loss: -2.9621, G_loss: 0.1627\n",
      "  Batch [350/1299] D_loss: -2.8548, G_loss: -2.1327\n",
      "  Batch [360/1299] D_loss: -1.8638, G_loss: 0.2811\n",
      "  Batch [370/1299] D_loss: -2.6490, G_loss: 3.9940\n",
      "  Batch [380/1299] D_loss: -3.0173, G_loss: 0.7469\n",
      "  Batch [390/1299] D_loss: -2.7937, G_loss: -0.2736\n",
      "  Batch [400/1299] D_loss: -2.5226, G_loss: -0.1736\n",
      "  Batch [410/1299] D_loss: -3.0766, G_loss: 2.3111\n",
      "  Batch [420/1299] D_loss: -3.9404, G_loss: -1.4047\n",
      "  Batch [430/1299] D_loss: -2.9352, G_loss: -0.7880\n",
      "  Batch [440/1299] D_loss: -3.2534, G_loss: 0.6083\n",
      "  Batch [450/1299] D_loss: -4.2230, G_loss: 1.0016\n",
      "  Batch [460/1299] D_loss: -2.5527, G_loss: 0.5715\n",
      "  Batch [470/1299] D_loss: -3.0225, G_loss: -1.5280\n",
      "  Batch [480/1299] D_loss: -2.6423, G_loss: 1.7759\n",
      "  Batch [490/1299] D_loss: -3.4608, G_loss: 0.6232\n",
      "  Batch [500/1299] D_loss: -3.9461, G_loss: -0.1768\n",
      "  Batch [510/1299] D_loss: -1.9118, G_loss: 0.0956\n",
      "  Batch [520/1299] D_loss: -3.7937, G_loss: 1.5938\n",
      "  Batch [530/1299] D_loss: -4.2919, G_loss: -2.7612\n",
      "  Batch [540/1299] D_loss: -2.4239, G_loss: -1.7594\n",
      "  Batch [550/1299] D_loss: -2.9271, G_loss: 1.3352\n",
      "  Batch [560/1299] D_loss: -3.6271, G_loss: -0.9629\n",
      "  Batch [570/1299] D_loss: -3.3705, G_loss: -2.0648\n",
      "  Batch [580/1299] D_loss: -2.6147, G_loss: 0.0061\n",
      "  Batch [590/1299] D_loss: -3.3497, G_loss: 1.1231\n",
      "  Batch [600/1299] D_loss: -3.0783, G_loss: 1.2815\n",
      "  Batch [610/1299] D_loss: -2.7302, G_loss: -3.7665\n",
      "  Batch [620/1299] D_loss: -2.6086, G_loss: 1.1551\n",
      "  Batch [630/1299] D_loss: -4.9474, G_loss: -1.0094\n",
      "  Batch [640/1299] D_loss: -4.0554, G_loss: -1.0384\n",
      "  Batch [650/1299] D_loss: -2.7269, G_loss: 2.2729\n",
      "  Batch [660/1299] D_loss: -2.9487, G_loss: 2.4432\n",
      "  Batch [670/1299] D_loss: -3.2509, G_loss: -0.9230\n",
      "  Batch [680/1299] D_loss: -3.3005, G_loss: -1.6020\n",
      "  Batch [690/1299] D_loss: -1.8461, G_loss: -1.3016\n",
      "  Batch [700/1299] D_loss: -4.0711, G_loss: -0.6108\n",
      "  Batch [710/1299] D_loss: -4.1864, G_loss: 3.3655\n",
      "  Batch [720/1299] D_loss: -3.2752, G_loss: 0.0085\n",
      "  Batch [730/1299] D_loss: -3.4381, G_loss: -3.2198\n",
      "  Batch [740/1299] D_loss: -2.0600, G_loss: -0.0807\n",
      "  Batch [750/1299] D_loss: -3.5850, G_loss: 0.4836\n",
      "  Batch [760/1299] D_loss: -4.0693, G_loss: -2.6007\n",
      "  Batch [770/1299] D_loss: -3.1521, G_loss: 0.3081\n",
      "  Batch [780/1299] D_loss: -3.0089, G_loss: -0.0227\n",
      "  Batch [790/1299] D_loss: -2.3380, G_loss: 0.4604\n",
      "  Batch [800/1299] D_loss: -3.2015, G_loss: 1.3105\n",
      "  Batch [810/1299] D_loss: -3.3720, G_loss: 0.1700\n",
      "  Batch [820/1299] D_loss: -4.1490, G_loss: -1.5543\n",
      "  Batch [830/1299] D_loss: -2.7878, G_loss: -0.7266\n",
      "  Batch [840/1299] D_loss: -3.1278, G_loss: -0.2707\n",
      "  Batch [850/1299] D_loss: -4.3792, G_loss: 1.6357\n",
      "  Batch [860/1299] D_loss: -2.0648, G_loss: 1.4721\n",
      "  Batch [870/1299] D_loss: -2.7993, G_loss: 2.1789\n",
      "  Batch [880/1299] D_loss: -4.2591, G_loss: -0.2227\n",
      "  Batch [890/1299] D_loss: -3.4782, G_loss: 0.0014\n",
      "  Batch [900/1299] D_loss: -1.7879, G_loss: -2.1524\n",
      "  Batch [910/1299] D_loss: -3.5312, G_loss: -1.8288\n",
      "  Batch [920/1299] D_loss: -4.0434, G_loss: 1.4198\n",
      "  Batch [930/1299] D_loss: -3.8025, G_loss: 1.3170\n",
      "  Batch [940/1299] D_loss: -3.5053, G_loss: -1.5094\n",
      "  Batch [950/1299] D_loss: -3.1805, G_loss: -2.2973\n",
      "  Batch [960/1299] D_loss: -3.1107, G_loss: 1.3686\n",
      "  Batch [970/1299] D_loss: -4.1118, G_loss: -0.6200\n",
      "  Batch [980/1299] D_loss: -2.9323, G_loss: 1.4159\n",
      "  Batch [990/1299] D_loss: -3.3401, G_loss: 0.1341\n",
      "  Batch [1000/1299] D_loss: -4.5862, G_loss: 1.6905\n",
      "  Batch [1010/1299] D_loss: -3.6777, G_loss: -0.7965\n",
      "  Batch [1020/1299] D_loss: -3.6582, G_loss: 0.2155\n",
      "  Batch [1030/1299] D_loss: -3.3836, G_loss: 1.3523\n",
      "  Batch [1040/1299] D_loss: -2.9773, G_loss: 0.1303\n",
      "  Batch [1050/1299] D_loss: -4.5478, G_loss: 3.0898\n",
      "  Batch [1060/1299] D_loss: -3.8812, G_loss: -0.5474\n",
      "  Batch [1070/1299] D_loss: -4.1195, G_loss: -2.1911\n",
      "  Batch [1080/1299] D_loss: -3.8393, G_loss: -3.1096\n",
      "  Batch [1090/1299] D_loss: -1.9922, G_loss: 2.1031\n",
      "  Batch [1100/1299] D_loss: -2.9805, G_loss: 0.3017\n",
      "  Batch [1110/1299] D_loss: -2.8995, G_loss: 1.8903\n",
      "  Batch [1120/1299] D_loss: -3.2476, G_loss: 0.6559\n",
      "  Batch [1130/1299] D_loss: -3.6743, G_loss: -0.7432\n",
      "  Batch [1140/1299] D_loss: -4.1844, G_loss: 0.9970\n",
      "  Batch [1150/1299] D_loss: -3.1642, G_loss: 0.4464\n",
      "  Batch [1160/1299] D_loss: -2.3360, G_loss: -0.4811\n",
      "  Batch [1170/1299] D_loss: -4.1321, G_loss: -0.2491\n",
      "  Batch [1180/1299] D_loss: -3.7736, G_loss: -0.7135\n",
      "  Batch [1190/1299] D_loss: -3.4671, G_loss: 1.0349\n",
      "  Batch [1200/1299] D_loss: -3.2807, G_loss: 2.4451\n",
      "  Batch [1210/1299] D_loss: -3.4732, G_loss: -1.2057\n",
      "  Batch [1220/1299] D_loss: -4.6499, G_loss: -1.1745\n",
      "  Batch [1230/1299] D_loss: -4.0979, G_loss: 1.6935\n",
      "  Batch [1240/1299] D_loss: -2.1380, G_loss: 0.2102\n",
      "  Batch [1250/1299] D_loss: -4.1710, G_loss: 0.7402\n",
      "  Batch [1260/1299] D_loss: -4.7128, G_loss: 0.8074\n",
      "  Batch [1270/1299] D_loss: -2.8406, G_loss: 0.6286\n",
      "  Batch [1280/1299] D_loss: -3.1696, G_loss: 2.4083\n",
      "  Batch [1290/1299] D_loss: -3.3114, G_loss: 1.5262\n",
      "\n",
      "Epoch 2 Summary:\n",
      "  Average D_loss: -3.0593\n",
      "  Average G_loss: 0.0947\n",
      "\n",
      "Epoch [3/100]\n",
      "  Batch [0/1299] D_loss: -4.0216, G_loss: -0.8364\n",
      "  Batch [10/1299] D_loss: -3.5188, G_loss: 0.7377\n",
      "  Batch [20/1299] D_loss: -3.2382, G_loss: 0.4002\n",
      "  Batch [30/1299] D_loss: -3.2190, G_loss: 0.0704\n",
      "  Batch [40/1299] D_loss: -3.4624, G_loss: 0.3140\n",
      "  Batch [50/1299] D_loss: -2.5720, G_loss: 1.2233\n",
      "  Batch [60/1299] D_loss: -4.1797, G_loss: 0.8438\n",
      "  Batch [70/1299] D_loss: -3.7431, G_loss: 1.3852\n",
      "  Batch [80/1299] D_loss: -3.3744, G_loss: 0.8208\n",
      "  Batch [90/1299] D_loss: -2.4971, G_loss: 1.2726\n",
      "  Batch [100/1299] D_loss: -3.2588, G_loss: 1.7791\n",
      "  Batch [110/1299] D_loss: -3.6520, G_loss: 1.5519\n",
      "  Batch [120/1299] D_loss: -3.3550, G_loss: -0.3923\n",
      "  Batch [130/1299] D_loss: -3.4531, G_loss: 0.3831\n",
      "  Batch [140/1299] D_loss: -3.5199, G_loss: 2.4132\n",
      "  Batch [150/1299] D_loss: -5.6597, G_loss: 1.5129\n",
      "  Batch [160/1299] D_loss: -4.2686, G_loss: 1.0996\n",
      "  Batch [170/1299] D_loss: -4.5328, G_loss: -1.0648\n",
      "  Batch [180/1299] D_loss: -3.5498, G_loss: 0.9405\n",
      "  Batch [190/1299] D_loss: -2.9150, G_loss: 1.6041\n",
      "  Batch [200/1299] D_loss: -3.7024, G_loss: 2.2921\n",
      "  Batch [210/1299] D_loss: -3.6300, G_loss: 0.7295\n",
      "  Batch [220/1299] D_loss: -3.0327, G_loss: 3.2547\n",
      "  Batch [230/1299] D_loss: -3.2872, G_loss: -0.3827\n",
      "  Batch [240/1299] D_loss: -3.9549, G_loss: 0.3784\n",
      "  Batch [250/1299] D_loss: -3.0784, G_loss: 1.2784\n",
      "  Batch [260/1299] D_loss: -4.6346, G_loss: 1.2160\n",
      "  Batch [270/1299] D_loss: -3.5015, G_loss: 1.4931\n",
      "  Batch [280/1299] D_loss: -3.6906, G_loss: -1.3295\n",
      "  Batch [290/1299] D_loss: -4.5839, G_loss: 2.7604\n",
      "  Batch [300/1299] D_loss: -4.0174, G_loss: 1.7107\n",
      "  Batch [310/1299] D_loss: -4.2326, G_loss: 1.3865\n",
      "  Batch [320/1299] D_loss: -2.7582, G_loss: -0.8667\n",
      "  Batch [330/1299] D_loss: -3.3620, G_loss: 2.0012\n",
      "  Batch [340/1299] D_loss: -5.0939, G_loss: -0.8037\n",
      "  Batch [350/1299] D_loss: -3.4725, G_loss: 1.9372\n",
      "  Batch [360/1299] D_loss: -3.3049, G_loss: 2.1446\n",
      "  Batch [370/1299] D_loss: -3.7908, G_loss: 3.8978\n",
      "  Batch [380/1299] D_loss: -3.9948, G_loss: 2.2291\n",
      "  Batch [390/1299] D_loss: -3.4027, G_loss: 1.0804\n",
      "  Batch [400/1299] D_loss: -4.4812, G_loss: -1.0256\n",
      "  Batch [410/1299] D_loss: -3.6930, G_loss: 0.6304\n",
      "  Batch [420/1299] D_loss: -2.8283, G_loss: 2.2211\n",
      "  Batch [430/1299] D_loss: -4.1351, G_loss: 1.4898\n",
      "  Batch [440/1299] D_loss: -4.0974, G_loss: 2.1516\n",
      "  Batch [450/1299] D_loss: -3.2064, G_loss: 1.5014\n",
      "  Batch [460/1299] D_loss: -2.9761, G_loss: 2.2557\n",
      "  Batch [470/1299] D_loss: -2.7328, G_loss: 2.3791\n",
      "  Batch [480/1299] D_loss: -4.8696, G_loss: 3.8535\n",
      "  Batch [490/1299] D_loss: -3.4481, G_loss: 1.1364\n",
      "  Batch [500/1299] D_loss: -4.6819, G_loss: 0.3097\n",
      "  Batch [510/1299] D_loss: -4.6042, G_loss: -0.7516\n",
      "  Batch [520/1299] D_loss: -3.2800, G_loss: 0.1157\n",
      "  Batch [530/1299] D_loss: -3.3596, G_loss: 3.5564\n",
      "  Batch [540/1299] D_loss: -3.3225, G_loss: 2.0907\n",
      "  Batch [550/1299] D_loss: -3.5157, G_loss: 3.8993\n",
      "  Batch [560/1299] D_loss: -3.6145, G_loss: -0.5346\n",
      "  Batch [570/1299] D_loss: -3.7292, G_loss: 1.1401\n",
      "  Batch [580/1299] D_loss: -4.4895, G_loss: -0.9892\n",
      "  Batch [590/1299] D_loss: -4.2225, G_loss: -1.6775\n",
      "  Batch [600/1299] D_loss: -2.5995, G_loss: 3.3378\n",
      "  Batch [610/1299] D_loss: -4.4493, G_loss: 1.0250\n",
      "  Batch [620/1299] D_loss: -3.3743, G_loss: 4.9945\n",
      "  Batch [630/1299] D_loss: -3.3256, G_loss: 1.6863\n",
      "  Batch [640/1299] D_loss: -3.6788, G_loss: 0.0469\n",
      "  Batch [650/1299] D_loss: -3.7629, G_loss: 2.1159\n",
      "  Batch [660/1299] D_loss: -3.5200, G_loss: 0.7987\n",
      "  Batch [670/1299] D_loss: -4.4877, G_loss: 3.3073\n",
      "  Batch [680/1299] D_loss: -3.8736, G_loss: 0.6224\n",
      "  Batch [690/1299] D_loss: -3.6672, G_loss: 2.6591\n",
      "  Batch [700/1299] D_loss: -2.9381, G_loss: 2.2984\n",
      "  Batch [710/1299] D_loss: -4.5524, G_loss: 3.5807\n",
      "  Batch [720/1299] D_loss: -2.4635, G_loss: 2.9460\n",
      "  Batch [730/1299] D_loss: -3.5774, G_loss: 2.2199\n",
      "  Batch [740/1299] D_loss: -3.1513, G_loss: 1.3494\n",
      "  Batch [750/1299] D_loss: -3.4385, G_loss: 3.7725\n",
      "  Batch [760/1299] D_loss: -4.3415, G_loss: 2.1068\n",
      "  Batch [770/1299] D_loss: -3.6018, G_loss: 3.8232\n",
      "  Batch [780/1299] D_loss: -2.6493, G_loss: 0.8164\n",
      "  Batch [790/1299] D_loss: -5.5293, G_loss: 1.9635\n",
      "  Batch [800/1299] D_loss: -4.2382, G_loss: 3.4130\n",
      "  Batch [810/1299] D_loss: -4.1872, G_loss: 3.4643\n",
      "  Batch [820/1299] D_loss: -3.7357, G_loss: 1.1690\n",
      "  Batch [830/1299] D_loss: -4.3022, G_loss: -0.3643\n",
      "  Batch [840/1299] D_loss: -5.1091, G_loss: 2.0954\n",
      "  Batch [850/1299] D_loss: -2.6994, G_loss: 3.3286\n",
      "  Batch [860/1299] D_loss: -2.6211, G_loss: 2.5415\n",
      "  Batch [870/1299] D_loss: -3.7107, G_loss: 1.3945\n",
      "  Batch [880/1299] D_loss: -3.5815, G_loss: 0.8406\n",
      "  Batch [890/1299] D_loss: -3.5808, G_loss: 1.2828\n",
      "  Batch [900/1299] D_loss: -3.3081, G_loss: 3.2563\n",
      "  Batch [910/1299] D_loss: -3.7659, G_loss: 2.2854\n",
      "  Batch [920/1299] D_loss: -2.9672, G_loss: 2.0897\n",
      "  Batch [930/1299] D_loss: -3.9775, G_loss: -0.9576\n",
      "  Batch [940/1299] D_loss: -3.3127, G_loss: 1.3464\n",
      "  Batch [950/1299] D_loss: -2.8659, G_loss: 2.6621\n",
      "  Batch [960/1299] D_loss: -4.7254, G_loss: 5.4282\n",
      "  Batch [970/1299] D_loss: -3.5475, G_loss: 2.7244\n",
      "  Batch [980/1299] D_loss: -2.3355, G_loss: 2.1596\n",
      "  Batch [990/1299] D_loss: -4.5224, G_loss: 3.4203\n",
      "  Batch [1000/1299] D_loss: -1.0566, G_loss: 0.1372\n",
      "  Batch [1010/1299] D_loss: -3.4869, G_loss: 2.3023\n",
      "  Batch [1020/1299] D_loss: -3.7161, G_loss: 3.4055\n",
      "  Batch [1030/1299] D_loss: -4.3858, G_loss: 3.4228\n",
      "  Batch [1040/1299] D_loss: -4.0154, G_loss: 4.2272\n",
      "  Batch [1050/1299] D_loss: -3.8012, G_loss: 3.0195\n",
      "  Batch [1060/1299] D_loss: -4.2254, G_loss: 3.5032\n",
      "  Batch [1070/1299] D_loss: -5.0136, G_loss: 5.5126\n",
      "  Batch [1080/1299] D_loss: -4.6338, G_loss: 0.8285\n",
      "  Batch [1090/1299] D_loss: -4.9635, G_loss: 4.4531\n",
      "  Batch [1100/1299] D_loss: -4.4256, G_loss: 4.3005\n",
      "  Batch [1110/1299] D_loss: -4.7370, G_loss: 2.1744\n",
      "  Batch [1120/1299] D_loss: -2.8575, G_loss: 2.5022\n",
      "  Batch [1130/1299] D_loss: -4.3697, G_loss: 2.9684\n",
      "  Batch [1140/1299] D_loss: -4.1490, G_loss: 4.0625\n",
      "  Batch [1150/1299] D_loss: -3.4472, G_loss: 3.7752\n",
      "  Batch [1160/1299] D_loss: -4.4555, G_loss: 1.3666\n",
      "  Batch [1170/1299] D_loss: -3.1999, G_loss: 3.4660\n",
      "  Batch [1180/1299] D_loss: -3.9861, G_loss: 5.6211\n",
      "  Batch [1190/1299] D_loss: -4.0928, G_loss: 2.7758\n",
      "  Batch [1200/1299] D_loss: -3.1405, G_loss: 2.2212\n",
      "  Batch [1210/1299] D_loss: -1.6744, G_loss: 2.4265\n",
      "  Batch [1220/1299] D_loss: -3.9415, G_loss: 3.2230\n",
      "  Batch [1230/1299] D_loss: -4.7383, G_loss: 2.1937\n",
      "  Batch [1240/1299] D_loss: -4.3356, G_loss: 2.8840\n",
      "  Batch [1250/1299] D_loss: -4.0361, G_loss: 1.9287\n",
      "  Batch [1260/1299] D_loss: -2.3615, G_loss: 2.1148\n",
      "  Batch [1270/1299] D_loss: -4.0534, G_loss: 3.8697\n",
      "  Batch [1280/1299] D_loss: -3.1032, G_loss: 3.9616\n",
      "  Batch [1290/1299] D_loss: -2.9590, G_loss: 7.4132\n",
      "\n",
      "Epoch 3 Summary:\n",
      "  Average D_loss: -3.3948\n",
      "  Average G_loss: 1.8301\n",
      "\n",
      "Epoch [4/100]\n",
      "  Batch [0/1299] D_loss: -3.4800, G_loss: 3.5575\n",
      "  Batch [10/1299] D_loss: -3.0527, G_loss: 1.1665\n",
      "  Batch [20/1299] D_loss: -4.7684, G_loss: 2.7785\n",
      "  Batch [30/1299] D_loss: -3.1604, G_loss: 4.0601\n",
      "  Batch [40/1299] D_loss: -2.9981, G_loss: 3.8662\n",
      "  Batch [50/1299] D_loss: -4.1661, G_loss: 4.4335\n",
      "  Batch [60/1299] D_loss: -5.1296, G_loss: 0.8613\n",
      "  Batch [70/1299] D_loss: -4.9423, G_loss: 3.0677\n",
      "  Batch [80/1299] D_loss: -3.8366, G_loss: 3.0869\n",
      "  Batch [90/1299] D_loss: -4.0564, G_loss: 2.3928\n",
      "  Batch [100/1299] D_loss: -4.4449, G_loss: 2.2346\n",
      "  Batch [110/1299] D_loss: -4.1458, G_loss: 2.6095\n",
      "  Batch [120/1299] D_loss: -4.4843, G_loss: 3.5610\n",
      "  Batch [130/1299] D_loss: -3.4174, G_loss: 3.9197\n",
      "  Batch [140/1299] D_loss: -3.6510, G_loss: 2.1626\n",
      "  Batch [150/1299] D_loss: -2.9813, G_loss: 3.8992\n",
      "  Batch [160/1299] D_loss: -4.9863, G_loss: 5.0663\n",
      "  Batch [170/1299] D_loss: -4.3457, G_loss: 3.0102\n",
      "  Batch [180/1299] D_loss: -3.9569, G_loss: 2.7372\n",
      "  Batch [190/1299] D_loss: -4.4220, G_loss: 3.7901\n",
      "  Batch [200/1299] D_loss: -3.3149, G_loss: 4.2510\n",
      "  Batch [210/1299] D_loss: -3.7625, G_loss: 4.4550\n",
      "  Batch [220/1299] D_loss: -2.6009, G_loss: 2.8174\n",
      "  Batch [230/1299] D_loss: -3.3799, G_loss: 3.5787\n",
      "  Batch [240/1299] D_loss: -5.2851, G_loss: 3.6612\n",
      "  Batch [250/1299] D_loss: -4.2283, G_loss: 4.0975\n",
      "  Batch [260/1299] D_loss: -3.6333, G_loss: 6.0253\n",
      "  Batch [270/1299] D_loss: -3.7712, G_loss: 3.7309\n",
      "  Batch [280/1299] D_loss: -3.6867, G_loss: 4.4234\n",
      "  Batch [290/1299] D_loss: -3.7550, G_loss: 2.6180\n",
      "  Batch [300/1299] D_loss: -3.0328, G_loss: 3.7456\n",
      "  Batch [310/1299] D_loss: -2.9469, G_loss: 5.9608\n",
      "  Batch [320/1299] D_loss: -3.8664, G_loss: 0.7848\n",
      "  Batch [330/1299] D_loss: -2.6743, G_loss: 1.4755\n",
      "  Batch [340/1299] D_loss: -4.5001, G_loss: 4.4911\n",
      "  Batch [350/1299] D_loss: -4.9724, G_loss: 5.2956\n",
      "  Batch [360/1299] D_loss: -5.0853, G_loss: 5.3069\n",
      "  Batch [370/1299] D_loss: -2.9415, G_loss: 3.3717\n",
      "  Batch [380/1299] D_loss: -3.5276, G_loss: 4.8223\n",
      "  Batch [390/1299] D_loss: -4.1283, G_loss: 2.0628\n",
      "  Batch [400/1299] D_loss: -4.2040, G_loss: 3.2473\n",
      "  Batch [410/1299] D_loss: -3.7980, G_loss: 2.8448\n",
      "  Batch [420/1299] D_loss: -3.3911, G_loss: 4.2629\n",
      "  Batch [430/1299] D_loss: -4.4282, G_loss: 4.2396\n",
      "  Batch [440/1299] D_loss: -3.5374, G_loss: 7.3379\n",
      "  Batch [450/1299] D_loss: -4.0137, G_loss: 4.5862\n",
      "  Batch [460/1299] D_loss: -5.4964, G_loss: 2.8335\n",
      "  Batch [470/1299] D_loss: -3.4976, G_loss: 1.6696\n",
      "  Batch [480/1299] D_loss: -3.1907, G_loss: 6.9214\n",
      "  Batch [490/1299] D_loss: -2.3677, G_loss: 3.7524\n",
      "  Batch [500/1299] D_loss: -3.8309, G_loss: 3.0403\n",
      "  Batch [510/1299] D_loss: -4.7806, G_loss: 1.5637\n",
      "  Batch [520/1299] D_loss: -4.0935, G_loss: 1.9263\n",
      "  Batch [530/1299] D_loss: -3.7666, G_loss: 3.6152\n",
      "  Batch [540/1299] D_loss: -2.3273, G_loss: 3.6890\n",
      "  Batch [550/1299] D_loss: -4.6660, G_loss: 3.1106\n",
      "  Batch [560/1299] D_loss: -4.1595, G_loss: 6.5952\n",
      "  Batch [570/1299] D_loss: -4.3607, G_loss: 5.2299\n",
      "  Batch [580/1299] D_loss: -4.6680, G_loss: 2.9056\n",
      "  Batch [590/1299] D_loss: -3.9281, G_loss: 5.0272\n",
      "  Batch [600/1299] D_loss: -3.8632, G_loss: 3.7597\n",
      "  Batch [610/1299] D_loss: -3.6253, G_loss: 3.7919\n",
      "  Batch [620/1299] D_loss: -2.6940, G_loss: 2.8214\n",
      "  Batch [630/1299] D_loss: -4.2969, G_loss: 2.4144\n",
      "  Batch [640/1299] D_loss: -4.4127, G_loss: 3.8059\n",
      "  Batch [650/1299] D_loss: -4.3586, G_loss: 4.0729\n",
      "  Batch [660/1299] D_loss: -3.9105, G_loss: 3.8747\n",
      "  Batch [670/1299] D_loss: -4.6854, G_loss: 4.7380\n",
      "  Batch [680/1299] D_loss: -4.1294, G_loss: 3.0690\n",
      "  Batch [690/1299] D_loss: -3.5135, G_loss: 2.8134\n",
      "  Batch [700/1299] D_loss: -5.3496, G_loss: 4.1626\n",
      "  Batch [710/1299] D_loss: -6.1370, G_loss: 2.4706\n",
      "  Batch [720/1299] D_loss: -3.4137, G_loss: 3.9003\n",
      "  Batch [730/1299] D_loss: -3.5943, G_loss: 4.5741\n",
      "  Batch [740/1299] D_loss: -3.3515, G_loss: 5.9027\n",
      "  Batch [750/1299] D_loss: -5.3252, G_loss: 4.9632\n",
      "  Batch [760/1299] D_loss: -5.0501, G_loss: 3.5263\n",
      "  Batch [770/1299] D_loss: -4.3101, G_loss: 0.3409\n",
      "  Batch [780/1299] D_loss: -3.5718, G_loss: 6.4643\n",
      "  Batch [790/1299] D_loss: -5.4992, G_loss: 3.5111\n",
      "  Batch [800/1299] D_loss: -2.8832, G_loss: 2.4068\n",
      "  Batch [810/1299] D_loss: -4.2292, G_loss: 4.3613\n",
      "  Batch [820/1299] D_loss: -4.2048, G_loss: 5.7960\n",
      "  Batch [830/1299] D_loss: -3.3403, G_loss: 4.6841\n",
      "  Batch [840/1299] D_loss: -3.7899, G_loss: 5.5071\n",
      "  Batch [850/1299] D_loss: -3.0523, G_loss: 2.7601\n",
      "  Batch [860/1299] D_loss: -4.6850, G_loss: 5.3135\n",
      "  Batch [870/1299] D_loss: -5.0252, G_loss: 0.9710\n",
      "  Batch [880/1299] D_loss: -3.6755, G_loss: 0.1083\n",
      "  Batch [890/1299] D_loss: -2.3848, G_loss: 4.9107\n",
      "  Batch [900/1299] D_loss: -3.9714, G_loss: 5.0860\n",
      "  Batch [910/1299] D_loss: -1.7175, G_loss: 5.1517\n",
      "  Batch [920/1299] D_loss: -3.8049, G_loss: 5.2852\n",
      "  Batch [930/1299] D_loss: -4.2846, G_loss: 4.8974\n",
      "  Batch [940/1299] D_loss: -4.2945, G_loss: 4.3816\n",
      "  Batch [950/1299] D_loss: -3.6399, G_loss: 1.9924\n",
      "  Batch [960/1299] D_loss: -3.1630, G_loss: 3.6021\n",
      "  Batch [970/1299] D_loss: -4.0566, G_loss: 3.5712\n",
      "  Batch [980/1299] D_loss: -3.0768, G_loss: 2.5459\n",
      "  Batch [990/1299] D_loss: -4.4215, G_loss: 4.3398\n",
      "  Batch [1000/1299] D_loss: -0.9257, G_loss: 5.1960\n",
      "  Batch [1010/1299] D_loss: -3.7744, G_loss: 3.6742\n",
      "  Batch [1020/1299] D_loss: -3.8520, G_loss: 4.7753\n",
      "  Batch [1030/1299] D_loss: -4.4382, G_loss: 5.3713\n",
      "  Batch [1040/1299] D_loss: -4.3309, G_loss: 7.9169\n",
      "  Batch [1050/1299] D_loss: -5.0128, G_loss: 3.9241\n",
      "  Batch [1060/1299] D_loss: -4.5064, G_loss: 5.6572\n",
      "  Batch [1070/1299] D_loss: -5.5739, G_loss: 4.7888\n",
      "  Batch [1080/1299] D_loss: -4.3531, G_loss: 4.0026\n",
      "  Batch [1090/1299] D_loss: -3.4879, G_loss: 3.8396\n",
      "  Batch [1100/1299] D_loss: -3.9025, G_loss: 3.7142\n",
      "  Batch [1110/1299] D_loss: -3.5861, G_loss: 5.4635\n",
      "  Batch [1120/1299] D_loss: -4.1606, G_loss: 5.6437\n",
      "  Batch [1130/1299] D_loss: -3.7495, G_loss: 3.4602\n",
      "  Batch [1140/1299] D_loss: -3.2135, G_loss: 4.2026\n",
      "  Batch [1150/1299] D_loss: -5.0991, G_loss: 4.8579\n",
      "  Batch [1160/1299] D_loss: -3.6429, G_loss: 3.6141\n",
      "  Batch [1170/1299] D_loss: -2.4776, G_loss: 4.8215\n",
      "  Batch [1180/1299] D_loss: -4.6382, G_loss: 0.8607\n",
      "  Batch [1190/1299] D_loss: -4.9861, G_loss: 1.1617\n",
      "  Batch [1200/1299] D_loss: -3.9488, G_loss: 2.8789\n",
      "  Batch [1210/1299] D_loss: -4.4220, G_loss: 5.5487\n",
      "  Batch [1220/1299] D_loss: -2.9297, G_loss: 3.7530\n",
      "  Batch [1230/1299] D_loss: -5.7735, G_loss: 2.7990\n",
      "  Batch [1240/1299] D_loss: -4.3755, G_loss: 3.0804\n",
      "  Batch [1250/1299] D_loss: -4.2876, G_loss: 5.8380\n",
      "  Batch [1260/1299] D_loss: -3.3805, G_loss: 4.8611\n",
      "  Batch [1270/1299] D_loss: -4.6222, G_loss: 3.2860\n",
      "  Batch [1280/1299] D_loss: -3.0654, G_loss: 2.6934\n",
      "  Batch [1290/1299] D_loss: -3.3412, G_loss: 3.2936\n",
      "\n",
      "Epoch 4 Summary:\n",
      "  Average D_loss: -3.4722\n",
      "  Average G_loss: 3.7434\n",
      "\n",
      "Epoch [5/100]\n",
      "  Batch [0/1299] D_loss: -4.8957, G_loss: 4.1798\n",
      "  Batch [10/1299] D_loss: -5.2280, G_loss: 6.1586\n",
      "  Batch [20/1299] D_loss: -3.8968, G_loss: 5.6082\n",
      "  Batch [30/1299] D_loss: -4.1762, G_loss: 2.8255\n",
      "  Batch [40/1299] D_loss: -4.5453, G_loss: 2.7509\n",
      "  Batch [50/1299] D_loss: -4.2474, G_loss: 3.1016\n",
      "  Batch [60/1299] D_loss: -4.6695, G_loss: 3.2989\n",
      "  Batch [70/1299] D_loss: -4.7282, G_loss: 5.4650\n",
      "  Batch [80/1299] D_loss: -3.8581, G_loss: 4.5076\n",
      "  Batch [90/1299] D_loss: -2.5841, G_loss: 4.3252\n",
      "  Batch [100/1299] D_loss: -5.2273, G_loss: 3.9373\n",
      "  Batch [110/1299] D_loss: -1.8467, G_loss: 4.6672\n",
      "  Batch [120/1299] D_loss: -3.7548, G_loss: 4.3065\n",
      "  Batch [130/1299] D_loss: -4.3665, G_loss: 4.0189\n",
      "  Batch [140/1299] D_loss: -3.4018, G_loss: 3.6219\n",
      "  Batch [150/1299] D_loss: -1.8540, G_loss: 3.8540\n",
      "  Batch [160/1299] D_loss: -3.1969, G_loss: 2.8983\n",
      "  Batch [170/1299] D_loss: -4.3033, G_loss: 3.8081\n",
      "  Batch [180/1299] D_loss: -2.6723, G_loss: 5.1983\n",
      "  Batch [190/1299] D_loss: -4.0640, G_loss: 4.8103\n",
      "  Batch [200/1299] D_loss: -5.4222, G_loss: 7.0659\n",
      "  Batch [210/1299] D_loss: -4.0502, G_loss: 4.2528\n",
      "  Batch [220/1299] D_loss: -3.9919, G_loss: 2.8820\n",
      "  Batch [230/1299] D_loss: -3.2814, G_loss: 2.8647\n",
      "  Batch [240/1299] D_loss: -2.3517, G_loss: 2.9288\n",
      "  Batch [250/1299] D_loss: -3.4404, G_loss: 4.8556\n",
      "  Batch [260/1299] D_loss: -4.0877, G_loss: 5.3535\n",
      "  Batch [270/1299] D_loss: -4.4632, G_loss: 4.4580\n",
      "  Batch [280/1299] D_loss: -3.9978, G_loss: 4.7168\n",
      "  Batch [290/1299] D_loss: -3.3563, G_loss: 3.4806\n",
      "  Batch [300/1299] D_loss: -3.7231, G_loss: 5.7206\n",
      "  Batch [310/1299] D_loss: -3.6232, G_loss: 5.1898\n",
      "  Batch [320/1299] D_loss: -2.9456, G_loss: 6.9248\n",
      "  Batch [330/1299] D_loss: -3.7663, G_loss: 3.6053\n",
      "  Batch [340/1299] D_loss: -4.4377, G_loss: 2.4022\n",
      "  Batch [350/1299] D_loss: -4.0564, G_loss: 3.7026\n",
      "  Batch [360/1299] D_loss: -3.3552, G_loss: 5.5647\n",
      "  Batch [370/1299] D_loss: -3.0660, G_loss: 5.4174\n",
      "  Batch [380/1299] D_loss: -4.6137, G_loss: 5.0787\n",
      "  Batch [390/1299] D_loss: -3.8868, G_loss: 4.5245\n",
      "  Batch [400/1299] D_loss: -4.6894, G_loss: 4.3973\n",
      "  Batch [410/1299] D_loss: -3.2713, G_loss: 4.7958\n",
      "  Batch [420/1299] D_loss: -2.9391, G_loss: 4.2055\n",
      "  Batch [430/1299] D_loss: -3.5325, G_loss: 4.6021\n",
      "  Batch [440/1299] D_loss: -3.2514, G_loss: 5.1135\n",
      "  Batch [450/1299] D_loss: -3.6345, G_loss: 3.2178\n",
      "  Batch [460/1299] D_loss: -4.6859, G_loss: 4.4383\n",
      "  Batch [470/1299] D_loss: -3.7138, G_loss: 6.1016\n",
      "  Batch [480/1299] D_loss: -3.9008, G_loss: 3.9998\n",
      "  Batch [490/1299] D_loss: -2.9683, G_loss: 5.0943\n",
      "  Batch [500/1299] D_loss: -4.7480, G_loss: 4.8715\n",
      "  Batch [510/1299] D_loss: -3.0627, G_loss: 4.5320\n",
      "  Batch [520/1299] D_loss: -3.7915, G_loss: 4.2735\n",
      "  Batch [530/1299] D_loss: -3.5041, G_loss: 7.5260\n",
      "  Batch [540/1299] D_loss: -5.4986, G_loss: 6.7621\n",
      "  Batch [550/1299] D_loss: -3.7181, G_loss: 2.6273\n",
      "  Batch [560/1299] D_loss: -3.4887, G_loss: 3.7138\n",
      "  Batch [570/1299] D_loss: -3.1591, G_loss: 4.9101\n",
      "  Batch [580/1299] D_loss: -2.3451, G_loss: 4.1349\n",
      "  Batch [590/1299] D_loss: -5.7277, G_loss: 6.3282\n",
      "  Batch [600/1299] D_loss: -5.2291, G_loss: 7.9376\n",
      "  Batch [610/1299] D_loss: -4.0232, G_loss: 4.5343\n",
      "  Batch [620/1299] D_loss: -2.6372, G_loss: 3.5983\n",
      "  Batch [630/1299] D_loss: -2.9886, G_loss: 5.5426\n",
      "  Batch [640/1299] D_loss: -3.6551, G_loss: 3.3177\n",
      "  Batch [650/1299] D_loss: -5.5803, G_loss: 4.9369\n",
      "  Batch [660/1299] D_loss: -2.4845, G_loss: 5.6599\n",
      "  Batch [670/1299] D_loss: -3.1621, G_loss: 6.2156\n",
      "  Batch [680/1299] D_loss: -4.0626, G_loss: 5.5379\n",
      "  Batch [690/1299] D_loss: -4.4876, G_loss: 6.9526\n",
      "  Batch [700/1299] D_loss: -1.1177, G_loss: 5.8839\n",
      "  Batch [710/1299] D_loss: -3.3556, G_loss: 4.2577\n",
      "  Batch [720/1299] D_loss: -4.0400, G_loss: 3.8491\n",
      "  Batch [730/1299] D_loss: -3.5756, G_loss: 5.1384\n",
      "  Batch [740/1299] D_loss: -5.5876, G_loss: 4.8133\n",
      "  Batch [750/1299] D_loss: -3.3629, G_loss: 6.0896\n",
      "  Batch [760/1299] D_loss: -5.5357, G_loss: 5.9599\n",
      "  Batch [770/1299] D_loss: -4.8895, G_loss: 6.7909\n",
      "  Batch [780/1299] D_loss: -3.6140, G_loss: 6.9038\n",
      "  Batch [790/1299] D_loss: -4.7151, G_loss: 5.6837\n",
      "  Batch [800/1299] D_loss: -4.2038, G_loss: 8.0261\n",
      "  Batch [810/1299] D_loss: -2.1639, G_loss: 5.5141\n",
      "  Batch [820/1299] D_loss: -5.2270, G_loss: 4.2454\n",
      "  Batch [830/1299] D_loss: -4.9220, G_loss: 4.5195\n",
      "  Batch [840/1299] D_loss: -4.1056, G_loss: 7.1524\n",
      "  Batch [850/1299] D_loss: -4.0426, G_loss: 4.7953\n",
      "  Batch [860/1299] D_loss: -4.0256, G_loss: 5.0848\n",
      "  Batch [870/1299] D_loss: -5.3572, G_loss: 4.6769\n",
      "  Batch [880/1299] D_loss: -3.5895, G_loss: 4.7138\n",
      "  Batch [890/1299] D_loss: -4.7107, G_loss: 5.3476\n",
      "  Batch [900/1299] D_loss: -3.4577, G_loss: 4.2005\n",
      "  Batch [910/1299] D_loss: -3.4453, G_loss: 3.7337\n",
      "  Batch [920/1299] D_loss: -3.9518, G_loss: 5.6047\n",
      "  Batch [930/1299] D_loss: -4.2876, G_loss: 5.1946\n",
      "  Batch [940/1299] D_loss: -4.3994, G_loss: 4.5735\n",
      "  Batch [950/1299] D_loss: -4.4695, G_loss: 7.2131\n",
      "  Batch [960/1299] D_loss: -3.1902, G_loss: 5.6265\n",
      "  Batch [970/1299] D_loss: -4.2925, G_loss: 5.4553\n",
      "  Batch [980/1299] D_loss: -2.6103, G_loss: 4.3340\n",
      "  Batch [990/1299] D_loss: -4.2209, G_loss: 4.0429\n",
      "  Batch [1000/1299] D_loss: -3.6826, G_loss: 7.1572\n",
      "  Batch [1010/1299] D_loss: -3.4736, G_loss: 6.8164\n",
      "  Batch [1020/1299] D_loss: -4.4083, G_loss: 5.5372\n",
      "  Batch [1030/1299] D_loss: -3.4758, G_loss: 6.2196\n",
      "  Batch [1040/1299] D_loss: -2.4725, G_loss: 5.6338\n",
      "  Batch [1050/1299] D_loss: -3.4020, G_loss: 6.5758\n",
      "  Batch [1060/1299] D_loss: -3.7058, G_loss: 6.7135\n",
      "  Batch [1070/1299] D_loss: -3.9247, G_loss: 5.1497\n",
      "  Batch [1080/1299] D_loss: -2.5936, G_loss: 4.1702\n",
      "  Batch [1090/1299] D_loss: -4.5402, G_loss: 7.4750\n",
      "  Batch [1100/1299] D_loss: -3.0692, G_loss: 6.9675\n",
      "  Batch [1110/1299] D_loss: -4.8302, G_loss: 5.3893\n",
      "  Batch [1120/1299] D_loss: -3.7677, G_loss: 4.7785\n",
      "  Batch [1130/1299] D_loss: -2.6729, G_loss: 5.2942\n",
      "  Batch [1140/1299] D_loss: -2.3326, G_loss: 5.1701\n",
      "  Batch [1150/1299] D_loss: -3.2045, G_loss: 4.2500\n",
      "  Batch [1160/1299] D_loss: -4.7954, G_loss: 5.6522\n",
      "  Batch [1170/1299] D_loss: -3.3958, G_loss: 4.9344\n",
      "  Batch [1180/1299] D_loss: -5.7727, G_loss: 8.2829\n",
      "  Batch [1190/1299] D_loss: -4.1259, G_loss: 6.3652\n",
      "  Batch [1200/1299] D_loss: -3.9676, G_loss: 5.8479\n",
      "  Batch [1210/1299] D_loss: -3.7606, G_loss: 2.5944\n",
      "  Batch [1220/1299] D_loss: -3.3189, G_loss: 6.9543\n",
      "  Batch [1230/1299] D_loss: -4.4499, G_loss: 7.2050\n",
      "  Batch [1240/1299] D_loss: -4.1664, G_loss: 6.5377\n",
      "  Batch [1250/1299] D_loss: -4.7673, G_loss: 6.2554\n",
      "  Batch [1260/1299] D_loss: -3.9537, G_loss: 5.8198\n",
      "  Batch [1270/1299] D_loss: -4.0188, G_loss: 5.2055\n",
      "  Batch [1280/1299] D_loss: -3.6854, G_loss: 6.6662\n",
      "  Batch [1290/1299] D_loss: -3.6629, G_loss: 6.5055\n",
      "\n",
      "Epoch 5 Summary:\n",
      "  Average D_loss: -3.4423\n",
      "  Average G_loss: 5.0640\n",
      "\n",
      "Epoch [6/100]\n",
      "  Batch [0/1299] D_loss: -3.4608, G_loss: 7.6954\n",
      "  Batch [10/1299] D_loss: -5.2350, G_loss: 6.9509\n",
      "  Batch [20/1299] D_loss: -2.8923, G_loss: 7.9808\n",
      "  Batch [30/1299] D_loss: -4.1358, G_loss: 5.4240\n",
      "  Batch [40/1299] D_loss: -5.0267, G_loss: 6.1702\n",
      "  Batch [50/1299] D_loss: -4.0650, G_loss: 7.4138\n",
      "  Batch [60/1299] D_loss: -4.1906, G_loss: 6.4932\n",
      "  Batch [70/1299] D_loss: -3.3739, G_loss: 6.7963\n",
      "  Batch [80/1299] D_loss: -5.7051, G_loss: 6.1503\n",
      "  Batch [90/1299] D_loss: -3.3683, G_loss: 6.6657\n",
      "  Batch [100/1299] D_loss: -2.7236, G_loss: 7.3243\n",
      "  Batch [110/1299] D_loss: -3.4436, G_loss: 7.0372\n",
      "  Batch [120/1299] D_loss: -4.8969, G_loss: 6.3169\n",
      "  Batch [130/1299] D_loss: -3.4503, G_loss: 7.0089\n",
      "  Batch [140/1299] D_loss: -3.9944, G_loss: 5.8800\n",
      "  Batch [150/1299] D_loss: -5.9288, G_loss: 6.6707\n",
      "  Batch [160/1299] D_loss: -3.3093, G_loss: 6.7408\n",
      "  Batch [170/1299] D_loss: -3.1810, G_loss: 7.2071\n",
      "  Batch [180/1299] D_loss: -3.4219, G_loss: 6.1518\n",
      "  Batch [190/1299] D_loss: -4.7040, G_loss: 8.5654\n",
      "  Batch [200/1299] D_loss: -2.4289, G_loss: 5.3154\n",
      "  Batch [210/1299] D_loss: -5.4145, G_loss: 6.2462\n",
      "  Batch [220/1299] D_loss: -4.9488, G_loss: 8.3257\n",
      "  Batch [230/1299] D_loss: -3.2335, G_loss: 4.8604\n",
      "  Batch [240/1299] D_loss: -2.2243, G_loss: 6.6637\n",
      "  Batch [250/1299] D_loss: -4.5290, G_loss: 5.8273\n",
      "  Batch [260/1299] D_loss: -3.8388, G_loss: 8.0174\n",
      "  Batch [270/1299] D_loss: -3.3879, G_loss: 5.9349\n",
      "  Batch [280/1299] D_loss: -4.0393, G_loss: 7.2900\n",
      "  Batch [290/1299] D_loss: -3.6159, G_loss: 6.4224\n",
      "  Batch [300/1299] D_loss: -2.0429, G_loss: 6.3416\n",
      "  Batch [310/1299] D_loss: -3.6210, G_loss: 8.5311\n",
      "  Batch [320/1299] D_loss: -3.6861, G_loss: 4.5252\n",
      "  Batch [330/1299] D_loss: -5.5801, G_loss: 9.0494\n",
      "  Batch [340/1299] D_loss: -3.5076, G_loss: 6.0046\n",
      "  Batch [350/1299] D_loss: -2.6837, G_loss: 5.4592\n",
      "  Batch [360/1299] D_loss: -4.3999, G_loss: 7.8763\n",
      "  Batch [370/1299] D_loss: -4.7166, G_loss: 5.8930\n",
      "  Batch [380/1299] D_loss: -4.4242, G_loss: 7.0060\n",
      "  Batch [390/1299] D_loss: -4.7910, G_loss: 10.7266\n",
      "  Batch [400/1299] D_loss: -2.3383, G_loss: 6.6219\n",
      "  Batch [410/1299] D_loss: -3.7924, G_loss: 6.7805\n",
      "  Batch [420/1299] D_loss: -4.1030, G_loss: 5.4003\n",
      "  Batch [430/1299] D_loss: -4.3306, G_loss: 4.8717\n",
      "  Batch [440/1299] D_loss: -4.3341, G_loss: 7.4255\n",
      "  Batch [450/1299] D_loss: -3.8302, G_loss: 9.1309\n",
      "  Batch [460/1299] D_loss: -4.3698, G_loss: 6.6134\n",
      "  Batch [470/1299] D_loss: -5.3587, G_loss: 5.8643\n",
      "  Batch [480/1299] D_loss: -2.9357, G_loss: 5.8352\n",
      "  Batch [490/1299] D_loss: -3.1069, G_loss: 6.0317\n",
      "  Batch [500/1299] D_loss: -4.6743, G_loss: 4.6309\n",
      "  Batch [510/1299] D_loss: -3.7842, G_loss: 5.3576\n",
      "  Batch [520/1299] D_loss: -4.8952, G_loss: 6.1406\n",
      "  Batch [530/1299] D_loss: -4.0347, G_loss: 7.6593\n",
      "  Batch [540/1299] D_loss: -5.1218, G_loss: 5.9881\n",
      "  Batch [550/1299] D_loss: -2.3000, G_loss: 6.2400\n",
      "  Batch [560/1299] D_loss: -3.9586, G_loss: 6.4176\n",
      "  Batch [570/1299] D_loss: -3.1925, G_loss: 6.1212\n",
      "  Batch [580/1299] D_loss: -3.7049, G_loss: 6.9929\n",
      "  Batch [590/1299] D_loss: -4.0808, G_loss: 8.2131\n",
      "  Batch [600/1299] D_loss: -2.8203, G_loss: 9.4246\n",
      "  Batch [610/1299] D_loss: -4.9314, G_loss: 7.5369\n",
      "  Batch [620/1299] D_loss: -3.7490, G_loss: 6.8785\n",
      "  Batch [630/1299] D_loss: -3.0803, G_loss: 6.5358\n",
      "  Batch [640/1299] D_loss: -3.0433, G_loss: 6.1500\n",
      "  Batch [650/1299] D_loss: -4.1312, G_loss: 6.5631\n",
      "  Batch [660/1299] D_loss: -3.0323, G_loss: 7.8447\n",
      "  Batch [670/1299] D_loss: -3.1565, G_loss: 8.3919\n",
      "  Batch [680/1299] D_loss: -3.7349, G_loss: 8.8241\n",
      "  Batch [690/1299] D_loss: -3.8777, G_loss: 9.4371\n",
      "  Batch [700/1299] D_loss: -5.4320, G_loss: 7.2463\n",
      "  Batch [710/1299] D_loss: -3.3448, G_loss: 7.3507\n",
      "  Batch [720/1299] D_loss: -5.0054, G_loss: 6.0154\n",
      "  Batch [730/1299] D_loss: -3.5362, G_loss: 4.8937\n",
      "  Batch [740/1299] D_loss: -2.4206, G_loss: 5.6938\n",
      "  Batch [750/1299] D_loss: -4.1922, G_loss: 8.7988\n",
      "  Batch [760/1299] D_loss: -4.2746, G_loss: 7.9800\n",
      "  Batch [770/1299] D_loss: -4.4283, G_loss: 7.5393\n",
      "  Batch [780/1299] D_loss: -2.4576, G_loss: 6.2083\n",
      "  Batch [790/1299] D_loss: -3.8758, G_loss: 8.6207\n",
      "  Batch [800/1299] D_loss: -3.2938, G_loss: 7.0772\n",
      "  Batch [810/1299] D_loss: -3.5801, G_loss: 6.6696\n",
      "  Batch [820/1299] D_loss: -3.1704, G_loss: 8.0512\n",
      "  Batch [830/1299] D_loss: -2.9299, G_loss: 6.3736\n",
      "  Batch [840/1299] D_loss: -3.8041, G_loss: 6.1529\n",
      "  Batch [850/1299] D_loss: -3.1646, G_loss: 7.6524\n",
      "  Batch [860/1299] D_loss: -3.7825, G_loss: 7.5641\n",
      "  Batch [870/1299] D_loss: -5.5988, G_loss: 8.3121\n",
      "  Batch [880/1299] D_loss: -4.5944, G_loss: 6.5972\n",
      "  Batch [890/1299] D_loss: -4.0100, G_loss: 7.1788\n",
      "  Batch [900/1299] D_loss: -3.7195, G_loss: 6.6384\n",
      "  Batch [910/1299] D_loss: -3.1041, G_loss: 8.5411\n",
      "  Batch [920/1299] D_loss: -3.1411, G_loss: 9.7339\n",
      "  Batch [930/1299] D_loss: -2.0189, G_loss: 4.7008\n",
      "  Batch [940/1299] D_loss: -4.3540, G_loss: 5.5789\n",
      "  Batch [950/1299] D_loss: -3.9820, G_loss: 7.7721\n",
      "  Batch [960/1299] D_loss: -4.4133, G_loss: 6.5557\n",
      "  Batch [970/1299] D_loss: -4.4127, G_loss: 9.8469\n",
      "  Batch [980/1299] D_loss: -4.1967, G_loss: 10.9962\n",
      "  Batch [990/1299] D_loss: -3.5343, G_loss: 10.5818\n",
      "  Batch [1000/1299] D_loss: -4.3451, G_loss: 8.8733\n",
      "  Batch [1010/1299] D_loss: -3.3025, G_loss: 6.6012\n",
      "  Batch [1020/1299] D_loss: -2.6869, G_loss: 8.0120\n",
      "  Batch [1030/1299] D_loss: -4.3522, G_loss: 6.8796\n",
      "  Batch [1040/1299] D_loss: -3.8123, G_loss: 6.1442\n",
      "  Batch [1050/1299] D_loss: -2.5173, G_loss: 8.9893\n",
      "  Batch [1060/1299] D_loss: -4.5245, G_loss: 8.9396\n",
      "  Batch [1070/1299] D_loss: -4.7137, G_loss: 7.0516\n",
      "  Batch [1080/1299] D_loss: -3.6876, G_loss: 7.6016\n",
      "  Batch [1090/1299] D_loss: -4.5344, G_loss: 6.1707\n",
      "  Batch [1100/1299] D_loss: -1.9851, G_loss: 9.1812\n",
      "  Batch [1110/1299] D_loss: -4.9765, G_loss: 7.2611\n",
      "  Batch [1120/1299] D_loss: -4.9915, G_loss: 10.8885\n",
      "  Batch [1130/1299] D_loss: -3.5696, G_loss: 6.8531\n",
      "  Batch [1140/1299] D_loss: -3.4162, G_loss: 9.3025\n",
      "  Batch [1150/1299] D_loss: -3.7324, G_loss: 10.9649\n",
      "  Batch [1160/1299] D_loss: -3.7408, G_loss: 7.6894\n",
      "  Batch [1170/1299] D_loss: -3.1534, G_loss: 6.5140\n",
      "  Batch [1180/1299] D_loss: -3.5210, G_loss: 6.0891\n",
      "  Batch [1190/1299] D_loss: -3.6073, G_loss: 6.7486\n",
      "  Batch [1200/1299] D_loss: -4.6957, G_loss: 8.6560\n",
      "  Batch [1210/1299] D_loss: -5.3755, G_loss: 9.1796\n",
      "  Batch [1220/1299] D_loss: -3.6726, G_loss: 7.9357\n",
      "  Batch [1230/1299] D_loss: -3.3553, G_loss: 9.2299\n",
      "  Batch [1240/1299] D_loss: -2.6679, G_loss: 8.9287\n",
      "  Batch [1250/1299] D_loss: -4.2753, G_loss: 8.4374\n",
      "  Batch [1260/1299] D_loss: -5.8282, G_loss: 7.7725\n",
      "  Batch [1270/1299] D_loss: -2.1940, G_loss: 8.5578\n",
      "  Batch [1280/1299] D_loss: -2.7563, G_loss: 11.2847\n",
      "  Batch [1290/1299] D_loss: -3.0850, G_loss: 11.1443\n",
      "\n",
      "Epoch 6 Summary:\n",
      "  Average D_loss: -3.3604\n",
      "  Average G_loss: 7.2974\n",
      "\n",
      "Epoch [7/100]\n",
      "  Batch [0/1299] D_loss: -3.1944, G_loss: 8.9225\n",
      "  Batch [10/1299] D_loss: -4.0389, G_loss: 4.9529\n",
      "  Batch [20/1299] D_loss: -3.2092, G_loss: 7.8400\n",
      "  Batch [30/1299] D_loss: -2.7352, G_loss: 7.2888\n",
      "  Batch [40/1299] D_loss: -3.1087, G_loss: 8.8481\n",
      "  Batch [50/1299] D_loss: -4.7953, G_loss: 5.6903\n",
      "  Batch [60/1299] D_loss: -2.7236, G_loss: 6.7960\n",
      "  Batch [70/1299] D_loss: -3.4461, G_loss: 5.0527\n",
      "  Batch [80/1299] D_loss: -4.4106, G_loss: 5.7632\n",
      "  Batch [90/1299] D_loss: -4.8020, G_loss: 8.4565\n",
      "  Batch [100/1299] D_loss: -4.5740, G_loss: 7.9081\n",
      "  Batch [110/1299] D_loss: -2.4874, G_loss: 7.7805\n",
      "  Batch [120/1299] D_loss: -3.3754, G_loss: 7.9876\n",
      "  Batch [130/1299] D_loss: -4.5072, G_loss: 8.0045\n",
      "  Batch [140/1299] D_loss: -4.7286, G_loss: 8.2650\n",
      "  Batch [150/1299] D_loss: -3.0936, G_loss: 6.5870\n",
      "  Batch [160/1299] D_loss: -3.3117, G_loss: 7.3511\n",
      "  Batch [170/1299] D_loss: -3.9207, G_loss: 10.0228\n",
      "  Batch [180/1299] D_loss: -3.5473, G_loss: 9.2202\n",
      "  Batch [190/1299] D_loss: -0.2699, G_loss: 6.7060\n",
      "  Batch [200/1299] D_loss: -4.0461, G_loss: 7.0730\n",
      "  Batch [210/1299] D_loss: -3.9671, G_loss: 6.0094\n",
      "  Batch [220/1299] D_loss: -3.7363, G_loss: 7.2145\n",
      "  Batch [230/1299] D_loss: -4.1222, G_loss: 10.2212\n",
      "  Batch [240/1299] D_loss: -3.6017, G_loss: 7.7716\n",
      "  Batch [250/1299] D_loss: -4.5257, G_loss: 7.1899\n",
      "  Batch [260/1299] D_loss: -3.6564, G_loss: 8.5822\n",
      "  Batch [270/1299] D_loss: -2.4299, G_loss: 9.0937\n",
      "  Batch [280/1299] D_loss: -2.9875, G_loss: 6.6924\n",
      "  Batch [290/1299] D_loss: -3.8092, G_loss: 8.0708\n",
      "  Batch [300/1299] D_loss: -4.9777, G_loss: 8.3978\n",
      "  Batch [310/1299] D_loss: -3.0193, G_loss: 5.8361\n",
      "  Batch [320/1299] D_loss: -4.2424, G_loss: 8.7332\n",
      "  Batch [330/1299] D_loss: -3.1663, G_loss: 8.7158\n",
      "  Batch [340/1299] D_loss: -4.0730, G_loss: 6.3569\n",
      "  Batch [350/1299] D_loss: -5.3784, G_loss: 9.9436\n",
      "  Batch [360/1299] D_loss: -4.7438, G_loss: 9.4392\n",
      "  Batch [370/1299] D_loss: -5.0172, G_loss: 7.6033\n",
      "  Batch [380/1299] D_loss: -3.5684, G_loss: 5.6424\n",
      "  Batch [390/1299] D_loss: -4.2177, G_loss: 6.0887\n",
      "  Batch [400/1299] D_loss: -3.2451, G_loss: 8.0326\n",
      "  Batch [410/1299] D_loss: -4.0965, G_loss: 8.7986\n",
      "  Batch [420/1299] D_loss: -2.5851, G_loss: 5.6374\n",
      "  Batch [430/1299] D_loss: -2.5389, G_loss: 7.4947\n",
      "  Batch [440/1299] D_loss: -3.0613, G_loss: 7.2461\n",
      "  Batch [450/1299] D_loss: -3.2224, G_loss: 7.4001\n",
      "  Batch [460/1299] D_loss: -2.7989, G_loss: 7.4408\n",
      "  Batch [470/1299] D_loss: -1.7428, G_loss: 8.6475\n",
      "  Batch [480/1299] D_loss: -3.0954, G_loss: 7.9438\n",
      "  Batch [490/1299] D_loss: -2.6517, G_loss: 8.9122\n",
      "  Batch [500/1299] D_loss: -3.4441, G_loss: 9.8898\n",
      "  Batch [510/1299] D_loss: -4.1764, G_loss: 10.1605\n",
      "  Batch [520/1299] D_loss: -2.7412, G_loss: 8.0199\n",
      "  Batch [530/1299] D_loss: -1.9007, G_loss: 7.6115\n",
      "  Batch [540/1299] D_loss: -5.7611, G_loss: 4.6463\n",
      "  Batch [550/1299] D_loss: -1.5181, G_loss: 7.8991\n",
      "  Batch [560/1299] D_loss: -1.7603, G_loss: 8.4338\n",
      "  Batch [570/1299] D_loss: -3.0736, G_loss: 9.4994\n",
      "  Batch [580/1299] D_loss: -2.8892, G_loss: 9.1903\n",
      "  Batch [590/1299] D_loss: -1.7032, G_loss: 7.2304\n",
      "  Batch [600/1299] D_loss: -3.8518, G_loss: 6.5998\n",
      "  Batch [610/1299] D_loss: -2.4938, G_loss: 8.9270\n",
      "  Batch [620/1299] D_loss: -4.3602, G_loss: 7.3769\n",
      "  Batch [630/1299] D_loss: -3.2768, G_loss: 6.8178\n",
      "  Batch [640/1299] D_loss: -2.6568, G_loss: 7.0551\n",
      "  Batch [650/1299] D_loss: -3.4789, G_loss: 8.0097\n",
      "  Batch [660/1299] D_loss: -2.4414, G_loss: 6.7754\n",
      "  Batch [670/1299] D_loss: -2.2384, G_loss: 7.6698\n",
      "  Batch [680/1299] D_loss: -3.4019, G_loss: 5.3329\n",
      "  Batch [690/1299] D_loss: -3.3810, G_loss: 8.3980\n",
      "  Batch [700/1299] D_loss: -3.7183, G_loss: 8.8430\n",
      "  Batch [710/1299] D_loss: -2.8219, G_loss: 7.1557\n",
      "  Batch [720/1299] D_loss: -4.0573, G_loss: 9.3540\n",
      "  Batch [730/1299] D_loss: -3.6540, G_loss: 6.4434\n",
      "  Batch [740/1299] D_loss: -2.6557, G_loss: 5.9618\n",
      "  Batch [750/1299] D_loss: -3.7895, G_loss: 6.3925\n",
      "  Batch [760/1299] D_loss: -2.7102, G_loss: 6.2057\n",
      "  Batch [770/1299] D_loss: -2.0905, G_loss: 6.0907\n",
      "  Batch [780/1299] D_loss: -3.8528, G_loss: 7.2151\n",
      "  Batch [790/1299] D_loss: -2.9008, G_loss: 7.4166\n",
      "  Batch [800/1299] D_loss: -3.7638, G_loss: 7.0863\n",
      "  Batch [810/1299] D_loss: -4.2365, G_loss: 7.8829\n",
      "  Batch [820/1299] D_loss: -3.7948, G_loss: 6.3554\n",
      "  Batch [830/1299] D_loss: -4.1645, G_loss: 9.7775\n",
      "  Batch [840/1299] D_loss: -4.6419, G_loss: 7.4845\n",
      "  Batch [850/1299] D_loss: -4.2868, G_loss: 9.6215\n",
      "  Batch [860/1299] D_loss: -3.2197, G_loss: 6.7116\n",
      "  Batch [870/1299] D_loss: -2.4536, G_loss: 7.7988\n",
      "  Batch [880/1299] D_loss: -3.4802, G_loss: 9.1085\n",
      "  Batch [890/1299] D_loss: -4.3398, G_loss: 6.8389\n",
      "  Batch [900/1299] D_loss: -4.1927, G_loss: 7.9681\n",
      "  Batch [910/1299] D_loss: -3.0818, G_loss: 6.5505\n",
      "  Batch [920/1299] D_loss: -3.4190, G_loss: 7.7502\n",
      "  Batch [930/1299] D_loss: -3.8935, G_loss: 8.1860\n",
      "  Batch [940/1299] D_loss: -4.8344, G_loss: 5.1953\n",
      "  Batch [950/1299] D_loss: -4.7009, G_loss: 7.6758\n",
      "  Batch [960/1299] D_loss: -2.4724, G_loss: 6.6186\n",
      "  Batch [970/1299] D_loss: -3.4002, G_loss: 6.7565\n",
      "  Batch [980/1299] D_loss: -2.9790, G_loss: 7.7027\n",
      "  Batch [990/1299] D_loss: -4.6125, G_loss: 5.8699\n",
      "  Batch [1000/1299] D_loss: -3.8426, G_loss: 7.1018\n",
      "  Batch [1010/1299] D_loss: -4.3252, G_loss: 5.4876\n",
      "  Batch [1020/1299] D_loss: -2.5758, G_loss: 7.0368\n",
      "  Batch [1030/1299] D_loss: -4.8406, G_loss: 7.5825\n",
      "  Batch [1040/1299] D_loss: -2.9410, G_loss: 9.2751\n",
      "  Batch [1050/1299] D_loss: -4.2390, G_loss: 10.0412\n",
      "  Batch [1060/1299] D_loss: -3.6545, G_loss: 7.6834\n",
      "  Batch [1070/1299] D_loss: -3.9509, G_loss: 7.5462\n",
      "  Batch [1080/1299] D_loss: -2.8377, G_loss: 5.9789\n",
      "  Batch [1090/1299] D_loss: -4.1317, G_loss: 8.2200\n",
      "  Batch [1100/1299] D_loss: -4.2886, G_loss: 7.9780\n",
      "  Batch [1110/1299] D_loss: -3.5065, G_loss: 5.4673\n",
      "  Batch [1120/1299] D_loss: -4.8493, G_loss: 8.2565\n",
      "  Batch [1130/1299] D_loss: -4.0607, G_loss: 8.6770\n",
      "  Batch [1140/1299] D_loss: -4.2389, G_loss: 5.7028\n",
      "  Batch [1150/1299] D_loss: -2.8399, G_loss: 7.2032\n",
      "  Batch [1160/1299] D_loss: -3.0521, G_loss: 7.8609\n",
      "  Batch [1170/1299] D_loss: -3.4839, G_loss: 7.1273\n",
      "  Batch [1180/1299] D_loss: -3.7097, G_loss: 6.7743\n",
      "  Batch [1190/1299] D_loss: -3.5901, G_loss: 7.9389\n",
      "  Batch [1200/1299] D_loss: -2.6516, G_loss: 5.9329\n",
      "  Batch [1210/1299] D_loss: -4.8392, G_loss: 7.7996\n",
      "  Batch [1220/1299] D_loss: -2.7499, G_loss: 6.6745\n",
      "  Batch [1230/1299] D_loss: -2.2017, G_loss: 7.1103\n",
      "  Batch [1240/1299] D_loss: -2.5466, G_loss: 7.5638\n",
      "  Batch [1250/1299] D_loss: -3.0958, G_loss: 8.1829\n",
      "  Batch [1260/1299] D_loss: -5.1783, G_loss: 9.4909\n",
      "  Batch [1270/1299] D_loss: -3.2039, G_loss: 8.1270\n",
      "  Batch [1280/1299] D_loss: -3.4635, G_loss: 6.5058\n",
      "  Batch [1290/1299] D_loss: -2.5960, G_loss: 7.8042\n",
      "\n",
      "Epoch 7 Summary:\n",
      "  Average D_loss: -3.2373\n",
      "  Average G_loss: 7.5792\n",
      "\n",
      "Epoch [8/100]\n",
      "  Batch [0/1299] D_loss: -3.5868, G_loss: 5.4283\n",
      "  Batch [10/1299] D_loss: -4.8117, G_loss: 7.9905\n",
      "  Batch [20/1299] D_loss: -4.0329, G_loss: 6.4619\n",
      "  Batch [30/1299] D_loss: -4.3481, G_loss: 8.6621\n",
      "  Batch [40/1299] D_loss: -2.7090, G_loss: 6.3007\n",
      "  Batch [50/1299] D_loss: -2.9673, G_loss: 7.5862\n",
      "  Batch [60/1299] D_loss: -4.1915, G_loss: 6.1531\n",
      "  Batch [70/1299] D_loss: -2.3743, G_loss: 8.7154\n",
      "  Batch [80/1299] D_loss: -4.0546, G_loss: 9.6330\n",
      "  Batch [90/1299] D_loss: -3.3595, G_loss: 7.6619\n",
      "  Batch [100/1299] D_loss: -3.3700, G_loss: 7.0516\n",
      "  Batch [110/1299] D_loss: -3.4831, G_loss: 6.5675\n",
      "  Batch [120/1299] D_loss: -3.4085, G_loss: 9.0467\n",
      "  Batch [130/1299] D_loss: -3.2283, G_loss: 5.3300\n",
      "  Batch [140/1299] D_loss: -5.7237, G_loss: 6.3693\n",
      "  Batch [150/1299] D_loss: -3.6324, G_loss: 6.9093\n",
      "  Batch [160/1299] D_loss: -2.5684, G_loss: 4.8538\n",
      "  Batch [170/1299] D_loss: -4.3886, G_loss: 6.1323\n",
      "  Batch [180/1299] D_loss: -3.2621, G_loss: 9.7680\n",
      "  Batch [190/1299] D_loss: -4.4783, G_loss: 9.7805\n",
      "  Batch [200/1299] D_loss: -2.7769, G_loss: 5.6688\n",
      "  Batch [210/1299] D_loss: -3.9189, G_loss: 6.1292\n",
      "  Batch [220/1299] D_loss: -3.8953, G_loss: 7.3371\n",
      "  Batch [230/1299] D_loss: -3.0590, G_loss: 5.4550\n",
      "  Batch [240/1299] D_loss: -1.3224, G_loss: 8.3517\n",
      "  Batch [250/1299] D_loss: -3.8322, G_loss: 6.2080\n",
      "  Batch [260/1299] D_loss: -2.9192, G_loss: 5.8115\n",
      "  Batch [270/1299] D_loss: -4.2268, G_loss: 6.4813\n",
      "  Batch [280/1299] D_loss: -1.9770, G_loss: 8.9961\n",
      "  Batch [290/1299] D_loss: -3.6752, G_loss: 7.3673\n",
      "  Batch [300/1299] D_loss: -2.2511, G_loss: 7.0939\n",
      "  Batch [310/1299] D_loss: -4.1214, G_loss: 8.1753\n",
      "  Batch [320/1299] D_loss: -2.7353, G_loss: 4.9028\n",
      "  Batch [330/1299] D_loss: -4.2381, G_loss: 9.2616\n",
      "  Batch [340/1299] D_loss: -4.8776, G_loss: 6.8681\n",
      "  Batch [350/1299] D_loss: -3.0170, G_loss: 5.9614\n",
      "  Batch [360/1299] D_loss: -4.4019, G_loss: 8.3525\n",
      "  Batch [370/1299] D_loss: -2.8136, G_loss: 7.2570\n",
      "  Batch [380/1299] D_loss: -2.4638, G_loss: 8.8615\n",
      "  Batch [390/1299] D_loss: -3.2896, G_loss: 7.9598\n",
      "  Batch [400/1299] D_loss: -5.2327, G_loss: 6.7212\n",
      "  Batch [410/1299] D_loss: -4.8595, G_loss: 6.7336\n",
      "  Batch [420/1299] D_loss: -3.3706, G_loss: 6.6039\n",
      "  Batch [430/1299] D_loss: -3.8436, G_loss: 8.3445\n",
      "  Batch [440/1299] D_loss: -4.4978, G_loss: 7.4945\n",
      "  Batch [450/1299] D_loss: -3.7730, G_loss: 5.8625\n",
      "  Batch [460/1299] D_loss: -3.8870, G_loss: 7.9345\n",
      "  Batch [470/1299] D_loss: -4.9648, G_loss: 6.6884\n",
      "  Batch [480/1299] D_loss: -3.3643, G_loss: 6.9636\n",
      "  Batch [490/1299] D_loss: -3.9059, G_loss: 6.5086\n",
      "  Batch [500/1299] D_loss: -3.8956, G_loss: 6.7735\n",
      "  Batch [510/1299] D_loss: -4.2924, G_loss: 8.5020\n",
      "  Batch [520/1299] D_loss: -4.2037, G_loss: 7.3204\n",
      "  Batch [530/1299] D_loss: -4.3123, G_loss: 7.6109\n",
      "  Batch [540/1299] D_loss: -2.5786, G_loss: 5.6539\n",
      "  Batch [550/1299] D_loss: -2.2511, G_loss: 5.7409\n",
      "  Batch [560/1299] D_loss: -3.0681, G_loss: 7.3135\n",
      "  Batch [570/1299] D_loss: -2.9035, G_loss: 3.9489\n",
      "  Batch [580/1299] D_loss: -3.2543, G_loss: 7.9159\n",
      "  Batch [590/1299] D_loss: -4.0856, G_loss: 7.1783\n",
      "  Batch [600/1299] D_loss: -4.8592, G_loss: 7.1898\n",
      "  Batch [610/1299] D_loss: -3.1974, G_loss: 7.2448\n",
      "  Batch [620/1299] D_loss: -3.2277, G_loss: 6.9471\n",
      "  Batch [630/1299] D_loss: -3.8780, G_loss: 8.8310\n",
      "  Batch [640/1299] D_loss: -3.4212, G_loss: 6.4487\n",
      "  Batch [650/1299] D_loss: -4.9812, G_loss: 6.2680\n",
      "  Batch [660/1299] D_loss: -3.7108, G_loss: 5.7242\n",
      "  Batch [670/1299] D_loss: -3.2588, G_loss: 5.2698\n",
      "  Batch [680/1299] D_loss: -3.6661, G_loss: 6.2708\n",
      "  Batch [690/1299] D_loss: -3.7870, G_loss: 7.4384\n",
      "  Batch [700/1299] D_loss: -3.3552, G_loss: 5.3305\n",
      "  Batch [710/1299] D_loss: -4.5349, G_loss: 6.3405\n",
      "  Batch [720/1299] D_loss: -3.6303, G_loss: 4.3339\n",
      "  Batch [730/1299] D_loss: -5.5002, G_loss: 6.7050\n",
      "  Batch [740/1299] D_loss: -3.8410, G_loss: 5.9055\n",
      "  Batch [750/1299] D_loss: -2.4479, G_loss: 5.1468\n",
      "  Batch [760/1299] D_loss: -4.1850, G_loss: 6.8844\n",
      "  Batch [770/1299] D_loss: -4.7753, G_loss: 6.1755\n",
      "  Batch [780/1299] D_loss: -3.5755, G_loss: 4.8311\n",
      "  Batch [790/1299] D_loss: -0.9022, G_loss: 5.4366\n",
      "  Batch [800/1299] D_loss: -2.9212, G_loss: 5.4429\n",
      "  Batch [810/1299] D_loss: -3.9808, G_loss: 9.3705\n",
      "  Batch [820/1299] D_loss: -3.0152, G_loss: 6.6116\n",
      "  Batch [830/1299] D_loss: -4.0051, G_loss: 7.6742\n",
      "  Batch [840/1299] D_loss: -4.0197, G_loss: 5.9272\n",
      "  Batch [850/1299] D_loss: -3.4176, G_loss: 7.9970\n",
      "  Batch [860/1299] D_loss: -2.0960, G_loss: 8.7086\n",
      "  Batch [870/1299] D_loss: -4.3085, G_loss: 7.5274\n",
      "  Batch [880/1299] D_loss: -3.4268, G_loss: 5.3061\n",
      "  Batch [890/1299] D_loss: -3.2253, G_loss: 5.9009\n",
      "  Batch [900/1299] D_loss: -3.1967, G_loss: 5.0150\n",
      "  Batch [910/1299] D_loss: -3.5933, G_loss: 5.9901\n",
      "  Batch [920/1299] D_loss: -3.6998, G_loss: 5.4406\n",
      "  Batch [930/1299] D_loss: -3.1020, G_loss: 6.7125\n",
      "  Batch [940/1299] D_loss: -3.5048, G_loss: 7.7217\n",
      "  Batch [950/1299] D_loss: -3.6689, G_loss: 6.9697\n",
      "  Batch [960/1299] D_loss: -3.4742, G_loss: 6.4528\n",
      "  Batch [970/1299] D_loss: -5.7625, G_loss: 5.9189\n",
      "  Batch [980/1299] D_loss: -2.7084, G_loss: 7.3233\n",
      "  Batch [990/1299] D_loss: -3.0628, G_loss: 6.7453\n",
      "  Batch [1000/1299] D_loss: -2.0500, G_loss: 6.0132\n",
      "  Batch [1010/1299] D_loss: -3.8520, G_loss: 5.3302\n",
      "  Batch [1020/1299] D_loss: -2.7243, G_loss: 7.0195\n",
      "  Batch [1030/1299] D_loss: -3.5519, G_loss: 6.5783\n",
      "  Batch [1040/1299] D_loss: -3.4011, G_loss: 6.8580\n",
      "  Batch [1050/1299] D_loss: -3.9481, G_loss: 8.0456\n",
      "  Batch [1060/1299] D_loss: -5.3516, G_loss: 8.7431\n",
      "  Batch [1070/1299] D_loss: -3.5620, G_loss: 4.8410\n",
      "  Batch [1080/1299] D_loss: -3.5118, G_loss: 7.2671\n",
      "  Batch [1090/1299] D_loss: -2.6135, G_loss: 7.4462\n",
      "  Batch [1100/1299] D_loss: -2.0707, G_loss: 5.5219\n",
      "  Batch [1110/1299] D_loss: -4.0963, G_loss: 6.0657\n",
      "  Batch [1120/1299] D_loss: -4.8432, G_loss: 8.3532\n",
      "  Batch [1130/1299] D_loss: -2.5584, G_loss: 6.3084\n",
      "  Batch [1140/1299] D_loss: -3.6774, G_loss: 7.6215\n",
      "  Batch [1150/1299] D_loss: -3.6413, G_loss: 6.3769\n",
      "  Batch [1160/1299] D_loss: -3.6763, G_loss: 8.2953\n",
      "  Batch [1170/1299] D_loss: -3.3059, G_loss: 5.4836\n",
      "  Batch [1180/1299] D_loss: -2.7865, G_loss: 7.4650\n",
      "  Batch [1190/1299] D_loss: -2.6391, G_loss: 6.2032\n",
      "  Batch [1200/1299] D_loss: -2.5326, G_loss: 6.6077\n",
      "  Batch [1210/1299] D_loss: -3.6021, G_loss: 6.6496\n",
      "  Batch [1220/1299] D_loss: -3.0885, G_loss: 4.4376\n",
      "  Batch [1230/1299] D_loss: -3.4458, G_loss: 6.7457\n",
      "  Batch [1240/1299] D_loss: -2.6809, G_loss: 6.4424\n",
      "  Batch [1250/1299] D_loss: -3.2987, G_loss: 4.8129\n",
      "  Batch [1260/1299] D_loss: -2.0895, G_loss: 7.4107\n",
      "  Batch [1270/1299] D_loss: -3.7239, G_loss: 5.3294\n",
      "  Batch [1280/1299] D_loss: -4.0847, G_loss: 5.7567\n",
      "  Batch [1290/1299] D_loss: -4.6140, G_loss: 6.7796\n",
      "\n",
      "Epoch 8 Summary:\n",
      "  Average D_loss: -3.1321\n",
      "  Average G_loss: 6.8929\n",
      "\n",
      "Epoch [9/100]\n",
      "  Batch [0/1299] D_loss: -2.9738, G_loss: 5.7251\n",
      "  Batch [10/1299] D_loss: -1.7755, G_loss: 6.2795\n",
      "  Batch [20/1299] D_loss: -4.5047, G_loss: 5.4446\n",
      "  Batch [30/1299] D_loss: -4.2321, G_loss: 4.6555\n",
      "  Batch [40/1299] D_loss: -3.3997, G_loss: 6.2758\n",
      "  Batch [50/1299] D_loss: -2.4568, G_loss: 4.6189\n",
      "  Batch [60/1299] D_loss: -4.3184, G_loss: 5.2096\n",
      "  Batch [70/1299] D_loss: -3.0456, G_loss: 5.8346\n",
      "  Batch [80/1299] D_loss: -2.9060, G_loss: 5.6125\n",
      "  Batch [90/1299] D_loss: -4.1083, G_loss: 5.3052\n",
      "  Batch [100/1299] D_loss: -3.6269, G_loss: 4.9370\n",
      "  Batch [110/1299] D_loss: -2.8494, G_loss: 6.5817\n",
      "  Batch [120/1299] D_loss: -3.6825, G_loss: 5.3128\n",
      "  Batch [130/1299] D_loss: -3.3051, G_loss: 4.2861\n",
      "  Batch [140/1299] D_loss: -4.4890, G_loss: 6.6755\n",
      "  Batch [150/1299] D_loss: -3.0091, G_loss: 6.1621\n",
      "  Batch [160/1299] D_loss: -2.5887, G_loss: 5.8265\n",
      "  Batch [170/1299] D_loss: -4.1447, G_loss: 7.3252\n",
      "  Batch [180/1299] D_loss: -2.7302, G_loss: 6.0102\n",
      "  Batch [190/1299] D_loss: -3.6934, G_loss: 4.8366\n",
      "  Batch [200/1299] D_loss: -2.3028, G_loss: 7.4265\n",
      "  Batch [210/1299] D_loss: -4.5756, G_loss: 4.6657\n",
      "  Batch [220/1299] D_loss: -3.5128, G_loss: 4.7775\n",
      "  Batch [230/1299] D_loss: -3.1977, G_loss: 5.5457\n",
      "  Batch [240/1299] D_loss: -3.3337, G_loss: 4.2677\n",
      "  Batch [250/1299] D_loss: -2.8930, G_loss: 5.6765\n",
      "  Batch [260/1299] D_loss: -3.9038, G_loss: 8.5607\n",
      "  Batch [270/1299] D_loss: -2.5176, G_loss: 7.0403\n",
      "  Batch [280/1299] D_loss: -4.9846, G_loss: 4.7315\n",
      "  Batch [290/1299] D_loss: -2.3214, G_loss: 6.3672\n",
      "  Batch [300/1299] D_loss: -3.2094, G_loss: 7.1330\n",
      "  Batch [310/1299] D_loss: -3.8052, G_loss: 6.4289\n",
      "  Batch [320/1299] D_loss: -4.8869, G_loss: 6.1864\n",
      "  Batch [330/1299] D_loss: -3.7429, G_loss: 5.0358\n",
      "  Batch [340/1299] D_loss: -3.0340, G_loss: 7.9029\n",
      "  Batch [350/1299] D_loss: -2.1975, G_loss: 5.7902\n",
      "  Batch [360/1299] D_loss: -3.2043, G_loss: 6.7860\n",
      "  Batch [370/1299] D_loss: -3.5573, G_loss: 6.9726\n",
      "  Batch [380/1299] D_loss: -2.3159, G_loss: 6.3986\n",
      "  Batch [390/1299] D_loss: -3.7254, G_loss: 6.9251\n",
      "  Batch [400/1299] D_loss: -4.2653, G_loss: 7.1901\n",
      "  Batch [410/1299] D_loss: -4.2256, G_loss: 5.9409\n",
      "  Batch [420/1299] D_loss: -4.1129, G_loss: 4.2051\n",
      "  Batch [430/1299] D_loss: -3.7134, G_loss: 5.1205\n",
      "  Batch [440/1299] D_loss: -4.1761, G_loss: 5.2041\n",
      "  Batch [450/1299] D_loss: -2.1556, G_loss: 7.5288\n",
      "  Batch [460/1299] D_loss: -3.1585, G_loss: 8.2913\n",
      "  Batch [470/1299] D_loss: -3.9964, G_loss: 7.2239\n",
      "  Batch [480/1299] D_loss: -2.6171, G_loss: 7.6561\n",
      "  Batch [490/1299] D_loss: -5.2903, G_loss: 6.4935\n",
      "  Batch [500/1299] D_loss: -3.2647, G_loss: 4.9991\n",
      "  Batch [510/1299] D_loss: -3.6984, G_loss: 8.7783\n",
      "  Batch [520/1299] D_loss: -4.7673, G_loss: 6.9113\n",
      "  Batch [530/1299] D_loss: -2.7307, G_loss: 5.2440\n",
      "  Batch [540/1299] D_loss: -3.5517, G_loss: 5.6196\n",
      "  Batch [550/1299] D_loss: -3.2126, G_loss: 6.7588\n",
      "  Batch [560/1299] D_loss: -3.6222, G_loss: 7.3745\n",
      "  Batch [570/1299] D_loss: -4.0228, G_loss: 5.4080\n",
      "  Batch [580/1299] D_loss: -2.3162, G_loss: 5.8469\n",
      "  Batch [590/1299] D_loss: -2.7389, G_loss: 4.7867\n",
      "  Batch [600/1299] D_loss: -3.0264, G_loss: 5.4488\n",
      "  Batch [610/1299] D_loss: -2.8129, G_loss: 7.6889\n",
      "  Batch [620/1299] D_loss: -3.8923, G_loss: 5.8817\n",
      "  Batch [630/1299] D_loss: -2.1278, G_loss: 4.2940\n",
      "  Batch [640/1299] D_loss: -2.3507, G_loss: 5.9722\n",
      "  Batch [650/1299] D_loss: -3.4615, G_loss: 6.8334\n",
      "  Batch [660/1299] D_loss: -4.3015, G_loss: 6.3249\n",
      "  Batch [670/1299] D_loss: -4.5360, G_loss: 5.1330\n",
      "  Batch [680/1299] D_loss: -2.8898, G_loss: 4.5003\n",
      "  Batch [690/1299] D_loss: -2.9308, G_loss: 5.9874\n",
      "  Batch [700/1299] D_loss: -2.0955, G_loss: 3.8582\n",
      "  Batch [710/1299] D_loss: -2.1452, G_loss: 6.3846\n",
      "  Batch [720/1299] D_loss: -4.4701, G_loss: 4.9592\n",
      "  Batch [730/1299] D_loss: -4.2057, G_loss: 6.1321\n",
      "  Batch [740/1299] D_loss: -2.7532, G_loss: 7.4406\n",
      "  Batch [750/1299] D_loss: -3.7444, G_loss: 5.2327\n",
      "  Batch [760/1299] D_loss: -3.9351, G_loss: 5.2839\n",
      "  Batch [770/1299] D_loss: -3.2450, G_loss: 3.7901\n",
      "  Batch [780/1299] D_loss: -3.7326, G_loss: 2.9932\n",
      "  Batch [790/1299] D_loss: -4.5622, G_loss: 4.6939\n",
      "  Batch [800/1299] D_loss: -3.3004, G_loss: 3.9593\n",
      "  Batch [810/1299] D_loss: -2.2517, G_loss: 2.8745\n",
      "  Batch [820/1299] D_loss: -3.5514, G_loss: 4.0174\n",
      "  Batch [830/1299] D_loss: -1.4783, G_loss: 4.4940\n",
      "  Batch [840/1299] D_loss: -2.4777, G_loss: 4.5199\n",
      "  Batch [850/1299] D_loss: -3.4399, G_loss: 5.2160\n",
      "  Batch [860/1299] D_loss: -2.9843, G_loss: 4.4276\n",
      "  Batch [870/1299] D_loss: -2.5097, G_loss: 5.6423\n",
      "  Batch [880/1299] D_loss: -3.6798, G_loss: 5.6277\n",
      "  Batch [890/1299] D_loss: -4.8142, G_loss: 6.7479\n",
      "  Batch [900/1299] D_loss: -3.1362, G_loss: 5.5946\n",
      "  Batch [910/1299] D_loss: -4.3087, G_loss: 6.4579\n",
      "  Batch [920/1299] D_loss: -3.5624, G_loss: 4.0497\n",
      "  Batch [930/1299] D_loss: -3.7405, G_loss: 4.0833\n",
      "  Batch [940/1299] D_loss: -4.8518, G_loss: 5.0980\n",
      "  Batch [950/1299] D_loss: -3.2310, G_loss: 7.1659\n",
      "  Batch [960/1299] D_loss: -2.2250, G_loss: 6.4273\n",
      "  Batch [970/1299] D_loss: -3.0557, G_loss: 4.4722\n",
      "  Batch [980/1299] D_loss: -2.8287, G_loss: 5.3420\n",
      "  Batch [990/1299] D_loss: -3.0742, G_loss: 5.9977\n",
      "  Batch [1000/1299] D_loss: -3.5290, G_loss: 4.4105\n",
      "  Batch [1010/1299] D_loss: -4.0117, G_loss: 2.8849\n",
      "  Batch [1020/1299] D_loss: -3.6072, G_loss: 6.8865\n",
      "  Batch [1030/1299] D_loss: -2.4229, G_loss: 4.9680\n",
      "  Batch [1040/1299] D_loss: -4.2011, G_loss: 5.9527\n",
      "  Batch [1050/1299] D_loss: -2.3924, G_loss: 7.1815\n",
      "  Batch [1060/1299] D_loss: -4.6080, G_loss: 7.9028\n",
      "  Batch [1070/1299] D_loss: -2.8168, G_loss: 5.1433\n",
      "  Batch [1080/1299] D_loss: -2.7627, G_loss: 4.7542\n",
      "  Batch [1090/1299] D_loss: -2.5097, G_loss: 5.0695\n",
      "  Batch [1100/1299] D_loss: -3.7132, G_loss: 7.4656\n",
      "  Batch [1110/1299] D_loss: -3.1136, G_loss: 4.8341\n",
      "  Batch [1120/1299] D_loss: -4.0980, G_loss: 8.4524\n",
      "  Batch [1130/1299] D_loss: -3.2973, G_loss: 3.7564\n",
      "  Batch [1140/1299] D_loss: -4.2937, G_loss: 5.1371\n",
      "  Batch [1150/1299] D_loss: -4.1767, G_loss: 7.7318\n",
      "  Batch [1160/1299] D_loss: -1.0654, G_loss: 4.4484\n",
      "  Batch [1170/1299] D_loss: -5.9540, G_loss: 5.9650\n",
      "  Batch [1180/1299] D_loss: -3.3920, G_loss: 3.3323\n",
      "  Batch [1190/1299] D_loss: -1.1485, G_loss: 5.4530\n",
      "  Batch [1200/1299] D_loss: -4.3163, G_loss: 6.2134\n",
      "  Batch [1210/1299] D_loss: -3.0035, G_loss: 3.8688\n",
      "  Batch [1220/1299] D_loss: -2.8929, G_loss: 6.6788\n",
      "  Batch [1230/1299] D_loss: -1.3190, G_loss: 5.1313\n",
      "  Batch [1240/1299] D_loss: -2.8096, G_loss: 5.0118\n",
      "  Batch [1250/1299] D_loss: -3.7231, G_loss: 3.7818\n",
      "  Batch [1260/1299] D_loss: -3.2318, G_loss: 4.2728\n",
      "  Batch [1270/1299] D_loss: -2.3294, G_loss: 7.0592\n",
      "  Batch [1280/1299] D_loss: -2.6685, G_loss: 6.8584\n",
      "  Batch [1290/1299] D_loss: -3.9769, G_loss: 5.7911\n",
      "\n",
      "Epoch 9 Summary:\n",
      "  Average D_loss: -3.0120\n",
      "  Average G_loss: 5.7003\n",
      "\n",
      "Epoch [10/100]\n",
      "  Batch [0/1299] D_loss: -4.1889, G_loss: 3.8829\n",
      "  Batch [10/1299] D_loss: -2.3037, G_loss: 4.0415\n",
      "  Batch [20/1299] D_loss: -3.0253, G_loss: 4.6987\n",
      "  Batch [30/1299] D_loss: -4.7464, G_loss: 5.7311\n",
      "  Batch [40/1299] D_loss: -4.0996, G_loss: 5.2407\n",
      "  Batch [50/1299] D_loss: -4.4429, G_loss: 5.6675\n",
      "  Batch [60/1299] D_loss: -4.3179, G_loss: 6.2724\n",
      "  Batch [70/1299] D_loss: -3.2092, G_loss: 3.9108\n",
      "  Batch [80/1299] D_loss: -1.8270, G_loss: 3.1593\n",
      "  Batch [90/1299] D_loss: -3.7397, G_loss: 2.8552\n",
      "  Batch [100/1299] D_loss: -4.5790, G_loss: 3.5686\n",
      "  Batch [110/1299] D_loss: -2.8592, G_loss: 5.7820\n",
      "  Batch [120/1299] D_loss: -4.7570, G_loss: 5.4642\n",
      "  Batch [130/1299] D_loss: -3.0071, G_loss: 3.8657\n",
      "  Batch [140/1299] D_loss: -3.3477, G_loss: 5.5877\n",
      "  Batch [150/1299] D_loss: -2.5257, G_loss: 4.6524\n",
      "  Batch [160/1299] D_loss: -3.7225, G_loss: 4.9917\n",
      "  Batch [170/1299] D_loss: -2.3073, G_loss: 6.1675\n",
      "  Batch [180/1299] D_loss: -4.0523, G_loss: 7.7776\n",
      "  Batch [190/1299] D_loss: -3.6184, G_loss: 6.1691\n",
      "  Batch [200/1299] D_loss: -3.2990, G_loss: 6.1736\n",
      "  Batch [210/1299] D_loss: -4.0959, G_loss: 5.1681\n",
      "  Batch [220/1299] D_loss: -3.0202, G_loss: 5.4865\n",
      "  Batch [230/1299] D_loss: -3.5442, G_loss: 4.7568\n",
      "  Batch [240/1299] D_loss: -3.3210, G_loss: 6.7567\n",
      "  Batch [250/1299] D_loss: -3.3927, G_loss: 6.1524\n",
      "  Batch [260/1299] D_loss: -2.8964, G_loss: 5.4867\n",
      "  Batch [270/1299] D_loss: -4.5630, G_loss: 7.0834\n",
      "  Batch [280/1299] D_loss: -2.8497, G_loss: 6.8754\n",
      "  Batch [290/1299] D_loss: -2.3767, G_loss: 5.0594\n",
      "  Batch [300/1299] D_loss: -2.7569, G_loss: 6.0584\n",
      "  Batch [310/1299] D_loss: -3.9811, G_loss: 7.0144\n",
      "  Batch [320/1299] D_loss: -4.4405, G_loss: 5.0455\n",
      "  Batch [330/1299] D_loss: -3.5755, G_loss: 5.6985\n",
      "  Batch [340/1299] D_loss: -3.8587, G_loss: 6.0922\n",
      "  Batch [350/1299] D_loss: -2.8567, G_loss: 5.7879\n",
      "  Batch [360/1299] D_loss: -3.6228, G_loss: 4.9116\n",
      "  Batch [370/1299] D_loss: -4.5539, G_loss: 4.3984\n",
      "  Batch [380/1299] D_loss: -1.5405, G_loss: 4.1003\n",
      "  Batch [390/1299] D_loss: -4.2952, G_loss: 6.7520\n",
      "  Batch [400/1299] D_loss: -4.0238, G_loss: 6.4182\n",
      "  Batch [410/1299] D_loss: -2.7485, G_loss: 6.6660\n",
      "  Batch [420/1299] D_loss: -3.5488, G_loss: 5.0831\n",
      "  Batch [430/1299] D_loss: -1.0834, G_loss: 5.7917\n",
      "  Batch [440/1299] D_loss: -3.6990, G_loss: 7.1501\n",
      "  Batch [450/1299] D_loss: -4.3775, G_loss: 6.9004\n",
      "  Batch [460/1299] D_loss: -4.2230, G_loss: 5.5187\n",
      "  Batch [470/1299] D_loss: -3.1619, G_loss: 6.9356\n",
      "  Batch [480/1299] D_loss: -3.9781, G_loss: 5.8502\n",
      "  Batch [490/1299] D_loss: -2.7725, G_loss: 4.5065\n",
      "  Batch [500/1299] D_loss: -4.3441, G_loss: 5.1604\n",
      "  Batch [510/1299] D_loss: -3.6999, G_loss: 5.5108\n",
      "  Batch [520/1299] D_loss: -2.2177, G_loss: 5.1044\n",
      "  Batch [530/1299] D_loss: -3.0220, G_loss: 6.3982\n",
      "  Batch [540/1299] D_loss: -3.0292, G_loss: 6.5438\n",
      "  Batch [550/1299] D_loss: -0.2961, G_loss: 6.8175\n",
      "  Batch [560/1299] D_loss: -2.9579, G_loss: 3.4628\n",
      "  Batch [570/1299] D_loss: -3.9721, G_loss: 3.4989\n",
      "  Batch [580/1299] D_loss: -2.9621, G_loss: 3.7481\n",
      "  Batch [590/1299] D_loss: -4.9162, G_loss: 6.9303\n",
      "  Batch [600/1299] D_loss: -3.0561, G_loss: 7.3818\n",
      "  Batch [610/1299] D_loss: -3.0863, G_loss: 6.2637\n",
      "  Batch [620/1299] D_loss: -2.9944, G_loss: 6.9980\n",
      "  Batch [630/1299] D_loss: -3.6118, G_loss: 5.8739\n",
      "  Batch [640/1299] D_loss: -3.1848, G_loss: 6.5610\n",
      "  Batch [650/1299] D_loss: -4.2813, G_loss: 4.9707\n",
      "  Batch [660/1299] D_loss: -3.1951, G_loss: 4.2327\n",
      "  Batch [670/1299] D_loss: -4.6242, G_loss: 4.4227\n",
      "  Batch [680/1299] D_loss: -2.9086, G_loss: 6.1431\n",
      "  Batch [690/1299] D_loss: -2.6979, G_loss: 7.0937\n",
      "  Batch [700/1299] D_loss: -3.6147, G_loss: 4.9887\n",
      "  Batch [710/1299] D_loss: -2.8872, G_loss: 2.8565\n",
      "  Batch [720/1299] D_loss: -4.0470, G_loss: 3.6014\n",
      "  Batch [730/1299] D_loss: -2.7111, G_loss: 5.2310\n",
      "  Batch [740/1299] D_loss: -2.9487, G_loss: 7.2819\n",
      "  Batch [750/1299] D_loss: -3.3062, G_loss: 4.9264\n",
      "  Batch [760/1299] D_loss: -4.3433, G_loss: 6.5002\n",
      "  Batch [770/1299] D_loss: -2.8016, G_loss: 4.2425\n",
      "  Batch [780/1299] D_loss: -2.4230, G_loss: 4.3841\n",
      "  Batch [790/1299] D_loss: -3.4508, G_loss: 6.7396\n",
      "  Batch [800/1299] D_loss: -3.8156, G_loss: 7.2227\n",
      "  Batch [810/1299] D_loss: -2.7326, G_loss: 5.2504\n",
      "  Batch [820/1299] D_loss: -3.7654, G_loss: 7.2416\n",
      "  Batch [830/1299] D_loss: -3.3855, G_loss: 5.3058\n",
      "  Batch [840/1299] D_loss: -3.3111, G_loss: 5.5347\n",
      "  Batch [850/1299] D_loss: -3.2486, G_loss: 5.7949\n",
      "  Batch [860/1299] D_loss: -2.5868, G_loss: 4.2713\n",
      "  Batch [870/1299] D_loss: -4.3436, G_loss: 4.8033\n",
      "  Batch [880/1299] D_loss: -1.6026, G_loss: 6.5471\n",
      "  Batch [890/1299] D_loss: -2.4326, G_loss: 5.8645\n",
      "  Batch [900/1299] D_loss: -1.7710, G_loss: 4.8215\n",
      "  Batch [910/1299] D_loss: -3.5267, G_loss: 6.8079\n",
      "  Batch [920/1299] D_loss: -2.5121, G_loss: 5.3562\n",
      "  Batch [930/1299] D_loss: -3.4125, G_loss: 4.9589\n",
      "  Batch [940/1299] D_loss: -4.0885, G_loss: 4.5746\n",
      "  Batch [950/1299] D_loss: -3.4660, G_loss: 7.8236\n",
      "  Batch [960/1299] D_loss: -1.9055, G_loss: 4.5867\n",
      "  Batch [970/1299] D_loss: -5.0978, G_loss: 7.1130\n",
      "  Batch [980/1299] D_loss: -4.1672, G_loss: 6.3088\n",
      "  Batch [990/1299] D_loss: -2.6390, G_loss: 4.3080\n",
      "  Batch [1000/1299] D_loss: -2.2629, G_loss: 4.4876\n",
      "  Batch [1010/1299] D_loss: -3.2032, G_loss: 8.3313\n",
      "  Batch [1020/1299] D_loss: -3.3307, G_loss: 4.6760\n",
      "  Batch [1030/1299] D_loss: -3.4129, G_loss: 5.6343\n",
      "  Batch [1040/1299] D_loss: -3.2163, G_loss: 3.7028\n",
      "  Batch [1050/1299] D_loss: -3.7981, G_loss: 4.6248\n",
      "  Batch [1060/1299] D_loss: -3.2058, G_loss: 4.9510\n",
      "  Batch [1070/1299] D_loss: -3.9907, G_loss: 4.0410\n",
      "  Batch [1080/1299] D_loss: -2.7125, G_loss: 4.4302\n",
      "  Batch [1090/1299] D_loss: -4.6594, G_loss: 4.5122\n",
      "  Batch [1100/1299] D_loss: -4.3368, G_loss: 6.1928\n",
      "  Batch [1110/1299] D_loss: -3.5020, G_loss: 4.9512\n",
      "  Batch [1120/1299] D_loss: -3.8587, G_loss: 6.6127\n",
      "  Batch [1130/1299] D_loss: -1.5282, G_loss: 6.0039\n",
      "  Batch [1140/1299] D_loss: -3.4425, G_loss: 5.6577\n",
      "  Batch [1150/1299] D_loss: -3.3369, G_loss: 5.0269\n",
      "  Batch [1160/1299] D_loss: -3.1309, G_loss: 5.7519\n",
      "  Batch [1170/1299] D_loss: -4.3347, G_loss: 7.3805\n",
      "  Batch [1180/1299] D_loss: -2.5938, G_loss: 4.8723\n",
      "  Batch [1190/1299] D_loss: -3.3147, G_loss: 5.8598\n",
      "  Batch [1200/1299] D_loss: -3.8398, G_loss: 6.7780\n",
      "  Batch [1210/1299] D_loss: -2.8680, G_loss: 5.5338\n",
      "  Batch [1220/1299] D_loss: -3.1181, G_loss: 6.3861\n",
      "  Batch [1230/1299] D_loss: -3.6060, G_loss: 6.4014\n",
      "  Batch [1240/1299] D_loss: -4.3069, G_loss: 5.1814\n",
      "  Batch [1250/1299] D_loss: -3.1505, G_loss: 4.8566\n",
      "  Batch [1260/1299] D_loss: -1.6402, G_loss: 3.3398\n",
      "  Batch [1270/1299] D_loss: -3.7433, G_loss: 5.5971\n",
      "  Batch [1280/1299] D_loss: -4.9659, G_loss: 7.8170\n",
      "  Batch [1290/1299] D_loss: -3.7168, G_loss: 6.1534\n",
      "\n",
      "Epoch 10 Summary:\n",
      "  Average D_loss: -2.9392\n",
      "  Average G_loss: 5.5022\n",
      "\n",
      "Epoch [11/100]\n",
      "  Batch [0/1299] D_loss: -2.7878, G_loss: 4.5860\n",
      "  Batch [10/1299] D_loss: -2.4672, G_loss: 3.4225\n",
      "  Batch [20/1299] D_loss: -3.6916, G_loss: 3.8153\n",
      "  Batch [30/1299] D_loss: -2.1186, G_loss: 6.7924\n",
      "  Batch [40/1299] D_loss: -3.4841, G_loss: 4.3897\n",
      "  Batch [50/1299] D_loss: -1.8735, G_loss: 1.9284\n",
      "  Batch [60/1299] D_loss: -3.4246, G_loss: 4.3598\n",
      "  Batch [70/1299] D_loss: -2.9430, G_loss: 4.9180\n",
      "  Batch [80/1299] D_loss: -2.4423, G_loss: 6.6023\n",
      "  Batch [90/1299] D_loss: -2.8851, G_loss: 6.5960\n",
      "  Batch [100/1299] D_loss: -3.8494, G_loss: 5.4843\n",
      "  Batch [110/1299] D_loss: -4.4195, G_loss: 4.8354\n",
      "  Batch [120/1299] D_loss: -2.5753, G_loss: 3.5481\n",
      "  Batch [130/1299] D_loss: -3.2424, G_loss: 3.4470\n",
      "  Batch [140/1299] D_loss: -3.1214, G_loss: 4.2932\n",
      "  Batch [150/1299] D_loss: -2.0341, G_loss: 2.8084\n",
      "  Batch [160/1299] D_loss: -0.5179, G_loss: 5.0811\n",
      "  Batch [170/1299] D_loss: -4.2413, G_loss: 4.8004\n",
      "  Batch [180/1299] D_loss: -2.1168, G_loss: 4.9960\n",
      "  Batch [190/1299] D_loss: -1.7371, G_loss: 3.7283\n",
      "  Batch [200/1299] D_loss: -4.9769, G_loss: 1.7294\n",
      "  Batch [210/1299] D_loss: -3.6806, G_loss: 4.0825\n",
      "  Batch [220/1299] D_loss: -3.0545, G_loss: 4.2636\n",
      "  Batch [230/1299] D_loss: -3.6763, G_loss: 3.9959\n",
      "  Batch [240/1299] D_loss: -2.3817, G_loss: 5.6169\n",
      "  Batch [250/1299] D_loss: -5.0126, G_loss: 7.5439\n",
      "  Batch [260/1299] D_loss: -2.9907, G_loss: 4.9441\n",
      "  Batch [270/1299] D_loss: -2.7979, G_loss: 3.5896\n",
      "  Batch [280/1299] D_loss: -3.6471, G_loss: 3.4935\n",
      "  Batch [290/1299] D_loss: -3.0995, G_loss: 2.9444\n",
      "  Batch [300/1299] D_loss: -2.9800, G_loss: 3.5239\n",
      "  Batch [310/1299] D_loss: -4.4901, G_loss: 2.5534\n",
      "  Batch [320/1299] D_loss: -3.1283, G_loss: 4.0726\n",
      "  Batch [330/1299] D_loss: -2.9869, G_loss: 5.1362\n",
      "  Batch [340/1299] D_loss: -4.0226, G_loss: 5.2236\n",
      "  Batch [350/1299] D_loss: -3.3195, G_loss: 4.4802\n",
      "  Batch [360/1299] D_loss: -3.3086, G_loss: 4.2770\n",
      "  Batch [370/1299] D_loss: -4.2888, G_loss: 5.0861\n",
      "  Batch [380/1299] D_loss: -3.2570, G_loss: 6.2425\n",
      "  Batch [390/1299] D_loss: -3.2106, G_loss: 4.8241\n",
      "  Batch [400/1299] D_loss: -3.4146, G_loss: 4.6054\n",
      "  Batch [410/1299] D_loss: -3.0585, G_loss: 5.3043\n",
      "  Batch [420/1299] D_loss: -3.0328, G_loss: 3.1816\n",
      "  Batch [430/1299] D_loss: -3.3441, G_loss: 6.6519\n",
      "  Batch [440/1299] D_loss: -2.4015, G_loss: 4.1369\n",
      "  Batch [450/1299] D_loss: -2.6682, G_loss: 4.9671\n",
      "  Batch [460/1299] D_loss: -3.8647, G_loss: 5.4650\n",
      "  Batch [470/1299] D_loss: -4.3308, G_loss: 5.5266\n",
      "  Batch [480/1299] D_loss: -3.8375, G_loss: 6.5460\n",
      "  Batch [490/1299] D_loss: -3.7648, G_loss: 5.9936\n",
      "  Batch [500/1299] D_loss: -4.5006, G_loss: 3.2235\n",
      "  Batch [510/1299] D_loss: -2.0163, G_loss: 3.6812\n",
      "  Batch [520/1299] D_loss: -3.3369, G_loss: 3.4338\n",
      "  Batch [530/1299] D_loss: -4.3642, G_loss: 3.6770\n",
      "  Batch [540/1299] D_loss: -3.0682, G_loss: 3.9355\n",
      "  Batch [550/1299] D_loss: -3.2803, G_loss: 4.7019\n",
      "  Batch [560/1299] D_loss: -2.3795, G_loss: 3.8875\n",
      "  Batch [570/1299] D_loss: -3.8987, G_loss: 4.7749\n",
      "  Batch [580/1299] D_loss: -2.5369, G_loss: 4.7808\n",
      "  Batch [590/1299] D_loss: -3.6485, G_loss: 6.0948\n",
      "  Batch [600/1299] D_loss: -2.6841, G_loss: 5.1313\n",
      "  Batch [610/1299] D_loss: -2.9667, G_loss: 7.4595\n",
      "  Batch [620/1299] D_loss: -2.9657, G_loss: 4.6971\n",
      "  Batch [630/1299] D_loss: -2.7599, G_loss: 5.4625\n",
      "  Batch [640/1299] D_loss: -3.2759, G_loss: 5.6479\n",
      "  Batch [650/1299] D_loss: -2.9270, G_loss: 4.2692\n",
      "  Batch [660/1299] D_loss: -2.9013, G_loss: 3.7320\n",
      "  Batch [670/1299] D_loss: -4.6400, G_loss: 4.3216\n",
      "  Batch [680/1299] D_loss: -4.3447, G_loss: 4.3044\n",
      "  Batch [690/1299] D_loss: -2.4150, G_loss: 4.8659\n",
      "  Batch [700/1299] D_loss: -3.0905, G_loss: 5.3104\n",
      "  Batch [710/1299] D_loss: -3.0151, G_loss: 4.2961\n",
      "  Batch [720/1299] D_loss: -2.7836, G_loss: 5.8110\n",
      "  Batch [730/1299] D_loss: -1.7933, G_loss: 4.3959\n",
      "  Batch [740/1299] D_loss: -2.5676, G_loss: 6.7374\n",
      "  Batch [750/1299] D_loss: -2.6806, G_loss: 5.9900\n",
      "  Batch [760/1299] D_loss: -3.4053, G_loss: 5.5142\n",
      "  Batch [770/1299] D_loss: -4.3806, G_loss: 5.1591\n",
      "  Batch [780/1299] D_loss: -1.8335, G_loss: 2.9963\n",
      "  Batch [790/1299] D_loss: -4.7929, G_loss: 4.4264\n",
      "  Batch [800/1299] D_loss: -3.3236, G_loss: 5.7184\n",
      "  Batch [810/1299] D_loss: -1.6415, G_loss: 5.5234\n",
      "  Batch [820/1299] D_loss: -2.4035, G_loss: 8.1539\n",
      "  Batch [830/1299] D_loss: -4.4087, G_loss: 6.1623\n",
      "  Batch [840/1299] D_loss: -3.2949, G_loss: 5.0662\n",
      "  Batch [850/1299] D_loss: -2.5820, G_loss: 7.2426\n",
      "  Batch [860/1299] D_loss: -1.9714, G_loss: 5.5700\n",
      "  Batch [870/1299] D_loss: -2.0518, G_loss: 4.2883\n",
      "  Batch [880/1299] D_loss: -4.2710, G_loss: 4.1234\n",
      "  Batch [890/1299] D_loss: -1.4958, G_loss: 4.6777\n",
      "  Batch [900/1299] D_loss: -2.9055, G_loss: 6.5155\n",
      "  Batch [910/1299] D_loss: -4.8909, G_loss: 6.6411\n",
      "  Batch [920/1299] D_loss: -3.6313, G_loss: 4.6437\n",
      "  Batch [930/1299] D_loss: -3.6495, G_loss: 4.6648\n",
      "  Batch [940/1299] D_loss: -3.4067, G_loss: 2.7743\n",
      "  Batch [950/1299] D_loss: -2.4961, G_loss: 5.0945\n",
      "  Batch [960/1299] D_loss: -3.5775, G_loss: 5.7471\n",
      "  Batch [970/1299] D_loss: -3.3538, G_loss: 5.1787\n",
      "  Batch [980/1299] D_loss: -3.0584, G_loss: 7.2521\n",
      "  Batch [990/1299] D_loss: -3.2515, G_loss: 5.7902\n",
      "  Batch [1000/1299] D_loss: -5.4075, G_loss: 4.5571\n",
      "  Batch [1010/1299] D_loss: -3.6440, G_loss: 5.1246\n",
      "  Batch [1020/1299] D_loss: -3.5068, G_loss: 4.6418\n",
      "  Batch [1030/1299] D_loss: -3.6530, G_loss: 5.2887\n",
      "  Batch [1040/1299] D_loss: -3.2357, G_loss: 4.2392\n",
      "  Batch [1050/1299] D_loss: -3.3010, G_loss: 5.5041\n",
      "  Batch [1060/1299] D_loss: -3.1892, G_loss: 3.2394\n",
      "  Batch [1070/1299] D_loss: -3.6371, G_loss: 4.3152\n",
      "  Batch [1080/1299] D_loss: -3.7982, G_loss: 4.5304\n",
      "  Batch [1090/1299] D_loss: -4.0039, G_loss: 4.9559\n",
      "  Batch [1100/1299] D_loss: -4.1455, G_loss: 4.7652\n",
      "  Batch [1110/1299] D_loss: -2.9291, G_loss: 6.4743\n",
      "  Batch [1120/1299] D_loss: -3.3619, G_loss: 5.5083\n",
      "  Batch [1130/1299] D_loss: -3.9337, G_loss: 5.0308\n",
      "  Batch [1140/1299] D_loss: -4.0746, G_loss: 5.0000\n",
      "  Batch [1150/1299] D_loss: -3.3372, G_loss: 5.5453\n",
      "  Batch [1160/1299] D_loss: -1.8931, G_loss: 2.0446\n",
      "  Batch [1170/1299] D_loss: -2.7304, G_loss: 4.5030\n",
      "  Batch [1180/1299] D_loss: -3.9567, G_loss: 3.8990\n",
      "  Batch [1190/1299] D_loss: -3.8565, G_loss: 5.5443\n",
      "  Batch [1200/1299] D_loss: -4.6487, G_loss: 4.1665\n",
      "  Batch [1210/1299] D_loss: -3.1373, G_loss: 3.5922\n",
      "  Batch [1220/1299] D_loss: -2.8443, G_loss: 4.1196\n",
      "  Batch [1230/1299] D_loss: -4.3160, G_loss: 4.5249\n",
      "  Batch [1240/1299] D_loss: -2.6521, G_loss: 3.3384\n",
      "  Batch [1250/1299] D_loss: -2.6523, G_loss: 4.0269\n",
      "  Batch [1260/1299] D_loss: -1.4416, G_loss: 4.0695\n",
      "  Batch [1270/1299] D_loss: -3.7430, G_loss: 5.4123\n",
      "  Batch [1280/1299] D_loss: -3.7231, G_loss: 4.2807\n",
      "  Batch [1290/1299] D_loss: -2.2426, G_loss: 4.6359\n",
      "\n",
      "Epoch 11 Summary:\n",
      "  Average D_loss: -2.8658\n",
      "  Average G_loss: 4.8129\n",
      "\n",
      "Epoch [12/100]\n",
      "  Batch [0/1299] D_loss: -2.9094, G_loss: 4.0101\n",
      "  Batch [10/1299] D_loss: -2.5940, G_loss: 4.9856\n",
      "  Batch [20/1299] D_loss: -3.6919, G_loss: 6.2642\n",
      "  Batch [30/1299] D_loss: -2.4505, G_loss: 3.5398\n",
      "  Batch [40/1299] D_loss: -4.0694, G_loss: 4.7544\n",
      "  Batch [50/1299] D_loss: -2.6239, G_loss: 3.5574\n",
      "  Batch [60/1299] D_loss: -3.0549, G_loss: 2.9435\n",
      "  Batch [70/1299] D_loss: -2.5659, G_loss: 4.8694\n",
      "  Batch [80/1299] D_loss: -3.4443, G_loss: 3.4926\n",
      "  Batch [90/1299] D_loss: -3.1273, G_loss: 6.7972\n",
      "  Batch [100/1299] D_loss: -3.6313, G_loss: 5.9086\n",
      "  Batch [110/1299] D_loss: -2.7436, G_loss: 4.7652\n",
      "  Batch [120/1299] D_loss: -3.9665, G_loss: 3.6075\n",
      "  Batch [130/1299] D_loss: -3.6418, G_loss: 5.4307\n",
      "  Batch [140/1299] D_loss: -3.1055, G_loss: 4.9124\n",
      "  Batch [150/1299] D_loss: -3.2799, G_loss: 8.7740\n",
      "  Batch [160/1299] D_loss: -3.9212, G_loss: 2.7936\n",
      "  Batch [170/1299] D_loss: -1.3494, G_loss: 3.2577\n",
      "  Batch [180/1299] D_loss: -2.5604, G_loss: 2.6055\n",
      "  Batch [190/1299] D_loss: -3.1773, G_loss: 4.0422\n",
      "  Batch [200/1299] D_loss: -4.0997, G_loss: 5.7826\n",
      "  Batch [210/1299] D_loss: -2.3622, G_loss: 3.4387\n",
      "  Batch [220/1299] D_loss: -5.6833, G_loss: 4.9686\n",
      "  Batch [230/1299] D_loss: -3.1384, G_loss: 4.9501\n",
      "  Batch [240/1299] D_loss: -5.1516, G_loss: 7.8126\n",
      "  Batch [250/1299] D_loss: -4.1795, G_loss: 4.2734\n",
      "  Batch [260/1299] D_loss: -2.0064, G_loss: 2.0550\n",
      "  Batch [270/1299] D_loss: -4.2276, G_loss: 3.2692\n",
      "  Batch [280/1299] D_loss: -4.8214, G_loss: 5.3174\n",
      "  Batch [290/1299] D_loss: -2.8531, G_loss: 4.3851\n",
      "  Batch [300/1299] D_loss: -3.6260, G_loss: 6.5157\n",
      "  Batch [310/1299] D_loss: -2.6686, G_loss: 5.2570\n",
      "  Batch [320/1299] D_loss: -3.3906, G_loss: 5.1928\n",
      "  Batch [330/1299] D_loss: -3.8343, G_loss: 4.6701\n",
      "  Batch [340/1299] D_loss: -2.9448, G_loss: 4.0772\n",
      "  Batch [350/1299] D_loss: -4.2555, G_loss: 6.0497\n",
      "  Batch [360/1299] D_loss: -3.0400, G_loss: 5.2851\n",
      "  Batch [370/1299] D_loss: -2.2965, G_loss: 3.6062\n",
      "  Batch [380/1299] D_loss: -3.0748, G_loss: 4.0684\n",
      "  Batch [390/1299] D_loss: -2.1063, G_loss: 3.1108\n",
      "  Batch [400/1299] D_loss: -3.4199, G_loss: 4.6098\n",
      "  Batch [410/1299] D_loss: -2.8956, G_loss: 3.6397\n",
      "  Batch [420/1299] D_loss: -3.4452, G_loss: 5.0655\n",
      "  Batch [430/1299] D_loss: -3.1825, G_loss: 5.5835\n",
      "  Batch [440/1299] D_loss: -3.5990, G_loss: 3.3618\n",
      "  Batch [450/1299] D_loss: -3.9055, G_loss: 1.7619\n",
      "  Batch [460/1299] D_loss: -2.8946, G_loss: 4.5140\n",
      "  Batch [470/1299] D_loss: -4.0771, G_loss: 4.5995\n",
      "  Batch [480/1299] D_loss: -3.9263, G_loss: 4.0969\n",
      "  Batch [490/1299] D_loss: -3.4354, G_loss: 3.1583\n",
      "  Batch [500/1299] D_loss: -3.4349, G_loss: 4.4468\n",
      "  Batch [510/1299] D_loss: -2.3722, G_loss: 4.5715\n",
      "  Batch [520/1299] D_loss: -3.3843, G_loss: 3.5016\n",
      "  Batch [530/1299] D_loss: -4.2262, G_loss: 4.9308\n",
      "  Batch [540/1299] D_loss: -2.9331, G_loss: 4.6383\n",
      "  Batch [550/1299] D_loss: -3.2688, G_loss: 5.6497\n",
      "  Batch [560/1299] D_loss: -2.9727, G_loss: 2.8853\n",
      "  Batch [570/1299] D_loss: -2.4495, G_loss: 1.0509\n",
      "  Batch [580/1299] D_loss: -4.4798, G_loss: 4.1121\n",
      "  Batch [590/1299] D_loss: -3.0725, G_loss: 5.4923\n",
      "  Batch [600/1299] D_loss: -2.9502, G_loss: 4.4268\n",
      "  Batch [610/1299] D_loss: -3.5856, G_loss: 4.6905\n",
      "  Batch [620/1299] D_loss: -3.0012, G_loss: 3.4254\n",
      "  Batch [630/1299] D_loss: -2.4950, G_loss: 4.1049\n",
      "  Batch [640/1299] D_loss: -4.3222, G_loss: 5.0913\n",
      "  Batch [650/1299] D_loss: -3.2415, G_loss: 3.3891\n",
      "  Batch [660/1299] D_loss: -4.9505, G_loss: 6.3219\n",
      "  Batch [670/1299] D_loss: -3.9988, G_loss: 4.8698\n",
      "  Batch [680/1299] D_loss: -3.6821, G_loss: 5.4349\n",
      "  Batch [690/1299] D_loss: -2.5407, G_loss: 3.9543\n",
      "  Batch [700/1299] D_loss: -3.4039, G_loss: 3.0971\n",
      "  Batch [710/1299] D_loss: -3.8285, G_loss: 3.7312\n",
      "  Batch [720/1299] D_loss: -1.9851, G_loss: 6.4387\n",
      "  Batch [730/1299] D_loss: -3.8524, G_loss: 6.4058\n",
      "  Batch [740/1299] D_loss: -4.2920, G_loss: 3.9977\n",
      "  Batch [750/1299] D_loss: -2.9155, G_loss: 5.0568\n",
      "  Batch [760/1299] D_loss: -2.5969, G_loss: 6.7610\n",
      "  Batch [770/1299] D_loss: -1.8119, G_loss: 5.4088\n",
      "  Batch [780/1299] D_loss: -2.3745, G_loss: 2.8255\n",
      "  Batch [790/1299] D_loss: -3.8199, G_loss: 5.9678\n",
      "  Batch [800/1299] D_loss: -3.6297, G_loss: 5.1152\n",
      "  Batch [810/1299] D_loss: -2.1218, G_loss: 3.6193\n",
      "  Batch [820/1299] D_loss: -2.2066, G_loss: 3.9245\n",
      "  Batch [830/1299] D_loss: -2.0258, G_loss: 5.4891\n",
      "  Batch [840/1299] D_loss: -3.9997, G_loss: 3.6265\n",
      "  Batch [850/1299] D_loss: -3.1361, G_loss: 5.8657\n",
      "  Batch [860/1299] D_loss: -1.5828, G_loss: 3.9370\n",
      "  Batch [870/1299] D_loss: -3.6206, G_loss: 5.2334\n",
      "  Batch [880/1299] D_loss: -2.9451, G_loss: 4.4751\n",
      "  Batch [890/1299] D_loss: -2.7861, G_loss: 2.4143\n",
      "  Batch [900/1299] D_loss: -1.9840, G_loss: 3.7719\n",
      "  Batch [910/1299] D_loss: -3.4234, G_loss: 2.5471\n",
      "  Batch [920/1299] D_loss: -3.1498, G_loss: 1.8778\n",
      "  Batch [930/1299] D_loss: -4.7284, G_loss: 3.4634\n",
      "  Batch [940/1299] D_loss: -2.0657, G_loss: 4.6545\n",
      "  Batch [950/1299] D_loss: -2.9927, G_loss: 5.0819\n",
      "  Batch [960/1299] D_loss: -3.5068, G_loss: 2.7799\n",
      "  Batch [970/1299] D_loss: -2.2628, G_loss: 5.6723\n",
      "  Batch [980/1299] D_loss: -3.0612, G_loss: 2.5896\n",
      "  Batch [990/1299] D_loss: -3.8893, G_loss: 2.6737\n",
      "  Batch [1000/1299] D_loss: -4.4592, G_loss: 4.2733\n",
      "  Batch [1010/1299] D_loss: -3.8477, G_loss: 5.4905\n",
      "  Batch [1020/1299] D_loss: -3.2144, G_loss: 4.2741\n",
      "  Batch [1030/1299] D_loss: -1.7689, G_loss: 5.5970\n",
      "  Batch [1040/1299] D_loss: -3.5188, G_loss: 3.8519\n",
      "  Batch [1050/1299] D_loss: -2.6267, G_loss: 6.5949\n",
      "  Batch [1060/1299] D_loss: -2.7095, G_loss: 5.3782\n",
      "  Batch [1070/1299] D_loss: -4.1503, G_loss: 1.4544\n",
      "  Batch [1080/1299] D_loss: -4.2155, G_loss: 6.3491\n",
      "  Batch [1090/1299] D_loss: -3.3018, G_loss: 7.7328\n",
      "  Batch [1100/1299] D_loss: -3.3620, G_loss: 4.5447\n",
      "  Batch [1110/1299] D_loss: -3.1481, G_loss: 3.6986\n",
      "  Batch [1120/1299] D_loss: -4.1942, G_loss: 4.3160\n",
      "  Batch [1130/1299] D_loss: -3.1355, G_loss: 4.8052\n",
      "  Batch [1140/1299] D_loss: -2.7302, G_loss: 6.6009\n",
      "  Batch [1150/1299] D_loss: -2.0284, G_loss: 3.9932\n",
      "  Batch [1160/1299] D_loss: -4.7365, G_loss: 4.6926\n",
      "  Batch [1170/1299] D_loss: -2.6728, G_loss: 3.3567\n",
      "  Batch [1180/1299] D_loss: -3.1763, G_loss: 4.4791\n",
      "  Batch [1190/1299] D_loss: -3.1756, G_loss: 3.0097\n",
      "  Batch [1200/1299] D_loss: -1.9710, G_loss: 5.3506\n",
      "  Batch [1210/1299] D_loss: -4.5526, G_loss: 6.0877\n",
      "  Batch [1220/1299] D_loss: -3.2125, G_loss: 6.0453\n",
      "  Batch [1230/1299] D_loss: -3.4735, G_loss: 5.9058\n",
      "  Batch [1240/1299] D_loss: -2.7378, G_loss: 4.7194\n",
      "  Batch [1250/1299] D_loss: -3.3646, G_loss: 3.9653\n",
      "  Batch [1260/1299] D_loss: -3.2203, G_loss: 4.7269\n",
      "  Batch [1270/1299] D_loss: -3.4005, G_loss: 6.2527\n",
      "  Batch [1280/1299] D_loss: -3.0770, G_loss: 2.3713\n",
      "  Batch [1290/1299] D_loss: -4.4328, G_loss: 4.7189\n",
      "\n",
      "Epoch 12 Summary:\n",
      "  Average D_loss: -2.7882\n",
      "  Average G_loss: 4.4301\n",
      "\n",
      "Epoch [13/100]\n",
      "  Batch [0/1299] D_loss: -4.1058, G_loss: 4.9678\n",
      "  Batch [10/1299] D_loss: -3.4360, G_loss: 4.4068\n",
      "  Batch [20/1299] D_loss: -4.0029, G_loss: 5.2312\n",
      "  Batch [30/1299] D_loss: -4.3777, G_loss: 4.6447\n",
      "  Batch [40/1299] D_loss: -2.7029, G_loss: 5.3911\n",
      "  Batch [50/1299] D_loss: -2.6198, G_loss: 4.0762\n",
      "  Batch [60/1299] D_loss: -1.9043, G_loss: 3.4415\n",
      "  Batch [70/1299] D_loss: -4.6343, G_loss: 5.1581\n",
      "  Batch [80/1299] D_loss: -3.1960, G_loss: 5.9195\n",
      "  Batch [90/1299] D_loss: -3.1127, G_loss: 4.6730\n",
      "  Batch [100/1299] D_loss: -2.9426, G_loss: 4.7430\n",
      "  Batch [110/1299] D_loss: -1.7881, G_loss: 4.8010\n",
      "  Batch [120/1299] D_loss: -1.8967, G_loss: 3.8702\n",
      "  Batch [130/1299] D_loss: -3.0069, G_loss: 4.3448\n",
      "  Batch [140/1299] D_loss: -3.1921, G_loss: 3.0606\n",
      "  Batch [150/1299] D_loss: -2.4594, G_loss: 0.6740\n",
      "  Batch [160/1299] D_loss: -2.5823, G_loss: 2.5143\n",
      "  Batch [170/1299] D_loss: -2.4662, G_loss: 3.7430\n",
      "  Batch [180/1299] D_loss: -3.5288, G_loss: 5.4465\n",
      "  Batch [190/1299] D_loss: -2.5513, G_loss: 4.4977\n",
      "  Batch [200/1299] D_loss: -1.8432, G_loss: 4.4714\n",
      "  Batch [210/1299] D_loss: -1.9234, G_loss: 5.4079\n",
      "  Batch [220/1299] D_loss: -2.7279, G_loss: 3.8960\n",
      "  Batch [230/1299] D_loss: -3.2496, G_loss: 4.0956\n",
      "  Batch [240/1299] D_loss: -3.1435, G_loss: 4.8287\n",
      "  Batch [250/1299] D_loss: -3.3793, G_loss: 7.2433\n",
      "  Batch [260/1299] D_loss: -2.8292, G_loss: 5.6601\n",
      "  Batch [270/1299] D_loss: -3.6221, G_loss: 4.4403\n",
      "  Batch [280/1299] D_loss: -3.5986, G_loss: 5.4515\n",
      "  Batch [290/1299] D_loss: -3.5223, G_loss: 4.6613\n",
      "  Batch [300/1299] D_loss: -1.7735, G_loss: 6.0175\n",
      "  Batch [310/1299] D_loss: -3.3909, G_loss: 3.9069\n",
      "  Batch [320/1299] D_loss: -2.8048, G_loss: 5.8881\n",
      "  Batch [330/1299] D_loss: -4.1427, G_loss: 6.7929\n",
      "  Batch [340/1299] D_loss: -1.9870, G_loss: 3.5290\n",
      "  Batch [350/1299] D_loss: -2.9712, G_loss: 4.5280\n",
      "  Batch [360/1299] D_loss: -2.7377, G_loss: 3.4041\n",
      "  Batch [370/1299] D_loss: -2.7562, G_loss: 4.7996\n",
      "  Batch [380/1299] D_loss: -2.7193, G_loss: 3.3987\n",
      "  Batch [390/1299] D_loss: -2.6268, G_loss: 2.7267\n",
      "  Batch [400/1299] D_loss: -3.2813, G_loss: 3.8064\n",
      "  Batch [410/1299] D_loss: -3.1812, G_loss: 5.1237\n",
      "  Batch [420/1299] D_loss: -3.3624, G_loss: 4.5294\n",
      "  Batch [430/1299] D_loss: -4.2244, G_loss: 4.7037\n",
      "  Batch [440/1299] D_loss: -3.5717, G_loss: 4.3470\n",
      "  Batch [450/1299] D_loss: -2.1246, G_loss: 3.6482\n",
      "  Batch [460/1299] D_loss: -3.0535, G_loss: 5.1009\n",
      "  Batch [470/1299] D_loss: -3.3759, G_loss: 2.4409\n",
      "  Batch [480/1299] D_loss: -3.6467, G_loss: 5.2387\n",
      "  Batch [490/1299] D_loss: -2.3368, G_loss: 5.7811\n",
      "  Batch [500/1299] D_loss: -3.5944, G_loss: 5.8214\n",
      "  Batch [510/1299] D_loss: -2.5386, G_loss: 2.2880\n",
      "  Batch [520/1299] D_loss: -2.4398, G_loss: 3.3223\n",
      "  Batch [530/1299] D_loss: -3.1683, G_loss: 3.4267\n",
      "  Batch [540/1299] D_loss: -2.6580, G_loss: 3.4575\n",
      "  Batch [550/1299] D_loss: -0.8578, G_loss: 2.1676\n",
      "  Batch [560/1299] D_loss: -2.8321, G_loss: 3.7972\n",
      "  Batch [570/1299] D_loss: -3.6570, G_loss: 4.7709\n",
      "  Batch [580/1299] D_loss: -1.4663, G_loss: 4.0102\n",
      "  Batch [590/1299] D_loss: -4.6057, G_loss: 4.7871\n",
      "  Batch [600/1299] D_loss: -2.8147, G_loss: 2.4186\n",
      "  Batch [610/1299] D_loss: -2.4998, G_loss: 2.8174\n",
      "  Batch [620/1299] D_loss: -3.3539, G_loss: 4.0692\n",
      "  Batch [630/1299] D_loss: -3.1417, G_loss: 4.4399\n",
      "  Batch [640/1299] D_loss: -2.1884, G_loss: 4.7224\n",
      "  Batch [650/1299] D_loss: -2.2139, G_loss: 2.9783\n",
      "  Batch [660/1299] D_loss: -3.8293, G_loss: 2.3172\n",
      "  Batch [670/1299] D_loss: -3.5839, G_loss: 3.8519\n",
      "  Batch [680/1299] D_loss: -2.3776, G_loss: 3.3141\n",
      "  Batch [690/1299] D_loss: -2.2497, G_loss: 5.1259\n",
      "  Batch [700/1299] D_loss: -2.5167, G_loss: 5.4270\n",
      "  Batch [710/1299] D_loss: -3.8867, G_loss: 4.3478\n",
      "  Batch [720/1299] D_loss: -2.5028, G_loss: 3.4874\n",
      "  Batch [730/1299] D_loss: -3.6398, G_loss: 4.5916\n",
      "  Batch [740/1299] D_loss: -3.7586, G_loss: 4.9870\n",
      "  Batch [750/1299] D_loss: -2.9389, G_loss: 3.5450\n",
      "  Batch [760/1299] D_loss: -2.8554, G_loss: 2.2743\n",
      "  Batch [770/1299] D_loss: -3.8216, G_loss: 3.9954\n",
      "  Batch [780/1299] D_loss: -3.4205, G_loss: 3.5638\n",
      "  Batch [790/1299] D_loss: -3.8300, G_loss: 4.4150\n",
      "  Batch [800/1299] D_loss: -3.0982, G_loss: 4.6353\n",
      "  Batch [810/1299] D_loss: -2.5570, G_loss: 5.4001\n",
      "  Batch [820/1299] D_loss: -2.3915, G_loss: 3.8684\n",
      "  Batch [830/1299] D_loss: -2.4875, G_loss: 4.3452\n",
      "  Batch [840/1299] D_loss: -2.5952, G_loss: 4.2174\n",
      "  Batch [850/1299] D_loss: -3.7693, G_loss: 3.7259\n",
      "  Batch [860/1299] D_loss: -3.1445, G_loss: 3.9914\n",
      "  Batch [870/1299] D_loss: -2.9576, G_loss: 3.1649\n",
      "  Batch [880/1299] D_loss: -3.1810, G_loss: 2.2901\n",
      "  Batch [890/1299] D_loss: -3.9163, G_loss: 4.0086\n",
      "  Batch [900/1299] D_loss: -3.9944, G_loss: 3.1149\n",
      "  Batch [910/1299] D_loss: -3.1711, G_loss: 2.2685\n",
      "  Batch [920/1299] D_loss: -4.1296, G_loss: 1.9320\n",
      "  Batch [930/1299] D_loss: -2.7810, G_loss: 5.1663\n",
      "  Batch [940/1299] D_loss: -3.9267, G_loss: 4.1916\n",
      "  Batch [950/1299] D_loss: -2.7157, G_loss: 2.0569\n",
      "  Batch [960/1299] D_loss: -4.2689, G_loss: 3.4197\n",
      "  Batch [970/1299] D_loss: -3.8073, G_loss: 1.2400\n",
      "  Batch [980/1299] D_loss: -2.5481, G_loss: 4.7409\n",
      "  Batch [990/1299] D_loss: -3.0530, G_loss: 4.1539\n",
      "  Batch [1000/1299] D_loss: -4.2636, G_loss: 3.2620\n",
      "  Batch [1010/1299] D_loss: -3.5543, G_loss: 2.9590\n",
      "  Batch [1020/1299] D_loss: -2.4361, G_loss: 3.8184\n",
      "  Batch [1030/1299] D_loss: -2.9651, G_loss: 1.8973\n",
      "  Batch [1040/1299] D_loss: -4.9713, G_loss: 0.5556\n",
      "  Batch [1050/1299] D_loss: -3.4176, G_loss: 3.0924\n",
      "  Batch [1060/1299] D_loss: -3.2565, G_loss: 4.4868\n",
      "  Batch [1070/1299] D_loss: -4.5500, G_loss: 2.8681\n",
      "  Batch [1080/1299] D_loss: -3.5047, G_loss: 3.7552\n",
      "  Batch [1090/1299] D_loss: -3.8112, G_loss: 3.8425\n",
      "  Batch [1100/1299] D_loss: -2.0961, G_loss: 3.3719\n",
      "  Batch [1110/1299] D_loss: -2.3791, G_loss: 5.0859\n",
      "  Batch [1120/1299] D_loss: -4.8950, G_loss: 1.0174\n",
      "  Batch [1130/1299] D_loss: -3.2540, G_loss: 0.6471\n",
      "  Batch [1140/1299] D_loss: -4.5303, G_loss: 0.6088\n",
      "  Batch [1150/1299] D_loss: -3.8710, G_loss: 3.7684\n",
      "  Batch [1160/1299] D_loss: -3.9928, G_loss: 2.1700\n",
      "  Batch [1170/1299] D_loss: -3.1108, G_loss: 3.5471\n",
      "  Batch [1180/1299] D_loss: -4.6874, G_loss: 0.9150\n",
      "  Batch [1190/1299] D_loss: -2.9167, G_loss: 5.1690\n",
      "  Batch [1200/1299] D_loss: -3.5296, G_loss: 5.6112\n",
      "  Batch [1210/1299] D_loss: -3.6082, G_loss: 3.7227\n",
      "  Batch [1220/1299] D_loss: -2.9484, G_loss: 4.7394\n",
      "  Batch [1230/1299] D_loss: -3.3287, G_loss: 2.7234\n",
      "  Batch [1240/1299] D_loss: -2.6579, G_loss: 4.7636\n",
      "  Batch [1250/1299] D_loss: -3.2013, G_loss: 2.5812\n",
      "  Batch [1260/1299] D_loss: -3.1048, G_loss: 4.1441\n",
      "  Batch [1270/1299] D_loss: -1.5399, G_loss: 5.0695\n",
      "  Batch [1280/1299] D_loss: -3.2373, G_loss: 3.6748\n",
      "  Batch [1290/1299] D_loss: -3.8754, G_loss: 4.9518\n",
      "\n",
      "Epoch 13 Summary:\n",
      "  Average D_loss: -2.7534\n",
      "  Average G_loss: 3.9232\n",
      "\n",
      "Epoch [14/100]\n",
      "  Batch [0/1299] D_loss: -1.5487, G_loss: 3.0817\n",
      "  Batch [10/1299] D_loss: -3.2850, G_loss: 4.0209\n",
      "  Batch [20/1299] D_loss: -1.7014, G_loss: 2.8763\n",
      "  Batch [30/1299] D_loss: -3.5917, G_loss: 4.3711\n",
      "  Batch [40/1299] D_loss: -3.0520, G_loss: 5.4064\n",
      "  Batch [50/1299] D_loss: -2.7642, G_loss: 4.0587\n",
      "  Batch [60/1299] D_loss: -2.2295, G_loss: 3.0647\n",
      "  Batch [70/1299] D_loss: -3.3264, G_loss: 5.2486\n",
      "  Batch [80/1299] D_loss: -3.3625, G_loss: 4.1680\n",
      "  Batch [90/1299] D_loss: -3.3478, G_loss: 3.1059\n",
      "  Batch [100/1299] D_loss: -3.1516, G_loss: 3.5045\n",
      "  Batch [110/1299] D_loss: -2.1349, G_loss: 3.6721\n",
      "  Batch [120/1299] D_loss: -2.0613, G_loss: 4.1534\n",
      "  Batch [130/1299] D_loss: -3.7448, G_loss: 4.3432\n",
      "  Batch [140/1299] D_loss: -4.6176, G_loss: 2.5046\n",
      "  Batch [150/1299] D_loss: -4.0006, G_loss: 3.4426\n",
      "  Batch [160/1299] D_loss: -3.5394, G_loss: 3.2890\n",
      "  Batch [170/1299] D_loss: -2.2837, G_loss: 2.9088\n",
      "  Batch [180/1299] D_loss: -3.0470, G_loss: 4.4721\n",
      "  Batch [190/1299] D_loss: -3.6016, G_loss: 2.0789\n",
      "  Batch [200/1299] D_loss: -2.6108, G_loss: 3.4642\n",
      "  Batch [210/1299] D_loss: -3.3144, G_loss: 2.1636\n",
      "  Batch [220/1299] D_loss: -1.9417, G_loss: 3.2226\n",
      "  Batch [230/1299] D_loss: -2.7864, G_loss: 2.6408\n",
      "  Batch [240/1299] D_loss: -3.6146, G_loss: 2.1451\n",
      "  Batch [250/1299] D_loss: -1.9374, G_loss: 2.1019\n",
      "  Batch [260/1299] D_loss: -2.4066, G_loss: 2.9513\n",
      "  Batch [270/1299] D_loss: -1.7073, G_loss: 4.1016\n",
      "  Batch [280/1299] D_loss: -2.1756, G_loss: 3.5914\n",
      "  Batch [290/1299] D_loss: -4.9760, G_loss: 3.0896\n",
      "  Batch [300/1299] D_loss: -4.4849, G_loss: 3.5871\n",
      "  Batch [310/1299] D_loss: -3.0566, G_loss: 3.0233\n",
      "  Batch [320/1299] D_loss: -4.2375, G_loss: 3.2142\n",
      "  Batch [330/1299] D_loss: -4.5381, G_loss: 1.8853\n",
      "  Batch [340/1299] D_loss: -3.4729, G_loss: 2.4564\n",
      "  Batch [350/1299] D_loss: -3.0387, G_loss: 3.0722\n",
      "  Batch [360/1299] D_loss: -3.6993, G_loss: 5.2500\n",
      "  Batch [370/1299] D_loss: -2.5473, G_loss: 2.9832\n",
      "  Batch [380/1299] D_loss: -1.9704, G_loss: 4.1640\n",
      "  Batch [390/1299] D_loss: -2.1730, G_loss: 4.8195\n",
      "  Batch [400/1299] D_loss: -3.5175, G_loss: 5.7243\n",
      "  Batch [410/1299] D_loss: -2.4334, G_loss: 3.2537\n",
      "  Batch [420/1299] D_loss: -3.1509, G_loss: 0.7344\n",
      "  Batch [430/1299] D_loss: -1.7510, G_loss: 5.5900\n",
      "  Batch [440/1299] D_loss: -3.9449, G_loss: 6.6781\n",
      "  Batch [450/1299] D_loss: -3.9117, G_loss: 4.0593\n",
      "  Batch [460/1299] D_loss: -3.1081, G_loss: 2.8667\n",
      "  Batch [470/1299] D_loss: -2.5847, G_loss: 2.0883\n",
      "  Batch [480/1299] D_loss: -3.4096, G_loss: 4.2182\n",
      "  Batch [490/1299] D_loss: -3.3625, G_loss: 8.4697\n",
      "  Batch [500/1299] D_loss: -3.7861, G_loss: 7.5123\n",
      "  Batch [510/1299] D_loss: -4.2190, G_loss: 6.6315\n",
      "  Batch [520/1299] D_loss: -2.8796, G_loss: 4.2916\n",
      "  Batch [530/1299] D_loss: -4.2163, G_loss: 3.1057\n",
      "  Batch [540/1299] D_loss: -2.4521, G_loss: 3.3500\n",
      "  Batch [550/1299] D_loss: -2.6529, G_loss: 3.7557\n",
      "  Batch [560/1299] D_loss: -2.4460, G_loss: 3.8303\n",
      "  Batch [570/1299] D_loss: -1.5801, G_loss: 6.7213\n",
      "  Batch [580/1299] D_loss: -2.2053, G_loss: 2.8692\n",
      "  Batch [590/1299] D_loss: -4.4249, G_loss: 3.8119\n",
      "  Batch [600/1299] D_loss: -2.8045, G_loss: 6.0118\n",
      "  Batch [610/1299] D_loss: -3.9594, G_loss: 3.2274\n",
      "  Batch [620/1299] D_loss: -3.4780, G_loss: 5.6800\n",
      "  Batch [630/1299] D_loss: -2.7783, G_loss: 2.9992\n",
      "  Batch [640/1299] D_loss: -2.9231, G_loss: 4.2405\n",
      "  Batch [650/1299] D_loss: -3.1629, G_loss: 4.3185\n",
      "  Batch [660/1299] D_loss: -1.6537, G_loss: 4.1700\n",
      "  Batch [670/1299] D_loss: -2.7191, G_loss: 3.7919\n",
      "  Batch [680/1299] D_loss: -4.8944, G_loss: 4.6817\n",
      "  Batch [690/1299] D_loss: -2.6702, G_loss: 5.3691\n",
      "  Batch [700/1299] D_loss: -2.6801, G_loss: 3.9399\n",
      "  Batch [710/1299] D_loss: -2.6305, G_loss: 3.0394\n",
      "  Batch [720/1299] D_loss: -2.3367, G_loss: 4.7599\n",
      "  Batch [730/1299] D_loss: -1.9138, G_loss: 4.6041\n",
      "  Batch [740/1299] D_loss: -3.0076, G_loss: 3.5392\n",
      "  Batch [750/1299] D_loss: -3.0342, G_loss: 4.8997\n",
      "  Batch [760/1299] D_loss: -3.5198, G_loss: 4.3010\n",
      "  Batch [770/1299] D_loss: -3.7372, G_loss: 2.1235\n",
      "  Batch [780/1299] D_loss: -3.2708, G_loss: 4.7570\n",
      "  Batch [790/1299] D_loss: -2.7949, G_loss: 3.0701\n",
      "  Batch [800/1299] D_loss: -2.7813, G_loss: 4.3045\n",
      "  Batch [810/1299] D_loss: -2.3128, G_loss: 3.3637\n",
      "  Batch [820/1299] D_loss: -2.8478, G_loss: 4.4449\n",
      "  Batch [830/1299] D_loss: -2.6244, G_loss: 4.0101\n",
      "  Batch [840/1299] D_loss: -3.3762, G_loss: 5.2372\n",
      "  Batch [850/1299] D_loss: -2.2006, G_loss: 5.0958\n",
      "  Batch [860/1299] D_loss: -2.5033, G_loss: 4.6419\n",
      "  Batch [870/1299] D_loss: -2.8770, G_loss: 2.8329\n",
      "  Batch [880/1299] D_loss: -2.2897, G_loss: 4.4171\n",
      "  Batch [890/1299] D_loss: -1.9608, G_loss: 2.4202\n",
      "  Batch [900/1299] D_loss: -2.6336, G_loss: 4.2249\n",
      "  Batch [910/1299] D_loss: -3.4195, G_loss: 5.0593\n",
      "  Batch [920/1299] D_loss: -4.2180, G_loss: 4.3378\n",
      "  Batch [930/1299] D_loss: -3.4192, G_loss: 2.5223\n",
      "  Batch [940/1299] D_loss: -2.8268, G_loss: 2.8176\n",
      "  Batch [950/1299] D_loss: -2.7752, G_loss: 3.2656\n",
      "  Batch [960/1299] D_loss: -2.5344, G_loss: 3.2574\n",
      "  Batch [970/1299] D_loss: -2.5829, G_loss: 3.2128\n",
      "  Batch [980/1299] D_loss: -2.5324, G_loss: 4.2935\n",
      "  Batch [990/1299] D_loss: -2.9413, G_loss: 1.8854\n",
      "  Batch [1000/1299] D_loss: -2.4565, G_loss: 5.3762\n",
      "  Batch [1010/1299] D_loss: -3.1691, G_loss: 3.5732\n",
      "  Batch [1020/1299] D_loss: -2.8269, G_loss: 5.0876\n",
      "  Batch [1030/1299] D_loss: -4.2290, G_loss: 3.1986\n",
      "  Batch [1040/1299] D_loss: -2.0317, G_loss: 4.0065\n",
      "  Batch [1050/1299] D_loss: -4.6317, G_loss: 2.9327\n",
      "  Batch [1060/1299] D_loss: -2.9402, G_loss: 2.8744\n",
      "  Batch [1070/1299] D_loss: -2.0963, G_loss: 4.2073\n",
      "  Batch [1080/1299] D_loss: -3.8212, G_loss: 1.5334\n",
      "  Batch [1090/1299] D_loss: -2.5742, G_loss: 3.2549\n",
      "  Batch [1100/1299] D_loss: -4.6744, G_loss: 4.0506\n",
      "  Batch [1110/1299] D_loss: -3.3819, G_loss: 1.0217\n",
      "  Batch [1120/1299] D_loss: -2.2450, G_loss: 3.2824\n",
      "  Batch [1130/1299] D_loss: -3.0623, G_loss: 3.7086\n",
      "  Batch [1140/1299] D_loss: -3.9253, G_loss: 5.6300\n",
      "  Batch [1150/1299] D_loss: -3.2456, G_loss: 4.9307\n",
      "  Batch [1160/1299] D_loss: -3.0760, G_loss: 3.2968\n",
      "  Batch [1170/1299] D_loss: -3.0790, G_loss: 1.3581\n",
      "  Batch [1180/1299] D_loss: -3.1865, G_loss: 2.2498\n",
      "  Batch [1190/1299] D_loss: -2.4319, G_loss: 3.6714\n",
      "  Batch [1200/1299] D_loss: -2.5585, G_loss: 3.9485\n",
      "  Batch [1210/1299] D_loss: -3.3621, G_loss: 4.4315\n",
      "  Batch [1220/1299] D_loss: -2.9095, G_loss: 4.5134\n",
      "  Batch [1230/1299] D_loss: -1.9153, G_loss: 4.7645\n",
      "  Batch [1240/1299] D_loss: -2.9257, G_loss: 3.8649\n",
      "  Batch [1250/1299] D_loss: -2.8715, G_loss: 5.5467\n",
      "  Batch [1260/1299] D_loss: -3.2771, G_loss: 3.2165\n",
      "  Batch [1270/1299] D_loss: -1.4157, G_loss: 4.3928\n",
      "  Batch [1280/1299] D_loss: -3.3874, G_loss: 3.9558\n",
      "  Batch [1290/1299] D_loss: -3.3392, G_loss: 5.5314\n",
      "\n",
      "Epoch 14 Summary:\n",
      "  Average D_loss: -2.6959\n",
      "  Average G_loss: 3.9356\n",
      "\n",
      "Epoch [15/100]\n",
      "  Batch [0/1299] D_loss: -2.7637, G_loss: 3.4091\n",
      "  Batch [10/1299] D_loss: -3.7790, G_loss: 4.3403\n",
      "  Batch [20/1299] D_loss: -4.1609, G_loss: 2.5847\n",
      "  Batch [30/1299] D_loss: -2.5965, G_loss: 2.0938\n",
      "  Batch [40/1299] D_loss: -2.3739, G_loss: 4.8501\n",
      "  Batch [50/1299] D_loss: -3.3349, G_loss: 3.4662\n",
      "  Batch [60/1299] D_loss: -2.6980, G_loss: 2.5820\n",
      "  Batch [70/1299] D_loss: -3.0698, G_loss: 5.4530\n",
      "  Batch [80/1299] D_loss: -4.0502, G_loss: 4.0584\n",
      "  Batch [90/1299] D_loss: -4.1478, G_loss: 1.4468\n",
      "  Batch [100/1299] D_loss: -3.6357, G_loss: 4.0589\n",
      "  Batch [110/1299] D_loss: -2.2763, G_loss: 4.6156\n",
      "  Batch [120/1299] D_loss: -4.0251, G_loss: 3.0487\n",
      "  Batch [130/1299] D_loss: -2.8771, G_loss: 5.5407\n",
      "  Batch [140/1299] D_loss: -2.3614, G_loss: 3.8855\n",
      "  Batch [150/1299] D_loss: -2.9920, G_loss: 4.1758\n",
      "  Batch [160/1299] D_loss: -3.8350, G_loss: 4.0131\n",
      "  Batch [170/1299] D_loss: -3.8918, G_loss: 4.3318\n",
      "  Batch [180/1299] D_loss: -2.3932, G_loss: 5.4029\n",
      "  Batch [190/1299] D_loss: -3.2188, G_loss: 3.5674\n",
      "  Batch [200/1299] D_loss: -3.7819, G_loss: 2.7643\n",
      "  Batch [210/1299] D_loss: -2.7854, G_loss: 3.3077\n",
      "  Batch [220/1299] D_loss: -3.3097, G_loss: 3.4264\n",
      "  Batch [230/1299] D_loss: -3.6850, G_loss: 3.3625\n",
      "  Batch [240/1299] D_loss: -5.1800, G_loss: 7.7165\n",
      "  Batch [250/1299] D_loss: -2.7950, G_loss: 2.9359\n",
      "  Batch [260/1299] D_loss: -4.4361, G_loss: 3.6141\n",
      "  Batch [270/1299] D_loss: -2.9591, G_loss: 3.6805\n",
      "  Batch [280/1299] D_loss: -4.6505, G_loss: 6.8397\n",
      "  Batch [290/1299] D_loss: -3.2744, G_loss: 4.5319\n",
      "  Batch [300/1299] D_loss: -5.1758, G_loss: 3.0786\n",
      "  Batch [310/1299] D_loss: -4.0091, G_loss: 4.0875\n",
      "  Batch [320/1299] D_loss: -2.6687, G_loss: 4.2901\n",
      "  Batch [330/1299] D_loss: -3.2348, G_loss: 3.6060\n",
      "  Batch [340/1299] D_loss: -2.1028, G_loss: 0.8800\n",
      "  Batch [350/1299] D_loss: -1.6049, G_loss: 3.9910\n",
      "  Batch [360/1299] D_loss: -2.8793, G_loss: 2.8783\n",
      "  Batch [370/1299] D_loss: -1.6117, G_loss: 5.1403\n",
      "  Batch [380/1299] D_loss: -3.7615, G_loss: 3.7792\n",
      "  Batch [390/1299] D_loss: -2.4607, G_loss: 3.7654\n",
      "  Batch [400/1299] D_loss: -3.1492, G_loss: 5.3752\n",
      "  Batch [410/1299] D_loss: -3.0767, G_loss: 4.4737\n",
      "  Batch [420/1299] D_loss: -3.3595, G_loss: 5.0281\n",
      "  Batch [430/1299] D_loss: -1.7567, G_loss: 4.3869\n",
      "  Batch [440/1299] D_loss: -2.0740, G_loss: 2.3481\n",
      "  Batch [450/1299] D_loss: -4.0456, G_loss: 5.5740\n",
      "  Batch [460/1299] D_loss: -2.2011, G_loss: 4.8679\n",
      "  Batch [470/1299] D_loss: -3.1704, G_loss: 5.5971\n",
      "  Batch [480/1299] D_loss: -2.2915, G_loss: 5.9651\n",
      "  Batch [490/1299] D_loss: -2.4968, G_loss: 4.5056\n",
      "  Batch [500/1299] D_loss: -3.4783, G_loss: 2.5686\n",
      "  Batch [510/1299] D_loss: -2.4610, G_loss: 3.1666\n",
      "  Batch [520/1299] D_loss: -2.8516, G_loss: 3.1685\n",
      "  Batch [530/1299] D_loss: -2.9858, G_loss: 4.8890\n",
      "  Batch [540/1299] D_loss: -5.0115, G_loss: 4.0834\n",
      "  Batch [550/1299] D_loss: -3.9826, G_loss: 3.8590\n",
      "  Batch [560/1299] D_loss: -3.6323, G_loss: 4.1630\n",
      "  Batch [570/1299] D_loss: -4.6092, G_loss: 5.2439\n",
      "  Batch [580/1299] D_loss: -3.7563, G_loss: 3.9188\n",
      "  Batch [590/1299] D_loss: -2.5369, G_loss: 2.6045\n",
      "  Batch [600/1299] D_loss: -2.9997, G_loss: 4.0180\n",
      "  Batch [610/1299] D_loss: -3.7554, G_loss: 7.2328\n",
      "  Batch [620/1299] D_loss: -3.2956, G_loss: 3.0165\n",
      "  Batch [630/1299] D_loss: -3.9877, G_loss: 5.1509\n",
      "  Batch [640/1299] D_loss: -4.2908, G_loss: 4.4470\n",
      "  Batch [650/1299] D_loss: -1.7016, G_loss: 3.5592\n",
      "  Batch [660/1299] D_loss: -3.0734, G_loss: 6.1315\n",
      "  Batch [670/1299] D_loss: -1.8799, G_loss: 2.9467\n",
      "  Batch [680/1299] D_loss: -3.0314, G_loss: 4.0525\n",
      "  Batch [690/1299] D_loss: -3.7813, G_loss: 5.0036\n",
      "  Batch [700/1299] D_loss: -3.9835, G_loss: 3.7487\n",
      "  Batch [710/1299] D_loss: -3.0779, G_loss: 2.7055\n",
      "  Batch [720/1299] D_loss: -4.3104, G_loss: 4.1242\n",
      "  Batch [730/1299] D_loss: -2.3703, G_loss: 4.4061\n",
      "  Batch [740/1299] D_loss: -1.9081, G_loss: 5.5704\n",
      "  Batch [750/1299] D_loss: -3.8815, G_loss: 3.2799\n",
      "  Batch [760/1299] D_loss: -2.4012, G_loss: 4.2532\n",
      "  Batch [770/1299] D_loss: -2.2782, G_loss: 4.7837\n",
      "  Batch [780/1299] D_loss: -2.3119, G_loss: 3.7728\n",
      "  Batch [790/1299] D_loss: -3.5608, G_loss: 2.0731\n",
      "  Batch [800/1299] D_loss: -4.2474, G_loss: 4.8550\n",
      "  Batch [810/1299] D_loss: -3.2693, G_loss: 4.0885\n",
      "  Batch [820/1299] D_loss: -2.7648, G_loss: 3.5417\n",
      "  Batch [830/1299] D_loss: -4.2634, G_loss: 1.7134\n",
      "  Batch [840/1299] D_loss: -2.6476, G_loss: 3.9925\n",
      "  Batch [850/1299] D_loss: -3.1517, G_loss: 4.7078\n",
      "  Batch [860/1299] D_loss: -1.0649, G_loss: 4.1625\n",
      "  Batch [870/1299] D_loss: -3.2499, G_loss: 5.3739\n",
      "  Batch [880/1299] D_loss: -3.2237, G_loss: 3.9166\n",
      "  Batch [890/1299] D_loss: -3.2590, G_loss: 4.2977\n",
      "  Batch [900/1299] D_loss: -3.2928, G_loss: 3.0759\n",
      "  Batch [910/1299] D_loss: -3.4644, G_loss: 5.4472\n",
      "  Batch [920/1299] D_loss: -2.6904, G_loss: 4.6305\n",
      "  Batch [930/1299] D_loss: -1.4007, G_loss: 3.9379\n",
      "  Batch [940/1299] D_loss: -3.6881, G_loss: 4.9565\n",
      "  Batch [950/1299] D_loss: -2.7938, G_loss: 2.9597\n",
      "  Batch [960/1299] D_loss: -2.6842, G_loss: 4.2020\n",
      "  Batch [970/1299] D_loss: -2.9768, G_loss: 3.7779\n",
      "  Batch [980/1299] D_loss: -3.9578, G_loss: 5.8074\n",
      "  Batch [990/1299] D_loss: -4.2079, G_loss: 5.7163\n",
      "  Batch [1000/1299] D_loss: -3.6788, G_loss: 3.4343\n",
      "  Batch [1010/1299] D_loss: -2.9357, G_loss: 0.3364\n",
      "  Batch [1020/1299] D_loss: -3.1605, G_loss: 3.2461\n",
      "  Batch [1030/1299] D_loss: -3.5145, G_loss: 4.3209\n",
      "  Batch [1040/1299] D_loss: -3.6865, G_loss: 2.6631\n",
      "  Batch [1050/1299] D_loss: -3.1310, G_loss: 3.1466\n",
      "  Batch [1060/1299] D_loss: -2.5643, G_loss: 4.7248\n",
      "  Batch [1070/1299] D_loss: -1.2886, G_loss: 2.4787\n",
      "  Batch [1080/1299] D_loss: -3.4255, G_loss: 4.1613\n",
      "  Batch [1090/1299] D_loss: -2.2752, G_loss: 4.0293\n",
      "  Batch [1100/1299] D_loss: -3.3003, G_loss: 3.1294\n",
      "  Batch [1110/1299] D_loss: -2.6367, G_loss: 3.4366\n",
      "  Batch [1120/1299] D_loss: -4.1109, G_loss: 3.6778\n",
      "  Batch [1130/1299] D_loss: -3.7409, G_loss: 3.5442\n",
      "  Batch [1140/1299] D_loss: -3.9569, G_loss: 1.7184\n",
      "  Batch [1150/1299] D_loss: -0.6069, G_loss: 5.9229\n",
      "  Batch [1160/1299] D_loss: -3.5462, G_loss: 4.7946\n",
      "  Batch [1170/1299] D_loss: -4.2325, G_loss: 4.4974\n",
      "  Batch [1180/1299] D_loss: -2.7697, G_loss: 3.1757\n",
      "  Batch [1190/1299] D_loss: -2.0535, G_loss: 5.3830\n",
      "  Batch [1200/1299] D_loss: -3.7631, G_loss: 5.6838\n",
      "  Batch [1210/1299] D_loss: -2.1053, G_loss: 3.9999\n",
      "  Batch [1220/1299] D_loss: -2.9905, G_loss: 3.8229\n",
      "  Batch [1230/1299] D_loss: -2.7272, G_loss: 3.4935\n",
      "  Batch [1240/1299] D_loss: -3.8109, G_loss: 6.8266\n",
      "  Batch [1250/1299] D_loss: -3.5794, G_loss: 2.8902\n",
      "  Batch [1260/1299] D_loss: -4.1626, G_loss: 3.5148\n",
      "  Batch [1270/1299] D_loss: -2.5429, G_loss: 2.5609\n",
      "  Batch [1280/1299] D_loss: -3.1097, G_loss: 4.7935\n",
      "  Batch [1290/1299] D_loss: -4.0688, G_loss: 1.9558\n",
      "\n",
      "Epoch 15 Summary:\n",
      "  Average D_loss: -2.6437\n",
      "  Average G_loss: 4.0631\n",
      "\n",
      "Epoch [16/100]\n",
      "  Batch [0/1299] D_loss: -2.1425, G_loss: 3.6993\n",
      "  Batch [10/1299] D_loss: -3.8827, G_loss: 5.0578\n",
      "  Batch [20/1299] D_loss: -3.0766, G_loss: 3.8479\n",
      "  Batch [30/1299] D_loss: -2.8686, G_loss: 2.2972\n",
      "  Batch [40/1299] D_loss: -2.9246, G_loss: 2.9747\n",
      "  Batch [50/1299] D_loss: -4.9999, G_loss: 5.9037\n",
      "  Batch [60/1299] D_loss: -2.4299, G_loss: 4.5385\n",
      "  Batch [70/1299] D_loss: -3.8026, G_loss: 3.3555\n",
      "  Batch [80/1299] D_loss: -2.4860, G_loss: 4.6017\n",
      "  Batch [90/1299] D_loss: -3.0419, G_loss: 5.1802\n",
      "  Batch [100/1299] D_loss: -3.0364, G_loss: 4.5016\n",
      "  Batch [110/1299] D_loss: -2.5422, G_loss: 3.8351\n",
      "  Batch [120/1299] D_loss: -3.1181, G_loss: 4.8658\n",
      "  Batch [130/1299] D_loss: -3.7346, G_loss: 3.4343\n",
      "  Batch [140/1299] D_loss: -4.3193, G_loss: 2.9818\n",
      "  Batch [150/1299] D_loss: -3.8330, G_loss: 5.1667\n",
      "  Batch [160/1299] D_loss: -2.0723, G_loss: 6.0842\n",
      "  Batch [170/1299] D_loss: -1.9515, G_loss: 4.0574\n",
      "  Batch [180/1299] D_loss: -3.1240, G_loss: 2.9076\n",
      "  Batch [190/1299] D_loss: -3.3641, G_loss: 3.1009\n",
      "  Batch [200/1299] D_loss: -3.0993, G_loss: 3.7115\n",
      "  Batch [210/1299] D_loss: -2.5213, G_loss: 2.2954\n",
      "  Batch [220/1299] D_loss: -1.7677, G_loss: 2.3271\n",
      "  Batch [230/1299] D_loss: -2.3333, G_loss: 5.4963\n",
      "  Batch [240/1299] D_loss: -2.9513, G_loss: 4.0826\n",
      "  Batch [250/1299] D_loss: -1.8276, G_loss: 5.0596\n",
      "  Batch [260/1299] D_loss: -3.0272, G_loss: 4.2418\n",
      "  Batch [270/1299] D_loss: -3.3279, G_loss: 4.6213\n",
      "  Batch [280/1299] D_loss: -3.2513, G_loss: 4.2441\n",
      "  Batch [290/1299] D_loss: -2.0067, G_loss: 3.4873\n",
      "  Batch [300/1299] D_loss: -1.6892, G_loss: 2.9957\n",
      "  Batch [310/1299] D_loss: -3.3599, G_loss: 4.8283\n",
      "  Batch [320/1299] D_loss: -4.0861, G_loss: 3.6469\n",
      "  Batch [330/1299] D_loss: -3.6461, G_loss: 3.2487\n",
      "  Batch [340/1299] D_loss: -2.3865, G_loss: 4.8106\n",
      "  Batch [350/1299] D_loss: -2.4119, G_loss: 3.1049\n",
      "  Batch [360/1299] D_loss: -2.9692, G_loss: 3.6962\n",
      "  Batch [370/1299] D_loss: -2.9171, G_loss: 4.7498\n",
      "  Batch [380/1299] D_loss: -2.7087, G_loss: 5.1651\n",
      "  Batch [390/1299] D_loss: -2.1915, G_loss: 6.1611\n",
      "  Batch [400/1299] D_loss: -3.2477, G_loss: 5.3148\n",
      "  Batch [410/1299] D_loss: -2.7012, G_loss: 3.4912\n",
      "  Batch [420/1299] D_loss: -2.5620, G_loss: 3.9306\n",
      "  Batch [430/1299] D_loss: -3.2847, G_loss: 5.4198\n",
      "  Batch [440/1299] D_loss: -3.2887, G_loss: 3.9576\n",
      "  Batch [450/1299] D_loss: -3.3326, G_loss: 5.2126\n",
      "  Batch [460/1299] D_loss: -2.2514, G_loss: 4.8574\n",
      "  Batch [470/1299] D_loss: -3.2719, G_loss: 3.9514\n",
      "  Batch [480/1299] D_loss: -3.3587, G_loss: 3.8014\n",
      "  Batch [490/1299] D_loss: -3.7439, G_loss: 4.0403\n",
      "  Batch [500/1299] D_loss: -2.6806, G_loss: 3.0279\n",
      "  Batch [510/1299] D_loss: -3.3999, G_loss: 4.1111\n",
      "  Batch [520/1299] D_loss: -2.2416, G_loss: 5.9575\n",
      "  Batch [530/1299] D_loss: -3.7179, G_loss: 3.9155\n",
      "  Batch [540/1299] D_loss: -4.2222, G_loss: 5.2815\n",
      "  Batch [550/1299] D_loss: -5.0388, G_loss: 5.1127\n",
      "  Batch [560/1299] D_loss: -3.2427, G_loss: 4.4088\n",
      "  Batch [570/1299] D_loss: -2.7947, G_loss: 5.4760\n",
      "  Batch [580/1299] D_loss: -3.8881, G_loss: 5.4545\n",
      "  Batch [590/1299] D_loss: -3.1667, G_loss: 2.5568\n",
      "  Batch [600/1299] D_loss: -2.0610, G_loss: 4.4654\n",
      "  Batch [610/1299] D_loss: -2.4817, G_loss: 7.0190\n",
      "  Batch [620/1299] D_loss: -2.4773, G_loss: 5.6850\n",
      "  Batch [630/1299] D_loss: -3.2124, G_loss: 3.9841\n",
      "  Batch [640/1299] D_loss: -3.5108, G_loss: 1.2642\n",
      "  Batch [650/1299] D_loss: -3.3294, G_loss: 6.8253\n",
      "  Batch [660/1299] D_loss: -2.1090, G_loss: 5.0558\n",
      "  Batch [670/1299] D_loss: -2.6866, G_loss: 5.9983\n",
      "  Batch [680/1299] D_loss: -2.1563, G_loss: 3.8033\n",
      "  Batch [690/1299] D_loss: -2.8976, G_loss: 5.0081\n",
      "  Batch [700/1299] D_loss: -2.6333, G_loss: 3.0283\n",
      "  Batch [710/1299] D_loss: -1.5071, G_loss: 4.2001\n",
      "  Batch [720/1299] D_loss: -2.3545, G_loss: 3.6143\n",
      "  Batch [730/1299] D_loss: -5.4635, G_loss: 6.1339\n",
      "  Batch [740/1299] D_loss: -4.4000, G_loss: 4.2574\n",
      "  Batch [750/1299] D_loss: -2.8415, G_loss: 4.7973\n",
      "  Batch [760/1299] D_loss: -2.7189, G_loss: 3.7244\n",
      "  Batch [770/1299] D_loss: -2.1976, G_loss: 4.8285\n",
      "  Batch [780/1299] D_loss: -2.8187, G_loss: 3.8604\n",
      "  Batch [790/1299] D_loss: -2.6757, G_loss: 5.4433\n",
      "  Batch [800/1299] D_loss: -3.9048, G_loss: 5.3306\n",
      "  Batch [810/1299] D_loss: -3.2386, G_loss: 4.0430\n",
      "  Batch [820/1299] D_loss: -2.5473, G_loss: 4.5285\n",
      "  Batch [830/1299] D_loss: -1.9914, G_loss: 6.7071\n",
      "  Batch [840/1299] D_loss: -2.3269, G_loss: 4.0005\n",
      "  Batch [850/1299] D_loss: -4.4341, G_loss: 5.9075\n",
      "  Batch [860/1299] D_loss: -3.4537, G_loss: 9.3214\n",
      "  Batch [870/1299] D_loss: -1.5502, G_loss: 6.8231\n",
      "  Batch [880/1299] D_loss: -1.7172, G_loss: 6.0446\n",
      "  Batch [890/1299] D_loss: -3.0707, G_loss: 4.9460\n",
      "  Batch [900/1299] D_loss: -1.5619, G_loss: 5.7162\n",
      "  Batch [910/1299] D_loss: -2.4073, G_loss: 6.4881\n",
      "  Batch [920/1299] D_loss: -2.3885, G_loss: 4.8563\n",
      "  Batch [930/1299] D_loss: -2.7502, G_loss: 3.5559\n",
      "  Batch [940/1299] D_loss: -2.8913, G_loss: 5.7050\n",
      "  Batch [950/1299] D_loss: -3.0595, G_loss: 5.5814\n",
      "  Batch [960/1299] D_loss: -5.3018, G_loss: 6.5336\n",
      "  Batch [970/1299] D_loss: -3.5776, G_loss: 5.3616\n",
      "  Batch [980/1299] D_loss: -3.0324, G_loss: 4.4118\n",
      "  Batch [990/1299] D_loss: -2.1194, G_loss: 4.9822\n",
      "  Batch [1000/1299] D_loss: -2.2355, G_loss: 6.3004\n",
      "  Batch [1010/1299] D_loss: -3.1985, G_loss: 5.3506\n",
      "  Batch [1020/1299] D_loss: -3.2503, G_loss: 6.7173\n",
      "  Batch [1030/1299] D_loss: -2.4229, G_loss: 3.9280\n",
      "  Batch [1040/1299] D_loss: -3.1248, G_loss: 3.9691\n",
      "  Batch [1050/1299] D_loss: -2.8276, G_loss: 3.7606\n",
      "  Batch [1060/1299] D_loss: -4.4835, G_loss: 3.7397\n",
      "  Batch [1070/1299] D_loss: -3.2011, G_loss: 5.1056\n",
      "  Batch [1080/1299] D_loss: -1.9879, G_loss: 4.7201\n",
      "  Batch [1090/1299] D_loss: -2.4726, G_loss: 6.0859\n",
      "  Batch [1100/1299] D_loss: -2.3547, G_loss: 5.7272\n",
      "  Batch [1110/1299] D_loss: -2.9512, G_loss: 4.8088\n",
      "  Batch [1120/1299] D_loss: -1.4073, G_loss: 3.5815\n",
      "  Batch [1130/1299] D_loss: -2.4296, G_loss: 4.3156\n",
      "  Batch [1140/1299] D_loss: -2.1348, G_loss: 5.1691\n",
      "  Batch [1150/1299] D_loss: -2.3960, G_loss: 4.5359\n",
      "  Batch [1160/1299] D_loss: -2.4081, G_loss: 3.3520\n",
      "  Batch [1170/1299] D_loss: -3.8718, G_loss: 4.9365\n",
      "  Batch [1180/1299] D_loss: -1.4564, G_loss: 5.4598\n",
      "  Batch [1190/1299] D_loss: -3.8231, G_loss: 3.6345\n",
      "  Batch [1200/1299] D_loss: -3.2303, G_loss: 4.2285\n",
      "  Batch [1210/1299] D_loss: -2.2934, G_loss: 5.3504\n",
      "  Batch [1220/1299] D_loss: -2.9475, G_loss: 6.0663\n",
      "  Batch [1230/1299] D_loss: -1.9894, G_loss: 5.0725\n",
      "  Batch [1240/1299] D_loss: -2.4941, G_loss: 3.9413\n",
      "  Batch [1250/1299] D_loss: -2.7806, G_loss: 6.2308\n",
      "  Batch [1260/1299] D_loss: -1.9513, G_loss: 3.9449\n",
      "  Batch [1270/1299] D_loss: -2.8427, G_loss: 3.7437\n",
      "  Batch [1280/1299] D_loss: -2.5008, G_loss: 4.3801\n",
      "  Batch [1290/1299] D_loss: -3.0137, G_loss: 4.1801\n",
      "\n",
      "Epoch 16 Summary:\n",
      "  Average D_loss: -2.6165\n",
      "  Average G_loss: 4.6237\n",
      "\n",
      "Epoch [17/100]\n",
      "  Batch [0/1299] D_loss: -2.7364, G_loss: 4.2512\n",
      "  Batch [10/1299] D_loss: -2.4362, G_loss: 6.1723\n",
      "  Batch [20/1299] D_loss: -2.5285, G_loss: 3.8724\n",
      "  Batch [30/1299] D_loss: -2.2770, G_loss: 2.9346\n",
      "  Batch [40/1299] D_loss: -4.0855, G_loss: 2.3163\n",
      "  Batch [50/1299] D_loss: -3.4433, G_loss: 6.8715\n",
      "  Batch [60/1299] D_loss: -3.8301, G_loss: 2.7600\n",
      "  Batch [70/1299] D_loss: -1.9759, G_loss: 3.7170\n",
      "  Batch [80/1299] D_loss: -2.5648, G_loss: 2.3052\n",
      "  Batch [90/1299] D_loss: -2.5685, G_loss: 4.2773\n",
      "  Batch [100/1299] D_loss: -1.9591, G_loss: 5.7507\n",
      "  Batch [110/1299] D_loss: -1.9090, G_loss: 4.2122\n",
      "  Batch [120/1299] D_loss: -2.9095, G_loss: 3.3168\n",
      "  Batch [130/1299] D_loss: -4.8677, G_loss: 6.9333\n",
      "  Batch [140/1299] D_loss: -2.8212, G_loss: 5.7358\n",
      "  Batch [150/1299] D_loss: -3.0734, G_loss: 4.5097\n",
      "  Batch [160/1299] D_loss: -3.8829, G_loss: 6.1834\n",
      "  Batch [170/1299] D_loss: -3.7681, G_loss: 4.4673\n",
      "  Batch [180/1299] D_loss: -3.6509, G_loss: 3.8196\n",
      "  Batch [190/1299] D_loss: -2.4685, G_loss: 5.7454\n",
      "  Batch [200/1299] D_loss: -2.8482, G_loss: 4.7361\n",
      "  Batch [210/1299] D_loss: -4.1513, G_loss: 2.6338\n",
      "  Batch [220/1299] D_loss: -2.3129, G_loss: 5.2145\n",
      "  Batch [230/1299] D_loss: -1.1008, G_loss: 4.0195\n",
      "  Batch [240/1299] D_loss: -3.8907, G_loss: 4.6038\n",
      "  Batch [250/1299] D_loss: -4.0979, G_loss: 3.3479\n",
      "  Batch [260/1299] D_loss: -2.6980, G_loss: 2.0243\n",
      "  Batch [270/1299] D_loss: -2.5701, G_loss: 4.3641\n",
      "  Batch [280/1299] D_loss: -0.6786, G_loss: 3.6720\n",
      "  Batch [290/1299] D_loss: -3.0566, G_loss: 3.5020\n",
      "  Batch [300/1299] D_loss: -1.2614, G_loss: 6.1123\n",
      "  Batch [310/1299] D_loss: -3.1503, G_loss: 4.6248\n",
      "  Batch [320/1299] D_loss: -3.0826, G_loss: 3.5463\n",
      "  Batch [330/1299] D_loss: -2.7109, G_loss: 1.8444\n",
      "  Batch [340/1299] D_loss: -4.1795, G_loss: 6.0137\n",
      "  Batch [350/1299] D_loss: -2.7793, G_loss: 4.6483\n",
      "  Batch [360/1299] D_loss: -3.3778, G_loss: 2.9296\n",
      "  Batch [370/1299] D_loss: -3.0819, G_loss: 4.9849\n",
      "  Batch [380/1299] D_loss: -3.1455, G_loss: 2.9584\n",
      "  Batch [390/1299] D_loss: -2.2586, G_loss: 5.3343\n",
      "  Batch [400/1299] D_loss: -3.5296, G_loss: 4.3339\n",
      "  Batch [410/1299] D_loss: -2.1736, G_loss: 4.2500\n",
      "  Batch [420/1299] D_loss: -3.0495, G_loss: 6.4705\n",
      "  Batch [430/1299] D_loss: -2.4845, G_loss: 1.9503\n",
      "  Batch [440/1299] D_loss: -4.1489, G_loss: 4.7160\n",
      "  Batch [450/1299] D_loss: -3.2614, G_loss: 3.8933\n",
      "  Batch [460/1299] D_loss: -1.8853, G_loss: 5.1404\n",
      "  Batch [470/1299] D_loss: -1.6796, G_loss: 3.6894\n",
      "  Batch [480/1299] D_loss: -2.9786, G_loss: 4.3807\n",
      "  Batch [490/1299] D_loss: -2.6449, G_loss: 5.3311\n",
      "  Batch [500/1299] D_loss: -1.5803, G_loss: 4.4567\n",
      "  Batch [510/1299] D_loss: -3.7995, G_loss: 5.0601\n",
      "  Batch [520/1299] D_loss: -1.4119, G_loss: 5.2481\n",
      "  Batch [530/1299] D_loss: -2.2963, G_loss: 3.5389\n",
      "  Batch [540/1299] D_loss: -2.1772, G_loss: 5.8342\n",
      "  Batch [550/1299] D_loss: -3.3250, G_loss: 4.1788\n",
      "  Batch [560/1299] D_loss: -1.9465, G_loss: 4.5824\n",
      "  Batch [570/1299] D_loss: -2.9958, G_loss: 3.3941\n",
      "  Batch [580/1299] D_loss: -2.3362, G_loss: 5.0358\n",
      "  Batch [590/1299] D_loss: -3.2004, G_loss: 4.7991\n",
      "  Batch [600/1299] D_loss: -2.9502, G_loss: 3.9060\n",
      "  Batch [610/1299] D_loss: -1.4826, G_loss: 3.4868\n",
      "  Batch [620/1299] D_loss: -3.4394, G_loss: 4.1397\n",
      "  Batch [630/1299] D_loss: -4.4527, G_loss: 3.5927\n",
      "  Batch [640/1299] D_loss: -2.9680, G_loss: 3.6589\n",
      "  Batch [650/1299] D_loss: -2.2502, G_loss: 4.8441\n",
      "  Batch [660/1299] D_loss: -2.5160, G_loss: 4.8627\n",
      "  Batch [670/1299] D_loss: -2.9391, G_loss: 4.3163\n",
      "  Batch [680/1299] D_loss: -1.5340, G_loss: 4.9201\n",
      "  Batch [690/1299] D_loss: -2.8738, G_loss: 4.4670\n",
      "  Batch [700/1299] D_loss: -3.1689, G_loss: 5.6867\n",
      "  Batch [710/1299] D_loss: -3.3486, G_loss: 3.2361\n",
      "  Batch [720/1299] D_loss: -3.6886, G_loss: 5.9572\n",
      "  Batch [730/1299] D_loss: -3.1127, G_loss: 6.7332\n",
      "  Batch [740/1299] D_loss: -2.8905, G_loss: 6.9801\n",
      "  Batch [750/1299] D_loss: -4.5209, G_loss: 6.6781\n",
      "  Batch [760/1299] D_loss: -3.4204, G_loss: 2.6717\n",
      "  Batch [770/1299] D_loss: -2.5917, G_loss: 5.5085\n",
      "  Batch [780/1299] D_loss: -2.6345, G_loss: 5.0038\n",
      "  Batch [790/1299] D_loss: -4.2332, G_loss: 4.1758\n",
      "  Batch [800/1299] D_loss: -4.6209, G_loss: 3.6055\n",
      "  Batch [810/1299] D_loss: -3.9391, G_loss: 3.2555\n",
      "  Batch [820/1299] D_loss: -3.7052, G_loss: 4.1427\n",
      "  Batch [830/1299] D_loss: -2.5142, G_loss: 5.1410\n",
      "  Batch [840/1299] D_loss: -3.4897, G_loss: 1.8634\n",
      "  Batch [850/1299] D_loss: -2.2051, G_loss: 5.4948\n",
      "  Batch [860/1299] D_loss: -3.0639, G_loss: 4.4298\n",
      "  Batch [870/1299] D_loss: -3.0720, G_loss: 5.9535\n",
      "  Batch [880/1299] D_loss: -2.8226, G_loss: 4.4596\n",
      "  Batch [890/1299] D_loss: -4.3813, G_loss: 3.4907\n",
      "  Batch [900/1299] D_loss: -3.2459, G_loss: 5.8615\n",
      "  Batch [910/1299] D_loss: -3.8559, G_loss: 5.8655\n",
      "  Batch [920/1299] D_loss: -3.7702, G_loss: 5.7469\n",
      "  Batch [930/1299] D_loss: -3.2039, G_loss: 3.0499\n",
      "  Batch [940/1299] D_loss: -2.4001, G_loss: 2.7323\n",
      "  Batch [950/1299] D_loss: -2.0710, G_loss: 1.8202\n",
      "  Batch [960/1299] D_loss: -2.0387, G_loss: 4.4157\n",
      "  Batch [970/1299] D_loss: -2.7275, G_loss: 3.6673\n",
      "  Batch [980/1299] D_loss: -2.0765, G_loss: 3.9363\n",
      "  Batch [990/1299] D_loss: -4.0493, G_loss: 3.0182\n",
      "  Batch [1000/1299] D_loss: -2.3764, G_loss: 4.6838\n",
      "  Batch [1010/1299] D_loss: -3.2853, G_loss: 5.6081\n",
      "  Batch [1020/1299] D_loss: -3.5985, G_loss: 3.9007\n",
      "  Batch [1030/1299] D_loss: -2.9341, G_loss: 4.7706\n",
      "  Batch [1040/1299] D_loss: -2.3842, G_loss: 4.5793\n",
      "  Batch [1050/1299] D_loss: -3.0102, G_loss: 3.1569\n",
      "  Batch [1060/1299] D_loss: -3.3886, G_loss: 4.6600\n",
      "  Batch [1070/1299] D_loss: -2.8860, G_loss: 3.5321\n",
      "  Batch [1080/1299] D_loss: -3.0079, G_loss: 5.0036\n",
      "  Batch [1090/1299] D_loss: -2.6808, G_loss: 3.2748\n",
      "  Batch [1100/1299] D_loss: -3.4599, G_loss: 4.0475\n",
      "  Batch [1110/1299] D_loss: -3.1701, G_loss: 6.0166\n",
      "  Batch [1120/1299] D_loss: -3.8590, G_loss: 7.2060\n",
      "  Batch [1130/1299] D_loss: -3.3251, G_loss: 6.1458\n",
      "  Batch [1140/1299] D_loss: -3.2011, G_loss: 3.8830\n",
      "  Batch [1150/1299] D_loss: -2.1650, G_loss: 3.5203\n",
      "  Batch [1160/1299] D_loss: -2.4143, G_loss: 3.7989\n",
      "  Batch [1170/1299] D_loss: -3.1009, G_loss: 3.5855\n",
      "  Batch [1180/1299] D_loss: -2.4738, G_loss: 3.9328\n",
      "  Batch [1190/1299] D_loss: -2.7110, G_loss: 3.6324\n",
      "  Batch [1200/1299] D_loss: -3.5239, G_loss: 3.2113\n",
      "  Batch [1210/1299] D_loss: -4.6555, G_loss: 3.2564\n",
      "  Batch [1220/1299] D_loss: -2.4646, G_loss: 3.5130\n",
      "  Batch [1230/1299] D_loss: -4.1448, G_loss: 4.1862\n",
      "  Batch [1240/1299] D_loss: -3.9405, G_loss: 5.4058\n",
      "  Batch [1250/1299] D_loss: -2.9530, G_loss: 4.0511\n",
      "  Batch [1260/1299] D_loss: -3.4899, G_loss: 2.5010\n",
      "  Batch [1270/1299] D_loss: -2.0567, G_loss: 1.6600\n",
      "  Batch [1280/1299] D_loss: -2.3789, G_loss: 3.2650\n",
      "  Batch [1290/1299] D_loss: -2.7149, G_loss: 2.2024\n",
      "\n",
      "Epoch 17 Summary:\n",
      "  Average D_loss: -2.5418\n",
      "  Average G_loss: 4.1718\n",
      "\n",
      "Epoch [18/100]\n",
      "  Batch [0/1299] D_loss: -2.2518, G_loss: 3.2651\n",
      "  Batch [10/1299] D_loss: -2.9889, G_loss: 3.6701\n",
      "  Batch [20/1299] D_loss: -2.1080, G_loss: 4.0385\n",
      "  Batch [30/1299] D_loss: -1.2383, G_loss: 2.1600\n",
      "  Batch [40/1299] D_loss: -4.2564, G_loss: 4.0524\n",
      "  Batch [50/1299] D_loss: -1.9233, G_loss: 4.1821\n",
      "  Batch [60/1299] D_loss: -4.3381, G_loss: 2.5920\n",
      "  Batch [70/1299] D_loss: -3.8910, G_loss: 2.6837\n",
      "  Batch [80/1299] D_loss: -2.6906, G_loss: 4.0989\n",
      "  Batch [90/1299] D_loss: -5.3673, G_loss: 4.0981\n",
      "  Batch [100/1299] D_loss: -1.3956, G_loss: 2.5824\n",
      "  Batch [110/1299] D_loss: -2.7798, G_loss: 4.8247\n",
      "  Batch [120/1299] D_loss: -2.7603, G_loss: 4.9725\n",
      "  Batch [130/1299] D_loss: -4.9580, G_loss: 4.3266\n",
      "  Batch [140/1299] D_loss: -3.0578, G_loss: 3.4627\n",
      "  Batch [150/1299] D_loss: -2.9943, G_loss: 3.8284\n",
      "  Batch [160/1299] D_loss: -2.2928, G_loss: 5.0184\n",
      "  Batch [170/1299] D_loss: -3.2892, G_loss: 4.0500\n",
      "  Batch [180/1299] D_loss: -2.9978, G_loss: 3.7395\n",
      "  Batch [190/1299] D_loss: -2.7581, G_loss: 2.9594\n",
      "  Batch [200/1299] D_loss: -2.3688, G_loss: 1.6285\n",
      "  Batch [210/1299] D_loss: -4.0268, G_loss: 3.8814\n",
      "  Batch [220/1299] D_loss: -1.7788, G_loss: 3.0072\n",
      "  Batch [230/1299] D_loss: -3.1919, G_loss: 5.0139\n",
      "  Batch [240/1299] D_loss: -3.7247, G_loss: 3.1025\n",
      "  Batch [250/1299] D_loss: -2.3449, G_loss: 3.7952\n",
      "  Batch [260/1299] D_loss: -2.7167, G_loss: 2.6417\n",
      "  Batch [270/1299] D_loss: -2.6981, G_loss: 0.8579\n",
      "  Batch [280/1299] D_loss: -2.8070, G_loss: 3.6715\n",
      "  Batch [290/1299] D_loss: -2.3084, G_loss: 3.1135\n",
      "  Batch [300/1299] D_loss: -2.6753, G_loss: 3.0360\n",
      "  Batch [310/1299] D_loss: -2.0971, G_loss: 4.1596\n",
      "  Batch [320/1299] D_loss: -3.8048, G_loss: 4.6662\n",
      "  Batch [330/1299] D_loss: -3.7931, G_loss: 2.5413\n",
      "  Batch [340/1299] D_loss: -3.8085, G_loss: 4.0267\n",
      "  Batch [350/1299] D_loss: -3.4263, G_loss: 2.7796\n",
      "  Batch [360/1299] D_loss: -3.1020, G_loss: 3.2060\n",
      "  Batch [370/1299] D_loss: -2.7198, G_loss: 4.3325\n",
      "  Batch [380/1299] D_loss: -3.0456, G_loss: 2.8800\n",
      "  Batch [390/1299] D_loss: -1.8391, G_loss: 3.7558\n",
      "  Batch [400/1299] D_loss: -3.8950, G_loss: 3.3590\n",
      "  Batch [410/1299] D_loss: -2.0810, G_loss: 4.0024\n",
      "  Batch [420/1299] D_loss: -2.3096, G_loss: 4.7621\n",
      "  Batch [430/1299] D_loss: -2.5566, G_loss: 6.5042\n",
      "  Batch [440/1299] D_loss: -2.4287, G_loss: 5.0476\n",
      "  Batch [450/1299] D_loss: -3.7211, G_loss: 4.1177\n",
      "  Batch [460/1299] D_loss: -2.6315, G_loss: 5.6360\n",
      "  Batch [470/1299] D_loss: -4.6843, G_loss: 3.7237\n",
      "  Batch [480/1299] D_loss: -3.6926, G_loss: 3.6183\n",
      "  Batch [490/1299] D_loss: -3.6664, G_loss: 4.1360\n",
      "  Batch [500/1299] D_loss: -2.9177, G_loss: 3.8225\n",
      "  Batch [510/1299] D_loss: -2.5019, G_loss: 2.4343\n",
      "  Batch [520/1299] D_loss: -2.5340, G_loss: 1.1559\n",
      "  Batch [530/1299] D_loss: -2.3849, G_loss: 3.5682\n",
      "  Batch [540/1299] D_loss: -3.2279, G_loss: 2.9726\n",
      "  Batch [550/1299] D_loss: -3.3815, G_loss: 2.8578\n",
      "  Batch [560/1299] D_loss: -3.6448, G_loss: 3.0494\n",
      "  Batch [570/1299] D_loss: -2.5406, G_loss: 2.6094\n",
      "  Batch [580/1299] D_loss: -3.1437, G_loss: 3.5338\n",
      "  Batch [590/1299] D_loss: -2.4554, G_loss: 4.7190\n",
      "  Batch [600/1299] D_loss: -1.9647, G_loss: 3.3146\n",
      "  Batch [610/1299] D_loss: -3.6060, G_loss: 4.6758\n",
      "  Batch [620/1299] D_loss: -1.4779, G_loss: 3.0284\n",
      "  Batch [630/1299] D_loss: -3.0046, G_loss: 3.6328\n",
      "  Batch [640/1299] D_loss: -3.8147, G_loss: 4.9204\n",
      "  Batch [650/1299] D_loss: -2.7955, G_loss: 2.7001\n",
      "  Batch [660/1299] D_loss: -3.9918, G_loss: 3.7610\n",
      "  Batch [670/1299] D_loss: -1.3758, G_loss: 3.8487\n",
      "  Batch [680/1299] D_loss: -0.7875, G_loss: 3.1822\n",
      "  Batch [690/1299] D_loss: -3.0676, G_loss: 2.2074\n",
      "  Batch [700/1299] D_loss: -4.5407, G_loss: 0.8537\n",
      "  Batch [710/1299] D_loss: -2.4442, G_loss: 2.3601\n",
      "  Batch [720/1299] D_loss: -3.2221, G_loss: 1.7836\n",
      "  Batch [730/1299] D_loss: -3.9284, G_loss: 1.9842\n",
      "  Batch [740/1299] D_loss: -4.0190, G_loss: 3.9498\n",
      "  Batch [750/1299] D_loss: -2.6693, G_loss: 0.2859\n",
      "  Batch [760/1299] D_loss: -2.2452, G_loss: 1.4920\n",
      "  Batch [770/1299] D_loss: -3.3511, G_loss: 2.3629\n",
      "  Batch [780/1299] D_loss: -4.0861, G_loss: 6.4480\n",
      "  Batch [790/1299] D_loss: -2.8427, G_loss: 4.1195\n",
      "  Batch [800/1299] D_loss: -2.3045, G_loss: 1.8652\n",
      "  Batch [810/1299] D_loss: -2.5428, G_loss: 3.2916\n",
      "  Batch [820/1299] D_loss: -3.3687, G_loss: 3.4984\n",
      "  Batch [830/1299] D_loss: -3.5649, G_loss: 3.7720\n",
      "  Batch [840/1299] D_loss: -2.8714, G_loss: 4.7835\n",
      "  Batch [850/1299] D_loss: -2.8951, G_loss: 2.8724\n",
      "  Batch [860/1299] D_loss: -3.8404, G_loss: 5.1854\n",
      "  Batch [870/1299] D_loss: -3.1233, G_loss: 2.4341\n",
      "  Batch [880/1299] D_loss: -2.6094, G_loss: 3.2102\n",
      "  Batch [890/1299] D_loss: -2.6725, G_loss: 2.4623\n",
      "  Batch [900/1299] D_loss: -2.6154, G_loss: 2.4187\n",
      "  Batch [910/1299] D_loss: -3.3478, G_loss: 2.1999\n",
      "  Batch [920/1299] D_loss: -3.7281, G_loss: 2.9759\n",
      "  Batch [930/1299] D_loss: -1.9579, G_loss: 3.7540\n",
      "  Batch [940/1299] D_loss: -3.7450, G_loss: 3.1803\n",
      "  Batch [950/1299] D_loss: -2.3162, G_loss: 3.5157\n",
      "  Batch [960/1299] D_loss: -2.5766, G_loss: 3.1281\n",
      "  Batch [970/1299] D_loss: -1.9831, G_loss: 4.4099\n",
      "  Batch [980/1299] D_loss: -2.8010, G_loss: 2.7240\n",
      "  Batch [990/1299] D_loss: -2.9140, G_loss: 2.9622\n",
      "  Batch [1000/1299] D_loss: -3.1191, G_loss: 4.1739\n",
      "  Batch [1010/1299] D_loss: -3.7132, G_loss: 3.4723\n",
      "  Batch [1020/1299] D_loss: -3.2568, G_loss: 2.6323\n",
      "  Batch [1030/1299] D_loss: -2.2697, G_loss: 3.4274\n",
      "  Batch [1040/1299] D_loss: -3.5779, G_loss: 4.5657\n",
      "  Batch [1050/1299] D_loss: -2.6158, G_loss: 4.2368\n",
      "  Batch [1060/1299] D_loss: -3.2879, G_loss: 2.7592\n",
      "  Batch [1070/1299] D_loss: -2.3960, G_loss: 2.6559\n",
      "  Batch [1080/1299] D_loss: -4.2624, G_loss: 3.8010\n",
      "  Batch [1090/1299] D_loss: -2.1014, G_loss: 2.7995\n",
      "  Batch [1100/1299] D_loss: -3.4218, G_loss: 3.2416\n",
      "  Batch [1110/1299] D_loss: -2.9584, G_loss: 3.1265\n",
      "  Batch [1120/1299] D_loss: -1.9656, G_loss: 5.0958\n",
      "  Batch [1130/1299] D_loss: -2.8411, G_loss: 4.4707\n",
      "  Batch [1140/1299] D_loss: -2.4492, G_loss: 5.0215\n",
      "  Batch [1150/1299] D_loss: -3.5120, G_loss: 5.8044\n",
      "  Batch [1160/1299] D_loss: -3.9259, G_loss: 4.2272\n",
      "  Batch [1170/1299] D_loss: -2.6018, G_loss: 2.9153\n",
      "  Batch [1180/1299] D_loss: -2.2749, G_loss: 4.4273\n",
      "  Batch [1190/1299] D_loss: -2.6670, G_loss: 3.7050\n",
      "  Batch [1200/1299] D_loss: -2.5213, G_loss: 4.3707\n",
      "  Batch [1210/1299] D_loss: -2.5627, G_loss: 2.8129\n",
      "  Batch [1220/1299] D_loss: -2.0444, G_loss: 4.2986\n",
      "  Batch [1230/1299] D_loss: -1.5262, G_loss: 4.5891\n",
      "  Batch [1240/1299] D_loss: -4.1701, G_loss: 3.3912\n",
      "  Batch [1250/1299] D_loss: -2.7561, G_loss: 2.9860\n",
      "  Batch [1260/1299] D_loss: -2.8091, G_loss: 1.5517\n",
      "  Batch [1270/1299] D_loss: -2.9340, G_loss: 3.5371\n",
      "  Batch [1280/1299] D_loss: -2.8875, G_loss: 3.6602\n",
      "  Batch [1290/1299] D_loss: -3.5245, G_loss: 4.3633\n",
      "\n",
      "Epoch 18 Summary:\n",
      "  Average D_loss: -2.4961\n",
      "  Average G_loss: 3.5536\n",
      "\n",
      "Epoch [19/100]\n",
      "  Batch [0/1299] D_loss: -3.6374, G_loss: 4.2468\n",
      "  Batch [10/1299] D_loss: -3.3616, G_loss: 6.1904\n",
      "  Batch [20/1299] D_loss: -1.8025, G_loss: 4.5074\n",
      "  Batch [30/1299] D_loss: -2.2596, G_loss: 4.7174\n",
      "  Batch [40/1299] D_loss: -2.2833, G_loss: 5.6190\n",
      "  Batch [50/1299] D_loss: -2.6695, G_loss: 4.7857\n",
      "  Batch [60/1299] D_loss: -3.2150, G_loss: 3.9241\n",
      "  Batch [70/1299] D_loss: -4.3440, G_loss: 4.3377\n",
      "  Batch [80/1299] D_loss: -2.3042, G_loss: 4.4170\n",
      "  Batch [90/1299] D_loss: -2.5545, G_loss: 2.9380\n",
      "  Batch [100/1299] D_loss: -0.6580, G_loss: 4.4670\n",
      "  Batch [110/1299] D_loss: -2.4844, G_loss: 4.7261\n",
      "  Batch [120/1299] D_loss: -2.1234, G_loss: 5.9119\n",
      "  Batch [130/1299] D_loss: -1.6063, G_loss: 4.8405\n",
      "  Batch [140/1299] D_loss: -2.4723, G_loss: 6.4315\n",
      "  Batch [150/1299] D_loss: -3.9732, G_loss: 5.3288\n",
      "  Batch [160/1299] D_loss: -2.8522, G_loss: 5.1337\n",
      "  Batch [170/1299] D_loss: -2.4360, G_loss: 1.7592\n",
      "  Batch [180/1299] D_loss: -2.9637, G_loss: 4.5477\n",
      "  Batch [190/1299] D_loss: -1.8778, G_loss: 5.1829\n",
      "  Batch [200/1299] D_loss: -2.2981, G_loss: 5.4755\n",
      "  Batch [210/1299] D_loss: -2.8304, G_loss: 4.1377\n",
      "  Batch [220/1299] D_loss: -1.8188, G_loss: 2.6057\n",
      "  Batch [230/1299] D_loss: -3.1504, G_loss: 2.7137\n",
      "  Batch [240/1299] D_loss: -3.0584, G_loss: 1.9923\n",
      "  Batch [250/1299] D_loss: -2.0358, G_loss: 2.7785\n",
      "  Batch [260/1299] D_loss: -2.6776, G_loss: 3.5055\n",
      "  Batch [270/1299] D_loss: -3.6059, G_loss: 3.8372\n",
      "  Batch [280/1299] D_loss: -2.6610, G_loss: 2.9463\n",
      "  Batch [290/1299] D_loss: -3.2843, G_loss: 1.3394\n",
      "  Batch [300/1299] D_loss: -4.1928, G_loss: 4.1056\n",
      "  Batch [310/1299] D_loss: -2.5746, G_loss: 2.7291\n",
      "  Batch [320/1299] D_loss: -2.6627, G_loss: 4.3773\n",
      "  Batch [330/1299] D_loss: -3.6847, G_loss: 4.2677\n",
      "  Batch [340/1299] D_loss: -3.3379, G_loss: 2.5433\n",
      "  Batch [350/1299] D_loss: -2.0623, G_loss: 0.8979\n",
      "  Batch [360/1299] D_loss: -1.7673, G_loss: 2.5047\n",
      "  Batch [370/1299] D_loss: -1.8002, G_loss: 3.7298\n",
      "  Batch [380/1299] D_loss: -2.1354, G_loss: 2.5277\n",
      "  Batch [390/1299] D_loss: -2.9068, G_loss: 3.1448\n",
      "  Batch [400/1299] D_loss: -2.9755, G_loss: 1.2488\n",
      "  Batch [410/1299] D_loss: -3.5127, G_loss: 2.2047\n",
      "  Batch [420/1299] D_loss: -2.1148, G_loss: 2.3040\n",
      "  Batch [430/1299] D_loss: -2.3226, G_loss: 4.0368\n",
      "  Batch [440/1299] D_loss: -3.4253, G_loss: 3.7261\n",
      "  Batch [450/1299] D_loss: -1.5143, G_loss: 4.5197\n",
      "  Batch [460/1299] D_loss: -4.0047, G_loss: 4.4816\n",
      "  Batch [470/1299] D_loss: -2.8007, G_loss: 5.4146\n",
      "  Batch [480/1299] D_loss: -3.8237, G_loss: 5.0277\n",
      "  Batch [490/1299] D_loss: -2.6873, G_loss: 6.7839\n",
      "  Batch [500/1299] D_loss: -3.4123, G_loss: 4.9850\n",
      "  Batch [510/1299] D_loss: -3.4052, G_loss: 4.0104\n",
      "  Batch [520/1299] D_loss: -2.2023, G_loss: 4.0943\n",
      "  Batch [530/1299] D_loss: -1.4941, G_loss: 5.3362\n",
      "  Batch [540/1299] D_loss: -3.9578, G_loss: 3.0034\n",
      "  Batch [550/1299] D_loss: -4.4495, G_loss: 2.7988\n",
      "  Batch [560/1299] D_loss: -1.4362, G_loss: 2.6042\n",
      "  Batch [570/1299] D_loss: -3.7500, G_loss: 3.1504\n",
      "  Batch [580/1299] D_loss: -2.3284, G_loss: 2.1836\n",
      "  Batch [590/1299] D_loss: -3.0877, G_loss: 5.3062\n",
      "  Batch [600/1299] D_loss: -4.3891, G_loss: 3.5036\n",
      "  Batch [610/1299] D_loss: -3.0977, G_loss: 3.3721\n",
      "  Batch [620/1299] D_loss: -2.6936, G_loss: 2.4311\n",
      "  Batch [630/1299] D_loss: -3.5308, G_loss: 2.9838\n",
      "  Batch [640/1299] D_loss: -3.0522, G_loss: 3.3244\n",
      "  Batch [650/1299] D_loss: -1.6494, G_loss: 5.0757\n",
      "  Batch [660/1299] D_loss: -2.3896, G_loss: 5.6914\n",
      "  Batch [670/1299] D_loss: -1.6281, G_loss: 4.6317\n",
      "  Batch [680/1299] D_loss: -2.3922, G_loss: 4.3334\n",
      "  Batch [690/1299] D_loss: -3.2651, G_loss: 4.2608\n",
      "  Batch [700/1299] D_loss: -3.7000, G_loss: 2.3655\n",
      "  Batch [710/1299] D_loss: -3.6547, G_loss: 4.5815\n",
      "  Batch [720/1299] D_loss: -3.5647, G_loss: 3.0747\n",
      "  Batch [730/1299] D_loss: -2.5261, G_loss: 3.7080\n",
      "  Batch [740/1299] D_loss: -2.2036, G_loss: 1.0569\n",
      "  Batch [750/1299] D_loss: -3.2073, G_loss: 1.4390\n",
      "  Batch [760/1299] D_loss: -2.0823, G_loss: 1.7867\n",
      "  Batch [770/1299] D_loss: -2.7620, G_loss: 4.6329\n",
      "  Batch [780/1299] D_loss: -1.5221, G_loss: 5.0198\n",
      "  Batch [790/1299] D_loss: -2.4571, G_loss: 4.2896\n",
      "  Batch [800/1299] D_loss: -1.2331, G_loss: 2.8292\n",
      "  Batch [810/1299] D_loss: -4.1064, G_loss: 3.8567\n",
      "  Batch [820/1299] D_loss: -2.7171, G_loss: 3.8263\n",
      "  Batch [830/1299] D_loss: -2.4676, G_loss: 2.6552\n",
      "  Batch [840/1299] D_loss: -3.0883, G_loss: 5.1873\n",
      "  Batch [850/1299] D_loss: -1.9080, G_loss: 3.2241\n",
      "  Batch [860/1299] D_loss: -3.1274, G_loss: 3.9690\n",
      "  Batch [870/1299] D_loss: -2.0448, G_loss: 2.1676\n",
      "  Batch [880/1299] D_loss: -2.5119, G_loss: 4.7047\n",
      "  Batch [890/1299] D_loss: -3.4063, G_loss: 3.1161\n",
      "  Batch [900/1299] D_loss: -3.0640, G_loss: 2.9163\n",
      "  Batch [910/1299] D_loss: -3.3402, G_loss: 3.3659\n",
      "  Batch [920/1299] D_loss: -1.9927, G_loss: 2.2495\n",
      "  Batch [930/1299] D_loss: -4.3647, G_loss: 0.8405\n",
      "  Batch [940/1299] D_loss: -3.3360, G_loss: 5.3957\n",
      "  Batch [950/1299] D_loss: -4.4668, G_loss: 2.8929\n",
      "  Batch [960/1299] D_loss: -3.2690, G_loss: 2.1240\n",
      "  Batch [970/1299] D_loss: -0.3841, G_loss: 1.9150\n",
      "  Batch [980/1299] D_loss: -2.5737, G_loss: 3.7649\n",
      "  Batch [990/1299] D_loss: -2.2567, G_loss: 3.9414\n",
      "  Batch [1000/1299] D_loss: -3.0382, G_loss: 3.1730\n",
      "  Batch [1010/1299] D_loss: -0.9068, G_loss: 3.5456\n",
      "  Batch [1020/1299] D_loss: -2.2204, G_loss: 2.0405\n",
      "  Batch [1030/1299] D_loss: -3.1672, G_loss: 2.5771\n",
      "  Batch [1040/1299] D_loss: -4.7486, G_loss: 3.8070\n",
      "  Batch [1050/1299] D_loss: -1.3847, G_loss: 4.4672\n",
      "  Batch [1060/1299] D_loss: -2.2280, G_loss: 5.2295\n",
      "  Batch [1070/1299] D_loss: -2.6979, G_loss: 5.2987\n",
      "  Batch [1080/1299] D_loss: -1.9499, G_loss: 5.0374\n",
      "  Batch [1090/1299] D_loss: -1.6343, G_loss: 3.7615\n",
      "  Batch [1100/1299] D_loss: -2.5530, G_loss: 2.4468\n",
      "  Batch [1110/1299] D_loss: -2.6172, G_loss: 6.0004\n",
      "  Batch [1120/1299] D_loss: -2.7355, G_loss: 4.6666\n",
      "  Batch [1130/1299] D_loss: -2.2798, G_loss: 4.7491\n",
      "  Batch [1140/1299] D_loss: -0.8548, G_loss: 3.2598\n",
      "  Batch [1150/1299] D_loss: -1.3894, G_loss: 4.6531\n",
      "  Batch [1160/1299] D_loss: -2.5678, G_loss: 4.7023\n",
      "  Batch [1170/1299] D_loss: -1.9944, G_loss: 5.7648\n",
      "  Batch [1180/1299] D_loss: -1.4270, G_loss: 5.9475\n",
      "  Batch [1190/1299] D_loss: -4.0322, G_loss: 2.3444\n",
      "  Batch [1200/1299] D_loss: -2.0383, G_loss: 3.0438\n",
      "  Batch [1210/1299] D_loss: -3.4849, G_loss: 4.3343\n",
      "  Batch [1220/1299] D_loss: -3.1840, G_loss: 4.5509\n",
      "  Batch [1230/1299] D_loss: -3.4211, G_loss: 4.5591\n",
      "  Batch [1240/1299] D_loss: -2.4841, G_loss: 4.8783\n",
      "  Batch [1250/1299] D_loss: -2.9864, G_loss: 4.4987\n",
      "  Batch [1260/1299] D_loss: -1.8874, G_loss: 7.3027\n",
      "  Batch [1270/1299] D_loss: -3.6841, G_loss: 5.5573\n",
      "  Batch [1280/1299] D_loss: -4.1476, G_loss: 3.9107\n",
      "  Batch [1290/1299] D_loss: -4.0922, G_loss: 2.5306\n",
      "\n",
      "Epoch 19 Summary:\n",
      "  Average D_loss: -2.4671\n",
      "  Average G_loss: 3.9748\n",
      "\n",
      "Epoch [20/100]\n",
      "  Batch [0/1299] D_loss: -3.7848, G_loss: 3.1601\n",
      "  Batch [10/1299] D_loss: -3.4883, G_loss: 5.8628\n",
      "  Batch [20/1299] D_loss: -2.4339, G_loss: 4.5749\n",
      "  Batch [30/1299] D_loss: -3.1042, G_loss: 3.1762\n",
      "  Batch [40/1299] D_loss: -1.7101, G_loss: 4.6842\n",
      "  Batch [50/1299] D_loss: -2.4972, G_loss: 2.9663\n",
      "  Batch [60/1299] D_loss: -2.7504, G_loss: 3.5851\n",
      "  Batch [70/1299] D_loss: -1.5354, G_loss: 4.0320\n",
      "  Batch [80/1299] D_loss: -2.3918, G_loss: 4.4441\n",
      "  Batch [90/1299] D_loss: -0.9022, G_loss: 3.9770\n",
      "  Batch [100/1299] D_loss: -3.5158, G_loss: 4.4106\n",
      "  Batch [110/1299] D_loss: -2.1599, G_loss: 4.6013\n",
      "  Batch [120/1299] D_loss: -2.9325, G_loss: 5.0851\n",
      "  Batch [130/1299] D_loss: -3.2806, G_loss: 2.8130\n",
      "  Batch [140/1299] D_loss: -3.8601, G_loss: 3.4569\n",
      "  Batch [150/1299] D_loss: -4.6382, G_loss: 5.0460\n",
      "  Batch [160/1299] D_loss: -3.2855, G_loss: 5.9985\n",
      "  Batch [170/1299] D_loss: -3.5763, G_loss: 3.1566\n",
      "  Batch [180/1299] D_loss: -2.8871, G_loss: 3.1603\n",
      "  Batch [190/1299] D_loss: -2.4874, G_loss: 3.9320\n",
      "  Batch [200/1299] D_loss: -1.7068, G_loss: 3.1218\n",
      "  Batch [210/1299] D_loss: -3.5057, G_loss: 3.8151\n",
      "  Batch [220/1299] D_loss: -3.3600, G_loss: 2.5707\n",
      "  Batch [230/1299] D_loss: -3.4933, G_loss: 3.3418\n",
      "  Batch [240/1299] D_loss: -1.5328, G_loss: 3.5471\n",
      "  Batch [250/1299] D_loss: -3.7230, G_loss: 5.3046\n",
      "  Batch [260/1299] D_loss: -3.6198, G_loss: 3.5568\n",
      "  Batch [270/1299] D_loss: -2.8741, G_loss: 3.6784\n",
      "  Batch [280/1299] D_loss: -2.8046, G_loss: 4.7817\n",
      "  Batch [290/1299] D_loss: -2.5022, G_loss: 3.0683\n",
      "  Batch [300/1299] D_loss: -2.5487, G_loss: 5.8304\n",
      "  Batch [310/1299] D_loss: -3.1535, G_loss: 3.9583\n",
      "  Batch [320/1299] D_loss: -3.7433, G_loss: 4.0396\n",
      "  Batch [330/1299] D_loss: -3.2668, G_loss: 3.7642\n",
      "  Batch [340/1299] D_loss: -3.9055, G_loss: 3.5347\n",
      "  Batch [350/1299] D_loss: -2.8273, G_loss: 5.1988\n",
      "  Batch [360/1299] D_loss: -3.9498, G_loss: 1.8089\n",
      "  Batch [370/1299] D_loss: -2.8464, G_loss: 2.0840\n",
      "  Batch [380/1299] D_loss: -1.2154, G_loss: 3.3357\n",
      "  Batch [390/1299] D_loss: -3.2485, G_loss: 2.0549\n",
      "  Batch [400/1299] D_loss: -2.5501, G_loss: 3.9226\n",
      "  Batch [410/1299] D_loss: -3.2412, G_loss: 4.2551\n",
      "  Batch [420/1299] D_loss: -2.7080, G_loss: 3.2324\n",
      "  Batch [430/1299] D_loss: -3.2619, G_loss: 3.3984\n",
      "  Batch [440/1299] D_loss: -1.9634, G_loss: 3.5524\n",
      "  Batch [450/1299] D_loss: -2.7504, G_loss: 2.6772\n",
      "  Batch [460/1299] D_loss: -3.7367, G_loss: 2.9947\n",
      "  Batch [470/1299] D_loss: -1.3390, G_loss: 4.8858\n",
      "  Batch [480/1299] D_loss: -3.0570, G_loss: 3.8621\n",
      "  Batch [490/1299] D_loss: -2.7695, G_loss: 4.4060\n",
      "  Batch [500/1299] D_loss: -2.3715, G_loss: 4.5928\n",
      "  Batch [510/1299] D_loss: -2.7532, G_loss: 2.9707\n",
      "  Batch [520/1299] D_loss: -3.6858, G_loss: 2.7517\n",
      "  Batch [530/1299] D_loss: -2.7628, G_loss: 2.2953\n",
      "  Batch [540/1299] D_loss: -3.1198, G_loss: 3.9143\n",
      "  Batch [550/1299] D_loss: -1.5759, G_loss: 5.4266\n",
      "  Batch [560/1299] D_loss: -3.2530, G_loss: 5.3657\n",
      "  Batch [570/1299] D_loss: -3.7747, G_loss: 4.1813\n",
      "  Batch [580/1299] D_loss: -2.9420, G_loss: 4.3618\n",
      "  Batch [590/1299] D_loss: -1.8682, G_loss: 2.0546\n",
      "  Batch [600/1299] D_loss: -2.2238, G_loss: 3.1189\n",
      "  Batch [610/1299] D_loss: -2.1770, G_loss: 2.5912\n",
      "  Batch [620/1299] D_loss: -3.1165, G_loss: 3.6799\n",
      "  Batch [630/1299] D_loss: -1.9587, G_loss: 4.7077\n",
      "  Batch [640/1299] D_loss: -3.4101, G_loss: 4.6510\n",
      "  Batch [650/1299] D_loss: -2.2719, G_loss: 4.7916\n",
      "  Batch [660/1299] D_loss: -2.3115, G_loss: 2.7074\n",
      "  Batch [670/1299] D_loss: -2.1773, G_loss: 3.3181\n",
      "  Batch [680/1299] D_loss: -1.7051, G_loss: 2.8786\n",
      "  Batch [690/1299] D_loss: -2.1634, G_loss: 3.4309\n",
      "  Batch [700/1299] D_loss: -2.2007, G_loss: 2.6742\n",
      "  Batch [710/1299] D_loss: -2.6391, G_loss: 2.0674\n",
      "  Batch [720/1299] D_loss: -3.1329, G_loss: 3.1263\n",
      "  Batch [730/1299] D_loss: -3.3226, G_loss: 3.1296\n",
      "  Batch [740/1299] D_loss: -3.8032, G_loss: 3.5556\n",
      "  Batch [750/1299] D_loss: -3.3822, G_loss: 3.3134\n",
      "  Batch [760/1299] D_loss: -4.7308, G_loss: 3.4384\n",
      "  Batch [770/1299] D_loss: -3.7315, G_loss: 3.0970\n",
      "  Batch [780/1299] D_loss: -2.2264, G_loss: 4.5753\n",
      "  Batch [790/1299] D_loss: -2.7152, G_loss: 3.1754\n",
      "  Batch [800/1299] D_loss: -3.3632, G_loss: 2.4169\n",
      "  Batch [810/1299] D_loss: -3.2767, G_loss: 2.8580\n",
      "  Batch [820/1299] D_loss: -2.2173, G_loss: 5.4434\n",
      "  Batch [830/1299] D_loss: -2.5107, G_loss: 4.7146\n",
      "  Batch [840/1299] D_loss: -1.2851, G_loss: 2.2522\n",
      "  Batch [850/1299] D_loss: -2.0630, G_loss: 3.7337\n",
      "  Batch [860/1299] D_loss: -1.8542, G_loss: 2.9757\n",
      "  Batch [870/1299] D_loss: -3.5315, G_loss: 2.8341\n",
      "  Batch [880/1299] D_loss: -2.9587, G_loss: 5.4270\n",
      "  Batch [890/1299] D_loss: -3.2566, G_loss: 5.3097\n",
      "  Batch [900/1299] D_loss: -2.5503, G_loss: 4.0352\n",
      "  Batch [910/1299] D_loss: -2.6574, G_loss: 5.1471\n",
      "  Batch [920/1299] D_loss: -2.9915, G_loss: 2.4900\n",
      "  Batch [930/1299] D_loss: -2.2072, G_loss: 1.9541\n",
      "  Batch [940/1299] D_loss: -3.5164, G_loss: 3.0691\n",
      "  Batch [950/1299] D_loss: -1.0267, G_loss: 5.2591\n",
      "  Batch [960/1299] D_loss: -1.6683, G_loss: 3.8762\n",
      "  Batch [970/1299] D_loss: -2.9705, G_loss: 3.9432\n",
      "  Batch [980/1299] D_loss: -2.1399, G_loss: 2.3283\n",
      "  Batch [990/1299] D_loss: -3.2635, G_loss: 1.4797\n",
      "  Batch [1000/1299] D_loss: -2.5181, G_loss: 1.4899\n",
      "  Batch [1010/1299] D_loss: -3.5866, G_loss: 3.0061\n",
      "  Batch [1020/1299] D_loss: -2.5318, G_loss: 5.1146\n",
      "  Batch [1030/1299] D_loss: -2.5782, G_loss: 2.9995\n",
      "  Batch [1040/1299] D_loss: -3.3587, G_loss: 4.6028\n",
      "  Batch [1050/1299] D_loss: -1.7829, G_loss: 5.4190\n",
      "  Batch [1060/1299] D_loss: -2.8664, G_loss: 3.4107\n",
      "  Batch [1070/1299] D_loss: -3.5478, G_loss: 1.9017\n",
      "  Batch [1080/1299] D_loss: -1.0975, G_loss: 3.2013\n",
      "  Batch [1090/1299] D_loss: -3.9729, G_loss: 3.9054\n",
      "  Batch [1100/1299] D_loss: -2.7812, G_loss: 3.8465\n",
      "  Batch [1110/1299] D_loss: -2.5850, G_loss: 4.3232\n",
      "  Batch [1120/1299] D_loss: -2.2608, G_loss: 3.2908\n",
      "  Batch [1130/1299] D_loss: -3.7011, G_loss: 4.7940\n",
      "  Batch [1140/1299] D_loss: -3.2393, G_loss: 5.6939\n",
      "  Batch [1150/1299] D_loss: -2.6534, G_loss: 3.6224\n",
      "  Batch [1160/1299] D_loss: -2.4157, G_loss: 5.0185\n",
      "  Batch [1170/1299] D_loss: -2.4640, G_loss: 4.2528\n",
      "  Batch [1180/1299] D_loss: -1.9083, G_loss: 3.6976\n",
      "  Batch [1190/1299] D_loss: -2.0113, G_loss: 5.8286\n",
      "  Batch [1200/1299] D_loss: -1.4771, G_loss: 5.8189\n",
      "  Batch [1210/1299] D_loss: -2.6179, G_loss: 5.5664\n",
      "  Batch [1220/1299] D_loss: -2.2184, G_loss: 2.6521\n",
      "  Batch [1230/1299] D_loss: -3.5173, G_loss: 4.5992\n",
      "  Batch [1240/1299] D_loss: -2.9897, G_loss: 3.6586\n",
      "  Batch [1250/1299] D_loss: -2.5167, G_loss: 5.0201\n",
      "  Batch [1260/1299] D_loss: -3.9905, G_loss: 4.1129\n",
      "  Batch [1270/1299] D_loss: -4.1046, G_loss: 1.1786\n",
      "  Batch [1280/1299] D_loss: -2.9579, G_loss: 3.6160\n",
      "  Batch [1290/1299] D_loss: -2.9806, G_loss: 3.2182\n",
      "\n",
      "Epoch 20 Summary:\n",
      "  Average D_loss: -2.4547\n",
      "  Average G_loss: 3.7470\n",
      "\n",
      "Epoch [21/100]\n",
      "  Batch [0/1299] D_loss: -2.6694, G_loss: 5.8545\n",
      "  Batch [10/1299] D_loss: -2.5451, G_loss: 4.7534\n",
      "  Batch [20/1299] D_loss: -4.0299, G_loss: 5.6019\n",
      "  Batch [30/1299] D_loss: -2.5758, G_loss: 4.0014\n",
      "  Batch [40/1299] D_loss: -4.3356, G_loss: 3.9506\n",
      "  Batch [50/1299] D_loss: -2.3344, G_loss: 4.0814\n",
      "  Batch [60/1299] D_loss: -2.8852, G_loss: 4.0135\n",
      "  Batch [70/1299] D_loss: -3.3629, G_loss: 5.1260\n",
      "  Batch [80/1299] D_loss: -3.3911, G_loss: 4.5354\n",
      "  Batch [90/1299] D_loss: -2.2761, G_loss: 4.7381\n",
      "  Batch [100/1299] D_loss: -3.2038, G_loss: 4.6993\n",
      "  Batch [110/1299] D_loss: -2.2380, G_loss: 3.8886\n",
      "  Batch [120/1299] D_loss: -3.2309, G_loss: 2.7407\n",
      "  Batch [130/1299] D_loss: -2.9480, G_loss: 4.4477\n",
      "  Batch [140/1299] D_loss: -2.5610, G_loss: 3.7595\n",
      "  Batch [150/1299] D_loss: -2.9724, G_loss: 5.5049\n",
      "  Batch [160/1299] D_loss: -2.7751, G_loss: 0.8907\n",
      "  Batch [170/1299] D_loss: -3.7210, G_loss: 2.5975\n",
      "  Batch [180/1299] D_loss: -1.8910, G_loss: 2.3107\n",
      "  Batch [190/1299] D_loss: -1.6621, G_loss: 4.0222\n",
      "  Batch [200/1299] D_loss: -4.2783, G_loss: 2.1935\n",
      "  Batch [210/1299] D_loss: -1.4948, G_loss: 0.6662\n",
      "  Batch [220/1299] D_loss: -4.0538, G_loss: 2.7444\n",
      "  Batch [230/1299] D_loss: -3.1232, G_loss: 4.1150\n",
      "  Batch [240/1299] D_loss: -2.2335, G_loss: 1.3652\n",
      "  Batch [250/1299] D_loss: -2.8745, G_loss: 3.0363\n",
      "  Batch [260/1299] D_loss: -2.6159, G_loss: 2.6686\n",
      "  Batch [270/1299] D_loss: -3.3153, G_loss: 3.8306\n",
      "  Batch [280/1299] D_loss: -2.4495, G_loss: 2.4667\n",
      "  Batch [290/1299] D_loss: -1.2103, G_loss: 3.1093\n",
      "  Batch [300/1299] D_loss: -2.2657, G_loss: 2.9107\n",
      "  Batch [310/1299] D_loss: -2.9218, G_loss: 5.0167\n",
      "  Batch [320/1299] D_loss: -1.9210, G_loss: 3.8536\n",
      "  Batch [330/1299] D_loss: -3.3334, G_loss: 1.9950\n",
      "  Batch [340/1299] D_loss: -3.1105, G_loss: 2.2201\n",
      "  Batch [350/1299] D_loss: -1.1360, G_loss: 5.0511\n",
      "  Batch [360/1299] D_loss: -2.9696, G_loss: 5.1647\n",
      "  Batch [370/1299] D_loss: -2.3218, G_loss: 3.3834\n",
      "  Batch [380/1299] D_loss: -2.8179, G_loss: 3.3697\n",
      "  Batch [390/1299] D_loss: -2.3877, G_loss: 3.5749\n",
      "  Batch [400/1299] D_loss: -2.5278, G_loss: 3.6922\n",
      "  Batch [410/1299] D_loss: -2.3945, G_loss: 3.1419\n",
      "  Batch [420/1299] D_loss: -1.5559, G_loss: 1.1011\n",
      "  Batch [430/1299] D_loss: -3.4314, G_loss: 2.0779\n",
      "  Batch [440/1299] D_loss: -3.3364, G_loss: 2.9323\n",
      "  Batch [450/1299] D_loss: -3.4509, G_loss: 3.9238\n",
      "  Batch [460/1299] D_loss: -4.3483, G_loss: 2.8449\n",
      "  Batch [470/1299] D_loss: -4.9156, G_loss: 4.3596\n",
      "  Batch [480/1299] D_loss: -3.0340, G_loss: 3.3737\n",
      "  Batch [490/1299] D_loss: -0.7816, G_loss: 5.3027\n",
      "  Batch [500/1299] D_loss: -2.2168, G_loss: 4.5779\n",
      "  Batch [510/1299] D_loss: -0.4017, G_loss: 2.2413\n",
      "  Batch [520/1299] D_loss: -3.6612, G_loss: 3.3771\n",
      "  Batch [530/1299] D_loss: -1.7714, G_loss: 2.8760\n",
      "  Batch [540/1299] D_loss: -1.2537, G_loss: 3.4949\n",
      "  Batch [550/1299] D_loss: -3.0132, G_loss: 1.8338\n",
      "  Batch [560/1299] D_loss: -3.1361, G_loss: 6.0788\n",
      "  Batch [570/1299] D_loss: -2.4409, G_loss: 4.4722\n",
      "  Batch [580/1299] D_loss: -2.5321, G_loss: 4.1302\n",
      "  Batch [590/1299] D_loss: -1.6268, G_loss: 4.8870\n",
      "  Batch [600/1299] D_loss: -3.1068, G_loss: 1.3548\n",
      "  Batch [610/1299] D_loss: -3.2084, G_loss: 3.0741\n",
      "  Batch [620/1299] D_loss: -3.4711, G_loss: 2.1093\n",
      "  Batch [630/1299] D_loss: -1.6232, G_loss: 4.6406\n",
      "  Batch [640/1299] D_loss: -2.0110, G_loss: 1.4032\n",
      "  Batch [650/1299] D_loss: -2.3922, G_loss: 3.2854\n",
      "  Batch [660/1299] D_loss: -3.7784, G_loss: 2.6161\n",
      "  Batch [670/1299] D_loss: -2.9964, G_loss: 0.8360\n",
      "  Batch [680/1299] D_loss: -4.2902, G_loss: 1.7043\n",
      "  Batch [690/1299] D_loss: -2.2062, G_loss: 1.3499\n",
      "  Batch [700/1299] D_loss: -3.0208, G_loss: 2.2575\n",
      "  Batch [710/1299] D_loss: -2.5289, G_loss: 1.9150\n",
      "  Batch [720/1299] D_loss: -4.1952, G_loss: 2.3317\n",
      "  Batch [730/1299] D_loss: -3.0024, G_loss: 2.6765\n",
      "  Batch [740/1299] D_loss: -3.1447, G_loss: 4.2058\n",
      "  Batch [750/1299] D_loss: -1.9392, G_loss: 3.8944\n",
      "  Batch [760/1299] D_loss: -2.7846, G_loss: 2.5913\n",
      "  Batch [770/1299] D_loss: -2.2501, G_loss: 3.1517\n",
      "  Batch [780/1299] D_loss: -2.8700, G_loss: 3.8009\n",
      "  Batch [790/1299] D_loss: -2.3594, G_loss: 3.9411\n",
      "  Batch [800/1299] D_loss: -3.7983, G_loss: 2.7169\n",
      "  Batch [810/1299] D_loss: -1.0759, G_loss: 2.6440\n",
      "  Batch [820/1299] D_loss: -2.5765, G_loss: 3.5015\n",
      "  Batch [830/1299] D_loss: -0.4709, G_loss: 3.1618\n",
      "  Batch [840/1299] D_loss: -1.9816, G_loss: 5.1000\n",
      "  Batch [850/1299] D_loss: -2.2603, G_loss: 2.7033\n",
      "  Batch [860/1299] D_loss: -1.8305, G_loss: 2.8117\n",
      "  Batch [870/1299] D_loss: -3.5788, G_loss: 3.7588\n",
      "  Batch [880/1299] D_loss: -2.1801, G_loss: 3.6797\n",
      "  Batch [890/1299] D_loss: -2.0654, G_loss: 3.6134\n",
      "  Batch [900/1299] D_loss: -2.9295, G_loss: 4.0793\n",
      "  Batch [910/1299] D_loss: -1.4678, G_loss: 3.4180\n",
      "  Batch [920/1299] D_loss: -2.5219, G_loss: 6.4359\n",
      "  Batch [930/1299] D_loss: -3.4331, G_loss: 5.2918\n",
      "  Batch [940/1299] D_loss: -2.1214, G_loss: 3.2274\n",
      "  Batch [950/1299] D_loss: -3.0610, G_loss: 3.1575\n",
      "  Batch [960/1299] D_loss: -2.8672, G_loss: 3.5797\n",
      "  Batch [970/1299] D_loss: -3.9972, G_loss: 4.5263\n",
      "  Batch [980/1299] D_loss: -3.1799, G_loss: 2.7302\n",
      "  Batch [990/1299] D_loss: -2.9262, G_loss: 3.4073\n",
      "  Batch [1000/1299] D_loss: -1.2775, G_loss: 3.7852\n",
      "  Batch [1010/1299] D_loss: -3.2802, G_loss: 3.5242\n",
      "  Batch [1020/1299] D_loss: -3.6971, G_loss: 5.2933\n",
      "  Batch [1030/1299] D_loss: -1.0298, G_loss: 2.0387\n",
      "  Batch [1040/1299] D_loss: -3.3025, G_loss: 3.4988\n",
      "  Batch [1050/1299] D_loss: -2.9772, G_loss: 2.5057\n",
      "  Batch [1060/1299] D_loss: -1.1288, G_loss: 2.7612\n",
      "  Batch [1070/1299] D_loss: -2.4039, G_loss: 3.3278\n",
      "  Batch [1080/1299] D_loss: -3.8935, G_loss: 1.7011\n",
      "  Batch [1090/1299] D_loss: -4.7129, G_loss: 2.7984\n",
      "  Batch [1100/1299] D_loss: -3.5541, G_loss: 6.5904\n",
      "  Batch [1110/1299] D_loss: -2.3749, G_loss: 3.7662\n",
      "  Batch [1120/1299] D_loss: -4.0718, G_loss: 2.8456\n",
      "  Batch [1130/1299] D_loss: -4.9525, G_loss: 3.9459\n",
      "  Batch [1140/1299] D_loss: -3.8278, G_loss: 4.8295\n",
      "  Batch [1150/1299] D_loss: -3.4447, G_loss: 4.0951\n",
      "  Batch [1160/1299] D_loss: -1.5403, G_loss: 2.8236\n",
      "  Batch [1170/1299] D_loss: -1.9737, G_loss: 1.2249\n",
      "  Batch [1180/1299] D_loss: -3.0769, G_loss: 4.3440\n",
      "  Batch [1190/1299] D_loss: -4.5023, G_loss: 1.3435\n",
      "  Batch [1200/1299] D_loss: -3.5211, G_loss: 2.0897\n",
      "  Batch [1210/1299] D_loss: -3.1567, G_loss: 2.5220\n",
      "  Batch [1220/1299] D_loss: -2.3999, G_loss: 5.3331\n",
      "  Batch [1230/1299] D_loss: -3.0506, G_loss: 2.7221\n",
      "  Batch [1240/1299] D_loss: -1.3612, G_loss: 3.0987\n",
      "  Batch [1250/1299] D_loss: -3.7755, G_loss: 4.4301\n",
      "  Batch [1260/1299] D_loss: -1.7713, G_loss: 4.2198\n",
      "  Batch [1270/1299] D_loss: -1.1216, G_loss: 4.1302\n",
      "  Batch [1280/1299] D_loss: -3.3952, G_loss: 2.2042\n",
      "  Batch [1290/1299] D_loss: -4.3829, G_loss: 0.4300\n",
      "\n",
      "Epoch 21 Summary:\n",
      "  Average D_loss: -2.4233\n",
      "  Average G_loss: 3.5059\n",
      "\n",
      "Models saved at epoch 21:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_135516_dataset+cell_type/generator_20250121_135516_dataset+cell_type_epoch_21.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_135516_dataset+cell_type/discriminator_20250121_135516_dataset+cell_type_epoch_21.pt\n",
      "\n",
      "Epoch [22/100]\n",
      "  Batch [0/1299] D_loss: -1.7301, G_loss: 2.1703\n",
      "  Batch [10/1299] D_loss: -3.8805, G_loss: 2.8675\n",
      "  Batch [20/1299] D_loss: -3.7631, G_loss: 6.6671\n",
      "  Batch [30/1299] D_loss: -1.9787, G_loss: 3.9323\n",
      "  Batch [40/1299] D_loss: -2.4205, G_loss: 4.4437\n",
      "  Batch [50/1299] D_loss: -2.0452, G_loss: 3.6763\n",
      "  Batch [60/1299] D_loss: -1.2906, G_loss: 5.8077\n",
      "  Batch [70/1299] D_loss: -3.0939, G_loss: 4.0020\n",
      "  Batch [80/1299] D_loss: -2.1558, G_loss: 2.9465\n",
      "  Batch [90/1299] D_loss: -2.0289, G_loss: 4.0454\n",
      "  Batch [100/1299] D_loss: -2.9697, G_loss: 4.2963\n",
      "  Batch [110/1299] D_loss: -2.4372, G_loss: 1.5097\n",
      "  Batch [120/1299] D_loss: -1.0987, G_loss: 6.0530\n",
      "  Batch [130/1299] D_loss: -2.4204, G_loss: 2.5762\n",
      "  Batch [140/1299] D_loss: -3.8746, G_loss: 2.9506\n",
      "  Batch [150/1299] D_loss: -2.0436, G_loss: 4.4533\n",
      "  Batch [160/1299] D_loss: -3.9945, G_loss: 4.2401\n",
      "  Batch [170/1299] D_loss: -3.5803, G_loss: 4.8334\n",
      "  Batch [180/1299] D_loss: -3.4566, G_loss: 3.5402\n",
      "  Batch [190/1299] D_loss: -3.2745, G_loss: 4.0742\n",
      "  Batch [200/1299] D_loss: -2.3777, G_loss: 5.6498\n",
      "  Batch [210/1299] D_loss: -2.0986, G_loss: 1.2092\n",
      "  Batch [220/1299] D_loss: -3.2014, G_loss: 1.8325\n",
      "  Batch [230/1299] D_loss: -2.5416, G_loss: 6.3115\n",
      "  Batch [240/1299] D_loss: -4.4033, G_loss: 3.7782\n",
      "  Batch [250/1299] D_loss: -1.7062, G_loss: 3.0546\n",
      "  Batch [260/1299] D_loss: -2.3155, G_loss: 2.9456\n",
      "  Batch [270/1299] D_loss: -1.8940, G_loss: 1.7306\n",
      "  Batch [280/1299] D_loss: -2.9066, G_loss: 3.6610\n",
      "  Batch [290/1299] D_loss: -2.4013, G_loss: 3.6855\n",
      "  Batch [300/1299] D_loss: -2.4216, G_loss: 3.9452\n",
      "  Batch [310/1299] D_loss: -1.6992, G_loss: 2.5005\n",
      "  Batch [320/1299] D_loss: -3.7449, G_loss: 2.8053\n",
      "  Batch [330/1299] D_loss: -3.2195, G_loss: 2.2527\n",
      "  Batch [340/1299] D_loss: -2.5976, G_loss: 3.3005\n",
      "  Batch [350/1299] D_loss: -2.2748, G_loss: 4.5578\n",
      "  Batch [360/1299] D_loss: -1.7859, G_loss: 2.9926\n",
      "  Batch [370/1299] D_loss: -3.1603, G_loss: 3.3597\n",
      "  Batch [380/1299] D_loss: -2.6536, G_loss: 4.1211\n",
      "  Batch [390/1299] D_loss: -2.0976, G_loss: 2.2781\n",
      "  Batch [400/1299] D_loss: -2.4421, G_loss: 5.3491\n",
      "  Batch [410/1299] D_loss: -2.8929, G_loss: 5.3110\n",
      "  Batch [420/1299] D_loss: -3.3091, G_loss: 3.4183\n",
      "  Batch [430/1299] D_loss: -3.1566, G_loss: 3.1926\n",
      "  Batch [440/1299] D_loss: -2.9247, G_loss: 4.1344\n",
      "  Batch [450/1299] D_loss: -1.4376, G_loss: 3.5470\n",
      "  Batch [460/1299] D_loss: -2.1051, G_loss: 1.8919\n",
      "  Batch [470/1299] D_loss: -2.9123, G_loss: 3.3380\n",
      "  Batch [480/1299] D_loss: -1.3857, G_loss: 2.9036\n",
      "  Batch [490/1299] D_loss: -2.6995, G_loss: 4.2505\n",
      "  Batch [500/1299] D_loss: -3.3378, G_loss: 3.6649\n",
      "  Batch [510/1299] D_loss: -2.2048, G_loss: 3.4503\n",
      "  Batch [520/1299] D_loss: -3.2805, G_loss: 2.1190\n",
      "  Batch [530/1299] D_loss: -2.3281, G_loss: 3.5280\n",
      "  Batch [540/1299] D_loss: -1.8514, G_loss: 3.2762\n",
      "  Batch [550/1299] D_loss: -3.0133, G_loss: 2.9217\n",
      "  Batch [560/1299] D_loss: -2.0153, G_loss: 2.4146\n",
      "  Batch [570/1299] D_loss: -4.2791, G_loss: 3.9402\n",
      "  Batch [580/1299] D_loss: -1.7285, G_loss: 3.8625\n",
      "  Batch [590/1299] D_loss: -1.1792, G_loss: 4.0837\n",
      "  Batch [600/1299] D_loss: -2.6860, G_loss: 4.7289\n",
      "  Batch [610/1299] D_loss: -2.3936, G_loss: 3.5824\n",
      "  Batch [620/1299] D_loss: -2.7994, G_loss: 5.4031\n",
      "  Batch [630/1299] D_loss: -2.8479, G_loss: 3.9208\n",
      "  Batch [640/1299] D_loss: -3.2496, G_loss: 3.8468\n",
      "  Batch [650/1299] D_loss: -3.2176, G_loss: 4.1940\n",
      "  Batch [660/1299] D_loss: -2.4556, G_loss: 1.7447\n",
      "  Batch [670/1299] D_loss: -1.6567, G_loss: 3.8202\n",
      "  Batch [680/1299] D_loss: -3.1686, G_loss: 5.0226\n",
      "  Batch [690/1299] D_loss: -1.6849, G_loss: 3.8666\n",
      "  Batch [700/1299] D_loss: -2.4192, G_loss: 2.5335\n",
      "  Batch [710/1299] D_loss: -4.3443, G_loss: 2.9938\n",
      "  Batch [720/1299] D_loss: -2.6891, G_loss: 3.1286\n",
      "  Batch [730/1299] D_loss: -1.7303, G_loss: 5.0298\n",
      "  Batch [740/1299] D_loss: -3.7208, G_loss: 5.4756\n",
      "  Batch [750/1299] D_loss: -3.4662, G_loss: 0.9244\n",
      "  Batch [760/1299] D_loss: -1.9887, G_loss: 4.1902\n",
      "  Batch [770/1299] D_loss: -3.2717, G_loss: 5.5102\n",
      "  Batch [780/1299] D_loss: -2.1465, G_loss: 4.0088\n",
      "  Batch [790/1299] D_loss: -2.3094, G_loss: 4.3012\n",
      "  Batch [800/1299] D_loss: -2.3639, G_loss: 3.7000\n",
      "  Batch [810/1299] D_loss: -1.6583, G_loss: 5.7775\n",
      "  Batch [820/1299] D_loss: -3.0371, G_loss: 5.3970\n",
      "  Batch [830/1299] D_loss: -1.7477, G_loss: 5.3728\n",
      "  Batch [840/1299] D_loss: -1.3134, G_loss: 6.3661\n",
      "  Batch [850/1299] D_loss: -1.7145, G_loss: 5.1491\n",
      "  Batch [860/1299] D_loss: -3.4212, G_loss: 6.2794\n",
      "  Batch [870/1299] D_loss: -2.7609, G_loss: 6.3405\n",
      "  Batch [880/1299] D_loss: -1.7787, G_loss: 6.0440\n",
      "  Batch [890/1299] D_loss: -2.3056, G_loss: 5.6569\n",
      "  Batch [900/1299] D_loss: -3.9738, G_loss: 4.7329\n",
      "  Batch [910/1299] D_loss: -3.0917, G_loss: 1.9531\n",
      "  Batch [920/1299] D_loss: -2.3246, G_loss: 1.2568\n",
      "  Batch [930/1299] D_loss: -2.3249, G_loss: 3.1024\n",
      "  Batch [940/1299] D_loss: -2.9288, G_loss: 3.4428\n",
      "  Batch [950/1299] D_loss: -3.3375, G_loss: 2.6174\n",
      "  Batch [960/1299] D_loss: -2.4224, G_loss: 2.0748\n",
      "  Batch [970/1299] D_loss: -2.9106, G_loss: 2.1632\n",
      "  Batch [980/1299] D_loss: -2.7156, G_loss: 3.3743\n",
      "  Batch [990/1299] D_loss: -3.0624, G_loss: 2.7994\n",
      "  Batch [1000/1299] D_loss: -4.3432, G_loss: 3.1835\n",
      "  Batch [1010/1299] D_loss: -3.0861, G_loss: 1.9073\n",
      "  Batch [1020/1299] D_loss: -2.9238, G_loss: 1.9575\n",
      "  Batch [1030/1299] D_loss: -2.1147, G_loss: 5.3572\n",
      "  Batch [1040/1299] D_loss: -2.7345, G_loss: 1.3989\n",
      "  Batch [1050/1299] D_loss: -2.2631, G_loss: 1.6400\n",
      "  Batch [1060/1299] D_loss: -3.8110, G_loss: 4.1747\n",
      "  Batch [1070/1299] D_loss: -3.3386, G_loss: 2.2686\n",
      "  Batch [1080/1299] D_loss: -2.9071, G_loss: 4.0273\n",
      "  Batch [1090/1299] D_loss: -3.0002, G_loss: 4.1955\n",
      "  Batch [1100/1299] D_loss: -4.1093, G_loss: 3.3721\n",
      "  Batch [1110/1299] D_loss: -3.6424, G_loss: 1.5762\n",
      "  Batch [1120/1299] D_loss: -2.0425, G_loss: 2.6900\n",
      "  Batch [1130/1299] D_loss: -2.3479, G_loss: 2.9855\n",
      "  Batch [1140/1299] D_loss: -2.4301, G_loss: 1.4105\n",
      "  Batch [1150/1299] D_loss: -1.6178, G_loss: 5.3328\n",
      "  Batch [1160/1299] D_loss: -4.0297, G_loss: 5.1402\n",
      "  Batch [1170/1299] D_loss: -2.9070, G_loss: 5.6028\n",
      "  Batch [1180/1299] D_loss: -2.0479, G_loss: 3.7842\n",
      "  Batch [1190/1299] D_loss: -2.0789, G_loss: 1.8273\n",
      "  Batch [1200/1299] D_loss: -1.6427, G_loss: 3.3614\n",
      "  Batch [1210/1299] D_loss: -2.9543, G_loss: 1.6327\n",
      "  Batch [1220/1299] D_loss: -4.3281, G_loss: 3.3646\n",
      "  Batch [1230/1299] D_loss: -2.3367, G_loss: 4.0016\n",
      "  Batch [1240/1299] D_loss: -2.6270, G_loss: 2.4293\n",
      "  Batch [1250/1299] D_loss: -2.6964, G_loss: 3.7786\n",
      "  Batch [1260/1299] D_loss: -2.5263, G_loss: 3.2786\n",
      "  Batch [1270/1299] D_loss: -2.3860, G_loss: 0.9830\n",
      "  Batch [1280/1299] D_loss: -2.7841, G_loss: 3.0211\n",
      "  Batch [1290/1299] D_loss: -2.1091, G_loss: 4.3835\n",
      "\n",
      "Epoch 22 Summary:\n",
      "  Average D_loss: -2.3726\n",
      "  Average G_loss: 3.6598\n",
      "\n",
      "Epoch [23/100]\n",
      "  Batch [0/1299] D_loss: -2.9313, G_loss: 3.0089\n",
      "  Batch [10/1299] D_loss: -2.9350, G_loss: 2.4473\n",
      "  Batch [20/1299] D_loss: -2.4124, G_loss: 1.4598\n",
      "  Batch [30/1299] D_loss: -2.4945, G_loss: 2.0087\n",
      "  Batch [40/1299] D_loss: -2.3531, G_loss: 2.6337\n",
      "  Batch [50/1299] D_loss: -2.9038, G_loss: 3.0976\n",
      "  Batch [60/1299] D_loss: -2.4032, G_loss: 2.9195\n",
      "  Batch [70/1299] D_loss: -2.1264, G_loss: 2.6217\n",
      "  Batch [80/1299] D_loss: -3.2460, G_loss: 1.0463\n",
      "  Batch [90/1299] D_loss: -3.8855, G_loss: 3.1535\n",
      "  Batch [100/1299] D_loss: -2.6595, G_loss: 1.5792\n",
      "  Batch [110/1299] D_loss: -2.1281, G_loss: 4.0503\n",
      "  Batch [120/1299] D_loss: -2.0183, G_loss: 4.3792\n",
      "  Batch [130/1299] D_loss: -2.8144, G_loss: 1.5752\n",
      "  Batch [140/1299] D_loss: -2.3833, G_loss: 1.7795\n",
      "  Batch [150/1299] D_loss: -3.3294, G_loss: 3.1529\n",
      "  Batch [160/1299] D_loss: -2.2148, G_loss: -0.0681\n",
      "  Batch [170/1299] D_loss: -1.4404, G_loss: 2.0795\n",
      "  Batch [180/1299] D_loss: -2.4930, G_loss: 3.5453\n",
      "  Batch [190/1299] D_loss: -2.4289, G_loss: 2.3096\n",
      "  Batch [200/1299] D_loss: -2.3939, G_loss: 2.5389\n",
      "  Batch [210/1299] D_loss: -3.2315, G_loss: 1.6135\n",
      "  Batch [220/1299] D_loss: -2.8676, G_loss: 2.7935\n",
      "  Batch [230/1299] D_loss: -1.5870, G_loss: 2.6927\n",
      "  Batch [240/1299] D_loss: -2.2099, G_loss: 5.4685\n",
      "  Batch [250/1299] D_loss: -3.4902, G_loss: 3.1836\n",
      "  Batch [260/1299] D_loss: -2.3069, G_loss: 4.0469\n",
      "  Batch [270/1299] D_loss: -2.3742, G_loss: 3.7990\n",
      "  Batch [280/1299] D_loss: -1.1112, G_loss: 1.0538\n",
      "  Batch [290/1299] D_loss: -2.6439, G_loss: 1.0422\n",
      "  Batch [300/1299] D_loss: -1.6475, G_loss: 0.8313\n",
      "  Batch [310/1299] D_loss: -3.3011, G_loss: 4.2622\n",
      "  Batch [320/1299] D_loss: -2.8020, G_loss: 1.3210\n",
      "  Batch [330/1299] D_loss: -3.0648, G_loss: 2.6489\n",
      "  Batch [340/1299] D_loss: -1.2769, G_loss: 2.0678\n",
      "  Batch [350/1299] D_loss: -1.6966, G_loss: 3.8745\n",
      "  Batch [360/1299] D_loss: -2.8358, G_loss: 1.9694\n",
      "  Batch [370/1299] D_loss: -2.0227, G_loss: 2.5874\n",
      "  Batch [380/1299] D_loss: -2.2938, G_loss: 1.4269\n",
      "  Batch [390/1299] D_loss: -2.3944, G_loss: 1.6998\n",
      "  Batch [400/1299] D_loss: -2.6791, G_loss: 1.1116\n",
      "  Batch [410/1299] D_loss: -2.6378, G_loss: 2.7740\n",
      "  Batch [420/1299] D_loss: -1.3314, G_loss: 3.0710\n",
      "  Batch [430/1299] D_loss: -2.1167, G_loss: 2.2222\n",
      "  Batch [440/1299] D_loss: -2.4928, G_loss: 3.5155\n",
      "  Batch [450/1299] D_loss: -2.3046, G_loss: 0.2324\n",
      "  Batch [460/1299] D_loss: -2.2342, G_loss: 2.2459\n",
      "  Batch [470/1299] D_loss: -2.5856, G_loss: 2.4410\n",
      "  Batch [480/1299] D_loss: -2.4637, G_loss: 3.1733\n",
      "  Batch [490/1299] D_loss: -1.6891, G_loss: 3.5442\n",
      "  Batch [500/1299] D_loss: -2.0796, G_loss: 4.7442\n",
      "  Batch [510/1299] D_loss: -3.5708, G_loss: 4.2205\n",
      "  Batch [520/1299] D_loss: -2.8091, G_loss: 3.1857\n",
      "  Batch [530/1299] D_loss: -3.6847, G_loss: 3.2095\n",
      "  Batch [540/1299] D_loss: -2.3698, G_loss: 4.1054\n",
      "  Batch [550/1299] D_loss: -2.3146, G_loss: 2.4373\n",
      "  Batch [560/1299] D_loss: -3.0699, G_loss: 6.0170\n",
      "  Batch [570/1299] D_loss: -2.0859, G_loss: 4.3863\n",
      "  Batch [580/1299] D_loss: -1.7006, G_loss: 1.5095\n",
      "  Batch [590/1299] D_loss: -1.7633, G_loss: 3.6262\n",
      "  Batch [600/1299] D_loss: -1.6450, G_loss: 1.6775\n",
      "  Batch [610/1299] D_loss: -2.7574, G_loss: 2.0135\n",
      "  Batch [620/1299] D_loss: -1.7190, G_loss: 4.2714\n",
      "  Batch [630/1299] D_loss: -2.5927, G_loss: 2.4688\n",
      "  Batch [640/1299] D_loss: -2.9534, G_loss: 5.0040\n",
      "  Batch [650/1299] D_loss: -3.3211, G_loss: 4.4000\n",
      "  Batch [660/1299] D_loss: -2.5386, G_loss: 2.0858\n",
      "  Batch [670/1299] D_loss: -3.0791, G_loss: 4.4300\n",
      "  Batch [680/1299] D_loss: -1.5544, G_loss: 1.5468\n",
      "  Batch [690/1299] D_loss: -0.8633, G_loss: -1.2379\n",
      "  Batch [700/1299] D_loss: -3.5136, G_loss: 2.2898\n",
      "  Batch [710/1299] D_loss: -1.9866, G_loss: 2.6542\n",
      "  Batch [720/1299] D_loss: -2.3946, G_loss: 2.9812\n",
      "  Batch [730/1299] D_loss: -2.5879, G_loss: 1.7538\n",
      "  Batch [740/1299] D_loss: -2.8880, G_loss: 2.4011\n",
      "  Batch [750/1299] D_loss: -3.1401, G_loss: 4.1704\n",
      "  Batch [760/1299] D_loss: -1.6734, G_loss: 1.9619\n",
      "  Batch [770/1299] D_loss: -2.1144, G_loss: 0.7412\n",
      "  Batch [780/1299] D_loss: -2.5191, G_loss: 1.2134\n",
      "  Batch [790/1299] D_loss: -2.6844, G_loss: 0.8142\n",
      "  Batch [800/1299] D_loss: -3.4961, G_loss: 1.7670\n",
      "  Batch [810/1299] D_loss: -2.2854, G_loss: 3.2929\n",
      "  Batch [820/1299] D_loss: -2.6646, G_loss: 2.3746\n",
      "  Batch [830/1299] D_loss: -1.7615, G_loss: 1.5736\n",
      "  Batch [840/1299] D_loss: -1.2062, G_loss: 2.8199\n",
      "  Batch [850/1299] D_loss: -4.2511, G_loss: 1.9095\n",
      "  Batch [860/1299] D_loss: -1.4175, G_loss: 2.5790\n",
      "  Batch [870/1299] D_loss: -3.0697, G_loss: 0.5695\n",
      "  Batch [880/1299] D_loss: -2.6320, G_loss: 2.8117\n",
      "  Batch [890/1299] D_loss: -2.3250, G_loss: 1.4065\n",
      "  Batch [900/1299] D_loss: -3.4345, G_loss: 3.5662\n",
      "  Batch [910/1299] D_loss: -3.3344, G_loss: 1.6060\n",
      "  Batch [920/1299] D_loss: -2.0414, G_loss: 6.2144\n",
      "  Batch [930/1299] D_loss: -2.5136, G_loss: 4.8677\n",
      "  Batch [940/1299] D_loss: -3.4082, G_loss: 1.7093\n",
      "  Batch [950/1299] D_loss: -3.8969, G_loss: 2.3196\n",
      "  Batch [960/1299] D_loss: -2.1085, G_loss: 0.6769\n",
      "  Batch [970/1299] D_loss: -1.7444, G_loss: 0.7966\n",
      "  Batch [980/1299] D_loss: -2.0132, G_loss: 2.6163\n",
      "  Batch [990/1299] D_loss: -2.2748, G_loss: 3.4576\n",
      "  Batch [1000/1299] D_loss: -3.0618, G_loss: 1.3723\n",
      "  Batch [1010/1299] D_loss: -2.4024, G_loss: 2.3636\n",
      "  Batch [1020/1299] D_loss: -1.9324, G_loss: 1.5664\n",
      "  Batch [1030/1299] D_loss: -2.5900, G_loss: 3.2276\n",
      "  Batch [1040/1299] D_loss: -3.5930, G_loss: 3.9251\n",
      "  Batch [1050/1299] D_loss: -2.6417, G_loss: 5.5675\n",
      "  Batch [1060/1299] D_loss: -4.6949, G_loss: 3.3361\n",
      "  Batch [1070/1299] D_loss: -3.9477, G_loss: 2.6802\n",
      "  Batch [1080/1299] D_loss: -2.3721, G_loss: 2.1060\n",
      "  Batch [1090/1299] D_loss: -3.6559, G_loss: 2.6098\n",
      "  Batch [1100/1299] D_loss: -1.8564, G_loss: 3.7340\n",
      "  Batch [1110/1299] D_loss: -3.9297, G_loss: 4.1352\n",
      "  Batch [1120/1299] D_loss: -1.0258, G_loss: 4.8583\n",
      "  Batch [1130/1299] D_loss: -2.6816, G_loss: 2.2970\n",
      "  Batch [1140/1299] D_loss: -2.1126, G_loss: 1.7243\n",
      "  Batch [1150/1299] D_loss: -1.6466, G_loss: 1.8264\n",
      "  Batch [1160/1299] D_loss: -3.2860, G_loss: 3.1078\n",
      "  Batch [1170/1299] D_loss: -3.4906, G_loss: 3.8011\n",
      "  Batch [1180/1299] D_loss: -2.2553, G_loss: 1.5282\n",
      "  Batch [1190/1299] D_loss: -2.1392, G_loss: 1.9238\n",
      "  Batch [1200/1299] D_loss: -2.1888, G_loss: 0.8379\n",
      "  Batch [1210/1299] D_loss: -2.3694, G_loss: 2.9695\n",
      "  Batch [1220/1299] D_loss: -1.9607, G_loss: 1.3576\n",
      "  Batch [1230/1299] D_loss: -3.8636, G_loss: 2.6165\n",
      "  Batch [1240/1299] D_loss: -2.2887, G_loss: 3.4819\n",
      "  Batch [1250/1299] D_loss: -2.5155, G_loss: 3.2995\n",
      "  Batch [1260/1299] D_loss: -2.4918, G_loss: 3.8372\n",
      "  Batch [1270/1299] D_loss: -2.7911, G_loss: 4.8904\n",
      "  Batch [1280/1299] D_loss: -1.4785, G_loss: 4.8949\n",
      "  Batch [1290/1299] D_loss: -2.9756, G_loss: 8.1039\n",
      "\n",
      "Epoch 23 Summary:\n",
      "  Average D_loss: -2.3679\n",
      "  Average G_loss: 2.7598\n",
      "\n",
      "Epoch [24/100]\n",
      "  Batch [0/1299] D_loss: -2.5008, G_loss: 5.9815\n",
      "  Batch [10/1299] D_loss: -2.7546, G_loss: 3.8499\n",
      "  Batch [20/1299] D_loss: -2.2274, G_loss: 5.9262\n",
      "  Batch [30/1299] D_loss: -3.2795, G_loss: 6.8208\n",
      "  Batch [40/1299] D_loss: -1.6255, G_loss: 4.9146\n",
      "  Batch [50/1299] D_loss: -3.5484, G_loss: 2.9114\n",
      "  Batch [60/1299] D_loss: -3.3484, G_loss: 4.2484\n",
      "  Batch [70/1299] D_loss: -2.7130, G_loss: 5.0345\n",
      "  Batch [80/1299] D_loss: -1.1870, G_loss: 2.3444\n",
      "  Batch [90/1299] D_loss: -1.3908, G_loss: 5.2358\n",
      "  Batch [100/1299] D_loss: -2.8170, G_loss: 2.8338\n",
      "  Batch [110/1299] D_loss: -2.2380, G_loss: 2.6135\n",
      "  Batch [120/1299] D_loss: -3.0742, G_loss: 3.9122\n",
      "  Batch [130/1299] D_loss: -2.5439, G_loss: 0.6834\n",
      "  Batch [140/1299] D_loss: -3.5498, G_loss: 3.6460\n",
      "  Batch [150/1299] D_loss: -2.1523, G_loss: 1.9777\n",
      "  Batch [160/1299] D_loss: -1.1624, G_loss: 3.8989\n",
      "  Batch [170/1299] D_loss: -2.7297, G_loss: 2.9525\n",
      "  Batch [180/1299] D_loss: -3.3544, G_loss: 1.4858\n",
      "  Batch [190/1299] D_loss: -2.1098, G_loss: 2.6304\n",
      "  Batch [200/1299] D_loss: -3.0182, G_loss: 2.4490\n",
      "  Batch [210/1299] D_loss: -1.8820, G_loss: 3.5231\n",
      "  Batch [220/1299] D_loss: -2.9428, G_loss: 3.7169\n",
      "  Batch [230/1299] D_loss: -3.3901, G_loss: 2.9449\n",
      "  Batch [240/1299] D_loss: -3.0757, G_loss: 2.2352\n",
      "  Batch [250/1299] D_loss: -2.8376, G_loss: -0.4747\n",
      "  Batch [260/1299] D_loss: -3.3699, G_loss: 4.5481\n",
      "  Batch [270/1299] D_loss: -2.3909, G_loss: 6.2586\n",
      "  Batch [280/1299] D_loss: -2.8906, G_loss: 4.0117\n",
      "  Batch [290/1299] D_loss: -3.0157, G_loss: 5.0390\n",
      "  Batch [300/1299] D_loss: -3.2170, G_loss: 4.2325\n",
      "  Batch [310/1299] D_loss: -2.4835, G_loss: 5.4609\n",
      "  Batch [320/1299] D_loss: -3.0527, G_loss: 3.0981\n",
      "  Batch [330/1299] D_loss: -3.0638, G_loss: 6.8300\n",
      "  Batch [340/1299] D_loss: -1.7342, G_loss: 1.3596\n",
      "  Batch [350/1299] D_loss: 0.3321, G_loss: 0.1812\n",
      "  Batch [360/1299] D_loss: -2.6489, G_loss: 1.8849\n",
      "  Batch [370/1299] D_loss: -3.5688, G_loss: 5.4208\n",
      "  Batch [380/1299] D_loss: -2.4936, G_loss: 1.9453\n",
      "  Batch [390/1299] D_loss: -2.9773, G_loss: 2.2561\n",
      "  Batch [400/1299] D_loss: -1.1124, G_loss: 1.2596\n",
      "  Batch [410/1299] D_loss: -4.9577, G_loss: 1.8806\n",
      "  Batch [420/1299] D_loss: -2.4314, G_loss: 4.6268\n",
      "  Batch [430/1299] D_loss: -3.3933, G_loss: 4.2277\n",
      "  Batch [440/1299] D_loss: -2.2308, G_loss: 2.3525\n",
      "  Batch [450/1299] D_loss: -1.1277, G_loss: 2.1394\n",
      "  Batch [460/1299] D_loss: -2.7851, G_loss: 1.5250\n",
      "  Batch [470/1299] D_loss: -2.3871, G_loss: 2.4598\n",
      "  Batch [480/1299] D_loss: -2.2392, G_loss: 1.5348\n",
      "  Batch [490/1299] D_loss: -1.9943, G_loss: 3.9870\n",
      "  Batch [500/1299] D_loss: -2.9177, G_loss: 3.6896\n",
      "  Batch [510/1299] D_loss: -3.9799, G_loss: 3.1767\n",
      "  Batch [520/1299] D_loss: -2.4949, G_loss: 2.4794\n",
      "  Batch [530/1299] D_loss: -3.0994, G_loss: 0.1046\n",
      "  Batch [540/1299] D_loss: -2.5429, G_loss: 2.6833\n",
      "  Batch [550/1299] D_loss: -1.1008, G_loss: 3.3364\n",
      "  Batch [560/1299] D_loss: -3.5253, G_loss: 3.4223\n",
      "  Batch [570/1299] D_loss: -2.0778, G_loss: 3.8927\n",
      "  Batch [580/1299] D_loss: -1.9603, G_loss: 3.3801\n",
      "  Batch [590/1299] D_loss: -2.5860, G_loss: 4.2145\n",
      "  Batch [600/1299] D_loss: -3.7308, G_loss: 2.9493\n",
      "  Batch [610/1299] D_loss: -2.5603, G_loss: 3.3837\n",
      "  Batch [620/1299] D_loss: -1.3591, G_loss: 4.8367\n",
      "  Batch [630/1299] D_loss: -3.0130, G_loss: 3.4653\n",
      "  Batch [640/1299] D_loss: -1.6687, G_loss: 2.0989\n",
      "  Batch [650/1299] D_loss: -2.9754, G_loss: 3.6239\n",
      "  Batch [660/1299] D_loss: -2.3096, G_loss: 0.2721\n",
      "  Batch [670/1299] D_loss: -2.3783, G_loss: 3.0936\n",
      "  Batch [680/1299] D_loss: -0.9737, G_loss: 3.7632\n",
      "  Batch [690/1299] D_loss: -2.4657, G_loss: 1.7626\n",
      "  Batch [700/1299] D_loss: -2.6996, G_loss: 2.1817\n",
      "  Batch [710/1299] D_loss: -1.3622, G_loss: 1.9847\n",
      "  Batch [720/1299] D_loss: -2.1462, G_loss: 1.2871\n",
      "  Batch [730/1299] D_loss: -1.4804, G_loss: 1.3106\n",
      "  Batch [740/1299] D_loss: -4.0012, G_loss: 0.9601\n",
      "  Batch [750/1299] D_loss: -2.7021, G_loss: 1.7958\n",
      "  Batch [760/1299] D_loss: -1.2283, G_loss: 3.9857\n",
      "  Batch [770/1299] D_loss: -3.1233, G_loss: 1.9230\n",
      "  Batch [780/1299] D_loss: -3.1578, G_loss: 4.6120\n",
      "  Batch [790/1299] D_loss: -1.6555, G_loss: 3.7420\n",
      "  Batch [800/1299] D_loss: -3.2130, G_loss: 1.2139\n",
      "  Batch [810/1299] D_loss: -3.9141, G_loss: 4.9095\n",
      "  Batch [820/1299] D_loss: -2.0279, G_loss: 1.3666\n",
      "  Batch [830/1299] D_loss: -2.8184, G_loss: 4.7038\n",
      "  Batch [840/1299] D_loss: -3.2227, G_loss: 3.1316\n",
      "  Batch [850/1299] D_loss: -2.5630, G_loss: 4.4785\n",
      "  Batch [860/1299] D_loss: -3.7094, G_loss: 4.0150\n",
      "  Batch [870/1299] D_loss: -2.6474, G_loss: 3.8355\n",
      "  Batch [880/1299] D_loss: -1.8025, G_loss: 2.4769\n",
      "  Batch [890/1299] D_loss: -2.5072, G_loss: 3.0230\n",
      "  Batch [900/1299] D_loss: -2.8144, G_loss: 3.1185\n",
      "  Batch [910/1299] D_loss: -3.1058, G_loss: 4.5226\n",
      "  Batch [920/1299] D_loss: -0.7936, G_loss: 1.9047\n",
      "  Batch [930/1299] D_loss: -2.0386, G_loss: 1.0449\n",
      "  Batch [940/1299] D_loss: -1.7115, G_loss: 2.9825\n",
      "  Batch [950/1299] D_loss: -4.2537, G_loss: 2.8867\n",
      "  Batch [960/1299] D_loss: -1.7921, G_loss: 2.1728\n",
      "  Batch [970/1299] D_loss: -3.1810, G_loss: 0.5995\n",
      "  Batch [980/1299] D_loss: -2.2848, G_loss: 3.8745\n",
      "  Batch [990/1299] D_loss: -2.5848, G_loss: 2.2304\n",
      "  Batch [1000/1299] D_loss: -2.8193, G_loss: 1.7187\n",
      "  Batch [1010/1299] D_loss: -1.8411, G_loss: 1.8626\n",
      "  Batch [1020/1299] D_loss: -2.3381, G_loss: 2.0703\n",
      "  Batch [1030/1299] D_loss: -3.0012, G_loss: 4.6729\n",
      "  Batch [1040/1299] D_loss: -2.9529, G_loss: 3.3660\n",
      "  Batch [1050/1299] D_loss: -3.0494, G_loss: 4.0867\n",
      "  Batch [1060/1299] D_loss: -3.9229, G_loss: 0.8095\n",
      "  Batch [1070/1299] D_loss: -2.6374, G_loss: 2.5415\n",
      "  Batch [1080/1299] D_loss: -1.9453, G_loss: 4.6914\n",
      "  Batch [1090/1299] D_loss: -3.5095, G_loss: 3.6198\n",
      "  Batch [1100/1299] D_loss: -2.3391, G_loss: 2.3385\n",
      "  Batch [1110/1299] D_loss: -1.7034, G_loss: 4.0282\n",
      "  Batch [1120/1299] D_loss: -1.8256, G_loss: 3.4150\n",
      "  Batch [1130/1299] D_loss: -1.8261, G_loss: 5.5645\n",
      "  Batch [1140/1299] D_loss: -2.9094, G_loss: 1.9403\n",
      "  Batch [1150/1299] D_loss: -3.6124, G_loss: 2.0369\n",
      "  Batch [1160/1299] D_loss: -2.7692, G_loss: 1.4002\n",
      "  Batch [1170/1299] D_loss: -4.1308, G_loss: 2.9786\n",
      "  Batch [1180/1299] D_loss: -3.8249, G_loss: 2.3848\n",
      "  Batch [1190/1299] D_loss: -1.3248, G_loss: 4.1436\n",
      "  Batch [1200/1299] D_loss: -2.1082, G_loss: 3.5017\n",
      "  Batch [1210/1299] D_loss: -2.2690, G_loss: 3.8419\n",
      "  Batch [1220/1299] D_loss: -2.2709, G_loss: 3.8870\n",
      "  Batch [1230/1299] D_loss: -1.7292, G_loss: 3.8257\n",
      "  Batch [1240/1299] D_loss: -2.5736, G_loss: 5.7521\n",
      "  Batch [1250/1299] D_loss: -1.9578, G_loss: 4.0993\n",
      "  Batch [1260/1299] D_loss: -2.7397, G_loss: 3.1467\n",
      "  Batch [1270/1299] D_loss: -3.1493, G_loss: 3.1362\n",
      "  Batch [1280/1299] D_loss: -3.6440, G_loss: 3.5269\n",
      "  Batch [1290/1299] D_loss: -2.3679, G_loss: 1.8495\n",
      "\n",
      "Epoch 24 Summary:\n",
      "  Average D_loss: -2.3431\n",
      "  Average G_loss: 3.1250\n",
      "\n",
      "Epoch [25/100]\n",
      "  Batch [0/1299] D_loss: -3.8723, G_loss: 2.0829\n",
      "  Batch [10/1299] D_loss: -2.4402, G_loss: 3.1446\n",
      "  Batch [20/1299] D_loss: -2.4422, G_loss: 2.1243\n",
      "  Batch [30/1299] D_loss: -2.4009, G_loss: 2.4711\n",
      "  Batch [40/1299] D_loss: -4.2032, G_loss: 3.1799\n",
      "  Batch [50/1299] D_loss: -2.7568, G_loss: 0.2998\n",
      "  Batch [60/1299] D_loss: -1.9340, G_loss: 2.7561\n",
      "  Batch [70/1299] D_loss: -3.1120, G_loss: 3.0908\n",
      "  Batch [80/1299] D_loss: -3.1043, G_loss: 2.5659\n",
      "  Batch [90/1299] D_loss: -4.1816, G_loss: 1.3346\n",
      "  Batch [100/1299] D_loss: -3.2520, G_loss: 3.2095\n",
      "  Batch [110/1299] D_loss: -2.8940, G_loss: 3.5743\n",
      "  Batch [120/1299] D_loss: -2.8810, G_loss: 2.9740\n",
      "  Batch [130/1299] D_loss: -1.6363, G_loss: 4.2309\n",
      "  Batch [140/1299] D_loss: -3.2089, G_loss: 5.7287\n",
      "  Batch [150/1299] D_loss: -3.2544, G_loss: 4.0750\n",
      "  Batch [160/1299] D_loss: -3.1745, G_loss: 3.3735\n",
      "  Batch [170/1299] D_loss: -3.8903, G_loss: 1.4695\n",
      "  Batch [180/1299] D_loss: -2.4869, G_loss: -0.6648\n",
      "  Batch [190/1299] D_loss: -3.8260, G_loss: 3.3948\n",
      "  Batch [200/1299] D_loss: -2.0857, G_loss: 3.7757\n",
      "  Batch [210/1299] D_loss: -2.5927, G_loss: 4.5994\n",
      "  Batch [220/1299] D_loss: -1.8581, G_loss: 4.5929\n",
      "  Batch [230/1299] D_loss: -3.7153, G_loss: 2.9474\n",
      "  Batch [240/1299] D_loss: -3.0897, G_loss: 1.5956\n",
      "  Batch [250/1299] D_loss: -3.0877, G_loss: 1.0632\n",
      "  Batch [260/1299] D_loss: -2.4828, G_loss: 4.0805\n",
      "  Batch [270/1299] D_loss: -1.7355, G_loss: 5.3660\n",
      "  Batch [280/1299] D_loss: -3.6847, G_loss: 6.2930\n",
      "  Batch [290/1299] D_loss: -3.4157, G_loss: 4.2950\n",
      "  Batch [300/1299] D_loss: -2.4480, G_loss: 4.2367\n",
      "  Batch [310/1299] D_loss: -2.0517, G_loss: 2.1567\n",
      "  Batch [320/1299] D_loss: -2.9775, G_loss: 2.4946\n",
      "  Batch [330/1299] D_loss: -2.3118, G_loss: 2.0257\n",
      "  Batch [340/1299] D_loss: -2.1596, G_loss: -0.2145\n",
      "  Batch [350/1299] D_loss: -2.2419, G_loss: 2.9498\n",
      "  Batch [360/1299] D_loss: -4.2609, G_loss: 2.2275\n",
      "  Batch [370/1299] D_loss: -3.2158, G_loss: 2.9775\n",
      "  Batch [380/1299] D_loss: -3.0966, G_loss: 3.4325\n",
      "  Batch [390/1299] D_loss: -2.1728, G_loss: 2.2694\n",
      "  Batch [400/1299] D_loss: -3.0755, G_loss: 3.8036\n",
      "  Batch [410/1299] D_loss: -1.9699, G_loss: 3.4135\n",
      "  Batch [420/1299] D_loss: -2.3557, G_loss: 2.9570\n",
      "  Batch [430/1299] D_loss: -2.0064, G_loss: 4.4198\n",
      "  Batch [440/1299] D_loss: -3.5504, G_loss: 2.5742\n",
      "  Batch [450/1299] D_loss: -2.0593, G_loss: 2.4702\n",
      "  Batch [460/1299] D_loss: -3.1303, G_loss: 1.8102\n",
      "  Batch [470/1299] D_loss: -2.4352, G_loss: 3.0795\n",
      "  Batch [480/1299] D_loss: -2.7248, G_loss: 1.7824\n",
      "  Batch [490/1299] D_loss: -2.5332, G_loss: 4.1653\n",
      "  Batch [500/1299] D_loss: -1.2122, G_loss: 2.1750\n",
      "  Batch [510/1299] D_loss: -3.5141, G_loss: 2.7299\n",
      "  Batch [520/1299] D_loss: -2.4078, G_loss: 3.2498\n",
      "  Batch [530/1299] D_loss: -2.8730, G_loss: 2.1927\n",
      "  Batch [540/1299] D_loss: -2.3582, G_loss: 3.2771\n",
      "  Batch [550/1299] D_loss: -0.8928, G_loss: 2.0149\n",
      "  Batch [560/1299] D_loss: -3.0976, G_loss: 4.8627\n",
      "  Batch [570/1299] D_loss: -3.3357, G_loss: 3.9748\n",
      "  Batch [580/1299] D_loss: -3.1077, G_loss: 3.1816\n",
      "  Batch [590/1299] D_loss: -1.9850, G_loss: 1.9864\n",
      "  Batch [600/1299] D_loss: -2.1270, G_loss: 5.6115\n",
      "  Batch [610/1299] D_loss: -1.0006, G_loss: 4.1353\n",
      "  Batch [620/1299] D_loss: -2.9237, G_loss: 3.2565\n",
      "  Batch [630/1299] D_loss: -2.7196, G_loss: 2.4079\n",
      "  Batch [640/1299] D_loss: -1.7901, G_loss: 2.2177\n",
      "  Batch [650/1299] D_loss: -2.7581, G_loss: 1.9282\n",
      "  Batch [660/1299] D_loss: -2.4879, G_loss: 2.0688\n",
      "  Batch [670/1299] D_loss: -1.5342, G_loss: 2.5310\n",
      "  Batch [680/1299] D_loss: -1.8073, G_loss: 2.2146\n",
      "  Batch [690/1299] D_loss: -1.6796, G_loss: 2.2452\n",
      "  Batch [700/1299] D_loss: -0.2746, G_loss: 4.8060\n",
      "  Batch [710/1299] D_loss: -1.5077, G_loss: 4.6607\n",
      "  Batch [720/1299] D_loss: -4.2633, G_loss: 4.4121\n",
      "  Batch [730/1299] D_loss: -1.5299, G_loss: 4.9390\n",
      "  Batch [740/1299] D_loss: -1.9686, G_loss: 3.2586\n",
      "  Batch [750/1299] D_loss: -4.1888, G_loss: 3.8615\n",
      "  Batch [760/1299] D_loss: -0.8894, G_loss: 4.0768\n",
      "  Batch [770/1299] D_loss: -1.9845, G_loss: 4.9612\n",
      "  Batch [780/1299] D_loss: -1.1027, G_loss: 4.3138\n",
      "  Batch [790/1299] D_loss: -2.1868, G_loss: 2.9869\n",
      "  Batch [800/1299] D_loss: -3.0748, G_loss: 2.9806\n",
      "  Batch [810/1299] D_loss: -3.5493, G_loss: 3.7961\n",
      "  Batch [820/1299] D_loss: -3.0349, G_loss: 3.9247\n",
      "  Batch [830/1299] D_loss: -3.4975, G_loss: 2.4968\n",
      "  Batch [840/1299] D_loss: -0.3569, G_loss: 2.9083\n",
      "  Batch [850/1299] D_loss: -3.0122, G_loss: 5.0698\n",
      "  Batch [860/1299] D_loss: -1.9782, G_loss: 2.0310\n",
      "  Batch [870/1299] D_loss: -2.2267, G_loss: 4.5778\n",
      "  Batch [880/1299] D_loss: -2.8152, G_loss: 3.6707\n",
      "  Batch [890/1299] D_loss: -2.9215, G_loss: 4.0496\n",
      "  Batch [900/1299] D_loss: -0.8493, G_loss: 3.1210\n",
      "  Batch [910/1299] D_loss: -2.5952, G_loss: 3.8410\n",
      "  Batch [920/1299] D_loss: -1.4260, G_loss: 1.8165\n",
      "  Batch [930/1299] D_loss: -3.2519, G_loss: 1.6906\n",
      "  Batch [940/1299] D_loss: -3.7705, G_loss: 3.1055\n",
      "  Batch [950/1299] D_loss: -3.0927, G_loss: 2.5725\n",
      "  Batch [960/1299] D_loss: -1.4702, G_loss: 2.2711\n",
      "  Batch [970/1299] D_loss: -1.9976, G_loss: 2.8588\n",
      "  Batch [980/1299] D_loss: -1.2583, G_loss: 2.8436\n",
      "  Batch [990/1299] D_loss: -2.9691, G_loss: 1.9242\n",
      "  Batch [1000/1299] D_loss: -1.8104, G_loss: 2.5967\n",
      "  Batch [1010/1299] D_loss: -2.6606, G_loss: 2.7688\n",
      "  Batch [1020/1299] D_loss: -2.1569, G_loss: 2.7252\n",
      "  Batch [1030/1299] D_loss: -2.5997, G_loss: 2.2530\n",
      "  Batch [1040/1299] D_loss: -2.4163, G_loss: 3.1635\n",
      "  Batch [1050/1299] D_loss: -3.5809, G_loss: 3.0272\n",
      "  Batch [1060/1299] D_loss: -3.4584, G_loss: 2.6541\n",
      "  Batch [1070/1299] D_loss: -2.6655, G_loss: 2.8783\n",
      "  Batch [1080/1299] D_loss: -2.8375, G_loss: 2.5969\n",
      "  Batch [1090/1299] D_loss: -2.2085, G_loss: 2.7619\n",
      "  Batch [1100/1299] D_loss: -4.0610, G_loss: 3.2959\n",
      "  Batch [1110/1299] D_loss: -2.5533, G_loss: 3.8027\n",
      "  Batch [1120/1299] D_loss: -3.8498, G_loss: 3.6521\n",
      "  Batch [1130/1299] D_loss: -3.0178, G_loss: 4.6429\n",
      "  Batch [1140/1299] D_loss: -1.5972, G_loss: 2.6166\n",
      "  Batch [1150/1299] D_loss: -0.6852, G_loss: 2.7289\n",
      "  Batch [1160/1299] D_loss: -2.3003, G_loss: 2.8960\n",
      "  Batch [1170/1299] D_loss: -2.3414, G_loss: 3.1642\n",
      "  Batch [1180/1299] D_loss: -3.1912, G_loss: 1.3839\n",
      "  Batch [1190/1299] D_loss: -1.4969, G_loss: 1.0602\n",
      "  Batch [1200/1299] D_loss: -3.6876, G_loss: 1.7861\n",
      "  Batch [1210/1299] D_loss: -3.2724, G_loss: 1.6543\n",
      "  Batch [1220/1299] D_loss: -3.5531, G_loss: 2.8782\n",
      "  Batch [1230/1299] D_loss: -3.8715, G_loss: 3.1183\n",
      "  Batch [1240/1299] D_loss: -2.1808, G_loss: 2.5115\n",
      "  Batch [1250/1299] D_loss: -2.3179, G_loss: 1.0438\n",
      "  Batch [1260/1299] D_loss: -2.1833, G_loss: 4.1962\n",
      "  Batch [1270/1299] D_loss: -2.3287, G_loss: 3.3845\n",
      "  Batch [1280/1299] D_loss: -3.2975, G_loss: 1.9222\n",
      "  Batch [1290/1299] D_loss: -1.5349, G_loss: 3.4370\n",
      "\n",
      "Epoch 25 Summary:\n",
      "  Average D_loss: -2.3288\n",
      "  Average G_loss: 2.9367\n",
      "\n",
      "Epoch [26/100]\n",
      "  Batch [0/1299] D_loss: -3.3419, G_loss: 2.6381\n",
      "  Batch [10/1299] D_loss: -1.5318, G_loss: 2.4742\n",
      "  Batch [20/1299] D_loss: -3.0757, G_loss: 3.7657\n",
      "  Batch [30/1299] D_loss: -4.2195, G_loss: 1.5295\n",
      "  Batch [40/1299] D_loss: -3.9971, G_loss: 2.8989\n",
      "  Batch [50/1299] D_loss: -1.3023, G_loss: 1.9269\n",
      "  Batch [60/1299] D_loss: -1.7244, G_loss: 2.6359\n",
      "  Batch [70/1299] D_loss: -2.9087, G_loss: 3.5030\n",
      "  Batch [80/1299] D_loss: -1.0128, G_loss: 2.1650\n",
      "  Batch [90/1299] D_loss: -3.4040, G_loss: 2.8481\n",
      "  Batch [100/1299] D_loss: -2.0635, G_loss: 0.3564\n",
      "  Batch [110/1299] D_loss: -1.4887, G_loss: 3.1550\n",
      "  Batch [120/1299] D_loss: -3.0336, G_loss: 3.2721\n",
      "  Batch [130/1299] D_loss: -2.1807, G_loss: 3.2538\n",
      "  Batch [140/1299] D_loss: -3.3923, G_loss: 0.9642\n",
      "  Batch [150/1299] D_loss: -3.3220, G_loss: 2.2162\n",
      "  Batch [160/1299] D_loss: -2.3866, G_loss: 2.1806\n",
      "  Batch [170/1299] D_loss: -1.7630, G_loss: 3.4485\n",
      "  Batch [180/1299] D_loss: -2.7579, G_loss: 1.8814\n",
      "  Batch [190/1299] D_loss: -1.6209, G_loss: 3.6563\n",
      "  Batch [200/1299] D_loss: -2.7565, G_loss: 3.1720\n",
      "  Batch [210/1299] D_loss: -1.9910, G_loss: 3.0964\n",
      "  Batch [220/1299] D_loss: -3.2580, G_loss: 4.4701\n",
      "  Batch [230/1299] D_loss: -2.4384, G_loss: 3.1369\n",
      "  Batch [240/1299] D_loss: -2.1366, G_loss: 2.3773\n",
      "  Batch [250/1299] D_loss: -2.3636, G_loss: 3.0236\n",
      "  Batch [260/1299] D_loss: -3.4146, G_loss: 2.1471\n",
      "  Batch [270/1299] D_loss: -2.3584, G_loss: 2.4627\n",
      "  Batch [280/1299] D_loss: -2.4171, G_loss: 3.2427\n",
      "  Batch [290/1299] D_loss: -1.9971, G_loss: 1.9631\n",
      "  Batch [300/1299] D_loss: -2.7730, G_loss: 2.6785\n",
      "  Batch [310/1299] D_loss: -2.6470, G_loss: 2.4112\n",
      "  Batch [320/1299] D_loss: -1.6675, G_loss: 3.2156\n",
      "  Batch [330/1299] D_loss: -2.8449, G_loss: 2.9904\n",
      "  Batch [340/1299] D_loss: -3.9234, G_loss: 3.4846\n",
      "  Batch [350/1299] D_loss: -2.5096, G_loss: 3.9531\n",
      "  Batch [360/1299] D_loss: -3.3825, G_loss: 3.5477\n",
      "  Batch [370/1299] D_loss: -2.7526, G_loss: 4.3245\n",
      "  Batch [380/1299] D_loss: -3.7321, G_loss: 4.8267\n",
      "  Batch [390/1299] D_loss: -1.9842, G_loss: 3.7452\n",
      "  Batch [400/1299] D_loss: -2.7068, G_loss: 3.9847\n",
      "  Batch [410/1299] D_loss: -1.6919, G_loss: 3.8689\n",
      "  Batch [420/1299] D_loss: -2.4765, G_loss: 3.3585\n",
      "  Batch [430/1299] D_loss: -1.8520, G_loss: 2.9226\n",
      "  Batch [440/1299] D_loss: -2.9239, G_loss: 2.4967\n",
      "  Batch [450/1299] D_loss: -2.7804, G_loss: 3.5074\n",
      "  Batch [460/1299] D_loss: -2.8330, G_loss: 2.7829\n",
      "  Batch [470/1299] D_loss: -3.2278, G_loss: 2.9691\n",
      "  Batch [480/1299] D_loss: -2.5147, G_loss: 3.4315\n",
      "  Batch [490/1299] D_loss: -3.4852, G_loss: 2.6234\n",
      "  Batch [500/1299] D_loss: -3.2910, G_loss: 1.8882\n",
      "  Batch [510/1299] D_loss: -1.8630, G_loss: 1.2717\n",
      "  Batch [520/1299] D_loss: -2.7682, G_loss: 3.3459\n",
      "  Batch [530/1299] D_loss: -2.8594, G_loss: 2.2387\n",
      "  Batch [540/1299] D_loss: -1.8474, G_loss: 2.2512\n",
      "  Batch [550/1299] D_loss: -2.7063, G_loss: 2.5561\n",
      "  Batch [560/1299] D_loss: -2.3185, G_loss: 3.0162\n",
      "  Batch [570/1299] D_loss: -3.4129, G_loss: 4.6479\n",
      "  Batch [580/1299] D_loss: -3.0011, G_loss: 3.2789\n",
      "  Batch [590/1299] D_loss: -3.1895, G_loss: 3.8387\n",
      "  Batch [600/1299] D_loss: -2.0851, G_loss: 4.8164\n",
      "  Batch [610/1299] D_loss: -2.1547, G_loss: 3.7862\n",
      "  Batch [620/1299] D_loss: -0.8444, G_loss: 3.4514\n",
      "  Batch [630/1299] D_loss: -1.6174, G_loss: 4.5377\n",
      "  Batch [640/1299] D_loss: -2.8025, G_loss: 5.0086\n",
      "  Batch [650/1299] D_loss: -3.1565, G_loss: 2.2660\n",
      "  Batch [660/1299] D_loss: -2.2663, G_loss: 3.5303\n",
      "  Batch [670/1299] D_loss: -3.1911, G_loss: 3.3960\n",
      "  Batch [680/1299] D_loss: -4.1481, G_loss: 4.0078\n",
      "  Batch [690/1299] D_loss: -2.6779, G_loss: 4.5399\n",
      "  Batch [700/1299] D_loss: -2.5140, G_loss: 4.7983\n",
      "  Batch [710/1299] D_loss: -3.4951, G_loss: 6.3432\n",
      "  Batch [720/1299] D_loss: -3.0889, G_loss: 4.5922\n",
      "  Batch [730/1299] D_loss: -2.9215, G_loss: 3.3533\n",
      "  Batch [740/1299] D_loss: -3.0373, G_loss: 2.1655\n",
      "  Batch [750/1299] D_loss: -1.2586, G_loss: 2.5903\n",
      "  Batch [760/1299] D_loss: -2.9883, G_loss: 1.5073\n",
      "  Batch [770/1299] D_loss: -3.3281, G_loss: 3.2744\n",
      "  Batch [780/1299] D_loss: -2.3090, G_loss: 2.5755\n",
      "  Batch [790/1299] D_loss: -1.4387, G_loss: 2.9951\n",
      "  Batch [800/1299] D_loss: -1.2914, G_loss: 4.6800\n",
      "  Batch [810/1299] D_loss: -4.0099, G_loss: 4.8918\n",
      "  Batch [820/1299] D_loss: -2.0629, G_loss: 4.8371\n",
      "  Batch [830/1299] D_loss: -2.1366, G_loss: 5.0617\n",
      "  Batch [840/1299] D_loss: -2.4429, G_loss: 3.3804\n",
      "  Batch [850/1299] D_loss: -1.7554, G_loss: 3.2391\n",
      "  Batch [860/1299] D_loss: -1.6812, G_loss: 5.1260\n",
      "  Batch [870/1299] D_loss: -1.0659, G_loss: 2.8706\n",
      "  Batch [880/1299] D_loss: -1.5508, G_loss: 5.3334\n",
      "  Batch [890/1299] D_loss: -2.9103, G_loss: 4.0523\n",
      "  Batch [900/1299] D_loss: -3.2563, G_loss: 4.5560\n",
      "  Batch [910/1299] D_loss: -2.5630, G_loss: 4.7139\n",
      "  Batch [920/1299] D_loss: -3.0558, G_loss: 3.1481\n",
      "  Batch [930/1299] D_loss: -3.3784, G_loss: 3.1663\n",
      "  Batch [940/1299] D_loss: -2.5970, G_loss: 3.5791\n",
      "  Batch [950/1299] D_loss: -2.5931, G_loss: 2.8825\n",
      "  Batch [960/1299] D_loss: -2.7152, G_loss: 3.2625\n",
      "  Batch [970/1299] D_loss: -3.1725, G_loss: 3.0239\n",
      "  Batch [980/1299] D_loss: -2.5815, G_loss: 4.6051\n",
      "  Batch [990/1299] D_loss: -2.7836, G_loss: 2.3746\n",
      "  Batch [1000/1299] D_loss: -1.2745, G_loss: 2.6623\n",
      "  Batch [1010/1299] D_loss: -2.2470, G_loss: 2.8692\n",
      "  Batch [1020/1299] D_loss: -2.5649, G_loss: 3.7760\n",
      "  Batch [1030/1299] D_loss: -1.4378, G_loss: 4.3809\n",
      "  Batch [1040/1299] D_loss: -1.8409, G_loss: 5.4878\n",
      "  Batch [1050/1299] D_loss: -2.0102, G_loss: 2.8959\n",
      "  Batch [1060/1299] D_loss: -2.5997, G_loss: 1.2898\n",
      "  Batch [1070/1299] D_loss: -2.0143, G_loss: 3.1200\n",
      "  Batch [1080/1299] D_loss: -3.0683, G_loss: 2.6307\n",
      "  Batch [1090/1299] D_loss: -2.2579, G_loss: 3.5042\n",
      "  Batch [1100/1299] D_loss: -4.2318, G_loss: 4.9043\n",
      "  Batch [1110/1299] D_loss: -2.2410, G_loss: 3.0293\n",
      "  Batch [1120/1299] D_loss: -2.4577, G_loss: 3.0128\n",
      "  Batch [1130/1299] D_loss: -2.4347, G_loss: 2.8386\n",
      "  Batch [1140/1299] D_loss: -1.7536, G_loss: 3.4970\n",
      "  Batch [1150/1299] D_loss: -3.0096, G_loss: 1.3399\n",
      "  Batch [1160/1299] D_loss: -2.5942, G_loss: 2.5738\n",
      "  Batch [1170/1299] D_loss: -2.6193, G_loss: 1.4309\n",
      "  Batch [1180/1299] D_loss: -2.8609, G_loss: 1.6170\n",
      "  Batch [1190/1299] D_loss: -2.2575, G_loss: 1.0875\n",
      "  Batch [1200/1299] D_loss: -2.7876, G_loss: 2.2373\n",
      "  Batch [1210/1299] D_loss: -3.4117, G_loss: 1.5301\n",
      "  Batch [1220/1299] D_loss: -3.3094, G_loss: 4.7865\n",
      "  Batch [1230/1299] D_loss: -1.7273, G_loss: 2.3900\n",
      "  Batch [1240/1299] D_loss: -2.7575, G_loss: 1.6559\n",
      "  Batch [1250/1299] D_loss: -1.9386, G_loss: 3.7102\n",
      "  Batch [1260/1299] D_loss: -1.9152, G_loss: 4.0826\n",
      "  Batch [1270/1299] D_loss: -1.6745, G_loss: 3.5576\n",
      "  Batch [1280/1299] D_loss: -2.9757, G_loss: 2.2935\n",
      "  Batch [1290/1299] D_loss: -2.2712, G_loss: 2.1288\n",
      "\n",
      "Epoch 26 Summary:\n",
      "  Average D_loss: -2.3219\n",
      "  Average G_loss: 3.2597\n",
      "\n",
      "Epoch [27/100]\n",
      "  Batch [0/1299] D_loss: -3.4956, G_loss: 1.7368\n",
      "  Batch [10/1299] D_loss: -2.3994, G_loss: 3.1794\n",
      "  Batch [20/1299] D_loss: -2.8403, G_loss: 5.0435\n",
      "  Batch [30/1299] D_loss: -1.8829, G_loss: 1.6968\n",
      "  Batch [40/1299] D_loss: -3.2461, G_loss: 0.9470\n",
      "  Batch [50/1299] D_loss: -2.6066, G_loss: 6.3306\n",
      "  Batch [60/1299] D_loss: -2.0215, G_loss: 2.6446\n",
      "  Batch [70/1299] D_loss: -2.6020, G_loss: 5.6691\n",
      "  Batch [80/1299] D_loss: -3.5542, G_loss: 1.9517\n",
      "  Batch [90/1299] D_loss: -2.5977, G_loss: 3.1293\n",
      "  Batch [100/1299] D_loss: -2.1116, G_loss: 5.3796\n",
      "  Batch [110/1299] D_loss: -3.4324, G_loss: 0.7769\n",
      "  Batch [120/1299] D_loss: -2.3650, G_loss: 1.9613\n",
      "  Batch [130/1299] D_loss: -1.9508, G_loss: 0.7476\n",
      "  Batch [140/1299] D_loss: -3.2968, G_loss: 1.6577\n",
      "  Batch [150/1299] D_loss: -3.7083, G_loss: 2.0528\n",
      "  Batch [160/1299] D_loss: -2.3511, G_loss: 3.3868\n",
      "  Batch [170/1299] D_loss: -1.1974, G_loss: -0.5166\n",
      "  Batch [180/1299] D_loss: -2.2859, G_loss: 1.3421\n",
      "  Batch [190/1299] D_loss: -2.6455, G_loss: 4.3630\n",
      "  Batch [200/1299] D_loss: -1.3068, G_loss: 4.1178\n",
      "  Batch [210/1299] D_loss: -2.0659, G_loss: 3.4912\n",
      "  Batch [220/1299] D_loss: -2.4317, G_loss: 4.0011\n",
      "  Batch [230/1299] D_loss: -0.7517, G_loss: 2.5132\n",
      "  Batch [240/1299] D_loss: -4.1114, G_loss: 1.9694\n",
      "  Batch [250/1299] D_loss: -2.3191, G_loss: 1.6660\n",
      "  Batch [260/1299] D_loss: -1.9591, G_loss: 0.4859\n",
      "  Batch [270/1299] D_loss: -2.3035, G_loss: 2.0300\n",
      "  Batch [280/1299] D_loss: -3.6782, G_loss: 1.6813\n",
      "  Batch [290/1299] D_loss: -3.0465, G_loss: 2.6163\n",
      "  Batch [300/1299] D_loss: -3.1741, G_loss: 3.2508\n",
      "  Batch [310/1299] D_loss: -1.5470, G_loss: 2.1178\n",
      "  Batch [320/1299] D_loss: -1.7565, G_loss: 3.6939\n",
      "  Batch [330/1299] D_loss: -1.9976, G_loss: 3.2010\n",
      "  Batch [340/1299] D_loss: -2.8533, G_loss: 2.9320\n",
      "  Batch [350/1299] D_loss: -3.3419, G_loss: 3.4829\n",
      "  Batch [360/1299] D_loss: -1.9480, G_loss: 4.8696\n",
      "  Batch [370/1299] D_loss: -1.8650, G_loss: 3.2650\n",
      "  Batch [380/1299] D_loss: -2.9952, G_loss: 5.1876\n",
      "  Batch [390/1299] D_loss: -2.4978, G_loss: 4.0073\n",
      "  Batch [400/1299] D_loss: -3.4664, G_loss: 1.8374\n",
      "  Batch [410/1299] D_loss: -2.8030, G_loss: 0.5909\n",
      "  Batch [420/1299] D_loss: -1.3713, G_loss: 2.4439\n",
      "  Batch [430/1299] D_loss: -2.2081, G_loss: 4.0235\n",
      "  Batch [440/1299] D_loss: -2.1033, G_loss: 3.1069\n",
      "  Batch [450/1299] D_loss: -2.8364, G_loss: 0.9493\n",
      "  Batch [460/1299] D_loss: -2.2310, G_loss: 1.8056\n",
      "  Batch [470/1299] D_loss: -3.5919, G_loss: 3.7925\n",
      "  Batch [480/1299] D_loss: -2.6777, G_loss: 3.3305\n",
      "  Batch [490/1299] D_loss: -2.6084, G_loss: 1.1179\n",
      "  Batch [500/1299] D_loss: -3.3288, G_loss: 2.3060\n",
      "  Batch [510/1299] D_loss: -4.4192, G_loss: 1.9216\n",
      "  Batch [520/1299] D_loss: -1.1994, G_loss: 2.1712\n",
      "  Batch [530/1299] D_loss: -3.3844, G_loss: 1.8510\n",
      "  Batch [540/1299] D_loss: -1.6593, G_loss: 1.0127\n",
      "  Batch [550/1299] D_loss: -3.3017, G_loss: 3.3846\n",
      "  Batch [560/1299] D_loss: -2.4278, G_loss: 1.7613\n",
      "  Batch [570/1299] D_loss: -1.8689, G_loss: 1.6676\n",
      "  Batch [580/1299] D_loss: -2.5999, G_loss: 2.7693\n",
      "  Batch [590/1299] D_loss: -2.9174, G_loss: 1.5758\n",
      "  Batch [600/1299] D_loss: -2.4420, G_loss: 0.9395\n",
      "  Batch [610/1299] D_loss: -2.4297, G_loss: 2.4843\n",
      "  Batch [620/1299] D_loss: -2.5725, G_loss: 2.6350\n",
      "  Batch [630/1299] D_loss: -3.1094, G_loss: 3.1460\n",
      "  Batch [640/1299] D_loss: -1.6036, G_loss: 3.3564\n",
      "  Batch [650/1299] D_loss: -2.2077, G_loss: 2.3386\n",
      "  Batch [660/1299] D_loss: -3.2522, G_loss: 2.0162\n",
      "  Batch [670/1299] D_loss: -3.7137, G_loss: 2.4055\n",
      "  Batch [680/1299] D_loss: -3.6361, G_loss: 1.2260\n",
      "  Batch [690/1299] D_loss: -0.8758, G_loss: 3.0423\n",
      "  Batch [700/1299] D_loss: -2.4488, G_loss: 2.6749\n",
      "  Batch [710/1299] D_loss: -1.5002, G_loss: 1.9987\n",
      "  Batch [720/1299] D_loss: -1.9177, G_loss: 1.8192\n",
      "  Batch [730/1299] D_loss: -2.7484, G_loss: 2.4158\n",
      "  Batch [740/1299] D_loss: -1.2969, G_loss: 1.6062\n",
      "  Batch [750/1299] D_loss: -2.5467, G_loss: 3.2617\n",
      "  Batch [760/1299] D_loss: -3.5802, G_loss: 2.2521\n",
      "  Batch [770/1299] D_loss: -2.2495, G_loss: 0.1566\n",
      "  Batch [780/1299] D_loss: -2.7063, G_loss: 3.2485\n",
      "  Batch [790/1299] D_loss: -1.4930, G_loss: 2.3460\n",
      "  Batch [800/1299] D_loss: -2.1071, G_loss: 2.2125\n",
      "  Batch [810/1299] D_loss: -2.3049, G_loss: 1.2119\n",
      "  Batch [820/1299] D_loss: -2.7793, G_loss: 2.1567\n",
      "  Batch [830/1299] D_loss: -1.9247, G_loss: 3.5847\n",
      "  Batch [840/1299] D_loss: -2.9612, G_loss: 3.2688\n",
      "  Batch [850/1299] D_loss: -3.4083, G_loss: 3.6345\n",
      "  Batch [860/1299] D_loss: -1.8571, G_loss: 4.5143\n",
      "  Batch [870/1299] D_loss: -2.0008, G_loss: 4.7120\n",
      "  Batch [880/1299] D_loss: -2.1584, G_loss: 3.7605\n",
      "  Batch [890/1299] D_loss: -2.3279, G_loss: 2.4296\n",
      "  Batch [900/1299] D_loss: -1.9379, G_loss: 1.8682\n",
      "  Batch [910/1299] D_loss: -2.0622, G_loss: 2.4233\n",
      "  Batch [920/1299] D_loss: -2.7270, G_loss: 0.1776\n",
      "  Batch [930/1299] D_loss: -2.8837, G_loss: 1.1883\n",
      "  Batch [940/1299] D_loss: -3.1162, G_loss: 4.4133\n",
      "  Batch [950/1299] D_loss: -2.8313, G_loss: 1.6147\n",
      "  Batch [960/1299] D_loss: -3.9223, G_loss: 1.4040\n",
      "  Batch [970/1299] D_loss: -2.5098, G_loss: 3.0371\n",
      "  Batch [980/1299] D_loss: -2.1846, G_loss: 2.9660\n",
      "  Batch [990/1299] D_loss: -2.0676, G_loss: 5.2443\n",
      "  Batch [1000/1299] D_loss: -2.2901, G_loss: 3.2983\n",
      "  Batch [1010/1299] D_loss: -1.7219, G_loss: 4.9864\n",
      "  Batch [1020/1299] D_loss: -2.5000, G_loss: 2.9334\n",
      "  Batch [1030/1299] D_loss: -1.4106, G_loss: 4.7599\n",
      "  Batch [1040/1299] D_loss: -1.9940, G_loss: 1.4595\n",
      "  Batch [1050/1299] D_loss: -2.3654, G_loss: 0.6404\n",
      "  Batch [1060/1299] D_loss: -1.2180, G_loss: 1.8048\n",
      "  Batch [1070/1299] D_loss: -2.6373, G_loss: 2.9051\n",
      "  Batch [1080/1299] D_loss: -3.2462, G_loss: 3.7796\n",
      "  Batch [1090/1299] D_loss: -1.4572, G_loss: 3.9206\n",
      "  Batch [1100/1299] D_loss: -3.4751, G_loss: 1.6245\n",
      "  Batch [1110/1299] D_loss: -2.8611, G_loss: 2.1880\n",
      "  Batch [1120/1299] D_loss: -3.1814, G_loss: 1.8556\n",
      "  Batch [1130/1299] D_loss: -2.0796, G_loss: 1.3312\n",
      "  Batch [1140/1299] D_loss: -3.5224, G_loss: 2.2354\n",
      "  Batch [1150/1299] D_loss: -1.9383, G_loss: 1.3470\n",
      "  Batch [1160/1299] D_loss: -2.2510, G_loss: 2.7779\n",
      "  Batch [1170/1299] D_loss: -1.3087, G_loss: 2.2602\n",
      "  Batch [1180/1299] D_loss: -1.4915, G_loss: 2.0967\n",
      "  Batch [1190/1299] D_loss: -2.8367, G_loss: 1.4448\n",
      "  Batch [1200/1299] D_loss: -2.9584, G_loss: 2.6645\n",
      "  Batch [1210/1299] D_loss: -3.5628, G_loss: 2.7959\n",
      "  Batch [1220/1299] D_loss: -1.5372, G_loss: 2.1627\n",
      "  Batch [1230/1299] D_loss: -3.9538, G_loss: 3.7927\n",
      "  Batch [1240/1299] D_loss: -3.0142, G_loss: 3.8684\n",
      "  Batch [1250/1299] D_loss: -1.7027, G_loss: 3.7511\n",
      "  Batch [1260/1299] D_loss: -1.8309, G_loss: 1.9376\n",
      "  Batch [1270/1299] D_loss: -2.4360, G_loss: 2.2249\n",
      "  Batch [1280/1299] D_loss: -4.1555, G_loss: 2.0026\n",
      "  Batch [1290/1299] D_loss: -3.0754, G_loss: 2.6078\n",
      "\n",
      "Epoch 27 Summary:\n",
      "  Average D_loss: -2.3049\n",
      "  Average G_loss: 2.6055\n",
      "\n",
      "Epoch [28/100]\n",
      "  Batch [0/1299] D_loss: -3.1537, G_loss: 1.4589\n",
      "  Batch [10/1299] D_loss: -1.3117, G_loss: 1.6559\n",
      "  Batch [20/1299] D_loss: -1.5636, G_loss: 1.9652\n",
      "  Batch [30/1299] D_loss: -2.6283, G_loss: 1.1767\n",
      "  Batch [40/1299] D_loss: -1.8057, G_loss: 0.7058\n",
      "  Batch [50/1299] D_loss: -3.5434, G_loss: 0.8765\n",
      "  Batch [60/1299] D_loss: -2.0435, G_loss: 2.9088\n",
      "  Batch [70/1299] D_loss: -2.6462, G_loss: 3.5033\n",
      "  Batch [80/1299] D_loss: -2.5776, G_loss: 1.7699\n",
      "  Batch [90/1299] D_loss: -2.7750, G_loss: 2.1568\n",
      "  Batch [100/1299] D_loss: -2.4748, G_loss: 2.0120\n",
      "  Batch [110/1299] D_loss: -3.4832, G_loss: 2.5292\n",
      "  Batch [120/1299] D_loss: -1.9508, G_loss: 3.0082\n",
      "  Batch [130/1299] D_loss: -1.3318, G_loss: 3.8379\n",
      "  Batch [140/1299] D_loss: -2.6530, G_loss: 2.0662\n",
      "  Batch [150/1299] D_loss: -2.7255, G_loss: 5.1193\n",
      "  Batch [160/1299] D_loss: -1.2264, G_loss: 3.8224\n",
      "  Batch [170/1299] D_loss: -2.1993, G_loss: 2.7286\n",
      "  Batch [180/1299] D_loss: -1.5290, G_loss: 3.5370\n",
      "  Batch [190/1299] D_loss: -3.0618, G_loss: 1.8982\n",
      "  Batch [200/1299] D_loss: -1.8587, G_loss: 2.8698\n",
      "  Batch [210/1299] D_loss: -4.7559, G_loss: 4.6171\n",
      "  Batch [220/1299] D_loss: -2.4916, G_loss: 4.1013\n",
      "  Batch [230/1299] D_loss: -3.0375, G_loss: 4.7624\n",
      "  Batch [240/1299] D_loss: -2.6631, G_loss: 4.1308\n",
      "  Batch [250/1299] D_loss: -2.3055, G_loss: 1.7095\n",
      "  Batch [260/1299] D_loss: -4.3712, G_loss: 1.8447\n",
      "  Batch [270/1299] D_loss: -2.4502, G_loss: 1.5942\n",
      "  Batch [280/1299] D_loss: -3.5629, G_loss: 3.1358\n",
      "  Batch [290/1299] D_loss: -1.3700, G_loss: 3.7056\n",
      "  Batch [300/1299] D_loss: -2.9103, G_loss: 4.0454\n",
      "  Batch [310/1299] D_loss: -1.7509, G_loss: 4.1932\n",
      "  Batch [320/1299] D_loss: -2.6437, G_loss: 3.2513\n",
      "  Batch [330/1299] D_loss: -3.7400, G_loss: 3.5138\n",
      "  Batch [340/1299] D_loss: -2.2112, G_loss: 3.8704\n",
      "  Batch [350/1299] D_loss: -2.7824, G_loss: 4.6696\n",
      "  Batch [360/1299] D_loss: -1.7841, G_loss: 3.6597\n",
      "  Batch [370/1299] D_loss: -1.9200, G_loss: 1.3540\n",
      "  Batch [380/1299] D_loss: -1.4706, G_loss: 3.0140\n",
      "  Batch [390/1299] D_loss: -3.1478, G_loss: 1.5614\n",
      "  Batch [400/1299] D_loss: -3.2725, G_loss: 2.5864\n",
      "  Batch [410/1299] D_loss: -3.7044, G_loss: 2.6666\n",
      "  Batch [420/1299] D_loss: -2.5577, G_loss: 4.9342\n",
      "  Batch [430/1299] D_loss: -2.1470, G_loss: 4.9443\n",
      "  Batch [440/1299] D_loss: -1.8780, G_loss: 3.7822\n",
      "  Batch [450/1299] D_loss: -1.8080, G_loss: 4.2594\n",
      "  Batch [460/1299] D_loss: -3.1771, G_loss: 2.6514\n",
      "  Batch [470/1299] D_loss: -3.5198, G_loss: 4.9264\n",
      "  Batch [480/1299] D_loss: -4.0999, G_loss: 2.6206\n",
      "  Batch [490/1299] D_loss: -2.9699, G_loss: 3.0972\n",
      "  Batch [500/1299] D_loss: -3.0759, G_loss: 2.4231\n",
      "  Batch [510/1299] D_loss: -3.7232, G_loss: -0.2399\n",
      "  Batch [520/1299] D_loss: -0.9954, G_loss: 1.7165\n",
      "  Batch [530/1299] D_loss: -3.1390, G_loss: 2.8887\n",
      "  Batch [540/1299] D_loss: -3.0775, G_loss: 0.5271\n",
      "  Batch [550/1299] D_loss: -2.2329, G_loss: 1.5066\n",
      "  Batch [560/1299] D_loss: -2.5003, G_loss: 1.2768\n",
      "  Batch [570/1299] D_loss: -3.3497, G_loss: 1.2682\n",
      "  Batch [580/1299] D_loss: -2.6948, G_loss: 3.2059\n",
      "  Batch [590/1299] D_loss: -2.5909, G_loss: 1.5633\n",
      "  Batch [600/1299] D_loss: -3.1872, G_loss: 0.3965\n",
      "  Batch [610/1299] D_loss: -1.8956, G_loss: 0.6995\n",
      "  Batch [620/1299] D_loss: -1.6642, G_loss: 0.8174\n",
      "  Batch [630/1299] D_loss: -2.7247, G_loss: 1.9603\n",
      "  Batch [640/1299] D_loss: -3.3277, G_loss: 0.8345\n",
      "  Batch [650/1299] D_loss: -2.8491, G_loss: 2.8080\n",
      "  Batch [660/1299] D_loss: -2.2638, G_loss: -0.4749\n",
      "  Batch [670/1299] D_loss: -1.6533, G_loss: 1.2731\n",
      "  Batch [680/1299] D_loss: -2.1551, G_loss: 1.1619\n",
      "  Batch [690/1299] D_loss: -3.6633, G_loss: 1.9856\n",
      "  Batch [700/1299] D_loss: -1.6328, G_loss: 1.1242\n",
      "  Batch [710/1299] D_loss: -3.3532, G_loss: 0.7874\n",
      "  Batch [720/1299] D_loss: -2.3946, G_loss: 0.8077\n",
      "  Batch [730/1299] D_loss: -3.6484, G_loss: 0.3444\n",
      "  Batch [740/1299] D_loss: -2.0206, G_loss: -0.6833\n",
      "  Batch [750/1299] D_loss: -2.8493, G_loss: 1.6088\n",
      "  Batch [760/1299] D_loss: -1.8325, G_loss: 2.4565\n",
      "  Batch [770/1299] D_loss: -2.4298, G_loss: 0.4312\n",
      "  Batch [780/1299] D_loss: -0.9975, G_loss: 1.2076\n",
      "  Batch [790/1299] D_loss: -2.0358, G_loss: 0.8396\n",
      "  Batch [800/1299] D_loss: -2.2343, G_loss: 1.8012\n",
      "  Batch [810/1299] D_loss: -3.1782, G_loss: 1.6210\n",
      "  Batch [820/1299] D_loss: -1.5098, G_loss: 2.9723\n",
      "  Batch [830/1299] D_loss: -2.1651, G_loss: 2.9784\n",
      "  Batch [840/1299] D_loss: -2.0417, G_loss: 2.7835\n",
      "  Batch [850/1299] D_loss: -2.4099, G_loss: 3.4999\n",
      "  Batch [860/1299] D_loss: -2.2464, G_loss: 4.0338\n",
      "  Batch [870/1299] D_loss: -1.1886, G_loss: 3.1361\n",
      "  Batch [880/1299] D_loss: -4.0145, G_loss: 2.7786\n",
      "  Batch [890/1299] D_loss: -3.4489, G_loss: 1.7227\n",
      "  Batch [900/1299] D_loss: -2.6803, G_loss: 1.9447\n",
      "  Batch [910/1299] D_loss: -2.3519, G_loss: 1.5342\n",
      "  Batch [920/1299] D_loss: -4.2264, G_loss: 3.6055\n",
      "  Batch [930/1299] D_loss: -2.1013, G_loss: 2.5325\n",
      "  Batch [940/1299] D_loss: -1.4979, G_loss: 1.6430\n",
      "  Batch [950/1299] D_loss: -3.3809, G_loss: 1.9994\n",
      "  Batch [960/1299] D_loss: -2.3070, G_loss: 1.9898\n",
      "  Batch [970/1299] D_loss: -3.2628, G_loss: 1.3651\n",
      "  Batch [980/1299] D_loss: -2.4745, G_loss: 2.3817\n",
      "  Batch [990/1299] D_loss: -3.3786, G_loss: 4.0764\n",
      "  Batch [1000/1299] D_loss: -2.1670, G_loss: 2.7341\n",
      "  Batch [1010/1299] D_loss: -2.2021, G_loss: 2.9149\n",
      "  Batch [1020/1299] D_loss: -2.3580, G_loss: 3.9912\n",
      "  Batch [1030/1299] D_loss: -2.4684, G_loss: 2.2702\n",
      "  Batch [1040/1299] D_loss: -2.5060, G_loss: 0.8535\n",
      "  Batch [1050/1299] D_loss: -1.7199, G_loss: 0.5686\n",
      "  Batch [1060/1299] D_loss: -2.3305, G_loss: 2.2243\n",
      "  Batch [1070/1299] D_loss: -2.3685, G_loss: 3.2540\n",
      "  Batch [1080/1299] D_loss: -3.3411, G_loss: 3.2703\n",
      "  Batch [1090/1299] D_loss: -2.7462, G_loss: 3.2388\n",
      "  Batch [1100/1299] D_loss: -2.2954, G_loss: 2.1069\n",
      "  Batch [1110/1299] D_loss: -3.6634, G_loss: 1.2203\n",
      "  Batch [1120/1299] D_loss: -2.9590, G_loss: 1.0916\n",
      "  Batch [1130/1299] D_loss: -2.0501, G_loss: 2.4455\n",
      "  Batch [1140/1299] D_loss: -3.1888, G_loss: 3.0200\n",
      "  Batch [1150/1299] D_loss: -3.5785, G_loss: 2.7671\n",
      "  Batch [1160/1299] D_loss: -2.4290, G_loss: 1.9296\n",
      "  Batch [1170/1299] D_loss: -2.8744, G_loss: 2.3111\n",
      "  Batch [1180/1299] D_loss: -2.1082, G_loss: 2.7913\n",
      "  Batch [1190/1299] D_loss: -1.6831, G_loss: 1.1730\n",
      "  Batch [1200/1299] D_loss: -1.7840, G_loss: 3.1743\n",
      "  Batch [1210/1299] D_loss: -1.6049, G_loss: 2.6960\n",
      "  Batch [1220/1299] D_loss: -3.4819, G_loss: 1.9258\n",
      "  Batch [1230/1299] D_loss: -2.1218, G_loss: 1.7443\n",
      "  Batch [1240/1299] D_loss: -2.0577, G_loss: 2.8168\n",
      "  Batch [1250/1299] D_loss: -2.2599, G_loss: 1.2135\n",
      "  Batch [1260/1299] D_loss: -2.5722, G_loss: 1.1850\n",
      "  Batch [1270/1299] D_loss: -2.5420, G_loss: 2.6923\n",
      "  Batch [1280/1299] D_loss: -2.6213, G_loss: 3.3536\n",
      "  Batch [1290/1299] D_loss: -1.5824, G_loss: 0.8908\n",
      "\n",
      "Epoch 28 Summary:\n",
      "  Average D_loss: -2.2860\n",
      "  Average G_loss: 2.3833\n",
      "\n",
      "Epoch [29/100]\n",
      "  Batch [0/1299] D_loss: -3.0217, G_loss: 2.4857\n",
      "  Batch [10/1299] D_loss: -4.1619, G_loss: 2.3245\n",
      "  Batch [20/1299] D_loss: -2.4447, G_loss: 2.3162\n",
      "  Batch [30/1299] D_loss: -2.0643, G_loss: 1.0524\n",
      "  Batch [40/1299] D_loss: -2.6495, G_loss: 2.7086\n",
      "  Batch [50/1299] D_loss: -1.6049, G_loss: 3.8822\n",
      "  Batch [60/1299] D_loss: -2.8976, G_loss: 2.1525\n",
      "  Batch [70/1299] D_loss: -2.9706, G_loss: 5.4440\n",
      "  Batch [80/1299] D_loss: -1.3989, G_loss: 4.5656\n",
      "  Batch [90/1299] D_loss: -1.2137, G_loss: 2.3822\n",
      "  Batch [100/1299] D_loss: -2.5352, G_loss: 1.2978\n",
      "  Batch [110/1299] D_loss: -1.7816, G_loss: -0.0572\n",
      "  Batch [120/1299] D_loss: -3.8765, G_loss: 0.9846\n",
      "  Batch [130/1299] D_loss: -2.5086, G_loss: 0.9240\n",
      "  Batch [140/1299] D_loss: -2.3668, G_loss: 0.8115\n",
      "  Batch [150/1299] D_loss: -2.3306, G_loss: 1.8410\n",
      "  Batch [160/1299] D_loss: -2.1158, G_loss: 2.2331\n",
      "  Batch [170/1299] D_loss: -2.7765, G_loss: 1.5509\n",
      "  Batch [180/1299] D_loss: -2.6614, G_loss: 1.7878\n",
      "  Batch [190/1299] D_loss: -2.9080, G_loss: 2.5506\n",
      "  Batch [200/1299] D_loss: -2.4771, G_loss: 2.6837\n",
      "  Batch [210/1299] D_loss: -2.6223, G_loss: 1.8878\n",
      "  Batch [220/1299] D_loss: -2.1932, G_loss: 2.5573\n",
      "  Batch [230/1299] D_loss: -3.8186, G_loss: 1.8448\n",
      "  Batch [240/1299] D_loss: -2.6499, G_loss: 3.5621\n",
      "  Batch [250/1299] D_loss: -2.6893, G_loss: 2.8430\n",
      "  Batch [260/1299] D_loss: -2.5289, G_loss: 0.8756\n",
      "  Batch [270/1299] D_loss: -1.5729, G_loss: 1.3171\n",
      "  Batch [280/1299] D_loss: -2.8448, G_loss: 2.8723\n",
      "  Batch [290/1299] D_loss: -3.1605, G_loss: 1.7923\n",
      "  Batch [300/1299] D_loss: -2.5614, G_loss: 1.5609\n",
      "  Batch [310/1299] D_loss: -2.8549, G_loss: 1.5886\n",
      "  Batch [320/1299] D_loss: -3.6003, G_loss: 4.4789\n",
      "  Batch [330/1299] D_loss: -1.5975, G_loss: 2.6373\n",
      "  Batch [340/1299] D_loss: -3.1800, G_loss: 1.0382\n",
      "  Batch [350/1299] D_loss: -1.5149, G_loss: 1.8466\n",
      "  Batch [360/1299] D_loss: -2.8475, G_loss: 3.0848\n",
      "  Batch [370/1299] D_loss: -1.4159, G_loss: 1.7411\n",
      "  Batch [380/1299] D_loss: -2.7332, G_loss: 2.4666\n",
      "  Batch [390/1299] D_loss: -4.1573, G_loss: 2.6414\n",
      "  Batch [400/1299] D_loss: -1.4674, G_loss: 3.3798\n",
      "  Batch [410/1299] D_loss: -2.9701, G_loss: 4.5226\n",
      "  Batch [420/1299] D_loss: -3.0611, G_loss: 3.4462\n",
      "  Batch [430/1299] D_loss: -2.7951, G_loss: 2.5606\n",
      "  Batch [440/1299] D_loss: -3.4005, G_loss: 2.9796\n",
      "  Batch [450/1299] D_loss: -3.5259, G_loss: 1.2735\n",
      "  Batch [460/1299] D_loss: -3.3272, G_loss: 2.5488\n",
      "  Batch [470/1299] D_loss: -2.8186, G_loss: 4.2902\n",
      "  Batch [480/1299] D_loss: -2.3065, G_loss: 2.7331\n",
      "  Batch [490/1299] D_loss: -3.2668, G_loss: 3.3329\n",
      "  Batch [500/1299] D_loss: -1.3345, G_loss: 3.7965\n",
      "  Batch [510/1299] D_loss: -1.5389, G_loss: 2.5883\n",
      "  Batch [520/1299] D_loss: -2.2300, G_loss: 2.5125\n",
      "  Batch [530/1299] D_loss: -1.5736, G_loss: 2.6465\n",
      "  Batch [540/1299] D_loss: -1.8249, G_loss: 2.4560\n",
      "  Batch [550/1299] D_loss: -3.7856, G_loss: 3.2873\n",
      "  Batch [560/1299] D_loss: -3.6779, G_loss: 1.9475\n",
      "  Batch [570/1299] D_loss: -2.4621, G_loss: 3.2175\n",
      "  Batch [580/1299] D_loss: -3.9119, G_loss: 3.9017\n",
      "  Batch [590/1299] D_loss: -1.8115, G_loss: 3.3472\n",
      "  Batch [600/1299] D_loss: -1.8365, G_loss: 3.9907\n",
      "  Batch [610/1299] D_loss: -2.5737, G_loss: 2.0039\n",
      "  Batch [620/1299] D_loss: -2.6771, G_loss: 0.9314\n",
      "  Batch [630/1299] D_loss: -4.0149, G_loss: 1.2932\n",
      "  Batch [640/1299] D_loss: -3.7057, G_loss: 1.3362\n",
      "  Batch [650/1299] D_loss: -3.0072, G_loss: 1.5963\n",
      "  Batch [660/1299] D_loss: -4.0872, G_loss: 1.6741\n",
      "  Batch [670/1299] D_loss: -1.5862, G_loss: 2.1311\n",
      "  Batch [680/1299] D_loss: -3.5920, G_loss: 2.7640\n",
      "  Batch [690/1299] D_loss: -3.5106, G_loss: 1.9585\n",
      "  Batch [700/1299] D_loss: -2.5619, G_loss: 2.7609\n",
      "  Batch [710/1299] D_loss: -2.7098, G_loss: 1.6649\n",
      "  Batch [720/1299] D_loss: -3.1350, G_loss: 1.4936\n",
      "  Batch [730/1299] D_loss: -3.0780, G_loss: 3.0595\n",
      "  Batch [740/1299] D_loss: 0.4055, G_loss: 2.7210\n",
      "  Batch [750/1299] D_loss: -1.7988, G_loss: 3.3423\n",
      "  Batch [760/1299] D_loss: -2.5087, G_loss: 2.9511\n",
      "  Batch [770/1299] D_loss: -2.4634, G_loss: 4.1497\n",
      "  Batch [780/1299] D_loss: -2.5838, G_loss: 3.0191\n",
      "  Batch [790/1299] D_loss: -2.5094, G_loss: 2.1298\n",
      "  Batch [800/1299] D_loss: -2.1458, G_loss: 3.7968\n",
      "  Batch [810/1299] D_loss: -2.2466, G_loss: 2.3808\n",
      "  Batch [820/1299] D_loss: -3.6375, G_loss: 2.2220\n",
      "  Batch [830/1299] D_loss: -2.6470, G_loss: 2.7937\n",
      "  Batch [840/1299] D_loss: -0.9462, G_loss: 2.2258\n",
      "  Batch [850/1299] D_loss: -1.6547, G_loss: 1.5184\n",
      "  Batch [860/1299] D_loss: -2.8715, G_loss: 1.7903\n",
      "  Batch [870/1299] D_loss: -1.2087, G_loss: 0.9776\n",
      "  Batch [880/1299] D_loss: -2.6105, G_loss: -0.0440\n",
      "  Batch [890/1299] D_loss: -2.3884, G_loss: 0.7090\n",
      "  Batch [900/1299] D_loss: -3.0408, G_loss: 0.3880\n",
      "  Batch [910/1299] D_loss: -1.8961, G_loss: 0.2983\n",
      "  Batch [920/1299] D_loss: -1.7102, G_loss: 1.9978\n",
      "  Batch [930/1299] D_loss: -1.7865, G_loss: 0.8042\n",
      "  Batch [940/1299] D_loss: -3.9205, G_loss: 2.1235\n",
      "  Batch [950/1299] D_loss: -2.6947, G_loss: 1.6606\n",
      "  Batch [960/1299] D_loss: -3.1744, G_loss: 0.4346\n",
      "  Batch [970/1299] D_loss: -2.2996, G_loss: 0.2266\n",
      "  Batch [980/1299] D_loss: -2.1962, G_loss: 1.9986\n",
      "  Batch [990/1299] D_loss: -2.0177, G_loss: 2.2908\n",
      "  Batch [1000/1299] D_loss: -3.1060, G_loss: 1.9023\n",
      "  Batch [1010/1299] D_loss: -1.2525, G_loss: 2.1758\n",
      "  Batch [1020/1299] D_loss: -0.5170, G_loss: 2.3551\n",
      "  Batch [1030/1299] D_loss: -2.8644, G_loss: 1.0582\n",
      "  Batch [1040/1299] D_loss: -2.2394, G_loss: 4.2355\n",
      "  Batch [1050/1299] D_loss: -3.1457, G_loss: 3.6177\n",
      "  Batch [1060/1299] D_loss: -2.4920, G_loss: 3.1488\n",
      "  Batch [1070/1299] D_loss: -2.3015, G_loss: 3.1630\n",
      "  Batch [1080/1299] D_loss: -3.0007, G_loss: 0.7595\n",
      "  Batch [1090/1299] D_loss: -3.9201, G_loss: 2.6201\n",
      "  Batch [1100/1299] D_loss: -1.6940, G_loss: 2.2817\n",
      "  Batch [1110/1299] D_loss: -2.8164, G_loss: 3.8876\n",
      "  Batch [1120/1299] D_loss: -2.2748, G_loss: 1.5503\n",
      "  Batch [1130/1299] D_loss: -1.7742, G_loss: 0.6973\n",
      "  Batch [1140/1299] D_loss: -3.0337, G_loss: 2.9812\n",
      "  Batch [1150/1299] D_loss: -2.2385, G_loss: 1.3784\n",
      "  Batch [1160/1299] D_loss: -2.6643, G_loss: 1.9270\n",
      "  Batch [1170/1299] D_loss: -2.6833, G_loss: 1.9504\n",
      "  Batch [1180/1299] D_loss: -2.8796, G_loss: 1.6078\n",
      "  Batch [1190/1299] D_loss: -2.8215, G_loss: 0.7069\n",
      "  Batch [1200/1299] D_loss: -1.8117, G_loss: 2.7066\n",
      "  Batch [1210/1299] D_loss: -2.2790, G_loss: 1.7119\n",
      "  Batch [1220/1299] D_loss: -2.0915, G_loss: 2.2425\n",
      "  Batch [1230/1299] D_loss: -2.2240, G_loss: 1.6189\n",
      "  Batch [1240/1299] D_loss: -2.7160, G_loss: 1.8214\n",
      "  Batch [1250/1299] D_loss: -1.6609, G_loss: 1.5361\n",
      "  Batch [1260/1299] D_loss: -2.6103, G_loss: 0.4409\n",
      "  Batch [1270/1299] D_loss: -1.4929, G_loss: 1.8161\n",
      "  Batch [1280/1299] D_loss: -1.9106, G_loss: 3.6568\n",
      "  Batch [1290/1299] D_loss: -1.7542, G_loss: 3.6997\n",
      "\n",
      "Epoch 29 Summary:\n",
      "  Average D_loss: -2.2704\n",
      "  Average G_loss: 2.2169\n",
      "\n",
      "Epoch [30/100]\n",
      "  Batch [0/1299] D_loss: -2.2423, G_loss: 2.4580\n",
      "  Batch [10/1299] D_loss: -2.4091, G_loss: 1.3994\n",
      "  Batch [20/1299] D_loss: -2.7770, G_loss: 1.8891\n",
      "  Batch [30/1299] D_loss: -1.6330, G_loss: 3.8368\n",
      "  Batch [40/1299] D_loss: -3.5966, G_loss: 2.3160\n",
      "  Batch [50/1299] D_loss: -4.1698, G_loss: 3.4442\n",
      "  Batch [60/1299] D_loss: -2.4214, G_loss: 2.2380\n",
      "  Batch [70/1299] D_loss: -2.4528, G_loss: 1.9540\n",
      "  Batch [80/1299] D_loss: -2.8202, G_loss: 0.9590\n",
      "  Batch [90/1299] D_loss: -2.5915, G_loss: 1.1780\n",
      "  Batch [100/1299] D_loss: -1.2114, G_loss: 2.7763\n",
      "  Batch [110/1299] D_loss: -2.4236, G_loss: 1.2248\n",
      "  Batch [120/1299] D_loss: -2.9271, G_loss: 1.4071\n",
      "  Batch [130/1299] D_loss: -2.1272, G_loss: 2.4413\n",
      "  Batch [140/1299] D_loss: -1.9503, G_loss: 0.8730\n",
      "  Batch [150/1299] D_loss: -2.0189, G_loss: 0.7536\n",
      "  Batch [160/1299] D_loss: -2.9786, G_loss: 2.0580\n",
      "  Batch [170/1299] D_loss: -2.5185, G_loss: 1.5952\n",
      "  Batch [180/1299] D_loss: -2.4213, G_loss: 2.1649\n",
      "  Batch [190/1299] D_loss: -3.3911, G_loss: 1.2137\n",
      "  Batch [200/1299] D_loss: -1.8247, G_loss: 2.0601\n",
      "  Batch [210/1299] D_loss: -2.3382, G_loss: 2.8339\n",
      "  Batch [220/1299] D_loss: -3.8231, G_loss: 3.9969\n",
      "  Batch [230/1299] D_loss: -3.1437, G_loss: 4.7008\n",
      "  Batch [240/1299] D_loss: -3.4346, G_loss: 1.7594\n",
      "  Batch [250/1299] D_loss: -2.7050, G_loss: 2.6141\n",
      "  Batch [260/1299] D_loss: -3.2397, G_loss: 4.5014\n",
      "  Batch [270/1299] D_loss: -1.4119, G_loss: 5.1324\n",
      "  Batch [280/1299] D_loss: -2.2833, G_loss: 2.0501\n",
      "  Batch [290/1299] D_loss: -2.4142, G_loss: 2.5951\n",
      "  Batch [300/1299] D_loss: -1.6369, G_loss: 3.7405\n",
      "  Batch [310/1299] D_loss: -4.0562, G_loss: 5.7820\n",
      "  Batch [320/1299] D_loss: -2.1239, G_loss: 4.0281\n",
      "  Batch [330/1299] D_loss: -2.1285, G_loss: 3.2937\n",
      "  Batch [340/1299] D_loss: -1.9578, G_loss: 2.6994\n",
      "  Batch [350/1299] D_loss: -3.2342, G_loss: 1.4185\n",
      "  Batch [360/1299] D_loss: -3.2558, G_loss: 2.7341\n",
      "  Batch [370/1299] D_loss: -3.7347, G_loss: 4.8140\n",
      "  Batch [380/1299] D_loss: -1.1507, G_loss: 2.8140\n",
      "  Batch [390/1299] D_loss: -1.3682, G_loss: 2.3977\n",
      "  Batch [400/1299] D_loss: -2.3050, G_loss: 1.8317\n",
      "  Batch [410/1299] D_loss: -2.5086, G_loss: 2.2630\n",
      "  Batch [420/1299] D_loss: -2.5043, G_loss: 2.8493\n",
      "  Batch [430/1299] D_loss: -2.4636, G_loss: 1.6406\n",
      "  Batch [440/1299] D_loss: -3.1030, G_loss: 1.0957\n",
      "  Batch [450/1299] D_loss: -3.6549, G_loss: 0.9922\n",
      "  Batch [460/1299] D_loss: -2.0400, G_loss: 2.2140\n",
      "  Batch [470/1299] D_loss: -1.6453, G_loss: 3.2459\n",
      "  Batch [480/1299] D_loss: -2.7731, G_loss: 0.6165\n",
      "  Batch [490/1299] D_loss: -2.2423, G_loss: 2.4962\n",
      "  Batch [500/1299] D_loss: -2.3199, G_loss: 2.2235\n",
      "  Batch [510/1299] D_loss: -2.4165, G_loss: 2.8280\n",
      "  Batch [520/1299] D_loss: -1.6486, G_loss: 2.8163\n",
      "  Batch [530/1299] D_loss: -2.7018, G_loss: 3.3372\n",
      "  Batch [540/1299] D_loss: -0.6438, G_loss: 1.7977\n",
      "  Batch [550/1299] D_loss: -2.1623, G_loss: 1.9511\n",
      "  Batch [560/1299] D_loss: -3.4945, G_loss: 1.5298\n",
      "  Batch [570/1299] D_loss: -2.9690, G_loss: 1.4612\n",
      "  Batch [580/1299] D_loss: -1.9238, G_loss: 1.6722\n",
      "  Batch [590/1299] D_loss: -1.8099, G_loss: 1.3103\n",
      "  Batch [600/1299] D_loss: -2.2000, G_loss: 0.6061\n",
      "  Batch [610/1299] D_loss: -1.5502, G_loss: 1.5521\n",
      "  Batch [620/1299] D_loss: -2.4940, G_loss: 2.3893\n",
      "  Batch [630/1299] D_loss: -2.8135, G_loss: 2.9751\n",
      "  Batch [640/1299] D_loss: -2.6681, G_loss: 2.5376\n",
      "  Batch [650/1299] D_loss: -2.1302, G_loss: 1.9458\n",
      "  Batch [660/1299] D_loss: -2.3466, G_loss: -0.1627\n",
      "  Batch [670/1299] D_loss: -3.6494, G_loss: 1.0931\n",
      "  Batch [680/1299] D_loss: -3.1238, G_loss: 1.8076\n",
      "  Batch [690/1299] D_loss: -2.1676, G_loss: 2.6474\n",
      "  Batch [700/1299] D_loss: -1.6780, G_loss: 3.2865\n",
      "  Batch [710/1299] D_loss: -1.5226, G_loss: 1.7874\n",
      "  Batch [720/1299] D_loss: -2.8556, G_loss: 1.8273\n",
      "  Batch [730/1299] D_loss: -2.3399, G_loss: 2.3849\n",
      "  Batch [740/1299] D_loss: -2.0502, G_loss: 1.2951\n",
      "  Batch [750/1299] D_loss: -2.6798, G_loss: 1.1310\n",
      "  Batch [760/1299] D_loss: -2.8571, G_loss: 2.5655\n",
      "  Batch [770/1299] D_loss: -4.1160, G_loss: 3.8018\n",
      "  Batch [780/1299] D_loss: -2.5842, G_loss: 3.1487\n",
      "  Batch [790/1299] D_loss: -2.0961, G_loss: 1.4689\n",
      "  Batch [800/1299] D_loss: -2.6103, G_loss: 0.6932\n",
      "  Batch [810/1299] D_loss: -2.2966, G_loss: 0.6687\n",
      "  Batch [820/1299] D_loss: -1.6189, G_loss: 0.0473\n",
      "  Batch [830/1299] D_loss: -1.7129, G_loss: 1.3202\n",
      "  Batch [840/1299] D_loss: -2.4277, G_loss: 2.0265\n",
      "  Batch [850/1299] D_loss: -2.4020, G_loss: 1.7470\n",
      "  Batch [860/1299] D_loss: -2.8867, G_loss: 3.7511\n",
      "  Batch [870/1299] D_loss: -1.8553, G_loss: 2.5795\n",
      "  Batch [880/1299] D_loss: -2.7319, G_loss: 2.1619\n",
      "  Batch [890/1299] D_loss: -2.8577, G_loss: 3.0828\n",
      "  Batch [900/1299] D_loss: -2.1086, G_loss: 0.8865\n",
      "  Batch [910/1299] D_loss: -2.8045, G_loss: 3.1615\n",
      "  Batch [920/1299] D_loss: -2.4036, G_loss: 2.4480\n",
      "  Batch [930/1299] D_loss: -2.8448, G_loss: 2.1721\n",
      "  Batch [940/1299] D_loss: -1.8020, G_loss: 3.2215\n",
      "  Batch [950/1299] D_loss: -1.9716, G_loss: 3.3197\n",
      "  Batch [960/1299] D_loss: -0.2574, G_loss: 4.4515\n",
      "  Batch [970/1299] D_loss: -3.4646, G_loss: 4.3454\n",
      "  Batch [980/1299] D_loss: -2.2656, G_loss: 2.9715\n",
      "  Batch [990/1299] D_loss: -3.4657, G_loss: 3.1417\n",
      "  Batch [1000/1299] D_loss: -2.7437, G_loss: 3.1189\n",
      "  Batch [1010/1299] D_loss: -3.3904, G_loss: 3.0740\n",
      "  Batch [1020/1299] D_loss: -2.0712, G_loss: 2.3102\n",
      "  Batch [1030/1299] D_loss: -1.8297, G_loss: 2.4674\n",
      "  Batch [1040/1299] D_loss: -2.7485, G_loss: 2.8452\n",
      "  Batch [1050/1299] D_loss: -1.2258, G_loss: 3.2488\n",
      "  Batch [1060/1299] D_loss: -1.3029, G_loss: 1.6900\n",
      "  Batch [1070/1299] D_loss: -1.7107, G_loss: 2.0565\n",
      "  Batch [1080/1299] D_loss: -4.1308, G_loss: 3.7646\n",
      "  Batch [1090/1299] D_loss: -1.9242, G_loss: 3.2209\n",
      "  Batch [1100/1299] D_loss: -2.4864, G_loss: 2.3307\n",
      "  Batch [1110/1299] D_loss: -1.8439, G_loss: 3.7933\n",
      "  Batch [1120/1299] D_loss: -1.2561, G_loss: -0.3224\n",
      "  Batch [1130/1299] D_loss: -3.3589, G_loss: 0.8906\n",
      "  Batch [1140/1299] D_loss: -1.6066, G_loss: -0.5000\n",
      "  Batch [1150/1299] D_loss: -2.7043, G_loss: 2.2785\n",
      "  Batch [1160/1299] D_loss: -3.4878, G_loss: 1.2851\n",
      "  Batch [1170/1299] D_loss: -2.3404, G_loss: 2.2488\n",
      "  Batch [1180/1299] D_loss: -3.3053, G_loss: 1.2796\n",
      "  Batch [1190/1299] D_loss: -3.9837, G_loss: -0.1928\n",
      "  Batch [1200/1299] D_loss: -1.6990, G_loss: 1.6486\n",
      "  Batch [1210/1299] D_loss: -3.4476, G_loss: 1.5398\n",
      "  Batch [1220/1299] D_loss: -1.8817, G_loss: 2.4385\n",
      "  Batch [1230/1299] D_loss: -2.2991, G_loss: 1.2999\n",
      "  Batch [1240/1299] D_loss: -3.1646, G_loss: 2.9263\n",
      "  Batch [1250/1299] D_loss: -2.3546, G_loss: 5.0700\n",
      "  Batch [1260/1299] D_loss: -3.6240, G_loss: 4.9128\n",
      "  Batch [1270/1299] D_loss: -2.0221, G_loss: 3.9819\n",
      "  Batch [1280/1299] D_loss: -2.9597, G_loss: 3.3763\n",
      "  Batch [1290/1299] D_loss: -3.1477, G_loss: 4.3462\n",
      "\n",
      "Epoch 30 Summary:\n",
      "  Average D_loss: -2.2667\n",
      "  Average G_loss: 2.3192\n",
      "\n",
      "Epoch [31/100]\n",
      "  Batch [0/1299] D_loss: -2.0410, G_loss: 5.1339\n",
      "  Batch [10/1299] D_loss: -2.1415, G_loss: 4.3799\n",
      "  Batch [20/1299] D_loss: -1.8360, G_loss: 3.6510\n",
      "  Batch [30/1299] D_loss: -1.7350, G_loss: 3.1680\n",
      "  Batch [40/1299] D_loss: -2.5379, G_loss: 2.6168\n",
      "  Batch [50/1299] D_loss: -2.5572, G_loss: 2.6489\n",
      "  Batch [60/1299] D_loss: -2.9954, G_loss: 3.6366\n",
      "  Batch [70/1299] D_loss: -1.2931, G_loss: 2.6146\n",
      "  Batch [80/1299] D_loss: -2.7046, G_loss: 3.2619\n",
      "  Batch [90/1299] D_loss: -3.4847, G_loss: 2.6834\n",
      "  Batch [100/1299] D_loss: -2.4066, G_loss: 3.1487\n",
      "  Batch [110/1299] D_loss: -3.8999, G_loss: 2.0972\n",
      "  Batch [120/1299] D_loss: -1.9768, G_loss: 2.7090\n",
      "  Batch [130/1299] D_loss: -1.7439, G_loss: 3.8555\n",
      "  Batch [140/1299] D_loss: -0.9208, G_loss: 3.4522\n",
      "  Batch [150/1299] D_loss: -1.0965, G_loss: 4.9854\n",
      "  Batch [160/1299] D_loss: -2.7486, G_loss: 1.4724\n",
      "  Batch [170/1299] D_loss: -1.8613, G_loss: 2.6271\n",
      "  Batch [180/1299] D_loss: -2.5328, G_loss: 1.5267\n",
      "  Batch [190/1299] D_loss: -1.7810, G_loss: 1.8799\n",
      "  Batch [200/1299] D_loss: -2.7527, G_loss: 2.2179\n",
      "  Batch [210/1299] D_loss: -1.6247, G_loss: 2.7818\n",
      "  Batch [220/1299] D_loss: -0.9081, G_loss: 2.3029\n",
      "  Batch [230/1299] D_loss: -0.2656, G_loss: 3.0077\n",
      "  Batch [240/1299] D_loss: -2.2904, G_loss: 2.8624\n",
      "  Batch [250/1299] D_loss: -2.3391, G_loss: 3.5517\n",
      "  Batch [260/1299] D_loss: -1.7473, G_loss: 2.9171\n",
      "  Batch [270/1299] D_loss: -2.4167, G_loss: 3.1785\n",
      "  Batch [280/1299] D_loss: -3.2922, G_loss: 2.5188\n",
      "  Batch [290/1299] D_loss: -3.7683, G_loss: 5.2786\n",
      "  Batch [300/1299] D_loss: -0.9427, G_loss: 3.4105\n",
      "  Batch [310/1299] D_loss: -2.0985, G_loss: 3.1377\n",
      "  Batch [320/1299] D_loss: -3.3169, G_loss: 2.7051\n",
      "  Batch [330/1299] D_loss: -3.8662, G_loss: 1.8365\n",
      "  Batch [340/1299] D_loss: -3.7320, G_loss: 1.9606\n",
      "  Batch [350/1299] D_loss: -2.9923, G_loss: 2.0280\n",
      "  Batch [360/1299] D_loss: -2.1000, G_loss: 2.6544\n",
      "  Batch [370/1299] D_loss: -2.9261, G_loss: 2.6755\n",
      "  Batch [380/1299] D_loss: -3.9675, G_loss: 1.6009\n",
      "  Batch [390/1299] D_loss: -0.5693, G_loss: 1.5264\n",
      "  Batch [400/1299] D_loss: -3.7580, G_loss: -0.9150\n",
      "  Batch [410/1299] D_loss: -3.3990, G_loss: -1.2448\n",
      "  Batch [420/1299] D_loss: -1.8264, G_loss: 1.0148\n",
      "  Batch [430/1299] D_loss: -2.9347, G_loss: -0.4388\n",
      "  Batch [440/1299] D_loss: -1.9193, G_loss: 0.6220\n",
      "  Batch [450/1299] D_loss: -2.6988, G_loss: -1.8156\n",
      "  Batch [460/1299] D_loss: -2.9567, G_loss: -0.8988\n",
      "  Batch [470/1299] D_loss: -1.0430, G_loss: 1.9214\n",
      "  Batch [480/1299] D_loss: -2.8707, G_loss: 3.5664\n",
      "  Batch [490/1299] D_loss: 0.1230, G_loss: 2.5656\n",
      "  Batch [500/1299] D_loss: -3.5967, G_loss: -1.3631\n",
      "  Batch [510/1299] D_loss: -0.7349, G_loss: -0.9927\n",
      "  Batch [520/1299] D_loss: -1.7905, G_loss: -0.4713\n",
      "  Batch [530/1299] D_loss: -4.5774, G_loss: 2.3092\n",
      "  Batch [540/1299] D_loss: -3.0976, G_loss: 3.4375\n",
      "  Batch [550/1299] D_loss: -3.3715, G_loss: 1.8566\n",
      "  Batch [560/1299] D_loss: -2.2641, G_loss: 2.0373\n",
      "  Batch [570/1299] D_loss: -3.0120, G_loss: -0.5274\n",
      "  Batch [580/1299] D_loss: -2.7642, G_loss: 1.4669\n",
      "  Batch [590/1299] D_loss: -3.2606, G_loss: 0.9303\n",
      "  Batch [600/1299] D_loss: -3.7037, G_loss: -1.4435\n",
      "  Batch [610/1299] D_loss: -1.9256, G_loss: 0.6464\n",
      "  Batch [620/1299] D_loss: -2.4489, G_loss: 2.2276\n",
      "  Batch [630/1299] D_loss: -1.9610, G_loss: 2.8588\n",
      "  Batch [640/1299] D_loss: -3.4658, G_loss: 0.8014\n",
      "  Batch [650/1299] D_loss: -3.6069, G_loss: -0.5928\n",
      "  Batch [660/1299] D_loss: -3.1794, G_loss: -1.5236\n",
      "  Batch [670/1299] D_loss: -1.7558, G_loss: -0.9164\n",
      "  Batch [680/1299] D_loss: -1.8873, G_loss: -0.0339\n",
      "  Batch [690/1299] D_loss: -3.9269, G_loss: 1.3673\n",
      "  Batch [700/1299] D_loss: -0.8933, G_loss: 2.8692\n",
      "  Batch [710/1299] D_loss: -4.6132, G_loss: 4.6180\n",
      "  Batch [720/1299] D_loss: -3.0044, G_loss: -0.3851\n",
      "  Batch [730/1299] D_loss: -2.9114, G_loss: 1.8096\n",
      "  Batch [740/1299] D_loss: -3.5446, G_loss: 1.8085\n",
      "  Batch [750/1299] D_loss: -2.5083, G_loss: 1.8294\n",
      "  Batch [760/1299] D_loss: -2.6966, G_loss: 2.0582\n",
      "  Batch [770/1299] D_loss: -2.2222, G_loss: 2.7091\n",
      "  Batch [780/1299] D_loss: -3.1829, G_loss: 3.6779\n",
      "  Batch [790/1299] D_loss: -3.0004, G_loss: 3.0028\n",
      "  Batch [800/1299] D_loss: -4.1421, G_loss: 3.1548\n",
      "  Batch [810/1299] D_loss: -2.8292, G_loss: 2.2480\n",
      "  Batch [820/1299] D_loss: -1.5841, G_loss: 4.3639\n",
      "  Batch [830/1299] D_loss: -3.0002, G_loss: 2.9391\n",
      "  Batch [840/1299] D_loss: -2.9903, G_loss: 5.0244\n",
      "  Batch [850/1299] D_loss: -2.7917, G_loss: 4.6252\n",
      "  Batch [860/1299] D_loss: -2.1635, G_loss: 4.1581\n",
      "  Batch [870/1299] D_loss: -2.2991, G_loss: 3.4580\n",
      "  Batch [880/1299] D_loss: -2.4505, G_loss: 4.7867\n",
      "  Batch [890/1299] D_loss: -3.3411, G_loss: 2.5471\n",
      "  Batch [900/1299] D_loss: -3.2198, G_loss: 0.7738\n",
      "  Batch [910/1299] D_loss: -3.1867, G_loss: 1.3001\n",
      "  Batch [920/1299] D_loss: -3.2295, G_loss: 2.1182\n",
      "  Batch [930/1299] D_loss: -3.0707, G_loss: 2.1438\n",
      "  Batch [940/1299] D_loss: -2.6131, G_loss: 2.8344\n",
      "  Batch [950/1299] D_loss: -3.2230, G_loss: 2.4442\n",
      "  Batch [960/1299] D_loss: -3.1625, G_loss: 1.5129\n",
      "  Batch [970/1299] D_loss: -2.5144, G_loss: 3.8401\n",
      "  Batch [980/1299] D_loss: -2.6777, G_loss: 1.6754\n",
      "  Batch [990/1299] D_loss: -1.0649, G_loss: -0.8872\n",
      "  Batch [1000/1299] D_loss: -3.4583, G_loss: 1.7897\n",
      "  Batch [1010/1299] D_loss: -2.1988, G_loss: 1.0827\n",
      "  Batch [1020/1299] D_loss: -3.7331, G_loss: 0.7422\n",
      "  Batch [1030/1299] D_loss: -2.2089, G_loss: -0.3962\n",
      "  Batch [1040/1299] D_loss: -3.5512, G_loss: 3.6995\n",
      "  Batch [1050/1299] D_loss: -2.7976, G_loss: 3.5251\n",
      "  Batch [1060/1299] D_loss: -2.8405, G_loss: 3.6363\n",
      "  Batch [1070/1299] D_loss: -2.5106, G_loss: 3.8880\n",
      "  Batch [1080/1299] D_loss: -1.7556, G_loss: 2.3031\n",
      "  Batch [1090/1299] D_loss: -0.7327, G_loss: 3.2930\n",
      "  Batch [1100/1299] D_loss: -2.8411, G_loss: 2.0634\n",
      "  Batch [1110/1299] D_loss: -2.7112, G_loss: 1.3096\n",
      "  Batch [1120/1299] D_loss: -2.0546, G_loss: 3.3884\n",
      "  Batch [1130/1299] D_loss: -2.6291, G_loss: 2.2300\n",
      "  Batch [1140/1299] D_loss: -2.3118, G_loss: 1.9314\n",
      "  Batch [1150/1299] D_loss: -3.6817, G_loss: 3.1716\n",
      "  Batch [1160/1299] D_loss: -2.8838, G_loss: 0.4047\n",
      "  Batch [1170/1299] D_loss: -2.8958, G_loss: -0.1704\n",
      "  Batch [1180/1299] D_loss: -3.0574, G_loss: 1.3311\n",
      "  Batch [1190/1299] D_loss: -1.7859, G_loss: 1.7646\n",
      "  Batch [1200/1299] D_loss: -2.7944, G_loss: 1.6065\n",
      "  Batch [1210/1299] D_loss: -3.7622, G_loss: 1.9425\n",
      "  Batch [1220/1299] D_loss: -2.4524, G_loss: 3.5304\n",
      "  Batch [1230/1299] D_loss: -1.9179, G_loss: 2.3893\n",
      "  Batch [1240/1299] D_loss: -2.9256, G_loss: 2.4967\n",
      "  Batch [1250/1299] D_loss: -0.9500, G_loss: 2.2518\n",
      "  Batch [1260/1299] D_loss: -2.9179, G_loss: 1.5070\n",
      "  Batch [1270/1299] D_loss: -1.4523, G_loss: 1.4578\n",
      "  Batch [1280/1299] D_loss: -2.7539, G_loss: 4.1019\n",
      "  Batch [1290/1299] D_loss: -3.6806, G_loss: 2.0229\n",
      "\n",
      "Epoch 31 Summary:\n",
      "  Average D_loss: -2.2703\n",
      "  Average G_loss: 2.0451\n",
      "\n",
      "Epoch [32/100]\n",
      "  Batch [0/1299] D_loss: -2.0696, G_loss: 1.5494\n",
      "  Batch [10/1299] D_loss: -2.2316, G_loss: 0.5219\n",
      "  Batch [20/1299] D_loss: -2.3179, G_loss: 1.7414\n",
      "  Batch [30/1299] D_loss: -3.0730, G_loss: 0.9098\n",
      "  Batch [40/1299] D_loss: -3.9724, G_loss: 2.2403\n",
      "  Batch [50/1299] D_loss: -1.4375, G_loss: 1.3944\n",
      "  Batch [60/1299] D_loss: -3.3998, G_loss: 2.0422\n",
      "  Batch [70/1299] D_loss: -3.0012, G_loss: 3.0581\n",
      "  Batch [80/1299] D_loss: -3.0953, G_loss: 1.1031\n",
      "  Batch [90/1299] D_loss: -2.6999, G_loss: 2.8336\n",
      "  Batch [100/1299] D_loss: -2.1656, G_loss: 0.4961\n",
      "  Batch [110/1299] D_loss: -1.4975, G_loss: 3.2305\n",
      "  Batch [120/1299] D_loss: -3.5155, G_loss: 3.3174\n",
      "  Batch [130/1299] D_loss: -2.8932, G_loss: 3.3619\n",
      "  Batch [140/1299] D_loss: -3.2348, G_loss: 2.6965\n",
      "  Batch [150/1299] D_loss: -1.6969, G_loss: 1.2148\n",
      "  Batch [160/1299] D_loss: -2.8609, G_loss: 0.0488\n",
      "  Batch [170/1299] D_loss: -2.2520, G_loss: 2.1781\n",
      "  Batch [180/1299] D_loss: -1.9104, G_loss: 2.6984\n",
      "  Batch [190/1299] D_loss: -3.9179, G_loss: 0.7871\n",
      "  Batch [200/1299] D_loss: -1.5974, G_loss: 0.8771\n",
      "  Batch [210/1299] D_loss: -2.9174, G_loss: -0.7827\n",
      "  Batch [220/1299] D_loss: -3.0156, G_loss: 0.1939\n",
      "  Batch [230/1299] D_loss: -2.0962, G_loss: 1.6008\n",
      "  Batch [240/1299] D_loss: -2.1771, G_loss: 1.0979\n",
      "  Batch [250/1299] D_loss: -3.3212, G_loss: 2.1640\n",
      "  Batch [260/1299] D_loss: -0.8593, G_loss: 3.4781\n",
      "  Batch [270/1299] D_loss: -3.3162, G_loss: 0.1181\n",
      "  Batch [280/1299] D_loss: -2.3670, G_loss: 2.1553\n",
      "  Batch [290/1299] D_loss: -1.7481, G_loss: 1.7696\n",
      "  Batch [300/1299] D_loss: -4.6203, G_loss: 1.4561\n",
      "  Batch [310/1299] D_loss: -2.7431, G_loss: 1.2679\n",
      "  Batch [320/1299] D_loss: -2.8199, G_loss: -0.4028\n",
      "  Batch [330/1299] D_loss: -1.2155, G_loss: 1.1559\n",
      "  Batch [340/1299] D_loss: -1.4942, G_loss: 0.4434\n",
      "  Batch [350/1299] D_loss: -2.8022, G_loss: 0.3057\n",
      "  Batch [360/1299] D_loss: -2.8966, G_loss: 1.2729\n",
      "  Batch [370/1299] D_loss: -3.9770, G_loss: 2.7545\n",
      "  Batch [380/1299] D_loss: -3.0965, G_loss: 1.2875\n",
      "  Batch [390/1299] D_loss: -2.3768, G_loss: 1.3252\n",
      "  Batch [400/1299] D_loss: -2.4731, G_loss: 2.4590\n",
      "  Batch [410/1299] D_loss: -3.5750, G_loss: 3.3669\n",
      "  Batch [420/1299] D_loss: -2.7514, G_loss: 4.0167\n",
      "  Batch [430/1299] D_loss: -2.8919, G_loss: 3.4634\n",
      "  Batch [440/1299] D_loss: -2.6534, G_loss: 2.2599\n",
      "  Batch [450/1299] D_loss: -0.8271, G_loss: 1.5713\n",
      "  Batch [460/1299] D_loss: -2.2038, G_loss: 3.4180\n",
      "  Batch [470/1299] D_loss: -2.8992, G_loss: 4.9329\n",
      "  Batch [480/1299] D_loss: -2.6120, G_loss: 3.8131\n",
      "  Batch [490/1299] D_loss: -2.2789, G_loss: 4.1401\n",
      "  Batch [500/1299] D_loss: -2.5280, G_loss: 6.1924\n",
      "  Batch [510/1299] D_loss: -3.9562, G_loss: 5.4493\n",
      "  Batch [520/1299] D_loss: -2.6915, G_loss: 3.9804\n",
      "  Batch [530/1299] D_loss: -1.4553, G_loss: 3.7271\n",
      "  Batch [540/1299] D_loss: -0.8087, G_loss: 3.1529\n",
      "  Batch [550/1299] D_loss: -3.5797, G_loss: 3.9252\n",
      "  Batch [560/1299] D_loss: -2.7491, G_loss: 2.0926\n",
      "  Batch [570/1299] D_loss: -2.6181, G_loss: 2.4328\n",
      "  Batch [580/1299] D_loss: -2.4991, G_loss: 2.4288\n",
      "  Batch [590/1299] D_loss: -3.3775, G_loss: 2.4262\n",
      "  Batch [600/1299] D_loss: -2.4815, G_loss: 1.9875\n",
      "  Batch [610/1299] D_loss: -2.3213, G_loss: 1.5333\n",
      "  Batch [620/1299] D_loss: -2.1548, G_loss: -0.4921\n",
      "  Batch [630/1299] D_loss: -1.2815, G_loss: 2.4969\n",
      "  Batch [640/1299] D_loss: -2.0473, G_loss: 0.0616\n",
      "  Batch [650/1299] D_loss: -2.5625, G_loss: 0.7864\n",
      "  Batch [660/1299] D_loss: -3.0411, G_loss: 2.6151\n",
      "  Batch [670/1299] D_loss: -2.1927, G_loss: 3.4121\n",
      "  Batch [680/1299] D_loss: -3.3877, G_loss: 3.2099\n",
      "  Batch [690/1299] D_loss: -3.8339, G_loss: 3.1157\n",
      "  Batch [700/1299] D_loss: -1.3667, G_loss: 1.4532\n",
      "  Batch [710/1299] D_loss: -1.2441, G_loss: 1.4786\n",
      "  Batch [720/1299] D_loss: -1.2748, G_loss: 3.5853\n",
      "  Batch [730/1299] D_loss: -2.7252, G_loss: 3.8034\n",
      "  Batch [740/1299] D_loss: -2.5480, G_loss: 4.5539\n",
      "  Batch [750/1299] D_loss: -3.0841, G_loss: 1.7488\n",
      "  Batch [760/1299] D_loss: -2.0995, G_loss: 1.6923\n",
      "  Batch [770/1299] D_loss: -1.6329, G_loss: -1.2192\n",
      "  Batch [780/1299] D_loss: -1.1743, G_loss: 0.5327\n",
      "  Batch [790/1299] D_loss: -3.3656, G_loss: 2.4113\n",
      "  Batch [800/1299] D_loss: -2.1550, G_loss: 2.5824\n",
      "  Batch [810/1299] D_loss: -3.9943, G_loss: 4.8183\n",
      "  Batch [820/1299] D_loss: -3.6991, G_loss: 0.3248\n",
      "  Batch [830/1299] D_loss: -4.4786, G_loss: 0.2538\n",
      "  Batch [840/1299] D_loss: -1.5280, G_loss: 2.0438\n",
      "  Batch [850/1299] D_loss: -2.5129, G_loss: 3.1622\n",
      "  Batch [860/1299] D_loss: -1.4704, G_loss: 0.6749\n",
      "  Batch [870/1299] D_loss: -1.4631, G_loss: 0.0920\n",
      "  Batch [880/1299] D_loss: -2.5764, G_loss: 4.0856\n",
      "  Batch [890/1299] D_loss: -2.0181, G_loss: 3.9284\n",
      "  Batch [900/1299] D_loss: -2.1845, G_loss: 2.8911\n",
      "  Batch [910/1299] D_loss: -2.3758, G_loss: 1.0309\n",
      "  Batch [920/1299] D_loss: -2.3612, G_loss: 0.5546\n",
      "  Batch [930/1299] D_loss: -0.8670, G_loss: 1.6075\n",
      "  Batch [940/1299] D_loss: -1.8681, G_loss: 2.5560\n",
      "  Batch [950/1299] D_loss: -2.7749, G_loss: 4.8901\n",
      "  Batch [960/1299] D_loss: -2.4442, G_loss: 5.1568\n",
      "  Batch [970/1299] D_loss: -0.8289, G_loss: 4.0237\n",
      "  Batch [980/1299] D_loss: -3.4742, G_loss: 2.3108\n",
      "  Batch [990/1299] D_loss: -2.6709, G_loss: 5.0074\n",
      "  Batch [1000/1299] D_loss: -1.9973, G_loss: 1.3310\n",
      "  Batch [1010/1299] D_loss: -3.1030, G_loss: 1.1831\n",
      "  Batch [1020/1299] D_loss: -2.1670, G_loss: 4.7833\n",
      "  Batch [1030/1299] D_loss: -3.7844, G_loss: 3.8675\n",
      "  Batch [1040/1299] D_loss: -2.9304, G_loss: 3.5454\n",
      "  Batch [1050/1299] D_loss: -3.1543, G_loss: 2.5716\n",
      "  Batch [1060/1299] D_loss: -0.3188, G_loss: -0.1341\n",
      "  Batch [1070/1299] D_loss: -2.3175, G_loss: 0.9862\n",
      "  Batch [1080/1299] D_loss: -2.7005, G_loss: 0.7834\n",
      "  Batch [1090/1299] D_loss: -1.5767, G_loss: 3.4698\n",
      "  Batch [1100/1299] D_loss: -2.4580, G_loss: 2.7248\n",
      "  Batch [1110/1299] D_loss: -2.1607, G_loss: 3.3925\n",
      "  Batch [1120/1299] D_loss: -2.4543, G_loss: 3.9667\n",
      "  Batch [1130/1299] D_loss: -3.2307, G_loss: 3.5556\n",
      "  Batch [1140/1299] D_loss: -2.8332, G_loss: 3.1309\n",
      "  Batch [1150/1299] D_loss: -2.1871, G_loss: 3.9979\n",
      "  Batch [1160/1299] D_loss: -3.2045, G_loss: 1.1255\n",
      "  Batch [1170/1299] D_loss: -4.2587, G_loss: 3.8470\n",
      "  Batch [1180/1299] D_loss: -1.3118, G_loss: 4.5605\n",
      "  Batch [1190/1299] D_loss: -2.2333, G_loss: 1.7739\n",
      "  Batch [1200/1299] D_loss: -3.2922, G_loss: 3.7831\n",
      "  Batch [1210/1299] D_loss: -0.7661, G_loss: 1.3026\n",
      "  Batch [1220/1299] D_loss: -1.5019, G_loss: 3.0690\n",
      "  Batch [1230/1299] D_loss: -4.0971, G_loss: 2.9879\n",
      "  Batch [1240/1299] D_loss: -3.1879, G_loss: 2.1106\n",
      "  Batch [1250/1299] D_loss: -2.8848, G_loss: 2.6857\n",
      "  Batch [1260/1299] D_loss: -2.0845, G_loss: 4.0413\n",
      "  Batch [1270/1299] D_loss: -2.7884, G_loss: 1.3738\n",
      "  Batch [1280/1299] D_loss: -0.9814, G_loss: 2.2417\n",
      "  Batch [1290/1299] D_loss: -1.7404, G_loss: 2.4971\n",
      "\n",
      "Epoch 32 Summary:\n",
      "  Average D_loss: -2.2725\n",
      "  Average G_loss: 2.3270\n",
      "\n",
      "Epoch [33/100]\n",
      "  Batch [0/1299] D_loss: -3.5754, G_loss: 2.7935\n",
      "  Batch [10/1299] D_loss: -3.2454, G_loss: 1.6857\n",
      "  Batch [20/1299] D_loss: -3.2204, G_loss: 1.6041\n",
      "  Batch [30/1299] D_loss: -3.3976, G_loss: 3.1732\n",
      "  Batch [40/1299] D_loss: -2.6280, G_loss: 1.7757\n",
      "  Batch [50/1299] D_loss: -3.1202, G_loss: 1.0128\n",
      "  Batch [60/1299] D_loss: -3.6113, G_loss: 1.8172\n",
      "  Batch [70/1299] D_loss: -2.8985, G_loss: 1.5029\n",
      "  Batch [80/1299] D_loss: -2.4310, G_loss: 2.6747\n",
      "  Batch [90/1299] D_loss: -3.3718, G_loss: 3.2863\n",
      "  Batch [100/1299] D_loss: -3.7051, G_loss: 0.4858\n",
      "  Batch [110/1299] D_loss: -2.6741, G_loss: 1.7490\n",
      "  Batch [120/1299] D_loss: -3.7565, G_loss: 0.6761\n",
      "  Batch [130/1299] D_loss: -3.6407, G_loss: 3.7218\n",
      "  Batch [140/1299] D_loss: -1.6105, G_loss: 2.9344\n",
      "  Batch [150/1299] D_loss: -2.9164, G_loss: 3.7784\n",
      "  Batch [160/1299] D_loss: -1.7146, G_loss: 5.6072\n",
      "  Batch [170/1299] D_loss: -2.0932, G_loss: 6.0000\n",
      "  Batch [180/1299] D_loss: -3.5252, G_loss: 3.7530\n",
      "  Batch [190/1299] D_loss: -1.8570, G_loss: 3.4511\n",
      "  Batch [200/1299] D_loss: -2.8886, G_loss: 3.7311\n",
      "  Batch [210/1299] D_loss: -1.7552, G_loss: 3.2146\n",
      "  Batch [220/1299] D_loss: -3.7670, G_loss: 3.8182\n",
      "  Batch [230/1299] D_loss: -2.9790, G_loss: 2.8994\n",
      "  Batch [240/1299] D_loss: -4.1550, G_loss: 4.1060\n",
      "  Batch [250/1299] D_loss: -2.2349, G_loss: 4.7722\n",
      "  Batch [260/1299] D_loss: -2.1740, G_loss: 3.1824\n",
      "  Batch [270/1299] D_loss: -3.4140, G_loss: 6.0056\n",
      "  Batch [280/1299] D_loss: -2.7810, G_loss: 5.8293\n",
      "  Batch [290/1299] D_loss: -0.8668, G_loss: 2.6344\n",
      "  Batch [300/1299] D_loss: -2.5818, G_loss: 3.5747\n",
      "  Batch [310/1299] D_loss: -3.8133, G_loss: 0.3761\n",
      "  Batch [320/1299] D_loss: -0.3709, G_loss: -0.2346\n",
      "  Batch [330/1299] D_loss: -2.4924, G_loss: 3.8684\n",
      "  Batch [340/1299] D_loss: -3.6573, G_loss: 2.6645\n",
      "  Batch [350/1299] D_loss: -2.4452, G_loss: 1.8892\n",
      "  Batch [360/1299] D_loss: -1.7342, G_loss: 1.4384\n",
      "  Batch [370/1299] D_loss: -1.5958, G_loss: 0.2011\n",
      "  Batch [380/1299] D_loss: -2.6709, G_loss: 3.9875\n",
      "  Batch [390/1299] D_loss: -1.8100, G_loss: 2.1514\n",
      "  Batch [400/1299] D_loss: -3.1526, G_loss: 2.3754\n",
      "  Batch [410/1299] D_loss: -1.4600, G_loss: 3.9840\n",
      "  Batch [420/1299] D_loss: -2.7197, G_loss: 2.9343\n",
      "  Batch [430/1299] D_loss: -1.7537, G_loss: 3.1509\n",
      "  Batch [440/1299] D_loss: -2.8781, G_loss: 2.4157\n",
      "  Batch [450/1299] D_loss: -3.6376, G_loss: 5.1720\n",
      "  Batch [460/1299] D_loss: -3.5291, G_loss: 4.3367\n",
      "  Batch [470/1299] D_loss: -3.5704, G_loss: 3.4350\n",
      "  Batch [480/1299] D_loss: -2.5720, G_loss: 2.8346\n",
      "  Batch [490/1299] D_loss: -1.2877, G_loss: 5.3089\n",
      "  Batch [500/1299] D_loss: -2.7515, G_loss: 4.2319\n",
      "  Batch [510/1299] D_loss: -1.8801, G_loss: 4.3296\n",
      "  Batch [520/1299] D_loss: -2.2450, G_loss: 3.6627\n",
      "  Batch [530/1299] D_loss: -3.5604, G_loss: -0.0301\n",
      "  Batch [540/1299] D_loss: -5.1606, G_loss: 1.4574\n",
      "  Batch [550/1299] D_loss: -2.0143, G_loss: 2.2864\n",
      "  Batch [560/1299] D_loss: -0.2949, G_loss: 2.5704\n",
      "  Batch [570/1299] D_loss: -2.0199, G_loss: 2.8483\n",
      "  Batch [580/1299] D_loss: -1.4915, G_loss: 1.0125\n",
      "  Batch [590/1299] D_loss: -4.2225, G_loss: 3.1472\n",
      "  Batch [600/1299] D_loss: 0.8921, G_loss: 1.1450\n",
      "  Batch [610/1299] D_loss: -2.3456, G_loss: 2.6215\n",
      "  Batch [620/1299] D_loss: -3.6827, G_loss: 2.9168\n",
      "  Batch [630/1299] D_loss: -2.2873, G_loss: 4.8548\n",
      "  Batch [640/1299] D_loss: -3.5470, G_loss: 2.0620\n",
      "  Batch [650/1299] D_loss: -3.1915, G_loss: 1.6257\n",
      "  Batch [660/1299] D_loss: -0.0808, G_loss: 1.8753\n",
      "  Batch [670/1299] D_loss: -2.1901, G_loss: 2.6158\n",
      "  Batch [680/1299] D_loss: -2.3714, G_loss: 2.5606\n",
      "  Batch [690/1299] D_loss: -1.7075, G_loss: 1.0228\n",
      "  Batch [700/1299] D_loss: -1.3462, G_loss: 2.3735\n",
      "  Batch [710/1299] D_loss: -0.9398, G_loss: 0.2458\n",
      "  Batch [720/1299] D_loss: -3.3248, G_loss: 1.0857\n",
      "  Batch [730/1299] D_loss: -3.4181, G_loss: 0.5564\n",
      "  Batch [740/1299] D_loss: -4.5447, G_loss: 0.6672\n",
      "  Batch [750/1299] D_loss: -1.0420, G_loss: 3.2519\n",
      "  Batch [760/1299] D_loss: -4.0924, G_loss: 0.6972\n",
      "  Batch [770/1299] D_loss: -2.5149, G_loss: 0.2868\n",
      "  Batch [780/1299] D_loss: -3.4365, G_loss: 0.2531\n",
      "  Batch [790/1299] D_loss: -2.1087, G_loss: 0.9691\n",
      "  Batch [800/1299] D_loss: -3.9638, G_loss: 0.0625\n",
      "  Batch [810/1299] D_loss: -3.9425, G_loss: 2.1065\n",
      "  Batch [820/1299] D_loss: -2.8337, G_loss: 3.6230\n",
      "  Batch [830/1299] D_loss: -2.3766, G_loss: 2.0311\n",
      "  Batch [840/1299] D_loss: -2.4728, G_loss: 4.3542\n",
      "  Batch [850/1299] D_loss: -0.7593, G_loss: 1.8035\n",
      "  Batch [860/1299] D_loss: -1.6922, G_loss: 2.4489\n",
      "  Batch [870/1299] D_loss: -3.7273, G_loss: 1.5678\n",
      "  Batch [880/1299] D_loss: -2.6910, G_loss: 4.2707\n",
      "  Batch [890/1299] D_loss: -1.4212, G_loss: 3.7476\n",
      "  Batch [900/1299] D_loss: -3.0604, G_loss: 2.7287\n",
      "  Batch [910/1299] D_loss: -3.7853, G_loss: 2.6837\n",
      "  Batch [920/1299] D_loss: -3.7973, G_loss: -0.3226\n",
      "  Batch [930/1299] D_loss: -2.3614, G_loss: 0.1802\n",
      "  Batch [940/1299] D_loss: -3.9178, G_loss: 3.8082\n",
      "  Batch [950/1299] D_loss: -1.9386, G_loss: 3.7739\n",
      "  Batch [960/1299] D_loss: -4.7313, G_loss: 2.1306\n",
      "  Batch [970/1299] D_loss: -2.1350, G_loss: 1.5693\n",
      "  Batch [980/1299] D_loss: -1.7955, G_loss: 2.2543\n",
      "  Batch [990/1299] D_loss: -1.7257, G_loss: 3.3277\n",
      "  Batch [1000/1299] D_loss: -2.7883, G_loss: 1.8281\n",
      "  Batch [1010/1299] D_loss: -2.4260, G_loss: 0.8464\n",
      "  Batch [1020/1299] D_loss: -3.6676, G_loss: 0.6657\n",
      "  Batch [1030/1299] D_loss: -4.5171, G_loss: 1.4652\n",
      "  Batch [1040/1299] D_loss: -2.8953, G_loss: 4.1536\n",
      "  Batch [1050/1299] D_loss: -3.6153, G_loss: 3.4489\n",
      "  Batch [1060/1299] D_loss: -1.3993, G_loss: 1.1642\n",
      "  Batch [1070/1299] D_loss: -1.1180, G_loss: -1.4656\n",
      "  Batch [1080/1299] D_loss: -3.1338, G_loss: 0.2544\n",
      "  Batch [1090/1299] D_loss: -2.2691, G_loss: 4.5534\n",
      "  Batch [1100/1299] D_loss: -2.7084, G_loss: 0.7025\n",
      "  Batch [1110/1299] D_loss: -0.1545, G_loss: 2.6495\n",
      "  Batch [1120/1299] D_loss: -1.6442, G_loss: 1.5226\n",
      "  Batch [1130/1299] D_loss: -1.3088, G_loss: 2.8069\n",
      "  Batch [1140/1299] D_loss: -1.4197, G_loss: 2.7082\n",
      "  Batch [1150/1299] D_loss: -3.1191, G_loss: 2.0731\n",
      "  Batch [1160/1299] D_loss: -1.8604, G_loss: 3.5608\n",
      "  Batch [1170/1299] D_loss: -3.0851, G_loss: 2.8791\n",
      "  Batch [1180/1299] D_loss: -2.4748, G_loss: 3.6920\n",
      "  Batch [1190/1299] D_loss: -2.3157, G_loss: 4.5094\n",
      "  Batch [1200/1299] D_loss: -2.8404, G_loss: 2.6974\n",
      "  Batch [1210/1299] D_loss: -0.9260, G_loss: 1.4099\n",
      "  Batch [1220/1299] D_loss: -1.4558, G_loss: 2.5040\n",
      "  Batch [1230/1299] D_loss: -2.5026, G_loss: 1.7532\n",
      "  Batch [1240/1299] D_loss: -2.1716, G_loss: 3.1541\n",
      "  Batch [1250/1299] D_loss: -2.0278, G_loss: 2.7840\n",
      "  Batch [1260/1299] D_loss: -1.8631, G_loss: 3.3359\n",
      "  Batch [1270/1299] D_loss: -0.7822, G_loss: 1.3152\n",
      "  Batch [1280/1299] D_loss: -1.3798, G_loss: 0.9593\n",
      "  Batch [1290/1299] D_loss: -2.1112, G_loss: 2.3640\n",
      "\n",
      "Epoch 33 Summary:\n",
      "  Average D_loss: -2.2552\n",
      "  Average G_loss: 2.3826\n",
      "\n",
      "Epoch [34/100]\n",
      "  Batch [0/1299] D_loss: -4.8495, G_loss: 2.4954\n",
      "  Batch [10/1299] D_loss: -4.8574, G_loss: 3.6920\n",
      "  Batch [20/1299] D_loss: -3.0720, G_loss: 3.4472\n",
      "  Batch [30/1299] D_loss: -3.2309, G_loss: 3.0438\n",
      "  Batch [40/1299] D_loss: -2.7682, G_loss: 3.3606\n",
      "  Batch [50/1299] D_loss: -2.4659, G_loss: 0.3518\n",
      "  Batch [60/1299] D_loss: -3.2115, G_loss: 1.4964\n",
      "  Batch [70/1299] D_loss: -3.4500, G_loss: 1.6991\n",
      "  Batch [80/1299] D_loss: -4.1379, G_loss: 4.1889\n",
      "  Batch [90/1299] D_loss: -3.5209, G_loss: 3.7886\n",
      "  Batch [100/1299] D_loss: -2.1435, G_loss: 3.5724\n",
      "  Batch [110/1299] D_loss: -2.0709, G_loss: 3.2070\n",
      "  Batch [120/1299] D_loss: -1.6523, G_loss: 1.1389\n",
      "  Batch [130/1299] D_loss: -2.3022, G_loss: 2.9472\n",
      "  Batch [140/1299] D_loss: -1.8988, G_loss: 2.4250\n",
      "  Batch [150/1299] D_loss: -2.9657, G_loss: 2.1259\n",
      "  Batch [160/1299] D_loss: -4.3198, G_loss: 1.3772\n",
      "  Batch [170/1299] D_loss: -2.8858, G_loss: 0.9706\n",
      "  Batch [180/1299] D_loss: -2.7403, G_loss: 2.3567\n",
      "  Batch [190/1299] D_loss: -2.0769, G_loss: 1.4018\n",
      "  Batch [200/1299] D_loss: -3.0204, G_loss: 2.2540\n",
      "  Batch [210/1299] D_loss: -2.8124, G_loss: 1.8477\n",
      "  Batch [220/1299] D_loss: -2.6936, G_loss: 2.1860\n",
      "  Batch [230/1299] D_loss: -2.1880, G_loss: 1.4628\n",
      "  Batch [240/1299] D_loss: -2.0889, G_loss: 2.7145\n",
      "  Batch [250/1299] D_loss: -3.0605, G_loss: 1.6075\n",
      "  Batch [260/1299] D_loss: -2.5980, G_loss: 1.9109\n",
      "  Batch [270/1299] D_loss: -0.6242, G_loss: 0.5273\n",
      "  Batch [280/1299] D_loss: -2.1568, G_loss: 0.8328\n",
      "  Batch [290/1299] D_loss: -1.1907, G_loss: 0.2009\n",
      "  Batch [300/1299] D_loss: -2.0162, G_loss: 1.4083\n",
      "  Batch [310/1299] D_loss: -2.8374, G_loss: 1.4142\n",
      "  Batch [320/1299] D_loss: -2.6295, G_loss: 1.4553\n",
      "  Batch [330/1299] D_loss: -2.7336, G_loss: -0.4913\n",
      "  Batch [340/1299] D_loss: -1.9992, G_loss: 0.6833\n",
      "  Batch [350/1299] D_loss: -4.1020, G_loss: 0.1535\n",
      "  Batch [360/1299] D_loss: -2.7527, G_loss: 2.9784\n",
      "  Batch [370/1299] D_loss: -4.1600, G_loss: 1.4590\n",
      "  Batch [380/1299] D_loss: -2.4073, G_loss: 0.4578\n",
      "  Batch [390/1299] D_loss: -2.6817, G_loss: 1.6157\n",
      "  Batch [400/1299] D_loss: -1.7203, G_loss: 3.7435\n",
      "  Batch [410/1299] D_loss: -1.1817, G_loss: 3.2901\n",
      "  Batch [420/1299] D_loss: -0.7743, G_loss: 2.4541\n",
      "  Batch [430/1299] D_loss: -0.9530, G_loss: 1.5886\n",
      "  Batch [440/1299] D_loss: -2.3777, G_loss: 1.5129\n",
      "  Batch [450/1299] D_loss: -2.0262, G_loss: 2.0960\n",
      "  Batch [460/1299] D_loss: -1.9972, G_loss: 0.5362\n",
      "  Batch [470/1299] D_loss: -3.1321, G_loss: 2.4545\n",
      "  Batch [480/1299] D_loss: -1.9510, G_loss: 2.7906\n",
      "  Batch [490/1299] D_loss: -4.0895, G_loss: 2.3835\n",
      "  Batch [500/1299] D_loss: -3.3676, G_loss: 2.3291\n",
      "  Batch [510/1299] D_loss: -2.6401, G_loss: 4.0461\n",
      "  Batch [520/1299] D_loss: -2.9915, G_loss: 2.8788\n",
      "  Batch [530/1299] D_loss: -3.7985, G_loss: 4.4874\n",
      "  Batch [540/1299] D_loss: -1.5894, G_loss: 3.2754\n",
      "  Batch [550/1299] D_loss: -2.7160, G_loss: 2.0973\n",
      "  Batch [560/1299] D_loss: -3.5184, G_loss: 1.6940\n",
      "  Batch [570/1299] D_loss: -2.6262, G_loss: 2.2288\n",
      "  Batch [580/1299] D_loss: -2.9996, G_loss: 2.4670\n",
      "  Batch [590/1299] D_loss: -2.9812, G_loss: 1.6419\n",
      "  Batch [600/1299] D_loss: -2.4179, G_loss: 0.8326\n",
      "  Batch [610/1299] D_loss: -3.1224, G_loss: 1.7574\n",
      "  Batch [620/1299] D_loss: -2.6005, G_loss: 2.8159\n",
      "  Batch [630/1299] D_loss: -3.5189, G_loss: 3.8058\n",
      "  Batch [640/1299] D_loss: -1.5658, G_loss: 1.1565\n",
      "  Batch [650/1299] D_loss: -3.0985, G_loss: -0.0614\n",
      "  Batch [660/1299] D_loss: -0.9438, G_loss: 1.1938\n",
      "  Batch [670/1299] D_loss: -1.6348, G_loss: 1.7111\n",
      "  Batch [680/1299] D_loss: -2.8502, G_loss: 3.9318\n",
      "  Batch [690/1299] D_loss: -3.0903, G_loss: 3.1473\n",
      "  Batch [700/1299] D_loss: -2.8961, G_loss: 1.6519\n",
      "  Batch [710/1299] D_loss: -1.9997, G_loss: 1.7883\n",
      "  Batch [720/1299] D_loss: -3.1927, G_loss: 2.5228\n",
      "  Batch [730/1299] D_loss: -2.6269, G_loss: -1.4522\n",
      "  Batch [740/1299] D_loss: -3.7355, G_loss: -0.8596\n",
      "  Batch [750/1299] D_loss: -3.3949, G_loss: -0.4067\n",
      "  Batch [760/1299] D_loss: -2.1867, G_loss: 1.7218\n",
      "  Batch [770/1299] D_loss: -1.7516, G_loss: -0.2876\n",
      "  Batch [780/1299] D_loss: -2.4559, G_loss: 1.6174\n",
      "  Batch [790/1299] D_loss: -1.3542, G_loss: 2.4429\n",
      "  Batch [800/1299] D_loss: -1.3848, G_loss: 2.2825\n",
      "  Batch [810/1299] D_loss: -2.7550, G_loss: 2.9203\n",
      "  Batch [820/1299] D_loss: -3.4006, G_loss: 0.5451\n",
      "  Batch [830/1299] D_loss: -2.3792, G_loss: 1.1731\n",
      "  Batch [840/1299] D_loss: -2.6460, G_loss: -2.6246\n",
      "  Batch [850/1299] D_loss: -2.8632, G_loss: 1.1315\n",
      "  Batch [860/1299] D_loss: -2.2958, G_loss: 0.7565\n",
      "  Batch [870/1299] D_loss: -1.7952, G_loss: 0.2720\n",
      "  Batch [880/1299] D_loss: -1.1622, G_loss: 2.4392\n",
      "  Batch [890/1299] D_loss: -3.1067, G_loss: 0.0074\n",
      "  Batch [900/1299] D_loss: -4.2002, G_loss: 1.3392\n",
      "  Batch [910/1299] D_loss: -2.3450, G_loss: 0.4641\n",
      "  Batch [920/1299] D_loss: -2.4593, G_loss: 2.1028\n",
      "  Batch [930/1299] D_loss: -1.1113, G_loss: -0.1281\n",
      "  Batch [940/1299] D_loss: -3.2831, G_loss: 0.7339\n",
      "  Batch [950/1299] D_loss: -3.5650, G_loss: 1.5133\n",
      "  Batch [960/1299] D_loss: -2.3395, G_loss: 1.7800\n",
      "  Batch [970/1299] D_loss: -2.7309, G_loss: 0.8481\n",
      "  Batch [980/1299] D_loss: -2.3997, G_loss: 1.5290\n",
      "  Batch [990/1299] D_loss: -2.4150, G_loss: 0.8937\n",
      "  Batch [1000/1299] D_loss: -2.7321, G_loss: 0.3857\n",
      "  Batch [1010/1299] D_loss: -4.0880, G_loss: 0.1890\n",
      "  Batch [1020/1299] D_loss: -1.7566, G_loss: 1.9024\n",
      "  Batch [1030/1299] D_loss: -2.6830, G_loss: 1.8557\n",
      "  Batch [1040/1299] D_loss: -2.0708, G_loss: 2.1769\n",
      "  Batch [1050/1299] D_loss: -2.5484, G_loss: 4.0004\n",
      "  Batch [1060/1299] D_loss: -3.9137, G_loss: 6.5051\n",
      "  Batch [1070/1299] D_loss: -2.8838, G_loss: 5.2960\n",
      "  Batch [1080/1299] D_loss: -3.4146, G_loss: 6.5033\n",
      "  Batch [1090/1299] D_loss: -1.5352, G_loss: 4.8162\n",
      "  Batch [1100/1299] D_loss: -2.4367, G_loss: 5.0484\n",
      "  Batch [1110/1299] D_loss: -3.8450, G_loss: 2.8506\n",
      "  Batch [1120/1299] D_loss: -2.4009, G_loss: 3.1527\n",
      "  Batch [1130/1299] D_loss: -1.1005, G_loss: 3.2896\n",
      "  Batch [1140/1299] D_loss: -1.6156, G_loss: 4.6915\n",
      "  Batch [1150/1299] D_loss: -0.4164, G_loss: 0.2195\n",
      "  Batch [1160/1299] D_loss: -1.6001, G_loss: 1.6074\n",
      "  Batch [1170/1299] D_loss: -1.7478, G_loss: 3.1012\n",
      "  Batch [1180/1299] D_loss: -3.2432, G_loss: 1.3996\n",
      "  Batch [1190/1299] D_loss: -1.9258, G_loss: 2.4039\n",
      "  Batch [1200/1299] D_loss: -2.7836, G_loss: 4.4054\n",
      "  Batch [1210/1299] D_loss: -1.9809, G_loss: 5.2236\n",
      "  Batch [1220/1299] D_loss: -2.6332, G_loss: 3.1924\n",
      "  Batch [1230/1299] D_loss: -3.6828, G_loss: 2.6016\n",
      "  Batch [1240/1299] D_loss: -1.3057, G_loss: -0.5038\n",
      "  Batch [1250/1299] D_loss: -3.6127, G_loss: 1.9307\n",
      "  Batch [1260/1299] D_loss: -3.1642, G_loss: 2.4240\n",
      "  Batch [1270/1299] D_loss: -3.2078, G_loss: 0.6531\n",
      "  Batch [1280/1299] D_loss: -3.0341, G_loss: 0.6132\n",
      "  Batch [1290/1299] D_loss: -2.4246, G_loss: 1.0707\n",
      "\n",
      "Epoch 34 Summary:\n",
      "  Average D_loss: -2.2541\n",
      "  Average G_loss: 1.8922\n",
      "\n",
      "Epoch [35/100]\n",
      "  Batch [0/1299] D_loss: -4.8436, G_loss: -0.0856\n",
      "  Batch [10/1299] D_loss: -1.9480, G_loss: 1.3481\n",
      "  Batch [20/1299] D_loss: -2.5229, G_loss: 0.5402\n",
      "  Batch [30/1299] D_loss: -3.9485, G_loss: 1.9882\n",
      "  Batch [40/1299] D_loss: -0.9284, G_loss: 2.1742\n",
      "  Batch [50/1299] D_loss: -4.1598, G_loss: 2.4618\n",
      "  Batch [60/1299] D_loss: -1.9350, G_loss: 3.2363\n",
      "  Batch [70/1299] D_loss: -2.6733, G_loss: 3.5283\n",
      "  Batch [80/1299] D_loss: -2.6309, G_loss: 2.4293\n",
      "  Batch [90/1299] D_loss: -1.8657, G_loss: 5.0198\n",
      "  Batch [100/1299] D_loss: -3.3269, G_loss: 4.4427\n",
      "  Batch [110/1299] D_loss: -1.6330, G_loss: 1.8563\n",
      "  Batch [120/1299] D_loss: -3.5049, G_loss: 2.9922\n",
      "  Batch [130/1299] D_loss: -1.0761, G_loss: 1.2720\n",
      "  Batch [140/1299] D_loss: -3.0624, G_loss: 5.0558\n",
      "  Batch [150/1299] D_loss: -3.6141, G_loss: 4.1398\n",
      "  Batch [160/1299] D_loss: -4.0992, G_loss: 4.3572\n",
      "  Batch [170/1299] D_loss: -2.2364, G_loss: 4.6122\n",
      "  Batch [180/1299] D_loss: -3.3503, G_loss: 4.2170\n",
      "  Batch [190/1299] D_loss: -4.5401, G_loss: 2.7373\n",
      "  Batch [200/1299] D_loss: -2.6433, G_loss: 2.8609\n",
      "  Batch [210/1299] D_loss: -3.2038, G_loss: 3.7136\n",
      "  Batch [220/1299] D_loss: -1.3294, G_loss: 1.4500\n",
      "  Batch [230/1299] D_loss: -2.5362, G_loss: 2.8863\n",
      "  Batch [240/1299] D_loss: -2.9894, G_loss: 1.8211\n",
      "  Batch [250/1299] D_loss: -1.7439, G_loss: 1.4112\n",
      "  Batch [260/1299] D_loss: -0.7548, G_loss: 3.1358\n",
      "  Batch [270/1299] D_loss: -1.4083, G_loss: 3.0142\n",
      "  Batch [280/1299] D_loss: -1.2498, G_loss: 3.4068\n",
      "  Batch [290/1299] D_loss: -2.2627, G_loss: 0.9527\n",
      "  Batch [300/1299] D_loss: -1.9413, G_loss: 1.3974\n",
      "  Batch [310/1299] D_loss: -3.6896, G_loss: -0.6728\n",
      "  Batch [320/1299] D_loss: -2.2997, G_loss: 2.1684\n",
      "  Batch [330/1299] D_loss: -4.3243, G_loss: 0.8807\n",
      "  Batch [340/1299] D_loss: -2.3329, G_loss: 0.9398\n",
      "  Batch [350/1299] D_loss: -1.3499, G_loss: 3.9249\n",
      "  Batch [360/1299] D_loss: -2.5474, G_loss: 1.7317\n",
      "  Batch [370/1299] D_loss: -3.6076, G_loss: 2.1175\n",
      "  Batch [380/1299] D_loss: -1.5463, G_loss: 1.4496\n",
      "  Batch [390/1299] D_loss: -2.4903, G_loss: 1.8205\n",
      "  Batch [400/1299] D_loss: -3.8542, G_loss: 3.1824\n",
      "  Batch [410/1299] D_loss: -2.6422, G_loss: 3.2016\n",
      "  Batch [420/1299] D_loss: -2.8123, G_loss: 2.6143\n",
      "  Batch [430/1299] D_loss: -3.1132, G_loss: 3.4847\n",
      "  Batch [440/1299] D_loss: -2.4466, G_loss: 2.3722\n",
      "  Batch [450/1299] D_loss: -1.4868, G_loss: 3.5389\n",
      "  Batch [460/1299] D_loss: -4.2022, G_loss: 2.8635\n",
      "  Batch [470/1299] D_loss: -2.7098, G_loss: 2.6243\n",
      "  Batch [480/1299] D_loss: -2.8619, G_loss: 2.9282\n",
      "  Batch [490/1299] D_loss: -1.1441, G_loss: 2.7333\n",
      "  Batch [500/1299] D_loss: -0.9183, G_loss: 1.7205\n",
      "  Batch [510/1299] D_loss: -1.6747, G_loss: 0.8323\n",
      "  Batch [520/1299] D_loss: -1.9806, G_loss: 2.6719\n",
      "  Batch [530/1299] D_loss: -1.4698, G_loss: 5.8330\n",
      "  Batch [540/1299] D_loss: -3.1967, G_loss: 6.0400\n",
      "  Batch [550/1299] D_loss: -2.4543, G_loss: 4.5300\n",
      "  Batch [560/1299] D_loss: -2.6548, G_loss: 4.5771\n",
      "  Batch [570/1299] D_loss: -1.9965, G_loss: 1.8179\n",
      "  Batch [580/1299] D_loss: -2.2147, G_loss: 0.9620\n",
      "  Batch [590/1299] D_loss: -3.6292, G_loss: 1.7152\n",
      "  Batch [600/1299] D_loss: -3.7774, G_loss: -1.5649\n",
      "  Batch [610/1299] D_loss: -2.5833, G_loss: -0.3390\n",
      "  Batch [620/1299] D_loss: -3.9958, G_loss: -0.4542\n",
      "  Batch [630/1299] D_loss: -1.3250, G_loss: 0.7178\n",
      "  Batch [640/1299] D_loss: -3.1191, G_loss: 0.9604\n",
      "  Batch [650/1299] D_loss: -1.6888, G_loss: -1.9058\n",
      "  Batch [660/1299] D_loss: -5.8479, G_loss: -1.6826\n",
      "  Batch [670/1299] D_loss: -2.7244, G_loss: 1.6159\n",
      "  Batch [680/1299] D_loss: -4.8752, G_loss: -0.4925\n",
      "  Batch [690/1299] D_loss: -1.3095, G_loss: 0.5281\n",
      "  Batch [700/1299] D_loss: -3.1286, G_loss: 1.1998\n",
      "  Batch [710/1299] D_loss: -2.7793, G_loss: -1.8496\n",
      "  Batch [720/1299] D_loss: -3.1415, G_loss: 1.5703\n",
      "  Batch [730/1299] D_loss: -1.2893, G_loss: 2.3106\n",
      "  Batch [740/1299] D_loss: -1.0063, G_loss: 0.7221\n",
      "  Batch [750/1299] D_loss: -1.1723, G_loss: 0.4540\n",
      "  Batch [760/1299] D_loss: -2.2314, G_loss: -1.8701\n",
      "  Batch [770/1299] D_loss: -2.1873, G_loss: -0.9417\n",
      "  Batch [780/1299] D_loss: -2.8809, G_loss: 0.0111\n",
      "  Batch [790/1299] D_loss: -3.4462, G_loss: 2.9455\n",
      "  Batch [800/1299] D_loss: -2.4664, G_loss: 2.6338\n",
      "  Batch [810/1299] D_loss: -2.6667, G_loss: 0.7936\n",
      "  Batch [820/1299] D_loss: -2.0639, G_loss: 2.8971\n",
      "  Batch [830/1299] D_loss: -1.5914, G_loss: -0.4434\n",
      "  Batch [840/1299] D_loss: -2.2657, G_loss: 1.1385\n",
      "  Batch [850/1299] D_loss: -1.5031, G_loss: 1.6800\n",
      "  Batch [860/1299] D_loss: -2.4964, G_loss: 1.1485\n",
      "  Batch [870/1299] D_loss: -3.4857, G_loss: 0.0004\n",
      "  Batch [880/1299] D_loss: -1.9284, G_loss: 1.7890\n",
      "  Batch [890/1299] D_loss: -0.8445, G_loss: 2.2005\n",
      "  Batch [900/1299] D_loss: -3.8623, G_loss: 3.0934\n",
      "  Batch [910/1299] D_loss: -3.2449, G_loss: 0.7159\n",
      "  Batch [920/1299] D_loss: -3.7957, G_loss: 3.0304\n",
      "  Batch [930/1299] D_loss: -2.6513, G_loss: 0.9171\n",
      "  Batch [940/1299] D_loss: -0.9988, G_loss: 0.6306\n",
      "  Batch [950/1299] D_loss: -1.1872, G_loss: 1.4856\n",
      "  Batch [960/1299] D_loss: -0.8171, G_loss: 0.7411\n",
      "  Batch [970/1299] D_loss: -2.3262, G_loss: 2.4522\n",
      "  Batch [980/1299] D_loss: -1.8352, G_loss: 0.5728\n",
      "  Batch [990/1299] D_loss: -4.9531, G_loss: 2.0429\n",
      "  Batch [1000/1299] D_loss: -3.6048, G_loss: 2.6098\n",
      "  Batch [1010/1299] D_loss: -2.2732, G_loss: 1.1777\n",
      "  Batch [1020/1299] D_loss: -0.9649, G_loss: 2.1095\n",
      "  Batch [1030/1299] D_loss: -2.6663, G_loss: 0.6422\n",
      "  Batch [1040/1299] D_loss: -1.5335, G_loss: 0.6473\n",
      "  Batch [1050/1299] D_loss: -2.9532, G_loss: 1.0600\n",
      "  Batch [1060/1299] D_loss: -2.9517, G_loss: 2.1653\n",
      "  Batch [1070/1299] D_loss: -1.7578, G_loss: 1.4776\n",
      "  Batch [1080/1299] D_loss: -2.8165, G_loss: 1.1355\n",
      "  Batch [1090/1299] D_loss: -1.4642, G_loss: 0.1320\n",
      "  Batch [1100/1299] D_loss: -1.1299, G_loss: 0.6241\n",
      "  Batch [1110/1299] D_loss: -2.4654, G_loss: 1.4865\n",
      "  Batch [1120/1299] D_loss: -3.7459, G_loss: 0.4679\n",
      "  Batch [1130/1299] D_loss: -2.6644, G_loss: 1.6158\n",
      "  Batch [1140/1299] D_loss: -1.2928, G_loss: 0.6390\n",
      "  Batch [1150/1299] D_loss: -1.2817, G_loss: 0.3943\n",
      "  Batch [1160/1299] D_loss: -2.3385, G_loss: 0.8956\n",
      "  Batch [1170/1299] D_loss: -1.8986, G_loss: 2.6783\n",
      "  Batch [1180/1299] D_loss: -1.7813, G_loss: 1.5966\n",
      "  Batch [1190/1299] D_loss: -1.3334, G_loss: 2.8519\n",
      "  Batch [1200/1299] D_loss: -2.9748, G_loss: 2.8966\n",
      "  Batch [1210/1299] D_loss: -2.5769, G_loss: 2.1055\n",
      "  Batch [1220/1299] D_loss: -1.9871, G_loss: 3.0032\n",
      "  Batch [1230/1299] D_loss: -3.6156, G_loss: 3.5600\n",
      "  Batch [1240/1299] D_loss: -1.5643, G_loss: 2.1818\n",
      "  Batch [1250/1299] D_loss: -3.5243, G_loss: 2.0723\n",
      "  Batch [1260/1299] D_loss: -3.2783, G_loss: 3.0436\n",
      "  Batch [1270/1299] D_loss: -2.7951, G_loss: 1.5490\n",
      "  Batch [1280/1299] D_loss: -2.2221, G_loss: 3.8154\n",
      "  Batch [1290/1299] D_loss: -1.4987, G_loss: 5.5896\n",
      "\n",
      "Epoch 35 Summary:\n",
      "  Average D_loss: -2.2547\n",
      "  Average G_loss: 1.8936\n",
      "\n",
      "Epoch [36/100]\n",
      "  Batch [0/1299] D_loss: -2.8359, G_loss: 1.9455\n",
      "  Batch [10/1299] D_loss: -2.7514, G_loss: 1.3510\n",
      "  Batch [20/1299] D_loss: -3.2446, G_loss: 0.3143\n",
      "  Batch [30/1299] D_loss: -0.8345, G_loss: -0.9451\n",
      "  Batch [40/1299] D_loss: 0.5024, G_loss: 3.5391\n",
      "  Batch [50/1299] D_loss: -2.5450, G_loss: 1.3379\n",
      "  Batch [60/1299] D_loss: -3.8371, G_loss: 0.2791\n",
      "  Batch [70/1299] D_loss: -2.7344, G_loss: 2.9920\n",
      "  Batch [80/1299] D_loss: -5.4451, G_loss: 5.2665\n",
      "  Batch [90/1299] D_loss: -2.3076, G_loss: 5.0689\n",
      "  Batch [100/1299] D_loss: -3.2295, G_loss: 7.1778\n",
      "  Batch [110/1299] D_loss: -2.6409, G_loss: 6.7459\n",
      "  Batch [120/1299] D_loss: -1.5923, G_loss: 4.0541\n",
      "  Batch [130/1299] D_loss: -2.7262, G_loss: 2.4409\n",
      "  Batch [140/1299] D_loss: -2.7521, G_loss: 0.2369\n",
      "  Batch [150/1299] D_loss: -0.4255, G_loss: 1.4023\n",
      "  Batch [160/1299] D_loss: -2.6444, G_loss: 4.2663\n",
      "  Batch [170/1299] D_loss: -2.4782, G_loss: 0.6202\n",
      "  Batch [180/1299] D_loss: -3.7003, G_loss: 0.5181\n",
      "  Batch [190/1299] D_loss: -2.4396, G_loss: 1.6036\n",
      "  Batch [200/1299] D_loss: -1.3959, G_loss: 2.4562\n",
      "  Batch [210/1299] D_loss: -3.0808, G_loss: 1.8573\n",
      "  Batch [220/1299] D_loss: -2.5213, G_loss: 2.9745\n",
      "  Batch [230/1299] D_loss: -3.3461, G_loss: 2.3465\n",
      "  Batch [240/1299] D_loss: -2.5087, G_loss: 3.0915\n",
      "  Batch [250/1299] D_loss: -2.6081, G_loss: 2.9059\n",
      "  Batch [260/1299] D_loss: -1.6893, G_loss: 0.2437\n",
      "  Batch [270/1299] D_loss: -5.4515, G_loss: 1.5055\n",
      "  Batch [280/1299] D_loss: -1.3635, G_loss: 2.1273\n",
      "  Batch [290/1299] D_loss: -2.1928, G_loss: 3.0572\n",
      "  Batch [300/1299] D_loss: -3.2258, G_loss: 1.3534\n",
      "  Batch [310/1299] D_loss: -3.4592, G_loss: 4.0194\n",
      "  Batch [320/1299] D_loss: -3.2138, G_loss: 0.8357\n",
      "  Batch [330/1299] D_loss: -3.0036, G_loss: 1.3792\n",
      "  Batch [340/1299] D_loss: -3.3380, G_loss: 2.0938\n",
      "  Batch [350/1299] D_loss: -1.9047, G_loss: 2.9556\n",
      "  Batch [360/1299] D_loss: -2.4560, G_loss: 0.5102\n",
      "  Batch [370/1299] D_loss: -2.2219, G_loss: 1.3415\n",
      "  Batch [380/1299] D_loss: -2.0886, G_loss: 1.6001\n",
      "  Batch [390/1299] D_loss: -3.0985, G_loss: -0.3375\n",
      "  Batch [400/1299] D_loss: -2.6301, G_loss: 1.1260\n",
      "  Batch [410/1299] D_loss: -2.7620, G_loss: 2.6212\n",
      "  Batch [420/1299] D_loss: -1.9436, G_loss: 1.8549\n",
      "  Batch [430/1299] D_loss: -2.0494, G_loss: 4.2361\n",
      "  Batch [440/1299] D_loss: -1.2962, G_loss: 2.6914\n",
      "  Batch [450/1299] D_loss: -2.0089, G_loss: 1.7982\n",
      "  Batch [460/1299] D_loss: -3.8071, G_loss: 1.4214\n",
      "  Batch [470/1299] D_loss: -1.7320, G_loss: 2.0661\n",
      "  Batch [480/1299] D_loss: -2.6443, G_loss: 0.4981\n",
      "  Batch [490/1299] D_loss: -3.1405, G_loss: 0.8903\n",
      "  Batch [500/1299] D_loss: -3.4056, G_loss: 0.9130\n",
      "  Batch [510/1299] D_loss: -2.4510, G_loss: 0.7786\n",
      "  Batch [520/1299] D_loss: -1.8444, G_loss: 2.5402\n",
      "  Batch [530/1299] D_loss: -1.0751, G_loss: 2.4885\n",
      "  Batch [540/1299] D_loss: -1.3354, G_loss: 1.5142\n",
      "  Batch [550/1299] D_loss: -0.0097, G_loss: 0.2714\n",
      "  Batch [560/1299] D_loss: -2.1303, G_loss: 2.8746\n",
      "  Batch [570/1299] D_loss: -0.3600, G_loss: 4.1064\n",
      "  Batch [580/1299] D_loss: -2.3877, G_loss: 3.5685\n",
      "  Batch [590/1299] D_loss: -2.5867, G_loss: 4.1285\n",
      "  Batch [600/1299] D_loss: -3.9160, G_loss: 2.7425\n",
      "  Batch [610/1299] D_loss: -0.4809, G_loss: 4.7536\n",
      "  Batch [620/1299] D_loss: -3.7977, G_loss: 2.9611\n",
      "  Batch [630/1299] D_loss: -2.2713, G_loss: 3.3929\n",
      "  Batch [640/1299] D_loss: -2.9975, G_loss: 1.2721\n",
      "  Batch [650/1299] D_loss: -0.2929, G_loss: 1.5047\n",
      "  Batch [660/1299] D_loss: -1.5072, G_loss: 2.0297\n",
      "  Batch [670/1299] D_loss: -2.2703, G_loss: 3.2231\n",
      "  Batch [680/1299] D_loss: -2.3240, G_loss: 1.3863\n",
      "  Batch [690/1299] D_loss: -3.2184, G_loss: 1.7897\n",
      "  Batch [700/1299] D_loss: -3.2664, G_loss: 1.6534\n",
      "  Batch [710/1299] D_loss: -1.5117, G_loss: 2.9522\n",
      "  Batch [720/1299] D_loss: -3.3272, G_loss: 3.1077\n",
      "  Batch [730/1299] D_loss: -0.6995, G_loss: 3.7803\n",
      "  Batch [740/1299] D_loss: -3.4869, G_loss: 2.9405\n",
      "  Batch [750/1299] D_loss: -1.7964, G_loss: 6.6121\n",
      "  Batch [760/1299] D_loss: -2.4408, G_loss: 4.7413\n",
      "  Batch [770/1299] D_loss: -3.2475, G_loss: 4.1444\n",
      "  Batch [780/1299] D_loss: -1.6421, G_loss: 3.2857\n",
      "  Batch [790/1299] D_loss: -2.3636, G_loss: 4.3936\n",
      "  Batch [800/1299] D_loss: -0.0314, G_loss: 2.5143\n",
      "  Batch [810/1299] D_loss: -1.5308, G_loss: 1.4948\n",
      "  Batch [820/1299] D_loss: -3.2654, G_loss: 3.6275\n",
      "  Batch [830/1299] D_loss: -2.0824, G_loss: 4.0811\n",
      "  Batch [840/1299] D_loss: -0.2503, G_loss: 2.7184\n",
      "  Batch [850/1299] D_loss: -2.7143, G_loss: 2.6162\n",
      "  Batch [860/1299] D_loss: -3.0334, G_loss: 3.1079\n",
      "  Batch [870/1299] D_loss: -1.4714, G_loss: 1.1261\n",
      "  Batch [880/1299] D_loss: -1.1337, G_loss: 1.4538\n",
      "  Batch [890/1299] D_loss: -3.2005, G_loss: 1.5285\n",
      "  Batch [900/1299] D_loss: -2.2848, G_loss: -0.6026\n",
      "  Batch [910/1299] D_loss: -2.1922, G_loss: 1.7331\n",
      "  Batch [920/1299] D_loss: -1.3433, G_loss: 1.9120\n",
      "  Batch [930/1299] D_loss: -1.1176, G_loss: 0.1671\n",
      "  Batch [940/1299] D_loss: -1.6322, G_loss: 4.2200\n",
      "  Batch [950/1299] D_loss: -1.7353, G_loss: 3.7349\n",
      "  Batch [960/1299] D_loss: -1.1653, G_loss: 3.8248\n",
      "  Batch [970/1299] D_loss: -3.4323, G_loss: 3.9243\n",
      "  Batch [980/1299] D_loss: -1.4607, G_loss: 3.9115\n",
      "  Batch [990/1299] D_loss: -2.2065, G_loss: 0.7281\n",
      "  Batch [1000/1299] D_loss: -2.4015, G_loss: 1.4179\n",
      "  Batch [1010/1299] D_loss: -2.0153, G_loss: 2.7262\n",
      "  Batch [1020/1299] D_loss: -3.0791, G_loss: 1.9724\n",
      "  Batch [1030/1299] D_loss: -3.6628, G_loss: 3.1865\n",
      "  Batch [1040/1299] D_loss: -3.3488, G_loss: 1.8966\n",
      "  Batch [1050/1299] D_loss: -3.2559, G_loss: 3.8949\n",
      "  Batch [1060/1299] D_loss: -0.4098, G_loss: 5.1190\n",
      "  Batch [1070/1299] D_loss: -2.6080, G_loss: 6.0386\n",
      "  Batch [1080/1299] D_loss: -3.9306, G_loss: 4.0995\n",
      "  Batch [1090/1299] D_loss: -2.1314, G_loss: 3.6269\n",
      "  Batch [1100/1299] D_loss: -3.3267, G_loss: 3.6506\n",
      "  Batch [1110/1299] D_loss: -4.2142, G_loss: 3.3952\n",
      "  Batch [1120/1299] D_loss: -2.9397, G_loss: 4.8150\n",
      "  Batch [1130/1299] D_loss: -1.3910, G_loss: 4.2515\n",
      "  Batch [1140/1299] D_loss: -2.4090, G_loss: 2.8511\n",
      "  Batch [1150/1299] D_loss: -2.6370, G_loss: 1.7591\n",
      "  Batch [1160/1299] D_loss: -3.1729, G_loss: 1.6826\n",
      "  Batch [1170/1299] D_loss: -1.7672, G_loss: 1.3742\n",
      "  Batch [1180/1299] D_loss: -2.8701, G_loss: 2.1953\n",
      "  Batch [1190/1299] D_loss: -1.5265, G_loss: 3.6912\n",
      "  Batch [1200/1299] D_loss: -3.4005, G_loss: 3.9225\n",
      "  Batch [1210/1299] D_loss: -1.9962, G_loss: 3.2501\n",
      "  Batch [1220/1299] D_loss: -3.2025, G_loss: 3.1826\n",
      "  Batch [1230/1299] D_loss: -3.2400, G_loss: 2.8863\n",
      "  Batch [1240/1299] D_loss: -1.4297, G_loss: 1.4681\n",
      "  Batch [1250/1299] D_loss: -2.6008, G_loss: 2.2097\n",
      "  Batch [1260/1299] D_loss: -2.4253, G_loss: 2.3590\n",
      "  Batch [1270/1299] D_loss: -3.3675, G_loss: 4.3736\n",
      "  Batch [1280/1299] D_loss: -2.3719, G_loss: 3.8571\n",
      "  Batch [1290/1299] D_loss: -1.9355, G_loss: 3.6212\n",
      "\n",
      "Epoch 36 Summary:\n",
      "  Average D_loss: -2.2353\n",
      "  Average G_loss: 2.6556\n",
      "\n",
      "Epoch [37/100]\n",
      "  Batch [0/1299] D_loss: -2.4728, G_loss: 4.1848\n",
      "  Batch [10/1299] D_loss: -3.3468, G_loss: 3.2891\n",
      "  Batch [20/1299] D_loss: -3.2195, G_loss: -1.8446\n",
      "  Batch [30/1299] D_loss: -2.7301, G_loss: 0.6290\n",
      "  Batch [40/1299] D_loss: -1.0402, G_loss: 1.9965\n",
      "  Batch [50/1299] D_loss: -2.9336, G_loss: 0.5339\n",
      "  Batch [60/1299] D_loss: -1.8747, G_loss: 1.7820\n",
      "  Batch [70/1299] D_loss: -2.8936, G_loss: 2.8520\n",
      "  Batch [80/1299] D_loss: -4.3555, G_loss: 2.8500\n",
      "  Batch [90/1299] D_loss: -1.4478, G_loss: 0.8901\n",
      "  Batch [100/1299] D_loss: -2.4481, G_loss: 2.1169\n",
      "  Batch [110/1299] D_loss: -2.5368, G_loss: 2.6202\n",
      "  Batch [120/1299] D_loss: -2.3186, G_loss: 2.4184\n",
      "  Batch [130/1299] D_loss: -1.2294, G_loss: 1.9178\n",
      "  Batch [140/1299] D_loss: -1.8690, G_loss: 1.0390\n",
      "  Batch [150/1299] D_loss: -4.1783, G_loss: 0.9241\n",
      "  Batch [160/1299] D_loss: -2.2621, G_loss: 1.6910\n",
      "  Batch [170/1299] D_loss: -3.5757, G_loss: -0.3454\n",
      "  Batch [180/1299] D_loss: -3.0770, G_loss: 3.0647\n",
      "  Batch [190/1299] D_loss: -3.1874, G_loss: 2.0570\n",
      "  Batch [200/1299] D_loss: -0.8018, G_loss: 1.4048\n",
      "  Batch [210/1299] D_loss: -3.9520, G_loss: 1.0409\n",
      "  Batch [220/1299] D_loss: -1.8633, G_loss: 4.0273\n",
      "  Batch [230/1299] D_loss: -2.6490, G_loss: 2.2997\n",
      "  Batch [240/1299] D_loss: -4.8068, G_loss: 2.1549\n",
      "  Batch [250/1299] D_loss: -2.1251, G_loss: 1.1273\n",
      "  Batch [260/1299] D_loss: -1.8584, G_loss: 1.4839\n",
      "  Batch [270/1299] D_loss: -1.9412, G_loss: 2.6373\n",
      "  Batch [280/1299] D_loss: -3.8061, G_loss: 1.8856\n",
      "  Batch [290/1299] D_loss: -2.1972, G_loss: 1.9228\n",
      "  Batch [300/1299] D_loss: -2.3521, G_loss: 3.0036\n",
      "  Batch [310/1299] D_loss: -2.1546, G_loss: 0.6795\n",
      "  Batch [320/1299] D_loss: -2.5688, G_loss: 2.2983\n",
      "  Batch [330/1299] D_loss: -3.4947, G_loss: 1.9244\n",
      "  Batch [340/1299] D_loss: -1.3815, G_loss: -0.3988\n",
      "  Batch [350/1299] D_loss: -3.0992, G_loss: 3.4145\n",
      "  Batch [360/1299] D_loss: -3.3010, G_loss: 2.9240\n",
      "  Batch [370/1299] D_loss: -2.8773, G_loss: 1.7586\n",
      "  Batch [380/1299] D_loss: -0.3789, G_loss: 1.2214\n",
      "  Batch [390/1299] D_loss: -2.7755, G_loss: 1.5263\n",
      "  Batch [400/1299] D_loss: -2.4777, G_loss: 4.1532\n",
      "  Batch [410/1299] D_loss: -2.5811, G_loss: 2.0914\n",
      "  Batch [420/1299] D_loss: -3.9883, G_loss: 1.5379\n",
      "  Batch [430/1299] D_loss: -1.8828, G_loss: 1.9758\n",
      "  Batch [440/1299] D_loss: -3.2877, G_loss: 3.5877\n",
      "  Batch [450/1299] D_loss: -2.9285, G_loss: 3.5694\n",
      "  Batch [460/1299] D_loss: -1.4324, G_loss: 3.6144\n",
      "  Batch [470/1299] D_loss: -0.9314, G_loss: 4.6814\n",
      "  Batch [480/1299] D_loss: -5.0924, G_loss: 4.4033\n",
      "  Batch [490/1299] D_loss: -3.0475, G_loss: 2.3451\n",
      "  Batch [500/1299] D_loss: -3.0257, G_loss: 2.2765\n",
      "  Batch [510/1299] D_loss: -1.6372, G_loss: 2.3192\n",
      "  Batch [520/1299] D_loss: -2.8842, G_loss: 2.3266\n",
      "  Batch [530/1299] D_loss: -3.1294, G_loss: 0.1012\n",
      "  Batch [540/1299] D_loss: -2.4501, G_loss: -0.0575\n",
      "  Batch [550/1299] D_loss: -2.9322, G_loss: 3.1753\n",
      "  Batch [560/1299] D_loss: -2.3997, G_loss: 1.3520\n",
      "  Batch [570/1299] D_loss: -3.5750, G_loss: 2.8843\n",
      "  Batch [580/1299] D_loss: -3.1925, G_loss: 0.9840\n",
      "  Batch [590/1299] D_loss: -2.3856, G_loss: 3.5423\n",
      "  Batch [600/1299] D_loss: -2.3684, G_loss: 4.1352\n",
      "  Batch [610/1299] D_loss: -1.7877, G_loss: 2.4712\n",
      "  Batch [620/1299] D_loss: -1.3394, G_loss: 5.9765\n",
      "  Batch [630/1299] D_loss: -2.7527, G_loss: 4.4125\n",
      "  Batch [640/1299] D_loss: -1.5730, G_loss: 3.4222\n",
      "  Batch [650/1299] D_loss: -2.6714, G_loss: 2.2610\n",
      "  Batch [660/1299] D_loss: -3.0037, G_loss: 3.8630\n",
      "  Batch [670/1299] D_loss: -3.6421, G_loss: 4.6205\n",
      "  Batch [680/1299] D_loss: -2.1623, G_loss: 4.0358\n",
      "  Batch [690/1299] D_loss: -2.6562, G_loss: 5.1085\n",
      "  Batch [700/1299] D_loss: -0.9227, G_loss: 1.5029\n",
      "  Batch [710/1299] D_loss: -3.5290, G_loss: 4.2100\n",
      "  Batch [720/1299] D_loss: -0.5809, G_loss: 2.1267\n",
      "  Batch [730/1299] D_loss: -2.4676, G_loss: 1.4181\n",
      "  Batch [740/1299] D_loss: -2.5524, G_loss: 3.4154\n",
      "  Batch [750/1299] D_loss: -3.3127, G_loss: 1.3600\n",
      "  Batch [760/1299] D_loss: -4.6638, G_loss: 1.5302\n",
      "  Batch [770/1299] D_loss: -1.1128, G_loss: 1.5541\n",
      "  Batch [780/1299] D_loss: -2.6558, G_loss: 0.4275\n",
      "  Batch [790/1299] D_loss: -1.6538, G_loss: 1.8764\n",
      "  Batch [800/1299] D_loss: -0.6545, G_loss: 2.2807\n",
      "  Batch [810/1299] D_loss: -2.0067, G_loss: 1.3396\n",
      "  Batch [820/1299] D_loss: -1.6387, G_loss: 0.3878\n",
      "  Batch [830/1299] D_loss: -4.2777, G_loss: 1.4286\n",
      "  Batch [840/1299] D_loss: -1.6199, G_loss: 3.2178\n",
      "  Batch [850/1299] D_loss: -4.1155, G_loss: 2.0746\n",
      "  Batch [860/1299] D_loss: -2.7717, G_loss: 3.2868\n",
      "  Batch [870/1299] D_loss: -3.0215, G_loss: 3.9901\n",
      "  Batch [880/1299] D_loss: -2.0610, G_loss: 2.0158\n",
      "  Batch [890/1299] D_loss: -2.5748, G_loss: 0.0235\n",
      "  Batch [900/1299] D_loss: -1.1190, G_loss: 2.0832\n",
      "  Batch [910/1299] D_loss: -2.8868, G_loss: 4.6291\n",
      "  Batch [920/1299] D_loss: -0.9403, G_loss: 3.8829\n",
      "  Batch [930/1299] D_loss: 0.1295, G_loss: 2.4586\n",
      "  Batch [940/1299] D_loss: -0.6806, G_loss: 3.0272\n",
      "  Batch [950/1299] D_loss: -1.3560, G_loss: 4.7091\n",
      "  Batch [960/1299] D_loss: -3.6466, G_loss: 2.5537\n",
      "  Batch [970/1299] D_loss: -2.5953, G_loss: 4.4961\n",
      "  Batch [980/1299] D_loss: -2.5790, G_loss: 3.6282\n",
      "  Batch [990/1299] D_loss: -3.6311, G_loss: 3.0595\n",
      "  Batch [1000/1299] D_loss: -1.7309, G_loss: 2.7088\n",
      "  Batch [1010/1299] D_loss: -2.6418, G_loss: 5.7036\n",
      "  Batch [1020/1299] D_loss: -1.8291, G_loss: 0.8703\n",
      "  Batch [1030/1299] D_loss: -1.4329, G_loss: 0.8513\n",
      "  Batch [1040/1299] D_loss: -2.3132, G_loss: -0.4479\n",
      "  Batch [1050/1299] D_loss: -1.7998, G_loss: 0.8701\n",
      "  Batch [1060/1299] D_loss: -3.8418, G_loss: 2.7923\n",
      "  Batch [1070/1299] D_loss: -2.5296, G_loss: 3.1181\n",
      "  Batch [1080/1299] D_loss: -1.6961, G_loss: 2.8331\n",
      "  Batch [1090/1299] D_loss: -2.1391, G_loss: 2.7742\n",
      "  Batch [1100/1299] D_loss: -3.0016, G_loss: 5.8187\n",
      "  Batch [1110/1299] D_loss: -2.3900, G_loss: 3.8664\n",
      "  Batch [1120/1299] D_loss: -2.5759, G_loss: 4.4374\n",
      "  Batch [1130/1299] D_loss: -1.2955, G_loss: 4.5803\n",
      "  Batch [1140/1299] D_loss: -3.7242, G_loss: 3.3749\n",
      "  Batch [1150/1299] D_loss: -3.2019, G_loss: 1.3279\n",
      "  Batch [1160/1299] D_loss: -3.3763, G_loss: 2.7791\n",
      "  Batch [1170/1299] D_loss: -0.9163, G_loss: 1.6936\n",
      "  Batch [1180/1299] D_loss: -3.5055, G_loss: -1.7989\n",
      "  Batch [1190/1299] D_loss: -4.4545, G_loss: -0.8304\n",
      "  Batch [1200/1299] D_loss: -3.6605, G_loss: 0.7639\n",
      "  Batch [1210/1299] D_loss: -2.8913, G_loss: 2.3097\n",
      "  Batch [1220/1299] D_loss: 0.0685, G_loss: -0.1557\n",
      "  Batch [1230/1299] D_loss: -1.6322, G_loss: 0.8107\n",
      "  Batch [1240/1299] D_loss: -3.4230, G_loss: 1.5699\n",
      "  Batch [1250/1299] D_loss: -0.8557, G_loss: 2.7952\n",
      "  Batch [1260/1299] D_loss: -2.7778, G_loss: 3.4424\n",
      "  Batch [1270/1299] D_loss: -3.8768, G_loss: 1.9393\n",
      "  Batch [1280/1299] D_loss: -2.2730, G_loss: 1.9753\n",
      "  Batch [1290/1299] D_loss: -2.3595, G_loss: 1.9694\n",
      "\n",
      "Epoch 37 Summary:\n",
      "  Average D_loss: -2.2591\n",
      "  Average G_loss: 2.4024\n",
      "\n",
      "Epoch [38/100]\n",
      "  Batch [0/1299] D_loss: -2.8609, G_loss: 3.1124\n",
      "  Batch [10/1299] D_loss: -3.0106, G_loss: 3.4557\n",
      "  Batch [20/1299] D_loss: -2.2170, G_loss: 2.6878\n",
      "  Batch [30/1299] D_loss: -4.2255, G_loss: 1.8130\n",
      "  Batch [40/1299] D_loss: -3.1676, G_loss: 2.7211\n",
      "  Batch [50/1299] D_loss: -1.4349, G_loss: 1.8688\n",
      "  Batch [60/1299] D_loss: -1.0952, G_loss: 2.3086\n",
      "  Batch [70/1299] D_loss: -3.0003, G_loss: 2.0036\n",
      "  Batch [80/1299] D_loss: -2.5141, G_loss: 0.7476\n",
      "  Batch [90/1299] D_loss: -1.4402, G_loss: 1.2832\n",
      "  Batch [100/1299] D_loss: -3.6491, G_loss: -0.4395\n",
      "  Batch [110/1299] D_loss: -2.6517, G_loss: 1.0953\n",
      "  Batch [120/1299] D_loss: -1.4114, G_loss: 0.1520\n",
      "  Batch [130/1299] D_loss: -1.1675, G_loss: 1.3060\n",
      "  Batch [140/1299] D_loss: -2.5571, G_loss: 1.4886\n",
      "  Batch [150/1299] D_loss: -2.6534, G_loss: 2.4256\n",
      "  Batch [160/1299] D_loss: -1.9374, G_loss: 2.1140\n",
      "  Batch [170/1299] D_loss: -2.4698, G_loss: 1.2812\n",
      "  Batch [180/1299] D_loss: -2.0208, G_loss: 3.1430\n",
      "  Batch [190/1299] D_loss: -2.6639, G_loss: 3.1917\n",
      "  Batch [200/1299] D_loss: -1.8817, G_loss: 2.3630\n",
      "  Batch [210/1299] D_loss: -0.5565, G_loss: 3.6458\n",
      "  Batch [220/1299] D_loss: -2.2804, G_loss: 4.4172\n",
      "  Batch [230/1299] D_loss: -4.0341, G_loss: 2.8562\n",
      "  Batch [240/1299] D_loss: -2.9905, G_loss: 2.3850\n",
      "  Batch [250/1299] D_loss: -0.9739, G_loss: 1.9395\n",
      "  Batch [260/1299] D_loss: -3.0002, G_loss: 0.6268\n",
      "  Batch [270/1299] D_loss: -3.1761, G_loss: 1.3722\n",
      "  Batch [280/1299] D_loss: -2.7331, G_loss: 1.1878\n",
      "  Batch [290/1299] D_loss: -2.8645, G_loss: 1.0787\n",
      "  Batch [300/1299] D_loss: -2.8869, G_loss: 2.4776\n",
      "  Batch [310/1299] D_loss: -2.7328, G_loss: 2.1196\n",
      "  Batch [320/1299] D_loss: -4.0131, G_loss: 1.9245\n",
      "  Batch [330/1299] D_loss: -3.2392, G_loss: 3.2409\n",
      "  Batch [340/1299] D_loss: -2.6832, G_loss: 2.5663\n",
      "  Batch [350/1299] D_loss: -3.9446, G_loss: 4.6526\n",
      "  Batch [360/1299] D_loss: -2.2844, G_loss: 2.5186\n",
      "  Batch [370/1299] D_loss: -3.4335, G_loss: 3.1782\n",
      "  Batch [380/1299] D_loss: -1.1178, G_loss: 2.8501\n",
      "  Batch [390/1299] D_loss: -0.9122, G_loss: 3.4193\n",
      "  Batch [400/1299] D_loss: -1.6480, G_loss: 5.0207\n",
      "  Batch [410/1299] D_loss: -1.7862, G_loss: 4.4878\n",
      "  Batch [420/1299] D_loss: -2.6395, G_loss: 4.1578\n",
      "  Batch [430/1299] D_loss: -2.5739, G_loss: 3.2514\n",
      "  Batch [440/1299] D_loss: -1.5875, G_loss: 3.6055\n",
      "  Batch [450/1299] D_loss: -2.3762, G_loss: 1.9809\n",
      "  Batch [460/1299] D_loss: -2.1152, G_loss: 2.5250\n",
      "  Batch [470/1299] D_loss: -2.0036, G_loss: 0.5880\n",
      "  Batch [480/1299] D_loss: -0.1407, G_loss: 1.0972\n",
      "  Batch [490/1299] D_loss: -2.6546, G_loss: 2.1514\n",
      "  Batch [500/1299] D_loss: -1.2576, G_loss: 1.5991\n",
      "  Batch [510/1299] D_loss: -0.5895, G_loss: 2.0861\n",
      "  Batch [520/1299] D_loss: -3.7016, G_loss: 2.9564\n",
      "  Batch [530/1299] D_loss: -3.3551, G_loss: 0.7385\n",
      "  Batch [540/1299] D_loss: -1.9420, G_loss: 2.8933\n",
      "  Batch [550/1299] D_loss: -2.4906, G_loss: 0.1334\n",
      "  Batch [560/1299] D_loss: -2.9446, G_loss: 0.4221\n",
      "  Batch [570/1299] D_loss: -1.7202, G_loss: 0.2835\n",
      "  Batch [580/1299] D_loss: -2.3486, G_loss: 0.3535\n",
      "  Batch [590/1299] D_loss: -3.2415, G_loss: 0.2413\n",
      "  Batch [600/1299] D_loss: -2.2514, G_loss: -1.0558\n",
      "  Batch [610/1299] D_loss: -1.8262, G_loss: 0.1877\n",
      "  Batch [620/1299] D_loss: -2.3025, G_loss: -0.1442\n",
      "  Batch [630/1299] D_loss: -1.5512, G_loss: -0.5895\n",
      "  Batch [640/1299] D_loss: -1.4200, G_loss: 0.8524\n",
      "  Batch [650/1299] D_loss: -1.7040, G_loss: 1.8036\n",
      "  Batch [660/1299] D_loss: -2.9761, G_loss: 2.0405\n",
      "  Batch [670/1299] D_loss: -4.8832, G_loss: 1.2651\n",
      "  Batch [680/1299] D_loss: -2.3117, G_loss: 0.2820\n",
      "  Batch [690/1299] D_loss: -3.0220, G_loss: -0.6127\n",
      "  Batch [700/1299] D_loss: -2.5449, G_loss: 1.1380\n",
      "  Batch [710/1299] D_loss: -2.4635, G_loss: 1.1894\n",
      "  Batch [720/1299] D_loss: -1.6955, G_loss: -0.9330\n",
      "  Batch [730/1299] D_loss: -2.2540, G_loss: -1.1521\n",
      "  Batch [740/1299] D_loss: -2.1783, G_loss: -0.5751\n",
      "  Batch [750/1299] D_loss: -2.2186, G_loss: 0.4155\n",
      "  Batch [760/1299] D_loss: -3.1070, G_loss: 0.6824\n",
      "  Batch [770/1299] D_loss: -1.4204, G_loss: 0.6980\n",
      "  Batch [780/1299] D_loss: -0.8226, G_loss: 0.6359\n",
      "  Batch [790/1299] D_loss: -2.9619, G_loss: 2.1349\n",
      "  Batch [800/1299] D_loss: -1.8202, G_loss: 1.7999\n",
      "  Batch [810/1299] D_loss: -3.4638, G_loss: 1.4008\n",
      "  Batch [820/1299] D_loss: -2.6711, G_loss: 1.7490\n",
      "  Batch [830/1299] D_loss: -3.4303, G_loss: -1.2060\n",
      "  Batch [840/1299] D_loss: -1.0859, G_loss: 3.0152\n",
      "  Batch [850/1299] D_loss: -4.2560, G_loss: 2.0710\n",
      "  Batch [860/1299] D_loss: -0.9226, G_loss: 2.4359\n",
      "  Batch [870/1299] D_loss: -0.5710, G_loss: 2.9419\n",
      "  Batch [880/1299] D_loss: -0.6133, G_loss: 2.5288\n",
      "  Batch [890/1299] D_loss: -2.0047, G_loss: 2.5734\n",
      "  Batch [900/1299] D_loss: -2.5977, G_loss: 0.9416\n",
      "  Batch [910/1299] D_loss: -3.1776, G_loss: 0.7933\n",
      "  Batch [920/1299] D_loss: -3.3487, G_loss: 5.1328\n",
      "  Batch [930/1299] D_loss: -2.5059, G_loss: 3.4292\n",
      "  Batch [940/1299] D_loss: -3.6101, G_loss: 2.8180\n",
      "  Batch [950/1299] D_loss: -3.5731, G_loss: 0.7282\n",
      "  Batch [960/1299] D_loss: -2.3859, G_loss: 1.2462\n",
      "  Batch [970/1299] D_loss: -1.9025, G_loss: 2.6853\n",
      "  Batch [980/1299] D_loss: -2.3239, G_loss: -0.6878\n",
      "  Batch [990/1299] D_loss: -3.3072, G_loss: 2.3520\n",
      "  Batch [1000/1299] D_loss: -3.0676, G_loss: 3.0251\n",
      "  Batch [1010/1299] D_loss: -1.7291, G_loss: 3.8109\n",
      "  Batch [1020/1299] D_loss: -1.0282, G_loss: 1.9530\n",
      "  Batch [1030/1299] D_loss: -3.2153, G_loss: 2.3979\n",
      "  Batch [1040/1299] D_loss: -0.8124, G_loss: 2.9835\n",
      "  Batch [1050/1299] D_loss: -1.1315, G_loss: 2.8421\n",
      "  Batch [1060/1299] D_loss: -3.4504, G_loss: 1.4359\n",
      "  Batch [1070/1299] D_loss: -2.5139, G_loss: 0.3017\n",
      "  Batch [1080/1299] D_loss: -2.1142, G_loss: 1.6059\n",
      "  Batch [1090/1299] D_loss: -1.5019, G_loss: 2.9841\n",
      "  Batch [1100/1299] D_loss: -2.4476, G_loss: 2.8289\n",
      "  Batch [1110/1299] D_loss: -3.1683, G_loss: 3.6683\n",
      "  Batch [1120/1299] D_loss: -1.8862, G_loss: 3.2483\n",
      "  Batch [1130/1299] D_loss: -2.4528, G_loss: 5.1363\n",
      "  Batch [1140/1299] D_loss: -2.0808, G_loss: 4.9607\n",
      "  Batch [1150/1299] D_loss: -3.8262, G_loss: 3.6251\n",
      "  Batch [1160/1299] D_loss: -1.5632, G_loss: 3.5939\n",
      "  Batch [1170/1299] D_loss: -4.6911, G_loss: 6.0233\n",
      "  Batch [1180/1299] D_loss: -2.2962, G_loss: 4.0756\n",
      "  Batch [1190/1299] D_loss: -2.2921, G_loss: 3.6687\n",
      "  Batch [1200/1299] D_loss: -2.0243, G_loss: 3.4905\n",
      "  Batch [1210/1299] D_loss: -3.1304, G_loss: 3.9379\n",
      "  Batch [1220/1299] D_loss: -3.3346, G_loss: 2.9682\n",
      "  Batch [1230/1299] D_loss: -1.7396, G_loss: 1.6377\n",
      "  Batch [1240/1299] D_loss: -2.2774, G_loss: 2.8319\n",
      "  Batch [1250/1299] D_loss: -3.2270, G_loss: 4.0542\n",
      "  Batch [1260/1299] D_loss: -1.3500, G_loss: 2.1172\n",
      "  Batch [1270/1299] D_loss: -2.8277, G_loss: 2.1049\n",
      "  Batch [1280/1299] D_loss: -3.7207, G_loss: 4.2070\n",
      "  Batch [1290/1299] D_loss: -1.9462, G_loss: 1.2687\n",
      "\n",
      "Epoch 38 Summary:\n",
      "  Average D_loss: -2.2539\n",
      "  Average G_loss: 2.0566\n",
      "\n",
      "Epoch [39/100]\n",
      "  Batch [0/1299] D_loss: -2.2703, G_loss: 1.9331\n",
      "  Batch [10/1299] D_loss: -2.9040, G_loss: 1.1588\n",
      "  Batch [20/1299] D_loss: -3.4729, G_loss: 3.4405\n",
      "  Batch [30/1299] D_loss: -3.7721, G_loss: 2.5572\n",
      "  Batch [40/1299] D_loss: -2.4212, G_loss: 3.6743\n",
      "  Batch [50/1299] D_loss: -1.8977, G_loss: 3.2364\n",
      "  Batch [60/1299] D_loss: -1.9191, G_loss: 5.2614\n",
      "  Batch [70/1299] D_loss: -4.2337, G_loss: 5.6062\n",
      "  Batch [80/1299] D_loss: -1.9826, G_loss: 2.9515\n",
      "  Batch [90/1299] D_loss: -1.3395, G_loss: 4.0008\n",
      "  Batch [100/1299] D_loss: -2.0222, G_loss: 5.2432\n",
      "  Batch [110/1299] D_loss: -2.4649, G_loss: 6.7306\n",
      "  Batch [120/1299] D_loss: -2.3210, G_loss: 6.0293\n",
      "  Batch [130/1299] D_loss: -4.3187, G_loss: 5.1351\n",
      "  Batch [140/1299] D_loss: -2.0698, G_loss: 2.0581\n",
      "  Batch [150/1299] D_loss: -3.1633, G_loss: 2.8518\n",
      "  Batch [160/1299] D_loss: -2.1093, G_loss: 1.1450\n",
      "  Batch [170/1299] D_loss: -3.8770, G_loss: 2.2341\n",
      "  Batch [180/1299] D_loss: -1.7859, G_loss: 1.5504\n",
      "  Batch [190/1299] D_loss: -1.9373, G_loss: 2.7287\n",
      "  Batch [200/1299] D_loss: -1.7510, G_loss: 2.9887\n",
      "  Batch [210/1299] D_loss: -2.6391, G_loss: 1.8964\n",
      "  Batch [220/1299] D_loss: -2.4390, G_loss: 2.3504\n",
      "  Batch [230/1299] D_loss: -0.8548, G_loss: 2.3092\n",
      "  Batch [240/1299] D_loss: -1.3473, G_loss: 3.4960\n",
      "  Batch [250/1299] D_loss: 0.0952, G_loss: 2.7529\n",
      "  Batch [260/1299] D_loss: -2.0497, G_loss: 1.7168\n",
      "  Batch [270/1299] D_loss: -1.9896, G_loss: 1.7588\n",
      "  Batch [280/1299] D_loss: -2.0812, G_loss: 2.5104\n",
      "  Batch [290/1299] D_loss: -0.5858, G_loss: 2.7288\n",
      "  Batch [300/1299] D_loss: -3.2185, G_loss: 1.4241\n",
      "  Batch [310/1299] D_loss: -3.1981, G_loss: 1.7497\n",
      "  Batch [320/1299] D_loss: -2.7336, G_loss: 3.4778\n",
      "  Batch [330/1299] D_loss: -0.4345, G_loss: 2.8209\n",
      "  Batch [340/1299] D_loss: -1.5262, G_loss: 2.3014\n",
      "  Batch [350/1299] D_loss: -2.1643, G_loss: 2.3644\n",
      "  Batch [360/1299] D_loss: -1.8415, G_loss: 2.4291\n",
      "  Batch [370/1299] D_loss: -3.8943, G_loss: 1.3659\n",
      "  Batch [380/1299] D_loss: -3.3424, G_loss: 2.2901\n",
      "  Batch [390/1299] D_loss: -4.6322, G_loss: 0.7203\n",
      "  Batch [400/1299] D_loss: -2.0459, G_loss: 2.3199\n",
      "  Batch [410/1299] D_loss: -1.7445, G_loss: 1.9140\n",
      "  Batch [420/1299] D_loss: -3.1652, G_loss: 1.8162\n",
      "  Batch [430/1299] D_loss: -2.8342, G_loss: 2.0456\n",
      "  Batch [440/1299] D_loss: -0.5783, G_loss: 1.3912\n",
      "  Batch [450/1299] D_loss: -3.8521, G_loss: 2.0411\n",
      "  Batch [460/1299] D_loss: -3.1300, G_loss: 0.1802\n",
      "  Batch [470/1299] D_loss: -0.4141, G_loss: 1.8949\n",
      "  Batch [480/1299] D_loss: -1.9676, G_loss: 1.9549\n",
      "  Batch [490/1299] D_loss: -2.2100, G_loss: 4.5460\n",
      "  Batch [500/1299] D_loss: -2.8864, G_loss: 3.4635\n",
      "  Batch [510/1299] D_loss: -2.4858, G_loss: 4.4169\n",
      "  Batch [520/1299] D_loss: -2.4180, G_loss: 4.3737\n",
      "  Batch [530/1299] D_loss: -1.6911, G_loss: 5.2826\n",
      "  Batch [540/1299] D_loss: -3.8876, G_loss: 4.9386\n",
      "  Batch [550/1299] D_loss: -2.2535, G_loss: 4.1918\n",
      "  Batch [560/1299] D_loss: -2.9611, G_loss: 3.0770\n",
      "  Batch [570/1299] D_loss: -2.1276, G_loss: 1.6080\n",
      "  Batch [580/1299] D_loss: -3.5986, G_loss: 2.9574\n",
      "  Batch [590/1299] D_loss: -2.3107, G_loss: 0.0388\n",
      "  Batch [600/1299] D_loss: -2.9378, G_loss: 5.2634\n",
      "  Batch [610/1299] D_loss: -1.3547, G_loss: 3.1014\n",
      "  Batch [620/1299] D_loss: -3.5146, G_loss: 2.3652\n",
      "  Batch [630/1299] D_loss: -2.5271, G_loss: 2.6471\n",
      "  Batch [640/1299] D_loss: -1.9327, G_loss: 4.4789\n",
      "  Batch [650/1299] D_loss: -2.8659, G_loss: 5.3800\n",
      "  Batch [660/1299] D_loss: -2.7293, G_loss: 1.7081\n",
      "  Batch [670/1299] D_loss: -1.5137, G_loss: 1.8839\n",
      "  Batch [680/1299] D_loss: -3.3496, G_loss: 5.5085\n",
      "  Batch [690/1299] D_loss: -4.8456, G_loss: 5.8616\n",
      "  Batch [700/1299] D_loss: -5.2259, G_loss: 4.0123\n",
      "  Batch [710/1299] D_loss: -1.6280, G_loss: 6.4450\n",
      "  Batch [720/1299] D_loss: -4.2751, G_loss: 2.5939\n",
      "  Batch [730/1299] D_loss: -0.8561, G_loss: 5.0921\n",
      "  Batch [740/1299] D_loss: -2.0716, G_loss: 2.9287\n",
      "  Batch [750/1299] D_loss: -3.0750, G_loss: 3.4252\n",
      "  Batch [760/1299] D_loss: -3.9352, G_loss: 2.4074\n",
      "  Batch [770/1299] D_loss: -1.9289, G_loss: 5.1330\n",
      "  Batch [780/1299] D_loss: -1.1962, G_loss: 3.1885\n",
      "  Batch [790/1299] D_loss: -2.8485, G_loss: 3.0081\n",
      "  Batch [800/1299] D_loss: -3.2641, G_loss: 2.9451\n",
      "  Batch [810/1299] D_loss: -2.4584, G_loss: 3.2851\n",
      "  Batch [820/1299] D_loss: -4.0743, G_loss: 0.6749\n",
      "  Batch [830/1299] D_loss: -3.6721, G_loss: 2.5202\n",
      "  Batch [840/1299] D_loss: -1.8137, G_loss: 3.2581\n",
      "  Batch [850/1299] D_loss: -2.8441, G_loss: 4.6839\n",
      "  Batch [860/1299] D_loss: -2.5352, G_loss: 3.3278\n",
      "  Batch [870/1299] D_loss: -4.3723, G_loss: 4.0066\n",
      "  Batch [880/1299] D_loss: -2.7998, G_loss: 1.3509\n",
      "  Batch [890/1299] D_loss: -2.0583, G_loss: 2.2760\n",
      "  Batch [900/1299] D_loss: 0.0412, G_loss: 4.4363\n",
      "  Batch [910/1299] D_loss: -3.7063, G_loss: 3.8686\n",
      "  Batch [920/1299] D_loss: -2.4993, G_loss: 2.7547\n",
      "  Batch [930/1299] D_loss: -3.3997, G_loss: 1.5397\n",
      "  Batch [940/1299] D_loss: -3.1245, G_loss: 4.5637\n",
      "  Batch [950/1299] D_loss: -1.6171, G_loss: 4.3555\n",
      "  Batch [960/1299] D_loss: -2.4288, G_loss: 4.1720\n",
      "  Batch [970/1299] D_loss: -2.8648, G_loss: 3.6500\n",
      "  Batch [980/1299] D_loss: -2.3803, G_loss: 4.1339\n",
      "  Batch [990/1299] D_loss: -2.6308, G_loss: 3.2928\n",
      "  Batch [1000/1299] D_loss: 0.7614, G_loss: 3.7745\n",
      "  Batch [1010/1299] D_loss: -4.2058, G_loss: 3.7415\n",
      "  Batch [1020/1299] D_loss: 0.3203, G_loss: 3.1502\n",
      "  Batch [1030/1299] D_loss: -1.1840, G_loss: 1.3424\n",
      "  Batch [1040/1299] D_loss: -2.1981, G_loss: 3.1168\n",
      "  Batch [1050/1299] D_loss: -3.1323, G_loss: 3.8162\n",
      "  Batch [1060/1299] D_loss: -2.9378, G_loss: 1.4519\n",
      "  Batch [1070/1299] D_loss: -4.0703, G_loss: 2.8458\n",
      "  Batch [1080/1299] D_loss: -2.9414, G_loss: 2.0198\n",
      "  Batch [1090/1299] D_loss: -2.6983, G_loss: 2.6714\n",
      "  Batch [1100/1299] D_loss: -1.2320, G_loss: 1.2874\n",
      "  Batch [1110/1299] D_loss: -3.4314, G_loss: 0.5813\n",
      "  Batch [1120/1299] D_loss: -2.7806, G_loss: 1.5437\n",
      "  Batch [1130/1299] D_loss: -3.2694, G_loss: 0.6497\n",
      "  Batch [1140/1299] D_loss: -2.8932, G_loss: 0.8869\n",
      "  Batch [1150/1299] D_loss: 0.1195, G_loss: 0.0540\n",
      "  Batch [1160/1299] D_loss: -0.9254, G_loss: 1.2029\n",
      "  Batch [1170/1299] D_loss: -2.5473, G_loss: 1.9340\n",
      "  Batch [1180/1299] D_loss: -4.3584, G_loss: 1.0732\n",
      "  Batch [1190/1299] D_loss: -1.5697, G_loss: 1.9201\n",
      "  Batch [1200/1299] D_loss: -2.1346, G_loss: 2.4738\n",
      "  Batch [1210/1299] D_loss: -1.6334, G_loss: 2.7169\n",
      "  Batch [1220/1299] D_loss: -3.0535, G_loss: 0.9255\n",
      "  Batch [1230/1299] D_loss: -1.9029, G_loss: 3.4480\n",
      "  Batch [1240/1299] D_loss: -3.1712, G_loss: -0.0063\n",
      "  Batch [1250/1299] D_loss: -2.3486, G_loss: 2.4020\n",
      "  Batch [1260/1299] D_loss: -2.6050, G_loss: 2.0795\n",
      "  Batch [1270/1299] D_loss: -2.9590, G_loss: -0.6513\n",
      "  Batch [1280/1299] D_loss: -2.7964, G_loss: 0.2052\n",
      "  Batch [1290/1299] D_loss: -1.9955, G_loss: 3.2747\n",
      "\n",
      "Epoch 39 Summary:\n",
      "  Average D_loss: -2.2708\n",
      "  Average G_loss: 2.7228\n",
      "\n",
      "Epoch [40/100]\n",
      "  Batch [0/1299] D_loss: -1.8787, G_loss: 1.3831\n",
      "  Batch [10/1299] D_loss: -1.9000, G_loss: 0.7450\n",
      "  Batch [20/1299] D_loss: -0.2272, G_loss: 0.3462\n",
      "  Batch [30/1299] D_loss: -2.4102, G_loss: -0.3342\n",
      "  Batch [40/1299] D_loss: -2.2262, G_loss: 3.8700\n",
      "  Batch [50/1299] D_loss: -2.4792, G_loss: 0.1716\n",
      "  Batch [60/1299] D_loss: 0.3792, G_loss: 2.3971\n",
      "  Batch [70/1299] D_loss: -3.3659, G_loss: 3.6071\n",
      "  Batch [80/1299] D_loss: -0.7412, G_loss: 1.1625\n",
      "  Batch [90/1299] D_loss: -3.4497, G_loss: 5.4340\n",
      "  Batch [100/1299] D_loss: -3.9554, G_loss: 3.5854\n",
      "  Batch [110/1299] D_loss: -3.0447, G_loss: 2.2518\n",
      "  Batch [120/1299] D_loss: -2.3847, G_loss: 2.4276\n",
      "  Batch [130/1299] D_loss: -2.2019, G_loss: 2.0881\n",
      "  Batch [140/1299] D_loss: -3.3244, G_loss: 2.7055\n",
      "  Batch [150/1299] D_loss: -1.9045, G_loss: -0.4162\n",
      "  Batch [160/1299] D_loss: -2.3061, G_loss: 1.3535\n",
      "  Batch [170/1299] D_loss: -2.3244, G_loss: -0.2253\n",
      "  Batch [180/1299] D_loss: -2.4771, G_loss: 1.6312\n",
      "  Batch [190/1299] D_loss: -1.4099, G_loss: 1.1041\n",
      "  Batch [200/1299] D_loss: -4.0523, G_loss: 4.0812\n",
      "  Batch [210/1299] D_loss: -1.9859, G_loss: 1.8596\n",
      "  Batch [220/1299] D_loss: -3.0108, G_loss: 2.2741\n",
      "  Batch [230/1299] D_loss: -2.1186, G_loss: 1.2664\n",
      "  Batch [240/1299] D_loss: -1.8891, G_loss: 2.4171\n",
      "  Batch [250/1299] D_loss: -1.4318, G_loss: 2.6637\n",
      "  Batch [260/1299] D_loss: -3.5286, G_loss: 3.4054\n",
      "  Batch [270/1299] D_loss: -4.0239, G_loss: 3.1598\n",
      "  Batch [280/1299] D_loss: -3.3662, G_loss: 2.6542\n",
      "  Batch [290/1299] D_loss: -2.8695, G_loss: 3.8628\n",
      "  Batch [300/1299] D_loss: -1.2343, G_loss: 5.4792\n",
      "  Batch [310/1299] D_loss: -2.2239, G_loss: 3.2584\n",
      "  Batch [320/1299] D_loss: -4.3783, G_loss: 3.3167\n",
      "  Batch [330/1299] D_loss: -2.3293, G_loss: 4.7123\n",
      "  Batch [340/1299] D_loss: -0.5628, G_loss: 5.5267\n",
      "  Batch [350/1299] D_loss: -3.1618, G_loss: 4.3768\n",
      "  Batch [360/1299] D_loss: -2.2522, G_loss: 3.5484\n",
      "  Batch [370/1299] D_loss: -1.4075, G_loss: 5.2029\n",
      "  Batch [380/1299] D_loss: -3.1328, G_loss: 2.7709\n",
      "  Batch [390/1299] D_loss: -4.7133, G_loss: 1.7160\n",
      "  Batch [400/1299] D_loss: -3.0557, G_loss: 2.6518\n",
      "  Batch [410/1299] D_loss: -1.9573, G_loss: 1.7275\n",
      "  Batch [420/1299] D_loss: -2.3284, G_loss: 0.7153\n",
      "  Batch [430/1299] D_loss: -1.7238, G_loss: 1.3702\n",
      "  Batch [440/1299] D_loss: -1.1072, G_loss: 1.8439\n",
      "  Batch [450/1299] D_loss: -3.7053, G_loss: 2.6189\n",
      "  Batch [460/1299] D_loss: -1.9248, G_loss: 0.5832\n",
      "  Batch [470/1299] D_loss: -2.3968, G_loss: 1.6477\n",
      "  Batch [480/1299] D_loss: -2.1684, G_loss: 2.0704\n",
      "  Batch [490/1299] D_loss: -2.4517, G_loss: 2.3339\n",
      "  Batch [500/1299] D_loss: -2.7938, G_loss: 2.7870\n",
      "  Batch [510/1299] D_loss: -2.1796, G_loss: 1.3230\n",
      "  Batch [520/1299] D_loss: -3.4204, G_loss: 1.0388\n",
      "  Batch [530/1299] D_loss: -2.9332, G_loss: 0.2617\n",
      "  Batch [540/1299] D_loss: -1.4721, G_loss: 2.5208\n",
      "  Batch [550/1299] D_loss: -2.1108, G_loss: 1.9164\n",
      "  Batch [560/1299] D_loss: -3.0396, G_loss: 1.7624\n",
      "  Batch [570/1299] D_loss: -3.5163, G_loss: 2.3377\n",
      "  Batch [580/1299] D_loss: -3.2167, G_loss: 2.7383\n",
      "  Batch [590/1299] D_loss: -2.0103, G_loss: 5.8663\n",
      "  Batch [600/1299] D_loss: -1.8697, G_loss: 6.2791\n",
      "  Batch [610/1299] D_loss: -3.7347, G_loss: 5.7906\n",
      "  Batch [620/1299] D_loss: -2.4120, G_loss: 6.0183\n",
      "  Batch [630/1299] D_loss: -3.3491, G_loss: 5.2916\n",
      "  Batch [640/1299] D_loss: -1.2088, G_loss: 4.0736\n",
      "  Batch [650/1299] D_loss: -1.9614, G_loss: 4.3526\n",
      "  Batch [660/1299] D_loss: -2.5334, G_loss: 4.7487\n",
      "  Batch [670/1299] D_loss: -3.5957, G_loss: 2.0211\n",
      "  Batch [680/1299] D_loss: -1.7083, G_loss: 3.9346\n",
      "  Batch [690/1299] D_loss: -1.4373, G_loss: 3.4669\n",
      "  Batch [700/1299] D_loss: -3.6233, G_loss: 4.1317\n",
      "  Batch [710/1299] D_loss: -3.9318, G_loss: 3.4665\n",
      "  Batch [720/1299] D_loss: -2.5524, G_loss: 3.8034\n",
      "  Batch [730/1299] D_loss: -3.3363, G_loss: 5.3348\n",
      "  Batch [740/1299] D_loss: -3.1237, G_loss: 5.3371\n",
      "  Batch [750/1299] D_loss: -0.3347, G_loss: 5.6775\n",
      "  Batch [760/1299] D_loss: -2.7611, G_loss: 4.1054\n",
      "  Batch [770/1299] D_loss: -3.6565, G_loss: 3.3745\n",
      "  Batch [780/1299] D_loss: -3.2935, G_loss: 4.3756\n",
      "  Batch [790/1299] D_loss: -2.4419, G_loss: 4.1620\n",
      "  Batch [800/1299] D_loss: -4.1539, G_loss: 2.8625\n",
      "  Batch [810/1299] D_loss: -2.3279, G_loss: 3.3365\n",
      "  Batch [820/1299] D_loss: -2.5054, G_loss: 5.3862\n",
      "  Batch [830/1299] D_loss: -2.0907, G_loss: 3.8824\n",
      "  Batch [840/1299] D_loss: -3.1657, G_loss: 3.7143\n",
      "  Batch [850/1299] D_loss: -3.1113, G_loss: 2.4685\n",
      "  Batch [860/1299] D_loss: -1.6150, G_loss: 0.7918\n",
      "  Batch [870/1299] D_loss: -2.2294, G_loss: 1.5169\n",
      "  Batch [880/1299] D_loss: -1.5316, G_loss: 1.2691\n",
      "  Batch [890/1299] D_loss: 0.0102, G_loss: 1.5560\n",
      "  Batch [900/1299] D_loss: -2.6184, G_loss: 3.6065\n",
      "  Batch [910/1299] D_loss: -1.8182, G_loss: 1.4760\n",
      "  Batch [920/1299] D_loss: -2.9447, G_loss: 3.0594\n",
      "  Batch [930/1299] D_loss: -4.0397, G_loss: 2.7846\n",
      "  Batch [940/1299] D_loss: -1.8592, G_loss: 2.5134\n",
      "  Batch [950/1299] D_loss: -3.4276, G_loss: 3.0163\n",
      "  Batch [960/1299] D_loss: -1.0772, G_loss: 0.8290\n",
      "  Batch [970/1299] D_loss: -1.3096, G_loss: 2.3577\n",
      "  Batch [980/1299] D_loss: -2.3784, G_loss: 3.0988\n",
      "  Batch [990/1299] D_loss: -4.0254, G_loss: 3.9735\n",
      "  Batch [1000/1299] D_loss: -3.3549, G_loss: 1.8385\n",
      "  Batch [1010/1299] D_loss: -3.3407, G_loss: 4.0350\n",
      "  Batch [1020/1299] D_loss: -3.4063, G_loss: 2.4103\n",
      "  Batch [1030/1299] D_loss: -1.2689, G_loss: 3.0862\n",
      "  Batch [1040/1299] D_loss: -3.1471, G_loss: 4.4965\n",
      "  Batch [1050/1299] D_loss: -1.9305, G_loss: 1.2980\n",
      "  Batch [1060/1299] D_loss: -2.0812, G_loss: 1.2868\n",
      "  Batch [1070/1299] D_loss: -1.3406, G_loss: 0.1992\n",
      "  Batch [1080/1299] D_loss: -3.1967, G_loss: 1.2431\n",
      "  Batch [1090/1299] D_loss: -2.0327, G_loss: 3.2336\n",
      "  Batch [1100/1299] D_loss: -0.9010, G_loss: 3.1461\n",
      "  Batch [1110/1299] D_loss: -1.5681, G_loss: 2.8765\n",
      "  Batch [1120/1299] D_loss: -1.4834, G_loss: 4.8634\n",
      "  Batch [1130/1299] D_loss: -1.8921, G_loss: 4.2171\n",
      "  Batch [1140/1299] D_loss: -3.9996, G_loss: 3.7026\n",
      "  Batch [1150/1299] D_loss: -1.1920, G_loss: 2.0686\n",
      "  Batch [1160/1299] D_loss: -3.0272, G_loss: 1.5659\n",
      "  Batch [1170/1299] D_loss: -3.1238, G_loss: 2.4198\n",
      "  Batch [1180/1299] D_loss: -4.4074, G_loss: 2.3900\n",
      "  Batch [1190/1299] D_loss: -3.1191, G_loss: -0.5395\n",
      "  Batch [1200/1299] D_loss: -1.2794, G_loss: 2.4900\n",
      "  Batch [1210/1299] D_loss: -2.3764, G_loss: 1.6478\n",
      "  Batch [1220/1299] D_loss: -3.3546, G_loss: 2.7878\n",
      "  Batch [1230/1299] D_loss: -3.8755, G_loss: 1.8715\n",
      "  Batch [1240/1299] D_loss: -1.0523, G_loss: 2.9840\n",
      "  Batch [1250/1299] D_loss: -0.9054, G_loss: 3.4514\n",
      "  Batch [1260/1299] D_loss: -3.7614, G_loss: 3.4449\n",
      "  Batch [1270/1299] D_loss: -3.1028, G_loss: 0.3368\n",
      "  Batch [1280/1299] D_loss: -1.9812, G_loss: 0.8081\n",
      "  Batch [1290/1299] D_loss: -2.5144, G_loss: 0.0606\n",
      "\n",
      "Epoch 40 Summary:\n",
      "  Average D_loss: -2.2456\n",
      "  Average G_loss: 2.6617\n",
      "\n",
      "Epoch [41/100]\n",
      "  Batch [0/1299] D_loss: -1.3187, G_loss: 1.4955\n",
      "  Batch [10/1299] D_loss: -2.1141, G_loss: 4.7939\n",
      "  Batch [20/1299] D_loss: -4.9624, G_loss: 1.1699\n",
      "  Batch [30/1299] D_loss: -2.5639, G_loss: 0.5976\n",
      "  Batch [40/1299] D_loss: -1.4427, G_loss: 2.6899\n",
      "  Batch [50/1299] D_loss: -2.5831, G_loss: 1.2856\n",
      "  Batch [60/1299] D_loss: -4.0221, G_loss: 0.7129\n",
      "  Batch [70/1299] D_loss: -0.8724, G_loss: 2.2997\n",
      "  Batch [80/1299] D_loss: -1.5199, G_loss: -2.0942\n",
      "  Batch [90/1299] D_loss: -2.7957, G_loss: 1.3483\n",
      "  Batch [100/1299] D_loss: -2.6563, G_loss: 2.3328\n",
      "  Batch [110/1299] D_loss: -4.1488, G_loss: 5.0302\n",
      "  Batch [120/1299] D_loss: -3.6244, G_loss: 0.4778\n",
      "  Batch [130/1299] D_loss: -0.7146, G_loss: 4.8536\n",
      "  Batch [140/1299] D_loss: -4.7805, G_loss: 3.5567\n",
      "  Batch [150/1299] D_loss: -2.9556, G_loss: 5.2751\n",
      "  Batch [160/1299] D_loss: -4.3341, G_loss: -0.3287\n",
      "  Batch [170/1299] D_loss: -1.8683, G_loss: 4.0211\n",
      "  Batch [180/1299] D_loss: -4.5266, G_loss: 1.2435\n",
      "  Batch [190/1299] D_loss: -0.4366, G_loss: 3.1840\n",
      "  Batch [200/1299] D_loss: -6.1590, G_loss: 3.8315\n",
      "  Batch [210/1299] D_loss: -1.6765, G_loss: 2.4410\n",
      "  Batch [220/1299] D_loss: -1.9830, G_loss: 2.8867\n",
      "  Batch [230/1299] D_loss: 0.2653, G_loss: 1.2053\n",
      "  Batch [240/1299] D_loss: -1.0889, G_loss: 0.9481\n",
      "  Batch [250/1299] D_loss: -4.1924, G_loss: 2.4932\n",
      "  Batch [260/1299] D_loss: -1.8666, G_loss: 0.7340\n",
      "  Batch [270/1299] D_loss: -2.6074, G_loss: 2.0278\n",
      "  Batch [280/1299] D_loss: -1.3525, G_loss: 2.5330\n",
      "  Batch [290/1299] D_loss: -1.3191, G_loss: 2.0779\n",
      "  Batch [300/1299] D_loss: -1.3195, G_loss: 2.6293\n",
      "  Batch [310/1299] D_loss: -1.6739, G_loss: 3.7489\n",
      "  Batch [320/1299] D_loss: -2.1776, G_loss: 0.7034\n",
      "  Batch [330/1299] D_loss: -2.3216, G_loss: 3.1667\n",
      "  Batch [340/1299] D_loss: -1.4929, G_loss: 3.4617\n",
      "  Batch [350/1299] D_loss: -6.3037, G_loss: 3.4243\n",
      "  Batch [360/1299] D_loss: -4.6797, G_loss: 1.3573\n",
      "  Batch [370/1299] D_loss: -1.9688, G_loss: 0.9554\n",
      "  Batch [380/1299] D_loss: 0.5136, G_loss: 3.0726\n",
      "  Batch [390/1299] D_loss: -0.9202, G_loss: 5.7440\n",
      "  Batch [400/1299] D_loss: -2.8235, G_loss: 2.5868\n",
      "  Batch [410/1299] D_loss: -0.3356, G_loss: 1.6064\n",
      "  Batch [420/1299] D_loss: -4.9671, G_loss: -0.0981\n",
      "  Batch [430/1299] D_loss: -3.4473, G_loss: 1.9708\n",
      "  Batch [440/1299] D_loss: -2.0372, G_loss: 0.3821\n",
      "  Batch [450/1299] D_loss: -1.4043, G_loss: 2.9854\n",
      "  Batch [460/1299] D_loss: -3.7605, G_loss: 3.4541\n",
      "  Batch [470/1299] D_loss: -2.6007, G_loss: 4.0691\n",
      "  Batch [480/1299] D_loss: -3.7868, G_loss: 2.1142\n",
      "  Batch [490/1299] D_loss: -1.7459, G_loss: 0.2275\n",
      "  Batch [500/1299] D_loss: -1.3352, G_loss: 1.5376\n",
      "  Batch [510/1299] D_loss: -1.3462, G_loss: 4.5438\n",
      "  Batch [520/1299] D_loss: -1.1285, G_loss: 1.9414\n",
      "  Batch [530/1299] D_loss: -3.9540, G_loss: 4.2437\n",
      "  Batch [540/1299] D_loss: -2.0782, G_loss: 4.3482\n",
      "  Batch [550/1299] D_loss: -1.3286, G_loss: 3.3392\n",
      "  Batch [560/1299] D_loss: -5.0368, G_loss: 2.0271\n",
      "  Batch [570/1299] D_loss: -0.2657, G_loss: -0.8640\n",
      "  Batch [580/1299] D_loss: -3.3717, G_loss: 3.6197\n",
      "  Batch [590/1299] D_loss: -2.3751, G_loss: 5.2341\n",
      "  Batch [600/1299] D_loss: -1.1737, G_loss: 4.3752\n",
      "  Batch [610/1299] D_loss: -4.3884, G_loss: 1.4798\n",
      "  Batch [620/1299] D_loss: -1.7296, G_loss: 4.0489\n",
      "  Batch [630/1299] D_loss: -3.9541, G_loss: 2.1038\n",
      "  Batch [640/1299] D_loss: -1.7105, G_loss: 4.8363\n",
      "  Batch [650/1299] D_loss: -1.5768, G_loss: 3.2784\n",
      "  Batch [660/1299] D_loss: 0.5443, G_loss: 4.0464\n",
      "  Batch [670/1299] D_loss: -3.1375, G_loss: 3.8836\n",
      "  Batch [680/1299] D_loss: -4.8217, G_loss: 2.6008\n",
      "  Batch [690/1299] D_loss: -1.7585, G_loss: 2.6359\n",
      "  Batch [700/1299] D_loss: -2.9307, G_loss: 3.9710\n",
      "  Batch [710/1299] D_loss: -2.7673, G_loss: 2.3200\n",
      "  Batch [720/1299] D_loss: -0.2949, G_loss: 3.6174\n",
      "  Batch [730/1299] D_loss: -0.2734, G_loss: 4.6773\n",
      "  Batch [740/1299] D_loss: -2.8102, G_loss: 4.5314\n",
      "  Batch [750/1299] D_loss: -1.0511, G_loss: 6.2344\n",
      "  Batch [760/1299] D_loss: -0.5598, G_loss: 7.9513\n",
      "  Batch [770/1299] D_loss: -2.8675, G_loss: 6.4794\n",
      "  Batch [780/1299] D_loss: -2.7143, G_loss: 3.8119\n",
      "  Batch [790/1299] D_loss: -4.1512, G_loss: 1.7031\n",
      "  Batch [800/1299] D_loss: -1.2567, G_loss: 5.0347\n",
      "  Batch [810/1299] D_loss: -1.6204, G_loss: 1.1638\n",
      "  Batch [820/1299] D_loss: -3.1365, G_loss: 1.4915\n",
      "  Batch [830/1299] D_loss: -0.9630, G_loss: 2.2949\n",
      "  Batch [840/1299] D_loss: -0.0741, G_loss: 2.8637\n",
      "  Batch [850/1299] D_loss: -2.9963, G_loss: 0.8747\n",
      "  Batch [860/1299] D_loss: -1.7433, G_loss: -0.7125\n",
      "  Batch [870/1299] D_loss: -3.7905, G_loss: 2.9697\n",
      "  Batch [880/1299] D_loss: -2.6438, G_loss: 2.5883\n",
      "  Batch [890/1299] D_loss: -2.2420, G_loss: 3.1193\n",
      "  Batch [900/1299] D_loss: -4.3943, G_loss: 1.3319\n",
      "  Batch [910/1299] D_loss: -3.8542, G_loss: 2.1814\n",
      "  Batch [920/1299] D_loss: -2.8738, G_loss: 1.5338\n",
      "  Batch [930/1299] D_loss: -3.0511, G_loss: 4.4686\n",
      "  Batch [940/1299] D_loss: -1.1800, G_loss: 3.8843\n",
      "  Batch [950/1299] D_loss: -3.5654, G_loss: 3.9452\n",
      "  Batch [960/1299] D_loss: -2.3720, G_loss: 0.7547\n",
      "  Batch [970/1299] D_loss: -2.7220, G_loss: 0.9826\n",
      "  Batch [980/1299] D_loss: -1.9823, G_loss: 1.6016\n",
      "  Batch [990/1299] D_loss: -3.8050, G_loss: 1.1450\n",
      "  Batch [1000/1299] D_loss: -0.5156, G_loss: 0.8250\n",
      "  Batch [1010/1299] D_loss: 0.1294, G_loss: 1.5505\n",
      "  Batch [1020/1299] D_loss: -1.0019, G_loss: 0.0020\n",
      "  Batch [1030/1299] D_loss: -2.5787, G_loss: 1.2748\n",
      "  Batch [1040/1299] D_loss: -2.7631, G_loss: 0.9509\n",
      "  Batch [1050/1299] D_loss: -2.5061, G_loss: -1.0613\n",
      "  Batch [1060/1299] D_loss: -3.9367, G_loss: -1.2112\n",
      "  Batch [1070/1299] D_loss: -2.8167, G_loss: 0.9706\n",
      "  Batch [1080/1299] D_loss: -2.0687, G_loss: -0.6389\n",
      "  Batch [1090/1299] D_loss: -1.3700, G_loss: 0.5411\n",
      "  Batch [1100/1299] D_loss: -2.8100, G_loss: 1.5597\n",
      "  Batch [1110/1299] D_loss: -5.2921, G_loss: 2.6803\n",
      "  Batch [1120/1299] D_loss: -3.9589, G_loss: 4.6891\n",
      "  Batch [1130/1299] D_loss: -0.4959, G_loss: 1.5451\n",
      "  Batch [1140/1299] D_loss: -4.0200, G_loss: 3.9958\n",
      "  Batch [1150/1299] D_loss: -3.9349, G_loss: 2.5387\n",
      "  Batch [1160/1299] D_loss: -3.0980, G_loss: 0.2236\n",
      "  Batch [1170/1299] D_loss: -2.6698, G_loss: 2.4655\n",
      "  Batch [1180/1299] D_loss: -4.3151, G_loss: 2.8317\n",
      "  Batch [1190/1299] D_loss: -0.6805, G_loss: 2.9555\n",
      "  Batch [1200/1299] D_loss: -1.6016, G_loss: 0.7147\n",
      "  Batch [1210/1299] D_loss: -1.8689, G_loss: 1.4029\n",
      "  Batch [1220/1299] D_loss: -2.5423, G_loss: 1.1788\n",
      "  Batch [1230/1299] D_loss: -2.5146, G_loss: 1.3932\n",
      "  Batch [1240/1299] D_loss: -3.1819, G_loss: 1.6626\n",
      "  Batch [1250/1299] D_loss: -1.4733, G_loss: -0.2713\n",
      "  Batch [1260/1299] D_loss: -2.6135, G_loss: -1.0321\n",
      "  Batch [1270/1299] D_loss: -4.4330, G_loss: -0.7823\n",
      "  Batch [1280/1299] D_loss: -2.3640, G_loss: 1.4902\n",
      "  Batch [1290/1299] D_loss: -0.5860, G_loss: 4.6987\n",
      "\n",
      "Epoch 41 Summary:\n",
      "  Average D_loss: -2.2064\n",
      "  Average G_loss: 2.2295\n",
      "\n",
      "Models saved at epoch 41:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_144239_dataset+cell_type/generator_20250121_144239_dataset+cell_type_epoch_41.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_144239_dataset+cell_type/discriminator_20250121_144239_dataset+cell_type_epoch_41.pt\n",
      "\n",
      "Epoch [42/100]\n",
      "  Batch [0/1299] D_loss: -2.2967, G_loss: 3.5291\n",
      "  Batch [10/1299] D_loss: -0.8968, G_loss: 4.5945\n",
      "  Batch [20/1299] D_loss: -3.0012, G_loss: 3.1499\n",
      "  Batch [30/1299] D_loss: -1.5957, G_loss: 4.5409\n",
      "  Batch [40/1299] D_loss: -2.1890, G_loss: 3.6426\n",
      "  Batch [50/1299] D_loss: -1.9453, G_loss: 2.2697\n",
      "  Batch [60/1299] D_loss: -2.6431, G_loss: 2.0822\n",
      "  Batch [70/1299] D_loss: -1.4773, G_loss: 1.6075\n",
      "  Batch [80/1299] D_loss: -2.6339, G_loss: 2.8846\n",
      "  Batch [90/1299] D_loss: -0.9315, G_loss: 0.1500\n",
      "  Batch [100/1299] D_loss: -2.9026, G_loss: 0.4268\n",
      "  Batch [110/1299] D_loss: -3.6744, G_loss: 2.2839\n",
      "  Batch [120/1299] D_loss: -3.0767, G_loss: 2.6514\n",
      "  Batch [130/1299] D_loss: -3.2894, G_loss: 2.6039\n",
      "  Batch [140/1299] D_loss: -1.3662, G_loss: 1.7216\n",
      "  Batch [150/1299] D_loss: -4.9796, G_loss: 3.4546\n",
      "  Batch [160/1299] D_loss: -4.7560, G_loss: 2.2718\n",
      "  Batch [170/1299] D_loss: -3.4729, G_loss: 3.3472\n",
      "  Batch [180/1299] D_loss: -1.3909, G_loss: 3.3645\n",
      "  Batch [190/1299] D_loss: -0.9945, G_loss: 1.8804\n",
      "  Batch [200/1299] D_loss: -2.5975, G_loss: 1.8488\n",
      "  Batch [210/1299] D_loss: -2.3240, G_loss: 2.0441\n",
      "  Batch [220/1299] D_loss: -2.7266, G_loss: 1.7249\n",
      "  Batch [230/1299] D_loss: -3.1456, G_loss: 3.0025\n",
      "  Batch [240/1299] D_loss: -0.0712, G_loss: 3.4046\n",
      "  Batch [250/1299] D_loss: -4.5453, G_loss: 2.5502\n",
      "  Batch [260/1299] D_loss: -3.6860, G_loss: 0.2624\n",
      "  Batch [270/1299] D_loss: -5.1397, G_loss: -0.4334\n",
      "  Batch [280/1299] D_loss: -1.6292, G_loss: 1.8002\n",
      "  Batch [290/1299] D_loss: -0.7040, G_loss: 2.2287\n",
      "  Batch [300/1299] D_loss: -3.8667, G_loss: -0.5832\n",
      "  Batch [310/1299] D_loss: -1.8684, G_loss: 3.1065\n",
      "  Batch [320/1299] D_loss: -2.1831, G_loss: 3.6256\n",
      "  Batch [330/1299] D_loss: -0.3637, G_loss: 3.4667\n",
      "  Batch [340/1299] D_loss: -1.3079, G_loss: 2.1680\n",
      "  Batch [350/1299] D_loss: -2.8601, G_loss: 0.8983\n",
      "  Batch [360/1299] D_loss: -4.1617, G_loss: 0.8946\n",
      "  Batch [370/1299] D_loss: -3.5880, G_loss: -0.2201\n",
      "  Batch [380/1299] D_loss: -0.7109, G_loss: -0.3057\n",
      "  Batch [390/1299] D_loss: -4.9396, G_loss: 3.9440\n",
      "  Batch [400/1299] D_loss: -2.4518, G_loss: 3.2833\n",
      "  Batch [410/1299] D_loss: -3.2356, G_loss: 1.5449\n",
      "  Batch [420/1299] D_loss: -4.9055, G_loss: 3.6228\n",
      "  Batch [430/1299] D_loss: -1.7643, G_loss: 2.7165\n",
      "  Batch [440/1299] D_loss: -0.9219, G_loss: 4.8475\n",
      "  Batch [450/1299] D_loss: -2.1510, G_loss: 4.4964\n",
      "  Batch [460/1299] D_loss: -3.8099, G_loss: 3.1306\n",
      "  Batch [470/1299] D_loss: -0.8742, G_loss: 4.8439\n",
      "  Batch [480/1299] D_loss: -3.4398, G_loss: 1.6368\n",
      "  Batch [490/1299] D_loss: -2.3916, G_loss: 0.3655\n",
      "  Batch [500/1299] D_loss: -3.5530, G_loss: -0.1883\n",
      "  Batch [510/1299] D_loss: -2.5537, G_loss: 0.6420\n",
      "  Batch [520/1299] D_loss: -1.5728, G_loss: -0.6323\n",
      "  Batch [530/1299] D_loss: -1.8446, G_loss: 0.3524\n",
      "  Batch [540/1299] D_loss: -2.8859, G_loss: -0.8141\n",
      "  Batch [550/1299] D_loss: -3.6930, G_loss: -0.5448\n",
      "  Batch [560/1299] D_loss: -1.4107, G_loss: 0.3624\n",
      "  Batch [570/1299] D_loss: -1.7729, G_loss: -0.0689\n",
      "  Batch [580/1299] D_loss: -1.0884, G_loss: 1.1390\n",
      "  Batch [590/1299] D_loss: -2.0838, G_loss: 1.5291\n",
      "  Batch [600/1299] D_loss: -3.3461, G_loss: 0.1068\n",
      "  Batch [610/1299] D_loss: -2.0559, G_loss: 0.6555\n",
      "  Batch [620/1299] D_loss: -5.4468, G_loss: 1.7144\n",
      "  Batch [630/1299] D_loss: -1.7978, G_loss: -0.1482\n",
      "  Batch [640/1299] D_loss: -2.0381, G_loss: 0.3664\n",
      "  Batch [650/1299] D_loss: -0.7088, G_loss: 0.6545\n",
      "  Batch [660/1299] D_loss: -0.8130, G_loss: 1.4476\n",
      "  Batch [670/1299] D_loss: -2.5461, G_loss: -0.3400\n",
      "  Batch [680/1299] D_loss: -2.3639, G_loss: -1.2799\n",
      "  Batch [690/1299] D_loss: -1.4071, G_loss: 1.2906\n",
      "  Batch [700/1299] D_loss: 0.1954, G_loss: 1.5093\n",
      "  Batch [710/1299] D_loss: -2.7496, G_loss: 3.6841\n",
      "  Batch [720/1299] D_loss: -2.1821, G_loss: 2.2993\n",
      "  Batch [730/1299] D_loss: -2.8820, G_loss: 2.3032\n",
      "  Batch [740/1299] D_loss: -2.7509, G_loss: 2.4352\n",
      "  Batch [750/1299] D_loss: -1.3930, G_loss: 1.1268\n",
      "  Batch [760/1299] D_loss: -1.7652, G_loss: 0.6920\n",
      "  Batch [770/1299] D_loss: -2.3548, G_loss: 1.5068\n",
      "  Batch [780/1299] D_loss: -4.5545, G_loss: 1.3454\n",
      "  Batch [790/1299] D_loss: -0.5904, G_loss: 0.9778\n",
      "  Batch [800/1299] D_loss: -1.1239, G_loss: 1.1448\n",
      "  Batch [810/1299] D_loss: -0.3344, G_loss: 1.5792\n",
      "  Batch [820/1299] D_loss: -3.3327, G_loss: 1.5070\n",
      "  Batch [830/1299] D_loss: -0.3890, G_loss: 1.6721\n",
      "  Batch [840/1299] D_loss: -2.6129, G_loss: 1.5645\n",
      "  Batch [850/1299] D_loss: -2.2646, G_loss: 1.7281\n",
      "  Batch [860/1299] D_loss: -1.3936, G_loss: 0.2508\n",
      "  Batch [870/1299] D_loss: -3.5807, G_loss: 1.5578\n",
      "  Batch [880/1299] D_loss: -2.4063, G_loss: -0.4752\n",
      "  Batch [890/1299] D_loss: -0.9621, G_loss: 0.8481\n",
      "  Batch [900/1299] D_loss: -1.4076, G_loss: -1.5019\n",
      "  Batch [910/1299] D_loss: -0.4714, G_loss: -1.3597\n",
      "  Batch [920/1299] D_loss: -2.9110, G_loss: 0.6104\n",
      "  Batch [930/1299] D_loss: -2.3466, G_loss: 2.1476\n",
      "  Batch [940/1299] D_loss: -2.1345, G_loss: 1.6306\n",
      "  Batch [950/1299] D_loss: -2.9391, G_loss: 0.5782\n",
      "  Batch [960/1299] D_loss: -1.2453, G_loss: 0.0438\n",
      "  Batch [970/1299] D_loss: -1.5249, G_loss: 2.8706\n",
      "  Batch [980/1299] D_loss: -1.5164, G_loss: 1.9679\n",
      "  Batch [990/1299] D_loss: -3.2398, G_loss: 3.0370\n",
      "  Batch [1000/1299] D_loss: -2.4063, G_loss: 1.6129\n",
      "  Batch [1010/1299] D_loss: -2.2044, G_loss: 2.4850\n",
      "  Batch [1020/1299] D_loss: -2.0313, G_loss: -0.1087\n",
      "  Batch [1030/1299] D_loss: -1.8156, G_loss: 0.7864\n",
      "  Batch [1040/1299] D_loss: -3.3528, G_loss: 1.0904\n",
      "  Batch [1050/1299] D_loss: -2.3511, G_loss: 1.7095\n",
      "  Batch [1060/1299] D_loss: -3.3838, G_loss: -1.3007\n",
      "  Batch [1070/1299] D_loss: -1.1341, G_loss: 0.6995\n",
      "  Batch [1080/1299] D_loss: -1.8597, G_loss: 1.7567\n",
      "  Batch [1090/1299] D_loss: -3.3997, G_loss: 0.1443\n",
      "  Batch [1100/1299] D_loss: -3.9498, G_loss: -0.1812\n",
      "  Batch [1110/1299] D_loss: -1.3425, G_loss: -2.0706\n",
      "  Batch [1120/1299] D_loss: -3.1912, G_loss: 1.7941\n",
      "  Batch [1130/1299] D_loss: -1.4456, G_loss: -1.1773\n",
      "  Batch [1140/1299] D_loss: -2.3431, G_loss: 1.2582\n",
      "  Batch [1150/1299] D_loss: -4.2150, G_loss: 1.4596\n",
      "  Batch [1160/1299] D_loss: -2.5655, G_loss: -1.1194\n",
      "  Batch [1170/1299] D_loss: -0.2067, G_loss: -3.8181\n",
      "  Batch [1180/1299] D_loss: -2.6927, G_loss: -4.3934\n",
      "  Batch [1190/1299] D_loss: -3.0046, G_loss: -2.0469\n",
      "  Batch [1200/1299] D_loss: -2.7346, G_loss: -1.7995\n",
      "  Batch [1210/1299] D_loss: -3.4337, G_loss: 0.1364\n",
      "  Batch [1220/1299] D_loss: -3.4963, G_loss: -1.1614\n",
      "  Batch [1230/1299] D_loss: -2.2129, G_loss: -1.7495\n",
      "  Batch [1240/1299] D_loss: -4.5441, G_loss: -1.4935\n",
      "  Batch [1250/1299] D_loss: -1.7244, G_loss: 0.6672\n",
      "  Batch [1260/1299] D_loss: -1.5357, G_loss: 0.5619\n",
      "  Batch [1270/1299] D_loss: -0.6642, G_loss: -1.2363\n",
      "  Batch [1280/1299] D_loss: -1.8901, G_loss: -2.2668\n",
      "  Batch [1290/1299] D_loss: -1.5631, G_loss: -3.1917\n",
      "\n",
      "Epoch 42 Summary:\n",
      "  Average D_loss: -2.1498\n",
      "  Average G_loss: 1.2248\n",
      "\n",
      "Epoch [43/100]\n",
      "  Batch [0/1299] D_loss: -3.7614, G_loss: -2.8758\n",
      "  Batch [10/1299] D_loss: -3.6394, G_loss: -1.5980\n",
      "  Batch [20/1299] D_loss: -1.4530, G_loss: 0.5060\n",
      "  Batch [30/1299] D_loss: -3.5883, G_loss: 0.2042\n",
      "  Batch [40/1299] D_loss: -3.3099, G_loss: -2.0790\n",
      "  Batch [50/1299] D_loss: -2.1261, G_loss: -0.8347\n",
      "  Batch [60/1299] D_loss: -3.5223, G_loss: -0.3334\n",
      "  Batch [70/1299] D_loss: -1.3896, G_loss: 2.1924\n",
      "  Batch [80/1299] D_loss: -1.9398, G_loss: -0.6579\n",
      "  Batch [90/1299] D_loss: -1.9655, G_loss: -0.1159\n",
      "  Batch [100/1299] D_loss: -2.4347, G_loss: 1.2943\n",
      "  Batch [110/1299] D_loss: -0.6910, G_loss: 1.7072\n",
      "  Batch [120/1299] D_loss: -3.8551, G_loss: 2.1759\n",
      "  Batch [130/1299] D_loss: -4.1978, G_loss: -0.0583\n",
      "  Batch [140/1299] D_loss: -2.5604, G_loss: 3.8418\n",
      "  Batch [150/1299] D_loss: -2.6693, G_loss: 3.9437\n",
      "  Batch [160/1299] D_loss: -4.7211, G_loss: 3.8670\n",
      "  Batch [170/1299] D_loss: -1.2915, G_loss: 5.9807\n",
      "  Batch [180/1299] D_loss: -3.1331, G_loss: 5.9848\n",
      "  Batch [190/1299] D_loss: -3.4605, G_loss: 4.7931\n",
      "  Batch [200/1299] D_loss: -4.9676, G_loss: 3.8569\n",
      "  Batch [210/1299] D_loss: -2.9147, G_loss: 1.1561\n",
      "  Batch [220/1299] D_loss: 0.7703, G_loss: 2.4809\n",
      "  Batch [230/1299] D_loss: -4.5345, G_loss: -1.9113\n",
      "  Batch [240/1299] D_loss: -2.9384, G_loss: 2.9928\n",
      "  Batch [250/1299] D_loss: -1.2908, G_loss: 1.7571\n",
      "  Batch [260/1299] D_loss: -3.9828, G_loss: 3.7070\n",
      "  Batch [270/1299] D_loss: -2.5239, G_loss: 4.5207\n",
      "  Batch [280/1299] D_loss: 0.1314, G_loss: 2.4268\n",
      "  Batch [290/1299] D_loss: -2.6115, G_loss: 2.0464\n",
      "  Batch [300/1299] D_loss: -0.7257, G_loss: 0.4084\n",
      "  Batch [310/1299] D_loss: -1.4780, G_loss: -0.4897\n",
      "  Batch [320/1299] D_loss: -1.3712, G_loss: -1.5468\n",
      "  Batch [330/1299] D_loss: -2.3947, G_loss: -2.3482\n",
      "  Batch [340/1299] D_loss: -2.9520, G_loss: 0.3304\n",
      "  Batch [350/1299] D_loss: -4.2425, G_loss: 0.6178\n",
      "  Batch [360/1299] D_loss: -3.0915, G_loss: 1.5762\n",
      "  Batch [370/1299] D_loss: -2.1389, G_loss: 0.9671\n",
      "  Batch [380/1299] D_loss: -3.9755, G_loss: 1.9348\n",
      "  Batch [390/1299] D_loss: -0.9280, G_loss: 2.5056\n",
      "  Batch [400/1299] D_loss: -0.6376, G_loss: 1.1754\n",
      "  Batch [410/1299] D_loss: -2.7554, G_loss: 1.9463\n",
      "  Batch [420/1299] D_loss: -3.2057, G_loss: 2.9793\n",
      "  Batch [430/1299] D_loss: -1.3614, G_loss: 0.0372\n",
      "  Batch [440/1299] D_loss: -3.1705, G_loss: 1.7562\n",
      "  Batch [450/1299] D_loss: -0.4829, G_loss: 2.7744\n",
      "  Batch [460/1299] D_loss: -3.8307, G_loss: 2.2397\n",
      "  Batch [470/1299] D_loss: -2.4102, G_loss: -1.1898\n",
      "  Batch [480/1299] D_loss: -2.4759, G_loss: 0.5631\n",
      "  Batch [490/1299] D_loss: -0.8560, G_loss: -1.2959\n",
      "  Batch [500/1299] D_loss: -3.9353, G_loss: -2.8374\n",
      "  Batch [510/1299] D_loss: -2.8086, G_loss: -1.2150\n",
      "  Batch [520/1299] D_loss: -0.1178, G_loss: 0.0268\n",
      "  Batch [530/1299] D_loss: -2.6121, G_loss: -2.0564\n",
      "  Batch [540/1299] D_loss: -4.4044, G_loss: 1.5014\n",
      "  Batch [550/1299] D_loss: -4.7669, G_loss: -0.9274\n",
      "  Batch [560/1299] D_loss: -5.0393, G_loss: -0.3704\n",
      "  Batch [570/1299] D_loss: -4.7976, G_loss: -0.4862\n",
      "  Batch [580/1299] D_loss: -2.8449, G_loss: -1.8322\n",
      "  Batch [590/1299] D_loss: 0.5060, G_loss: -1.3697\n",
      "  Batch [600/1299] D_loss: -2.0814, G_loss: -2.5828\n",
      "  Batch [610/1299] D_loss: -2.5004, G_loss: -0.0972\n",
      "  Batch [620/1299] D_loss: -0.6943, G_loss: 2.7869\n",
      "  Batch [630/1299] D_loss: -3.7002, G_loss: 6.3285\n",
      "  Batch [640/1299] D_loss: -2.4580, G_loss: 1.6851\n",
      "  Batch [650/1299] D_loss: -0.0191, G_loss: 3.1302\n",
      "  Batch [660/1299] D_loss: -2.0772, G_loss: 0.8052\n",
      "  Batch [670/1299] D_loss: -2.8329, G_loss: 2.2868\n",
      "  Batch [680/1299] D_loss: -2.2546, G_loss: -1.5480\n",
      "  Batch [690/1299] D_loss: -3.0385, G_loss: 0.9366\n",
      "  Batch [700/1299] D_loss: -2.1939, G_loss: 0.7952\n",
      "  Batch [710/1299] D_loss: -3.7346, G_loss: 0.0474\n",
      "  Batch [720/1299] D_loss: -1.7983, G_loss: -2.0133\n",
      "  Batch [730/1299] D_loss: -1.8030, G_loss: 2.2364\n",
      "  Batch [740/1299] D_loss: -4.5142, G_loss: 2.2964\n",
      "  Batch [750/1299] D_loss: -3.6833, G_loss: 5.2765\n",
      "  Batch [760/1299] D_loss: -2.9852, G_loss: 5.1624\n",
      "  Batch [770/1299] D_loss: -5.0929, G_loss: 3.8160\n",
      "  Batch [780/1299] D_loss: -2.6659, G_loss: 2.1614\n",
      "  Batch [790/1299] D_loss: -4.3193, G_loss: 0.3821\n",
      "  Batch [800/1299] D_loss: -2.2545, G_loss: 0.2371\n",
      "  Batch [810/1299] D_loss: -2.4918, G_loss: -2.8458\n",
      "  Batch [820/1299] D_loss: -1.7838, G_loss: -1.9289\n",
      "  Batch [830/1299] D_loss: -3.9224, G_loss: 0.4525\n",
      "  Batch [840/1299] D_loss: -4.3749, G_loss: 0.4773\n",
      "  Batch [850/1299] D_loss: -2.8625, G_loss: 0.6028\n",
      "  Batch [860/1299] D_loss: -2.3117, G_loss: 2.5340\n",
      "  Batch [870/1299] D_loss: -3.4506, G_loss: 0.0982\n",
      "  Batch [880/1299] D_loss: -1.2727, G_loss: 0.0435\n",
      "  Batch [890/1299] D_loss: -1.8192, G_loss: 1.2444\n",
      "  Batch [900/1299] D_loss: -2.4707, G_loss: 2.5077\n",
      "  Batch [910/1299] D_loss: -3.6432, G_loss: 3.2960\n",
      "  Batch [920/1299] D_loss: -2.4424, G_loss: 3.3505\n",
      "  Batch [930/1299] D_loss: -2.5963, G_loss: 4.3055\n",
      "  Batch [940/1299] D_loss: -1.8068, G_loss: 3.8545\n",
      "  Batch [950/1299] D_loss: -3.0167, G_loss: 3.1539\n",
      "  Batch [960/1299] D_loss: -4.2790, G_loss: 1.8565\n",
      "  Batch [970/1299] D_loss: -0.5498, G_loss: 2.6786\n",
      "  Batch [980/1299] D_loss: -1.8380, G_loss: 2.0739\n",
      "  Batch [990/1299] D_loss: -1.0301, G_loss: 3.9034\n",
      "  Batch [1000/1299] D_loss: -1.3961, G_loss: 4.1169\n",
      "  Batch [1010/1299] D_loss: -0.3814, G_loss: 4.1783\n",
      "  Batch [1020/1299] D_loss: -3.3502, G_loss: 2.4780\n",
      "  Batch [1030/1299] D_loss: -2.9336, G_loss: 0.8059\n",
      "  Batch [1040/1299] D_loss: -0.0474, G_loss: 5.5444\n",
      "  Batch [1050/1299] D_loss: -2.0293, G_loss: 3.2120\n",
      "  Batch [1060/1299] D_loss: -3.1582, G_loss: 5.9043\n",
      "  Batch [1070/1299] D_loss: -0.5299, G_loss: 3.2261\n",
      "  Batch [1080/1299] D_loss: -4.2383, G_loss: 5.3066\n",
      "  Batch [1090/1299] D_loss: -1.8671, G_loss: 7.2456\n",
      "  Batch [1100/1299] D_loss: -1.6244, G_loss: 5.7175\n",
      "  Batch [1110/1299] D_loss: -2.5823, G_loss: 2.6984\n",
      "  Batch [1120/1299] D_loss: -4.4035, G_loss: 3.6883\n",
      "  Batch [1130/1299] D_loss: -1.6234, G_loss: 2.0922\n",
      "  Batch [1140/1299] D_loss: -1.9270, G_loss: 4.0721\n",
      "  Batch [1150/1299] D_loss: -3.9457, G_loss: 3.4051\n",
      "  Batch [1160/1299] D_loss: -3.3674, G_loss: 3.1742\n",
      "  Batch [1170/1299] D_loss: -3.7864, G_loss: 1.8303\n",
      "  Batch [1180/1299] D_loss: -4.8001, G_loss: 1.5580\n",
      "  Batch [1190/1299] D_loss: -3.7670, G_loss: 0.6083\n",
      "  Batch [1200/1299] D_loss: -0.5501, G_loss: -1.2690\n",
      "  Batch [1210/1299] D_loss: -2.0433, G_loss: -1.0037\n",
      "  Batch [1220/1299] D_loss: -5.0527, G_loss: -0.6990\n",
      "  Batch [1230/1299] D_loss: -3.0269, G_loss: 1.8225\n",
      "  Batch [1240/1299] D_loss: -1.9602, G_loss: 1.7223\n",
      "  Batch [1250/1299] D_loss: -1.2430, G_loss: -0.1843\n",
      "  Batch [1260/1299] D_loss: -0.9712, G_loss: -0.1298\n",
      "  Batch [1270/1299] D_loss: -2.5016, G_loss: 2.3775\n",
      "  Batch [1280/1299] D_loss: -3.9833, G_loss: 0.5074\n",
      "  Batch [1290/1299] D_loss: -1.3625, G_loss: 2.2618\n",
      "\n",
      "Epoch 43 Summary:\n",
      "  Average D_loss: -2.1300\n",
      "  Average G_loss: 1.5270\n",
      "\n",
      "Epoch [44/100]\n",
      "  Batch [0/1299] D_loss: -1.3219, G_loss: 1.7419\n",
      "  Batch [10/1299] D_loss: -2.4649, G_loss: 1.0093\n",
      "  Batch [20/1299] D_loss: -4.0092, G_loss: 4.2751\n",
      "  Batch [30/1299] D_loss: -1.3121, G_loss: 2.1409\n",
      "  Batch [40/1299] D_loss: -2.4951, G_loss: 3.1887\n",
      "  Batch [50/1299] D_loss: -1.5764, G_loss: 3.7913\n",
      "  Batch [60/1299] D_loss: -1.9790, G_loss: 2.1619\n",
      "  Batch [70/1299] D_loss: -2.7485, G_loss: 3.7822\n",
      "  Batch [80/1299] D_loss: -2.3039, G_loss: 1.4850\n",
      "  Batch [90/1299] D_loss: -1.7093, G_loss: 1.1014\n",
      "  Batch [100/1299] D_loss: -3.3521, G_loss: -1.2559\n",
      "  Batch [110/1299] D_loss: -2.8151, G_loss: -0.4281\n",
      "  Batch [120/1299] D_loss: -0.6160, G_loss: 1.1219\n",
      "  Batch [130/1299] D_loss: -0.6285, G_loss: 4.1972\n",
      "  Batch [140/1299] D_loss: -3.5684, G_loss: 2.2930\n",
      "  Batch [150/1299] D_loss: -4.2479, G_loss: 3.4220\n",
      "  Batch [160/1299] D_loss: -0.9451, G_loss: 1.6908\n",
      "  Batch [170/1299] D_loss: -2.7300, G_loss: 2.5951\n",
      "  Batch [180/1299] D_loss: -2.3080, G_loss: 3.3385\n",
      "  Batch [190/1299] D_loss: -3.4315, G_loss: 3.7452\n",
      "  Batch [200/1299] D_loss: -2.3196, G_loss: 3.3888\n",
      "  Batch [210/1299] D_loss: 0.6111, G_loss: 2.0495\n",
      "  Batch [220/1299] D_loss: -2.5792, G_loss: 0.5220\n",
      "  Batch [230/1299] D_loss: -0.0280, G_loss: 1.6045\n",
      "  Batch [240/1299] D_loss: -1.9958, G_loss: -0.0062\n",
      "  Batch [250/1299] D_loss: -0.0027, G_loss: 0.9183\n",
      "  Batch [260/1299] D_loss: -1.4956, G_loss: -0.1469\n",
      "  Batch [270/1299] D_loss: -1.6333, G_loss: -0.0706\n",
      "  Batch [280/1299] D_loss: -1.7586, G_loss: 1.5531\n",
      "  Batch [290/1299] D_loss: -1.9316, G_loss: 1.0643\n",
      "  Batch [300/1299] D_loss: -2.6043, G_loss: 1.5535\n",
      "  Batch [310/1299] D_loss: -2.3837, G_loss: 0.9190\n",
      "  Batch [320/1299] D_loss: -2.0088, G_loss: 3.2067\n",
      "  Batch [330/1299] D_loss: -3.5135, G_loss: 2.8968\n",
      "  Batch [340/1299] D_loss: -1.0347, G_loss: 3.4883\n",
      "  Batch [350/1299] D_loss: -1.9686, G_loss: 2.4243\n",
      "  Batch [360/1299] D_loss: -1.5176, G_loss: 1.9568\n",
      "  Batch [370/1299] D_loss: -2.5390, G_loss: 2.3578\n",
      "  Batch [380/1299] D_loss: -4.6523, G_loss: 1.2765\n",
      "  Batch [390/1299] D_loss: -3.5735, G_loss: 1.4959\n",
      "  Batch [400/1299] D_loss: -1.9738, G_loss: 3.5063\n",
      "  Batch [410/1299] D_loss: -0.8064, G_loss: 1.7163\n",
      "  Batch [420/1299] D_loss: -1.7938, G_loss: 1.5413\n",
      "  Batch [430/1299] D_loss: -3.5832, G_loss: 0.7198\n",
      "  Batch [440/1299] D_loss: -1.3731, G_loss: 2.5376\n",
      "  Batch [450/1299] D_loss: -5.9844, G_loss: 0.2407\n",
      "  Batch [460/1299] D_loss: -2.1492, G_loss: -0.6559\n",
      "  Batch [470/1299] D_loss: -1.7427, G_loss: 1.4043\n",
      "  Batch [480/1299] D_loss: -3.2977, G_loss: 1.3065\n",
      "  Batch [490/1299] D_loss: -2.7305, G_loss: -1.0061\n",
      "  Batch [500/1299] D_loss: -2.8162, G_loss: 1.4764\n",
      "  Batch [510/1299] D_loss: -1.5767, G_loss: 1.5749\n",
      "  Batch [520/1299] D_loss: -2.8361, G_loss: 3.1181\n",
      "  Batch [530/1299] D_loss: -4.0956, G_loss: 2.3420\n",
      "  Batch [540/1299] D_loss: -2.1526, G_loss: 0.1814\n",
      "  Batch [550/1299] D_loss: -1.6980, G_loss: 0.5912\n",
      "  Batch [560/1299] D_loss: -4.1487, G_loss: 2.4122\n",
      "  Batch [570/1299] D_loss: -2.3122, G_loss: 2.2897\n",
      "  Batch [580/1299] D_loss: -1.6531, G_loss: 0.1421\n",
      "  Batch [590/1299] D_loss: -2.0115, G_loss: 1.1335\n",
      "  Batch [600/1299] D_loss: -0.9878, G_loss: 0.9703\n",
      "  Batch [610/1299] D_loss: -2.7293, G_loss: 2.6037\n",
      "  Batch [620/1299] D_loss: -4.1177, G_loss: 2.9942\n",
      "  Batch [630/1299] D_loss: -2.9835, G_loss: 2.7136\n",
      "  Batch [640/1299] D_loss: -1.3337, G_loss: 2.3607\n",
      "  Batch [650/1299] D_loss: -3.7276, G_loss: 2.2450\n",
      "  Batch [660/1299] D_loss: -4.2595, G_loss: 2.7178\n",
      "  Batch [670/1299] D_loss: -1.1984, G_loss: 1.7415\n",
      "  Batch [680/1299] D_loss: -1.0283, G_loss: 1.6281\n",
      "  Batch [690/1299] D_loss: -1.9200, G_loss: 0.1679\n",
      "  Batch [700/1299] D_loss: -2.7381, G_loss: 0.5234\n",
      "  Batch [710/1299] D_loss: -2.8531, G_loss: 1.1765\n",
      "  Batch [720/1299] D_loss: -1.4400, G_loss: 3.5962\n",
      "  Batch [730/1299] D_loss: -3.1051, G_loss: 2.8982\n",
      "  Batch [740/1299] D_loss: -2.1014, G_loss: 1.8646\n",
      "  Batch [750/1299] D_loss: -3.0770, G_loss: 0.3339\n",
      "  Batch [760/1299] D_loss: -5.0035, G_loss: -1.9388\n",
      "  Batch [770/1299] D_loss: -1.7052, G_loss: -1.1206\n",
      "  Batch [780/1299] D_loss: -2.4911, G_loss: -2.9739\n",
      "  Batch [790/1299] D_loss: -3.6697, G_loss: -2.1499\n",
      "  Batch [800/1299] D_loss: -2.4341, G_loss: -1.9742\n",
      "  Batch [810/1299] D_loss: -1.4006, G_loss: -0.6456\n",
      "  Batch [820/1299] D_loss: -2.1339, G_loss: 2.1012\n",
      "  Batch [830/1299] D_loss: -2.7364, G_loss: 4.2900\n",
      "  Batch [840/1299] D_loss: -3.2477, G_loss: 5.6538\n",
      "  Batch [850/1299] D_loss: -2.7047, G_loss: 3.9180\n",
      "  Batch [860/1299] D_loss: -2.4096, G_loss: 2.5913\n",
      "  Batch [870/1299] D_loss: -1.1439, G_loss: 2.7197\n",
      "  Batch [880/1299] D_loss: -1.1194, G_loss: 1.4119\n",
      "  Batch [890/1299] D_loss: -2.3610, G_loss: 1.4475\n",
      "  Batch [900/1299] D_loss: -3.1432, G_loss: 3.1474\n",
      "  Batch [910/1299] D_loss: -2.0689, G_loss: 1.5076\n",
      "  Batch [920/1299] D_loss: -2.7090, G_loss: 1.8985\n",
      "  Batch [930/1299] D_loss: -2.8836, G_loss: 2.8105\n",
      "  Batch [940/1299] D_loss: -0.8336, G_loss: 2.9329\n",
      "  Batch [950/1299] D_loss: -2.7312, G_loss: 2.6274\n",
      "  Batch [960/1299] D_loss: -2.6576, G_loss: 3.7970\n",
      "  Batch [970/1299] D_loss: -1.9073, G_loss: 2.1765\n",
      "  Batch [980/1299] D_loss: -2.4972, G_loss: 1.7548\n",
      "  Batch [990/1299] D_loss: -2.0528, G_loss: -0.8311\n",
      "  Batch [1000/1299] D_loss: -2.8316, G_loss: -0.1866\n",
      "  Batch [1010/1299] D_loss: -2.8182, G_loss: -0.5251\n",
      "  Batch [1020/1299] D_loss: -0.9023, G_loss: -2.1520\n",
      "  Batch [1030/1299] D_loss: -2.1187, G_loss: -2.4842\n",
      "  Batch [1040/1299] D_loss: -2.5226, G_loss: -3.1886\n",
      "  Batch [1050/1299] D_loss: -2.2618, G_loss: -2.7335\n",
      "  Batch [1060/1299] D_loss: -2.9835, G_loss: -3.5483\n",
      "  Batch [1070/1299] D_loss: -1.1116, G_loss: -2.4310\n",
      "  Batch [1080/1299] D_loss: -1.9610, G_loss: -2.7821\n",
      "  Batch [1090/1299] D_loss: -2.8693, G_loss: -2.3379\n",
      "  Batch [1100/1299] D_loss: -2.0774, G_loss: -0.2296\n",
      "  Batch [1110/1299] D_loss: -2.8857, G_loss: -0.0467\n",
      "  Batch [1120/1299] D_loss: -1.4351, G_loss: 0.2152\n",
      "  Batch [1130/1299] D_loss: -1.6857, G_loss: 0.5178\n",
      "  Batch [1140/1299] D_loss: -2.5735, G_loss: 1.7779\n",
      "  Batch [1150/1299] D_loss: -3.2967, G_loss: 1.2458\n",
      "  Batch [1160/1299] D_loss: -1.0700, G_loss: 0.1053\n",
      "  Batch [1170/1299] D_loss: -3.1692, G_loss: 0.6142\n",
      "  Batch [1180/1299] D_loss: -5.6004, G_loss: 0.9666\n",
      "  Batch [1190/1299] D_loss: -3.7784, G_loss: 2.5592\n",
      "  Batch [1200/1299] D_loss: -1.5615, G_loss: 2.3068\n",
      "  Batch [1210/1299] D_loss: -2.0180, G_loss: 2.0505\n",
      "  Batch [1220/1299] D_loss: -3.1867, G_loss: 2.3015\n",
      "  Batch [1230/1299] D_loss: -3.3802, G_loss: 1.9001\n",
      "  Batch [1240/1299] D_loss: -2.5076, G_loss: 1.0722\n",
      "  Batch [1250/1299] D_loss: -1.1383, G_loss: 1.3095\n",
      "  Batch [1260/1299] D_loss: -0.5366, G_loss: 2.2427\n",
      "  Batch [1270/1299] D_loss: -2.7580, G_loss: 0.5585\n",
      "  Batch [1280/1299] D_loss: -1.5247, G_loss: 0.3414\n",
      "  Batch [1290/1299] D_loss: -2.0137, G_loss: 1.3377\n",
      "\n",
      "Epoch 44 Summary:\n",
      "  Average D_loss: -2.0664\n",
      "  Average G_loss: 1.3292\n",
      "\n",
      "Epoch [45/100]\n",
      "  Batch [0/1299] D_loss: 0.2522, G_loss: -0.2553\n",
      "  Batch [10/1299] D_loss: -2.2028, G_loss: 0.2862\n",
      "  Batch [20/1299] D_loss: -2.7867, G_loss: -2.3909\n",
      "  Batch [30/1299] D_loss: -1.7831, G_loss: -2.9043\n",
      "  Batch [40/1299] D_loss: -1.4210, G_loss: -1.4263\n",
      "  Batch [50/1299] D_loss: -1.9379, G_loss: -1.8165\n",
      "  Batch [60/1299] D_loss: -2.5333, G_loss: 0.2281\n",
      "  Batch [70/1299] D_loss: -1.1795, G_loss: 1.3497\n",
      "  Batch [80/1299] D_loss: -1.6205, G_loss: 2.4515\n",
      "  Batch [90/1299] D_loss: -3.1273, G_loss: 2.8879\n",
      "  Batch [100/1299] D_loss: -2.3710, G_loss: 3.4027\n",
      "  Batch [110/1299] D_loss: -1.0602, G_loss: 2.1522\n",
      "  Batch [120/1299] D_loss: -2.1101, G_loss: 2.2057\n",
      "  Batch [130/1299] D_loss: -3.1855, G_loss: 3.5853\n",
      "  Batch [140/1299] D_loss: -0.5043, G_loss: 3.5443\n",
      "  Batch [150/1299] D_loss: -2.3130, G_loss: 3.0034\n",
      "  Batch [160/1299] D_loss: -2.4563, G_loss: 3.2879\n",
      "  Batch [170/1299] D_loss: -2.8033, G_loss: 1.8537\n",
      "  Batch [180/1299] D_loss: -1.2015, G_loss: 1.7362\n",
      "  Batch [190/1299] D_loss: -3.0012, G_loss: 1.3841\n",
      "  Batch [200/1299] D_loss: -0.3704, G_loss: 3.3123\n",
      "  Batch [210/1299] D_loss: -2.6317, G_loss: 1.7108\n",
      "  Batch [220/1299] D_loss: -1.2977, G_loss: 2.4014\n",
      "  Batch [230/1299] D_loss: -3.8625, G_loss: 4.8472\n",
      "  Batch [240/1299] D_loss: -2.8618, G_loss: 5.5745\n",
      "  Batch [250/1299] D_loss: -1.5463, G_loss: 3.2310\n",
      "  Batch [260/1299] D_loss: -1.8109, G_loss: 2.5610\n",
      "  Batch [270/1299] D_loss: -2.9190, G_loss: 0.4311\n",
      "  Batch [280/1299] D_loss: -2.5290, G_loss: 3.2592\n",
      "  Batch [290/1299] D_loss: -3.3666, G_loss: 4.4809\n",
      "  Batch [300/1299] D_loss: -3.1814, G_loss: 2.7287\n",
      "  Batch [310/1299] D_loss: -2.7239, G_loss: 3.0326\n",
      "  Batch [320/1299] D_loss: -1.7076, G_loss: 1.9429\n",
      "  Batch [330/1299] D_loss: -3.2232, G_loss: 1.4446\n",
      "  Batch [340/1299] D_loss: -2.7622, G_loss: 1.1022\n",
      "  Batch [350/1299] D_loss: -1.5521, G_loss: -0.6738\n",
      "  Batch [360/1299] D_loss: -2.6383, G_loss: 0.9961\n",
      "  Batch [370/1299] D_loss: -3.7342, G_loss: 1.2703\n",
      "  Batch [380/1299] D_loss: -2.0766, G_loss: 2.0402\n",
      "  Batch [390/1299] D_loss: -1.2002, G_loss: 0.9115\n",
      "  Batch [400/1299] D_loss: -2.6619, G_loss: 2.3571\n",
      "  Batch [410/1299] D_loss: -1.5138, G_loss: 1.6955\n",
      "  Batch [420/1299] D_loss: -3.0164, G_loss: 0.1694\n",
      "  Batch [430/1299] D_loss: -3.3465, G_loss: 0.9595\n",
      "  Batch [440/1299] D_loss: -3.5752, G_loss: 1.6554\n",
      "  Batch [450/1299] D_loss: -3.3015, G_loss: 3.8486\n",
      "  Batch [460/1299] D_loss: -2.6813, G_loss: 1.4948\n",
      "  Batch [470/1299] D_loss: -2.8426, G_loss: 1.6746\n",
      "  Batch [480/1299] D_loss: -3.0405, G_loss: -0.7215\n",
      "  Batch [490/1299] D_loss: -2.2606, G_loss: 1.1854\n",
      "  Batch [500/1299] D_loss: -0.9214, G_loss: -1.0295\n",
      "  Batch [510/1299] D_loss: -2.2227, G_loss: -0.4513\n",
      "  Batch [520/1299] D_loss: -1.8164, G_loss: 0.3582\n",
      "  Batch [530/1299] D_loss: -3.2668, G_loss: 0.2531\n",
      "  Batch [540/1299] D_loss: -2.3171, G_loss: 3.8843\n",
      "  Batch [550/1299] D_loss: -2.5425, G_loss: 4.0767\n",
      "  Batch [560/1299] D_loss: -3.2696, G_loss: 2.7750\n",
      "  Batch [570/1299] D_loss: -3.1366, G_loss: 1.1054\n",
      "  Batch [580/1299] D_loss: -1.2740, G_loss: -0.5919\n",
      "  Batch [590/1299] D_loss: -1.4190, G_loss: -0.1263\n",
      "  Batch [600/1299] D_loss: -2.0606, G_loss: 2.2644\n",
      "  Batch [610/1299] D_loss: -1.5407, G_loss: 0.5814\n",
      "  Batch [620/1299] D_loss: -1.1961, G_loss: 0.3332\n",
      "  Batch [630/1299] D_loss: -1.6929, G_loss: 0.0510\n",
      "  Batch [640/1299] D_loss: -1.5484, G_loss: 2.0329\n",
      "  Batch [650/1299] D_loss: -3.4301, G_loss: 2.8957\n",
      "  Batch [660/1299] D_loss: -2.6741, G_loss: 3.6429\n",
      "  Batch [670/1299] D_loss: -2.7427, G_loss: 3.2167\n",
      "  Batch [680/1299] D_loss: -2.3370, G_loss: -0.3154\n",
      "  Batch [690/1299] D_loss: -3.5465, G_loss: 1.1088\n",
      "  Batch [700/1299] D_loss: -3.4690, G_loss: 1.7783\n",
      "  Batch [710/1299] D_loss: -2.2559, G_loss: 2.7664\n",
      "  Batch [720/1299] D_loss: -1.7633, G_loss: 1.8367\n",
      "  Batch [730/1299] D_loss: -0.4964, G_loss: 3.2247\n",
      "  Batch [740/1299] D_loss: -1.5111, G_loss: 1.1377\n",
      "  Batch [750/1299] D_loss: -2.3592, G_loss: 1.3639\n",
      "  Batch [760/1299] D_loss: -1.2095, G_loss: 1.0264\n",
      "  Batch [770/1299] D_loss: -1.5717, G_loss: 2.4458\n",
      "  Batch [780/1299] D_loss: -2.4189, G_loss: 1.3523\n",
      "  Batch [790/1299] D_loss: -2.6824, G_loss: 1.8196\n",
      "  Batch [800/1299] D_loss: -3.3800, G_loss: 0.6006\n",
      "  Batch [810/1299] D_loss: -2.4069, G_loss: -0.4171\n",
      "  Batch [820/1299] D_loss: -1.6210, G_loss: -1.7928\n",
      "  Batch [830/1299] D_loss: -2.7996, G_loss: -0.9534\n",
      "  Batch [840/1299] D_loss: -0.8792, G_loss: 1.9458\n",
      "  Batch [850/1299] D_loss: -3.1440, G_loss: 0.4190\n",
      "  Batch [860/1299] D_loss: -2.8935, G_loss: 0.1375\n",
      "  Batch [870/1299] D_loss: -1.9989, G_loss: 0.0455\n",
      "  Batch [880/1299] D_loss: -2.3348, G_loss: 0.7891\n",
      "  Batch [890/1299] D_loss: -4.5796, G_loss: -0.0722\n",
      "  Batch [900/1299] D_loss: -1.0910, G_loss: -0.0200\n",
      "  Batch [910/1299] D_loss: -2.3544, G_loss: -0.4074\n",
      "  Batch [920/1299] D_loss: -3.0884, G_loss: -0.7544\n",
      "  Batch [930/1299] D_loss: -2.8944, G_loss: 0.0297\n",
      "  Batch [940/1299] D_loss: -0.0442, G_loss: -1.3795\n",
      "  Batch [950/1299] D_loss: -3.0864, G_loss: -1.0384\n",
      "  Batch [960/1299] D_loss: -1.7024, G_loss: 2.4678\n",
      "  Batch [970/1299] D_loss: -2.1401, G_loss: 2.8411\n",
      "  Batch [980/1299] D_loss: -4.3633, G_loss: 0.7865\n",
      "  Batch [990/1299] D_loss: -2.2844, G_loss: 1.8887\n",
      "  Batch [1000/1299] D_loss: -2.9592, G_loss: 0.3015\n",
      "  Batch [1010/1299] D_loss: -2.4266, G_loss: 3.9040\n",
      "  Batch [1020/1299] D_loss: 0.7394, G_loss: 2.3603\n",
      "  Batch [1030/1299] D_loss: -1.9995, G_loss: 4.1340\n",
      "  Batch [1040/1299] D_loss: -3.4585, G_loss: 2.4075\n",
      "  Batch [1050/1299] D_loss: -2.2402, G_loss: 2.0486\n",
      "  Batch [1060/1299] D_loss: -0.9432, G_loss: 1.3339\n",
      "  Batch [1070/1299] D_loss: -3.1290, G_loss: 1.0064\n",
      "  Batch [1080/1299] D_loss: -1.7879, G_loss: 1.6332\n",
      "  Batch [1090/1299] D_loss: -3.0146, G_loss: 2.2791\n",
      "  Batch [1100/1299] D_loss: 0.1098, G_loss: 0.9425\n",
      "  Batch [1110/1299] D_loss: -3.2657, G_loss: 3.1420\n",
      "  Batch [1120/1299] D_loss: -1.9063, G_loss: 2.9144\n",
      "  Batch [1130/1299] D_loss: -0.8376, G_loss: 3.1193\n",
      "  Batch [1140/1299] D_loss: -1.5299, G_loss: 1.8905\n",
      "  Batch [1150/1299] D_loss: -3.6242, G_loss: 1.0388\n",
      "  Batch [1160/1299] D_loss: -1.6382, G_loss: 2.3473\n",
      "  Batch [1170/1299] D_loss: -3.3207, G_loss: -0.9459\n",
      "  Batch [1180/1299] D_loss: -2.5035, G_loss: 0.1437\n",
      "  Batch [1190/1299] D_loss: -1.7338, G_loss: 0.8436\n",
      "  Batch [1200/1299] D_loss: -2.3124, G_loss: 1.9875\n",
      "  Batch [1210/1299] D_loss: -1.4806, G_loss: 3.3576\n",
      "  Batch [1220/1299] D_loss: -0.7109, G_loss: 2.4579\n",
      "  Batch [1230/1299] D_loss: -1.1675, G_loss: 3.0198\n",
      "  Batch [1240/1299] D_loss: -1.6612, G_loss: 3.6095\n",
      "  Batch [1250/1299] D_loss: -3.2660, G_loss: 0.6859\n",
      "  Batch [1260/1299] D_loss: -2.3128, G_loss: 0.1572\n",
      "  Batch [1270/1299] D_loss: -4.4802, G_loss: 1.6736\n",
      "  Batch [1280/1299] D_loss: -1.0795, G_loss: 2.3356\n",
      "  Batch [1290/1299] D_loss: -3.3310, G_loss: 0.6916\n",
      "\n",
      "Epoch 45 Summary:\n",
      "  Average D_loss: -2.0595\n",
      "  Average G_loss: 1.5530\n",
      "\n",
      "Epoch [46/100]\n",
      "  Batch [0/1299] D_loss: -0.5418, G_loss: -0.9041\n",
      "  Batch [10/1299] D_loss: 0.1281, G_loss: 1.8274\n",
      "  Batch [20/1299] D_loss: -2.6441, G_loss: -2.0948\n",
      "  Batch [30/1299] D_loss: -2.4505, G_loss: -0.6202\n",
      "  Batch [40/1299] D_loss: -0.2424, G_loss: -1.8565\n",
      "  Batch [50/1299] D_loss: -2.8334, G_loss: 2.3056\n",
      "  Batch [60/1299] D_loss: -5.6904, G_loss: 0.6553\n",
      "  Batch [70/1299] D_loss: -0.8035, G_loss: 0.0471\n",
      "  Batch [80/1299] D_loss: -2.0733, G_loss: -1.9790\n",
      "  Batch [90/1299] D_loss: -1.1543, G_loss: 0.0175\n",
      "  Batch [100/1299] D_loss: -3.2084, G_loss: 2.8965\n",
      "  Batch [110/1299] D_loss: -1.3645, G_loss: 3.9985\n",
      "  Batch [120/1299] D_loss: -3.4807, G_loss: 4.6002\n",
      "  Batch [130/1299] D_loss: -2.7815, G_loss: 1.2285\n",
      "  Batch [140/1299] D_loss: -2.8356, G_loss: 3.0479\n",
      "  Batch [150/1299] D_loss: -2.9925, G_loss: 2.1484\n",
      "  Batch [160/1299] D_loss: -0.2250, G_loss: 3.2532\n",
      "  Batch [170/1299] D_loss: -2.6131, G_loss: 3.3527\n",
      "  Batch [180/1299] D_loss: -1.9380, G_loss: 1.2134\n",
      "  Batch [190/1299] D_loss: -2.5939, G_loss: 2.7649\n",
      "  Batch [200/1299] D_loss: -1.2568, G_loss: 1.3468\n",
      "  Batch [210/1299] D_loss: -0.7777, G_loss: 1.0368\n",
      "  Batch [220/1299] D_loss: -3.3103, G_loss: 1.4676\n",
      "  Batch [230/1299] D_loss: -2.7152, G_loss: 1.8525\n",
      "  Batch [240/1299] D_loss: -4.1263, G_loss: 2.2480\n",
      "  Batch [250/1299] D_loss: -2.3616, G_loss: 0.4715\n",
      "  Batch [260/1299] D_loss: -0.1511, G_loss: 0.8977\n",
      "  Batch [270/1299] D_loss: -2.2328, G_loss: 2.5673\n",
      "  Batch [280/1299] D_loss: -3.4348, G_loss: 3.9864\n",
      "  Batch [290/1299] D_loss: -0.3635, G_loss: 4.6457\n",
      "  Batch [300/1299] D_loss: -2.8737, G_loss: 2.6694\n",
      "  Batch [310/1299] D_loss: -1.7121, G_loss: 1.2097\n",
      "  Batch [320/1299] D_loss: -2.6858, G_loss: 2.4773\n",
      "  Batch [330/1299] D_loss: -2.7332, G_loss: 1.9614\n",
      "  Batch [340/1299] D_loss: -1.6377, G_loss: 2.1144\n",
      "  Batch [350/1299] D_loss: -3.1004, G_loss: -0.2473\n",
      "  Batch [360/1299] D_loss: -0.4188, G_loss: -0.0014\n",
      "  Batch [370/1299] D_loss: -2.5165, G_loss: -2.6755\n",
      "  Batch [380/1299] D_loss: -4.8746, G_loss: -0.0234\n",
      "  Batch [390/1299] D_loss: -5.4988, G_loss: -3.8920\n",
      "  Batch [400/1299] D_loss: -1.1505, G_loss: -0.3983\n",
      "  Batch [410/1299] D_loss: -1.9584, G_loss: 0.6394\n",
      "  Batch [420/1299] D_loss: 0.2377, G_loss: -0.5569\n",
      "  Batch [430/1299] D_loss: -0.5856, G_loss: 0.2261\n",
      "  Batch [440/1299] D_loss: -1.8094, G_loss: -0.7345\n",
      "  Batch [450/1299] D_loss: -2.3746, G_loss: -0.4345\n",
      "  Batch [460/1299] D_loss: -3.0904, G_loss: 1.0781\n",
      "  Batch [470/1299] D_loss: -3.7607, G_loss: 1.0330\n",
      "  Batch [480/1299] D_loss: -1.6029, G_loss: 1.7882\n",
      "  Batch [490/1299] D_loss: -3.4212, G_loss: 1.1431\n",
      "  Batch [500/1299] D_loss: -2.9145, G_loss: 3.4333\n",
      "  Batch [510/1299] D_loss: -2.0248, G_loss: 2.7271\n",
      "  Batch [520/1299] D_loss: -2.7669, G_loss: 3.2828\n",
      "  Batch [530/1299] D_loss: -2.1220, G_loss: 2.7999\n",
      "  Batch [540/1299] D_loss: -2.0498, G_loss: 1.9724\n",
      "  Batch [550/1299] D_loss: -5.1298, G_loss: 1.6242\n",
      "  Batch [560/1299] D_loss: -2.6546, G_loss: 3.7929\n",
      "  Batch [570/1299] D_loss: -0.1379, G_loss: 0.1253\n",
      "  Batch [580/1299] D_loss: -2.2102, G_loss: 2.3317\n",
      "  Batch [590/1299] D_loss: -4.4191, G_loss: 3.7612\n",
      "  Batch [600/1299] D_loss: -1.4206, G_loss: 1.6754\n",
      "  Batch [610/1299] D_loss: -2.8477, G_loss: 0.3714\n",
      "  Batch [620/1299] D_loss: -4.2490, G_loss: 0.9411\n",
      "  Batch [630/1299] D_loss: -4.9356, G_loss: -0.0127\n",
      "  Batch [640/1299] D_loss: -1.9851, G_loss: 1.7977\n",
      "  Batch [650/1299] D_loss: -4.0863, G_loss: -0.0777\n",
      "  Batch [660/1299] D_loss: -1.4402, G_loss: -1.4993\n",
      "  Batch [670/1299] D_loss: -1.9351, G_loss: -1.8294\n",
      "  Batch [680/1299] D_loss: -3.2917, G_loss: -2.8932\n",
      "  Batch [690/1299] D_loss: -2.6608, G_loss: -1.4351\n",
      "  Batch [700/1299] D_loss: 0.4101, G_loss: 0.0264\n",
      "  Batch [710/1299] D_loss: -3.8247, G_loss: -0.4692\n",
      "  Batch [720/1299] D_loss: -1.7402, G_loss: 1.3983\n",
      "  Batch [730/1299] D_loss: -1.5923, G_loss: 2.5399\n",
      "  Batch [740/1299] D_loss: -1.4988, G_loss: 2.7687\n",
      "  Batch [750/1299] D_loss: -0.3777, G_loss: 2.2679\n",
      "  Batch [760/1299] D_loss: -3.1361, G_loss: 2.3841\n",
      "  Batch [770/1299] D_loss: -3.4558, G_loss: 2.7296\n",
      "  Batch [780/1299] D_loss: -2.7895, G_loss: 2.0210\n",
      "  Batch [790/1299] D_loss: -2.6447, G_loss: 1.8287\n",
      "  Batch [800/1299] D_loss: -1.3348, G_loss: 2.1428\n",
      "  Batch [810/1299] D_loss: -0.5651, G_loss: 3.0691\n",
      "  Batch [820/1299] D_loss: -0.6779, G_loss: 1.1906\n",
      "  Batch [830/1299] D_loss: -1.7583, G_loss: 0.1796\n",
      "  Batch [840/1299] D_loss: -3.1048, G_loss: -0.8076\n",
      "  Batch [850/1299] D_loss: -3.1200, G_loss: 0.4914\n",
      "  Batch [860/1299] D_loss: -1.0696, G_loss: 2.4141\n",
      "  Batch [870/1299] D_loss: -2.1046, G_loss: -0.2088\n",
      "  Batch [880/1299] D_loss: -2.5524, G_loss: 0.2191\n",
      "  Batch [890/1299] D_loss: -2.4880, G_loss: 1.5710\n",
      "  Batch [900/1299] D_loss: -1.3039, G_loss: 0.5030\n",
      "  Batch [910/1299] D_loss: -3.5360, G_loss: -0.1354\n",
      "  Batch [920/1299] D_loss: -1.0243, G_loss: -0.3178\n",
      "  Batch [930/1299] D_loss: -3.4360, G_loss: 1.1731\n",
      "  Batch [940/1299] D_loss: -2.4893, G_loss: 1.6547\n",
      "  Batch [950/1299] D_loss: -1.3241, G_loss: 1.1465\n",
      "  Batch [960/1299] D_loss: -3.4842, G_loss: 0.6414\n",
      "  Batch [970/1299] D_loss: -1.4814, G_loss: 0.9171\n",
      "  Batch [980/1299] D_loss: 1.3999, G_loss: 3.3375\n",
      "  Batch [990/1299] D_loss: -3.5977, G_loss: 1.5007\n",
      "  Batch [1000/1299] D_loss: -3.3358, G_loss: 2.6360\n",
      "  Batch [1010/1299] D_loss: -2.0997, G_loss: -0.3772\n",
      "  Batch [1020/1299] D_loss: -1.6915, G_loss: -0.1953\n",
      "  Batch [1030/1299] D_loss: -1.3548, G_loss: 1.8672\n",
      "  Batch [1040/1299] D_loss: -2.0243, G_loss: -0.8665\n",
      "  Batch [1050/1299] D_loss: -2.4090, G_loss: -1.1128\n",
      "  Batch [1060/1299] D_loss: -1.9058, G_loss: 1.6795\n",
      "  Batch [1070/1299] D_loss: -3.1558, G_loss: 4.2738\n",
      "  Batch [1080/1299] D_loss: -2.3125, G_loss: 4.0132\n",
      "  Batch [1090/1299] D_loss: -1.5834, G_loss: 3.9904\n",
      "  Batch [1100/1299] D_loss: -3.5546, G_loss: 2.4605\n",
      "  Batch [1110/1299] D_loss: -1.6784, G_loss: 2.9661\n",
      "  Batch [1120/1299] D_loss: -2.3085, G_loss: 2.6058\n",
      "  Batch [1130/1299] D_loss: -1.9914, G_loss: 0.8579\n",
      "  Batch [1140/1299] D_loss: -3.3793, G_loss: 2.3996\n",
      "  Batch [1150/1299] D_loss: 0.1644, G_loss: 3.2931\n",
      "  Batch [1160/1299] D_loss: -1.7886, G_loss: 3.4957\n",
      "  Batch [1170/1299] D_loss: -1.7834, G_loss: 3.8282\n",
      "  Batch [1180/1299] D_loss: -0.7431, G_loss: 3.8459\n",
      "  Batch [1190/1299] D_loss: -1.1922, G_loss: 1.8905\n",
      "  Batch [1200/1299] D_loss: -4.6108, G_loss: -0.7347\n",
      "  Batch [1210/1299] D_loss: -2.1380, G_loss: 0.1549\n",
      "  Batch [1220/1299] D_loss: -2.7759, G_loss: 0.7280\n",
      "  Batch [1230/1299] D_loss: -1.1425, G_loss: -0.3152\n",
      "  Batch [1240/1299] D_loss: -3.1417, G_loss: 0.4486\n",
      "  Batch [1250/1299] D_loss: -2.3104, G_loss: 2.9492\n",
      "  Batch [1260/1299] D_loss: -2.2207, G_loss: 2.3242\n",
      "  Batch [1270/1299] D_loss: -1.8280, G_loss: 1.3105\n",
      "  Batch [1280/1299] D_loss: -3.5025, G_loss: 1.0271\n",
      "  Batch [1290/1299] D_loss: -3.4501, G_loss: 1.2543\n",
      "\n",
      "Epoch 46 Summary:\n",
      "  Average D_loss: -2.0160\n",
      "  Average G_loss: 1.3915\n",
      "\n",
      "Epoch [47/100]\n",
      "  Batch [0/1299] D_loss: -2.3429, G_loss: 0.2683\n",
      "  Batch [10/1299] D_loss: -3.6141, G_loss: 2.0005\n",
      "  Batch [20/1299] D_loss: -4.9265, G_loss: 2.1066\n",
      "  Batch [30/1299] D_loss: -0.3287, G_loss: 0.7221\n",
      "  Batch [40/1299] D_loss: -1.9987, G_loss: 3.6230\n",
      "  Batch [50/1299] D_loss: -2.4777, G_loss: 2.5704\n",
      "  Batch [60/1299] D_loss: -2.6322, G_loss: 3.2664\n",
      "  Batch [70/1299] D_loss: -1.5505, G_loss: 5.6577\n",
      "  Batch [80/1299] D_loss: -1.3467, G_loss: 3.4001\n",
      "  Batch [90/1299] D_loss: -2.0453, G_loss: 3.5265\n",
      "  Batch [100/1299] D_loss: -4.2035, G_loss: 2.0952\n",
      "  Batch [110/1299] D_loss: -0.1870, G_loss: 5.1977\n",
      "  Batch [120/1299] D_loss: -1.3896, G_loss: 4.4295\n",
      "  Batch [130/1299] D_loss: -2.5611, G_loss: 3.7552\n",
      "  Batch [140/1299] D_loss: -4.4361, G_loss: 3.4044\n",
      "  Batch [150/1299] D_loss: -3.3934, G_loss: 2.2616\n",
      "  Batch [160/1299] D_loss: -2.9102, G_loss: 3.4133\n",
      "  Batch [170/1299] D_loss: -0.8981, G_loss: 2.1243\n",
      "  Batch [180/1299] D_loss: -2.8230, G_loss: 4.0809\n",
      "  Batch [190/1299] D_loss: -2.0939, G_loss: 2.3943\n",
      "  Batch [200/1299] D_loss: -2.8039, G_loss: 3.8987\n",
      "  Batch [210/1299] D_loss: -0.2336, G_loss: 5.2934\n",
      "  Batch [220/1299] D_loss: -2.7166, G_loss: 5.1547\n",
      "  Batch [230/1299] D_loss: -1.9200, G_loss: 2.5653\n",
      "  Batch [240/1299] D_loss: -2.8455, G_loss: 2.3295\n",
      "  Batch [250/1299] D_loss: -1.4106, G_loss: 2.0242\n",
      "  Batch [260/1299] D_loss: -2.6494, G_loss: 3.3032\n",
      "  Batch [270/1299] D_loss: -1.6617, G_loss: 1.4794\n",
      "  Batch [280/1299] D_loss: -2.4145, G_loss: 0.5386\n",
      "  Batch [290/1299] D_loss: -2.8676, G_loss: 0.3397\n",
      "  Batch [300/1299] D_loss: -1.5926, G_loss: 0.3006\n",
      "  Batch [310/1299] D_loss: -1.3395, G_loss: 0.1388\n",
      "  Batch [320/1299] D_loss: -3.6303, G_loss: -0.3066\n",
      "  Batch [330/1299] D_loss: -2.6811, G_loss: 0.1425\n",
      "  Batch [340/1299] D_loss: -4.4680, G_loss: -1.4762\n",
      "  Batch [350/1299] D_loss: -2.2245, G_loss: -0.3258\n",
      "  Batch [360/1299] D_loss: 0.0215, G_loss: 0.9937\n",
      "  Batch [370/1299] D_loss: -2.8059, G_loss: 1.6668\n",
      "  Batch [380/1299] D_loss: -3.7153, G_loss: 2.2087\n",
      "  Batch [390/1299] D_loss: -2.5481, G_loss: 1.8646\n",
      "  Batch [400/1299] D_loss: -0.7972, G_loss: 0.4583\n",
      "  Batch [410/1299] D_loss: 0.0093, G_loss: -0.7709\n",
      "  Batch [420/1299] D_loss: -0.0886, G_loss: 0.9145\n",
      "  Batch [430/1299] D_loss: -4.2870, G_loss: -1.4148\n",
      "  Batch [440/1299] D_loss: -1.3131, G_loss: 0.1484\n",
      "  Batch [450/1299] D_loss: -2.0491, G_loss: 0.5373\n",
      "  Batch [460/1299] D_loss: -2.6253, G_loss: 0.4227\n",
      "  Batch [470/1299] D_loss: -1.5653, G_loss: -1.5353\n",
      "  Batch [480/1299] D_loss: -4.5605, G_loss: 0.7640\n",
      "  Batch [490/1299] D_loss: -2.3428, G_loss: 1.8169\n",
      "  Batch [500/1299] D_loss: -1.5373, G_loss: 2.4016\n",
      "  Batch [510/1299] D_loss: -1.5754, G_loss: 2.8816\n",
      "  Batch [520/1299] D_loss: -2.1766, G_loss: 1.3546\n",
      "  Batch [530/1299] D_loss: -2.3749, G_loss: 1.6941\n",
      "  Batch [540/1299] D_loss: -4.6261, G_loss: 2.8262\n",
      "  Batch [550/1299] D_loss: -1.0833, G_loss: 1.5475\n",
      "  Batch [560/1299] D_loss: -2.4844, G_loss: 0.8013\n",
      "  Batch [570/1299] D_loss: -1.9952, G_loss: 1.0231\n",
      "  Batch [580/1299] D_loss: -0.4400, G_loss: 3.1572\n",
      "  Batch [590/1299] D_loss: -1.9499, G_loss: -0.2764\n",
      "  Batch [600/1299] D_loss: -2.6924, G_loss: 1.0657\n",
      "  Batch [610/1299] D_loss: -1.4430, G_loss: 1.6186\n",
      "  Batch [620/1299] D_loss: -4.0085, G_loss: 1.1784\n",
      "  Batch [630/1299] D_loss: 0.9650, G_loss: 1.8851\n",
      "  Batch [640/1299] D_loss: -1.4095, G_loss: 0.7360\n",
      "  Batch [650/1299] D_loss: -1.3947, G_loss: 1.2196\n",
      "  Batch [660/1299] D_loss: -2.3879, G_loss: 0.9642\n",
      "  Batch [670/1299] D_loss: -3.3324, G_loss: 2.7463\n",
      "  Batch [680/1299] D_loss: -3.2418, G_loss: 2.8041\n",
      "  Batch [690/1299] D_loss: -1.4374, G_loss: 0.3162\n",
      "  Batch [700/1299] D_loss: -1.1433, G_loss: -0.7880\n",
      "  Batch [710/1299] D_loss: -2.0789, G_loss: 0.3734\n",
      "  Batch [720/1299] D_loss: -1.3449, G_loss: 1.6782\n",
      "  Batch [730/1299] D_loss: 0.1046, G_loss: 4.1819\n",
      "  Batch [740/1299] D_loss: -1.6728, G_loss: 1.9600\n",
      "  Batch [750/1299] D_loss: -1.2490, G_loss: 0.4568\n",
      "  Batch [760/1299] D_loss: -2.8106, G_loss: -0.4432\n",
      "  Batch [770/1299] D_loss: -1.1976, G_loss: -1.8353\n",
      "  Batch [780/1299] D_loss: -1.1243, G_loss: 1.5702\n",
      "  Batch [790/1299] D_loss: -3.8922, G_loss: 1.9501\n",
      "  Batch [800/1299] D_loss: -1.6827, G_loss: 3.8753\n",
      "  Batch [810/1299] D_loss: -2.7654, G_loss: 2.4001\n",
      "  Batch [820/1299] D_loss: -2.3381, G_loss: 3.5668\n",
      "  Batch [830/1299] D_loss: -2.8283, G_loss: 0.4448\n",
      "  Batch [840/1299] D_loss: -2.2068, G_loss: 1.0123\n",
      "  Batch [850/1299] D_loss: 1.1431, G_loss: 3.6843\n",
      "  Batch [860/1299] D_loss: -1.4687, G_loss: -1.0140\n",
      "  Batch [870/1299] D_loss: -1.6901, G_loss: 3.4346\n",
      "  Batch [880/1299] D_loss: -1.7967, G_loss: 2.9119\n",
      "  Batch [890/1299] D_loss: -2.4225, G_loss: 3.3966\n",
      "  Batch [900/1299] D_loss: -1.3144, G_loss: 4.4691\n",
      "  Batch [910/1299] D_loss: -0.9577, G_loss: 1.4927\n",
      "  Batch [920/1299] D_loss: -0.6843, G_loss: 2.7330\n",
      "  Batch [930/1299] D_loss: -1.9088, G_loss: 3.5328\n",
      "  Batch [940/1299] D_loss: -4.5687, G_loss: 3.9330\n",
      "  Batch [950/1299] D_loss: -2.6391, G_loss: 2.9227\n",
      "  Batch [960/1299] D_loss: 0.0238, G_loss: 2.2288\n",
      "  Batch [970/1299] D_loss: -2.8900, G_loss: 3.0249\n",
      "  Batch [980/1299] D_loss: -1.2603, G_loss: 2.9230\n",
      "  Batch [990/1299] D_loss: -1.2830, G_loss: 1.4198\n",
      "  Batch [1000/1299] D_loss: -2.9161, G_loss: 1.5572\n",
      "  Batch [1010/1299] D_loss: -4.0382, G_loss: -1.0738\n",
      "  Batch [1020/1299] D_loss: -1.5975, G_loss: -0.4691\n",
      "  Batch [1030/1299] D_loss: -1.5960, G_loss: 1.4281\n",
      "  Batch [1040/1299] D_loss: -3.8945, G_loss: -0.0159\n",
      "  Batch [1050/1299] D_loss: -1.8158, G_loss: 0.0473\n",
      "  Batch [1060/1299] D_loss: -1.7983, G_loss: 0.2440\n",
      "  Batch [1070/1299] D_loss: -1.6165, G_loss: 1.6078\n",
      "  Batch [1080/1299] D_loss: -1.1801, G_loss: 0.4188\n",
      "  Batch [1090/1299] D_loss: -1.4347, G_loss: -0.0613\n",
      "  Batch [1100/1299] D_loss: -4.4851, G_loss: 1.8470\n",
      "  Batch [1110/1299] D_loss: -0.8622, G_loss: -1.8366\n",
      "  Batch [1120/1299] D_loss: -2.3056, G_loss: -2.1190\n",
      "  Batch [1130/1299] D_loss: -2.6582, G_loss: -3.7547\n",
      "  Batch [1140/1299] D_loss: -2.5885, G_loss: -2.9696\n",
      "  Batch [1150/1299] D_loss: -1.2635, G_loss: -1.9541\n",
      "  Batch [1160/1299] D_loss: -1.5651, G_loss: -1.6926\n",
      "  Batch [1170/1299] D_loss: -1.9692, G_loss: 1.3171\n",
      "  Batch [1180/1299] D_loss: -3.2244, G_loss: 0.5343\n",
      "  Batch [1190/1299] D_loss: -2.0834, G_loss: -0.1978\n",
      "  Batch [1200/1299] D_loss: -3.1756, G_loss: -1.2811\n",
      "  Batch [1210/1299] D_loss: -1.4518, G_loss: 0.3988\n",
      "  Batch [1220/1299] D_loss: -1.2013, G_loss: 1.8581\n",
      "  Batch [1230/1299] D_loss: -4.3617, G_loss: 0.7686\n",
      "  Batch [1240/1299] D_loss: -3.4620, G_loss: 2.2198\n",
      "  Batch [1250/1299] D_loss: -2.9187, G_loss: 2.0278\n",
      "  Batch [1260/1299] D_loss: -1.2761, G_loss: 1.5052\n",
      "  Batch [1270/1299] D_loss: -2.7837, G_loss: 0.8125\n",
      "  Batch [1280/1299] D_loss: -2.6842, G_loss: -0.8209\n",
      "  Batch [1290/1299] D_loss: -1.1234, G_loss: 1.0836\n",
      "\n",
      "Epoch 47 Summary:\n",
      "  Average D_loss: -2.0200\n",
      "  Average G_loss: 1.3822\n",
      "\n",
      "Epoch [48/100]\n",
      "  Batch [0/1299] D_loss: -2.5900, G_loss: -0.2217\n",
      "  Batch [10/1299] D_loss: -1.8959, G_loss: 1.3772\n",
      "  Batch [20/1299] D_loss: -1.5407, G_loss: -0.2711\n",
      "  Batch [30/1299] D_loss: -3.0151, G_loss: 1.3106\n",
      "  Batch [40/1299] D_loss: -1.4168, G_loss: -0.9075\n",
      "  Batch [50/1299] D_loss: -2.4160, G_loss: -2.8879\n",
      "  Batch [60/1299] D_loss: -0.3899, G_loss: -1.9323\n",
      "  Batch [70/1299] D_loss: -2.5172, G_loss: -1.3312\n",
      "  Batch [80/1299] D_loss: -0.7684, G_loss: -3.8069\n",
      "  Batch [90/1299] D_loss: -1.6593, G_loss: -1.2344\n",
      "  Batch [100/1299] D_loss: -2.0596, G_loss: -0.2528\n",
      "  Batch [110/1299] D_loss: -1.8844, G_loss: 0.0546\n",
      "  Batch [120/1299] D_loss: -0.5245, G_loss: -0.8851\n",
      "  Batch [130/1299] D_loss: -3.6645, G_loss: 0.0195\n",
      "  Batch [140/1299] D_loss: -1.2259, G_loss: 1.4690\n",
      "  Batch [150/1299] D_loss: -2.0525, G_loss: 0.5875\n",
      "  Batch [160/1299] D_loss: -1.7128, G_loss: -1.3568\n",
      "  Batch [170/1299] D_loss: -1.1657, G_loss: -1.2086\n",
      "  Batch [180/1299] D_loss: -2.6576, G_loss: 0.7839\n",
      "  Batch [190/1299] D_loss: -1.6705, G_loss: 1.6405\n",
      "  Batch [200/1299] D_loss: -2.8496, G_loss: 0.5522\n",
      "  Batch [210/1299] D_loss: -2.6208, G_loss: 0.3318\n",
      "  Batch [220/1299] D_loss: -3.1258, G_loss: 0.4536\n",
      "  Batch [230/1299] D_loss: -2.3059, G_loss: 0.9780\n",
      "  Batch [240/1299] D_loss: -3.0486, G_loss: -0.1108\n",
      "  Batch [250/1299] D_loss: -1.7502, G_loss: 1.8036\n",
      "  Batch [260/1299] D_loss: -3.3757, G_loss: 0.4827\n",
      "  Batch [270/1299] D_loss: -0.9997, G_loss: -0.8681\n",
      "  Batch [280/1299] D_loss: -1.0299, G_loss: -0.9115\n",
      "  Batch [290/1299] D_loss: -1.4107, G_loss: -1.7132\n",
      "  Batch [300/1299] D_loss: -1.6999, G_loss: -1.4921\n",
      "  Batch [310/1299] D_loss: -2.4064, G_loss: -0.3784\n",
      "  Batch [320/1299] D_loss: -3.3858, G_loss: -1.1025\n",
      "  Batch [330/1299] D_loss: -1.7561, G_loss: 3.4831\n",
      "  Batch [340/1299] D_loss: -2.4105, G_loss: 2.7022\n",
      "  Batch [350/1299] D_loss: -1.4719, G_loss: 2.0614\n",
      "  Batch [360/1299] D_loss: -3.4448, G_loss: 1.4841\n",
      "  Batch [370/1299] D_loss: -2.3662, G_loss: 1.9864\n",
      "  Batch [380/1299] D_loss: -4.4140, G_loss: 5.1627\n",
      "  Batch [390/1299] D_loss: -3.3911, G_loss: 3.8162\n",
      "  Batch [400/1299] D_loss: -4.2243, G_loss: 4.3788\n",
      "  Batch [410/1299] D_loss: -3.0136, G_loss: 4.0946\n",
      "  Batch [420/1299] D_loss: -2.3605, G_loss: 2.0366\n",
      "  Batch [430/1299] D_loss: -2.0159, G_loss: 3.4311\n",
      "  Batch [440/1299] D_loss: -3.0892, G_loss: 3.4687\n",
      "  Batch [450/1299] D_loss: -3.1713, G_loss: 1.6348\n",
      "  Batch [460/1299] D_loss: -0.6035, G_loss: 2.4489\n",
      "  Batch [470/1299] D_loss: -2.3736, G_loss: 3.8378\n",
      "  Batch [480/1299] D_loss: -1.2349, G_loss: 1.8729\n",
      "  Batch [490/1299] D_loss: -1.8460, G_loss: 4.9568\n",
      "  Batch [500/1299] D_loss: -2.4417, G_loss: -0.2086\n",
      "  Batch [510/1299] D_loss: -0.4298, G_loss: 0.8595\n",
      "  Batch [520/1299] D_loss: -2.1803, G_loss: 2.1341\n",
      "  Batch [530/1299] D_loss: -1.9513, G_loss: -0.5809\n",
      "  Batch [540/1299] D_loss: -1.4898, G_loss: 0.7258\n",
      "  Batch [550/1299] D_loss: -2.7699, G_loss: 2.9734\n",
      "  Batch [560/1299] D_loss: -1.0618, G_loss: 1.2846\n",
      "  Batch [570/1299] D_loss: -2.9861, G_loss: 1.2034\n",
      "  Batch [580/1299] D_loss: -4.9188, G_loss: 0.9186\n",
      "  Batch [590/1299] D_loss: 0.0752, G_loss: 2.4380\n",
      "  Batch [600/1299] D_loss: -2.9820, G_loss: 4.9458\n",
      "  Batch [610/1299] D_loss: -2.9210, G_loss: 2.0961\n",
      "  Batch [620/1299] D_loss: 0.5137, G_loss: 2.1288\n",
      "  Batch [630/1299] D_loss: 0.0384, G_loss: 0.3627\n",
      "  Batch [640/1299] D_loss: -3.4162, G_loss: 1.9807\n",
      "  Batch [650/1299] D_loss: -0.5708, G_loss: 2.4951\n",
      "  Batch [660/1299] D_loss: -1.8045, G_loss: 3.1277\n",
      "  Batch [670/1299] D_loss: -2.5421, G_loss: 3.5202\n",
      "  Batch [680/1299] D_loss: -2.5326, G_loss: 1.2213\n",
      "  Batch [690/1299] D_loss: -4.7459, G_loss: 0.6797\n",
      "  Batch [700/1299] D_loss: -2.5763, G_loss: 3.4300\n",
      "  Batch [710/1299] D_loss: -1.6441, G_loss: 3.6356\n",
      "  Batch [720/1299] D_loss: -4.1049, G_loss: 3.1676\n",
      "  Batch [730/1299] D_loss: -2.2918, G_loss: 3.4800\n",
      "  Batch [740/1299] D_loss: -2.9307, G_loss: 3.1010\n",
      "  Batch [750/1299] D_loss: -2.2438, G_loss: 3.6898\n",
      "  Batch [760/1299] D_loss: -1.2286, G_loss: 3.8650\n",
      "  Batch [770/1299] D_loss: -1.7940, G_loss: 3.0385\n",
      "  Batch [780/1299] D_loss: -1.9901, G_loss: 3.0352\n",
      "  Batch [790/1299] D_loss: -1.0896, G_loss: 4.9733\n",
      "  Batch [800/1299] D_loss: -2.9263, G_loss: 2.4197\n",
      "  Batch [810/1299] D_loss: -2.2312, G_loss: 0.9535\n",
      "  Batch [820/1299] D_loss: -2.5372, G_loss: 0.8337\n",
      "  Batch [830/1299] D_loss: -4.2490, G_loss: -0.1681\n",
      "  Batch [840/1299] D_loss: -2.5298, G_loss: -1.0016\n",
      "  Batch [850/1299] D_loss: -2.2644, G_loss: -2.2807\n",
      "  Batch [860/1299] D_loss: -3.1692, G_loss: -0.8683\n",
      "  Batch [870/1299] D_loss: -3.7631, G_loss: -0.8359\n",
      "  Batch [880/1299] D_loss: 0.2460, G_loss: -0.4010\n",
      "  Batch [890/1299] D_loss: -1.8126, G_loss: -0.8277\n",
      "  Batch [900/1299] D_loss: -3.2465, G_loss: -1.4620\n",
      "  Batch [910/1299] D_loss: -2.4490, G_loss: 0.4897\n",
      "  Batch [920/1299] D_loss: -2.3651, G_loss: 0.4821\n",
      "  Batch [930/1299] D_loss: -0.7992, G_loss: 2.1462\n",
      "  Batch [940/1299] D_loss: -3.6884, G_loss: 1.3055\n",
      "  Batch [950/1299] D_loss: -1.1100, G_loss: -0.3327\n",
      "  Batch [960/1299] D_loss: -1.9022, G_loss: -0.8079\n",
      "  Batch [970/1299] D_loss: -2.0387, G_loss: 1.1928\n",
      "  Batch [980/1299] D_loss: -2.1426, G_loss: 1.4355\n",
      "  Batch [990/1299] D_loss: -2.6752, G_loss: 2.5666\n",
      "  Batch [1000/1299] D_loss: -1.5276, G_loss: 4.1313\n",
      "  Batch [1010/1299] D_loss: -0.7672, G_loss: 2.6065\n",
      "  Batch [1020/1299] D_loss: -3.6514, G_loss: 4.2828\n",
      "  Batch [1030/1299] D_loss: -2.3976, G_loss: 2.7194\n",
      "  Batch [1040/1299] D_loss: -2.9943, G_loss: 1.9154\n",
      "  Batch [1050/1299] D_loss: -0.6282, G_loss: 2.4063\n",
      "  Batch [1060/1299] D_loss: -3.2555, G_loss: 2.8637\n",
      "  Batch [1070/1299] D_loss: -2.0589, G_loss: 3.1344\n",
      "  Batch [1080/1299] D_loss: -1.1266, G_loss: 1.3408\n",
      "  Batch [1090/1299] D_loss: -2.1152, G_loss: 3.9977\n",
      "  Batch [1100/1299] D_loss: -1.8758, G_loss: 2.1597\n",
      "  Batch [1110/1299] D_loss: -4.8522, G_loss: 1.4975\n",
      "  Batch [1120/1299] D_loss: -3.9106, G_loss: 1.0620\n",
      "  Batch [1130/1299] D_loss: -2.1536, G_loss: 0.4960\n",
      "  Batch [1140/1299] D_loss: -1.1398, G_loss: 0.5331\n",
      "  Batch [1150/1299] D_loss: -2.3666, G_loss: 2.9379\n",
      "  Batch [1160/1299] D_loss: -2.6453, G_loss: 0.5303\n",
      "  Batch [1170/1299] D_loss: -1.6115, G_loss: -0.5339\n",
      "  Batch [1180/1299] D_loss: -2.6835, G_loss: -0.3985\n",
      "  Batch [1190/1299] D_loss: -1.9231, G_loss: 1.0570\n",
      "  Batch [1200/1299] D_loss: -0.5326, G_loss: 0.1147\n",
      "  Batch [1210/1299] D_loss: 0.0198, G_loss: 2.8416\n",
      "  Batch [1220/1299] D_loss: -1.5680, G_loss: 1.3471\n",
      "  Batch [1230/1299] D_loss: -1.3122, G_loss: 0.1126\n",
      "  Batch [1240/1299] D_loss: 0.2425, G_loss: -0.2055\n",
      "  Batch [1250/1299] D_loss: -2.4467, G_loss: -1.4465\n",
      "  Batch [1260/1299] D_loss: -1.2547, G_loss: -3.1132\n",
      "  Batch [1270/1299] D_loss: -2.3047, G_loss: -2.7500\n",
      "  Batch [1280/1299] D_loss: -2.6342, G_loss: -2.5069\n",
      "  Batch [1290/1299] D_loss: -0.8662, G_loss: 1.8826\n",
      "\n",
      "Epoch 48 Summary:\n",
      "  Average D_loss: -2.0259\n",
      "  Average G_loss: 1.1123\n",
      "\n",
      "Epoch [49/100]\n",
      "  Batch [0/1299] D_loss: -2.5316, G_loss: 1.8617\n",
      "  Batch [10/1299] D_loss: -1.1103, G_loss: 2.9176\n",
      "  Batch [20/1299] D_loss: -1.9947, G_loss: 2.9509\n",
      "  Batch [30/1299] D_loss: -2.0480, G_loss: 0.7839\n",
      "  Batch [40/1299] D_loss: -2.6731, G_loss: 2.6106\n",
      "  Batch [50/1299] D_loss: -0.5373, G_loss: 3.4645\n",
      "  Batch [60/1299] D_loss: -0.3745, G_loss: 4.2659\n",
      "  Batch [70/1299] D_loss: -0.0154, G_loss: 2.5255\n",
      "  Batch [80/1299] D_loss: -2.2043, G_loss: 1.4281\n",
      "  Batch [90/1299] D_loss: -2.2412, G_loss: 1.7222\n",
      "  Batch [100/1299] D_loss: -2.1008, G_loss: 2.1330\n",
      "  Batch [110/1299] D_loss: -3.4993, G_loss: 0.9224\n",
      "  Batch [120/1299] D_loss: -1.6452, G_loss: -0.2025\n",
      "  Batch [130/1299] D_loss: -2.4459, G_loss: -0.5944\n",
      "  Batch [140/1299] D_loss: -1.7513, G_loss: 0.4925\n",
      "  Batch [150/1299] D_loss: -3.4055, G_loss: 0.3384\n",
      "  Batch [160/1299] D_loss: -0.4198, G_loss: 0.9344\n",
      "  Batch [170/1299] D_loss: -1.4776, G_loss: 0.0359\n",
      "  Batch [180/1299] D_loss: -1.7366, G_loss: 0.9625\n",
      "  Batch [190/1299] D_loss: -1.7286, G_loss: 1.3654\n",
      "  Batch [200/1299] D_loss: -3.4519, G_loss: 2.6335\n",
      "  Batch [210/1299] D_loss: -1.4140, G_loss: 3.2534\n",
      "  Batch [220/1299] D_loss: -0.9889, G_loss: -0.1606\n",
      "  Batch [230/1299] D_loss: -4.5010, G_loss: 1.0763\n",
      "  Batch [240/1299] D_loss: -2.0883, G_loss: 0.6583\n",
      "  Batch [250/1299] D_loss: -1.1249, G_loss: 2.6053\n",
      "  Batch [260/1299] D_loss: -2.1598, G_loss: -0.1931\n",
      "  Batch [270/1299] D_loss: -3.6969, G_loss: -2.7351\n",
      "  Batch [280/1299] D_loss: 0.0397, G_loss: 0.7502\n",
      "  Batch [290/1299] D_loss: -0.8485, G_loss: -1.2117\n",
      "  Batch [300/1299] D_loss: -3.4679, G_loss: 1.1746\n",
      "  Batch [310/1299] D_loss: -1.8785, G_loss: 2.3768\n",
      "  Batch [320/1299] D_loss: -2.7433, G_loss: 0.6444\n",
      "  Batch [330/1299] D_loss: -2.6249, G_loss: 2.9821\n",
      "  Batch [340/1299] D_loss: -2.5152, G_loss: 2.0094\n",
      "  Batch [350/1299] D_loss: -2.4696, G_loss: 1.5210\n",
      "  Batch [360/1299] D_loss: -1.6200, G_loss: 0.6655\n",
      "  Batch [370/1299] D_loss: -0.8657, G_loss: 0.1284\n",
      "  Batch [380/1299] D_loss: -3.0155, G_loss: -2.9498\n",
      "  Batch [390/1299] D_loss: -2.7100, G_loss: -1.2320\n",
      "  Batch [400/1299] D_loss: -2.9940, G_loss: -3.5756\n",
      "  Batch [410/1299] D_loss: -2.3287, G_loss: -3.5757\n",
      "  Batch [420/1299] D_loss: -1.6003, G_loss: -5.7749\n",
      "  Batch [430/1299] D_loss: -2.0683, G_loss: -3.1502\n",
      "  Batch [440/1299] D_loss: -2.0124, G_loss: -3.4898\n",
      "  Batch [450/1299] D_loss: -2.5067, G_loss: -3.4978\n",
      "  Batch [460/1299] D_loss: -1.6190, G_loss: 1.0106\n",
      "  Batch [470/1299] D_loss: -3.1697, G_loss: 0.6632\n",
      "  Batch [480/1299] D_loss: -2.1711, G_loss: -0.5944\n",
      "  Batch [490/1299] D_loss: -1.5705, G_loss: -0.7344\n",
      "  Batch [500/1299] D_loss: -0.6099, G_loss: -1.2280\n",
      "  Batch [510/1299] D_loss: -1.2641, G_loss: -0.4680\n",
      "  Batch [520/1299] D_loss: -1.7053, G_loss: -0.8178\n",
      "  Batch [530/1299] D_loss: -1.0158, G_loss: -0.0934\n",
      "  Batch [540/1299] D_loss: -1.8429, G_loss: 0.2355\n",
      "  Batch [550/1299] D_loss: -1.7144, G_loss: 0.6829\n",
      "  Batch [560/1299] D_loss: -2.8854, G_loss: 2.0302\n",
      "  Batch [570/1299] D_loss: -3.8632, G_loss: 2.0489\n",
      "  Batch [580/1299] D_loss: -0.9354, G_loss: 2.4223\n",
      "  Batch [590/1299] D_loss: -1.3750, G_loss: -0.0669\n",
      "  Batch [600/1299] D_loss: -3.3985, G_loss: 3.6143\n",
      "  Batch [610/1299] D_loss: -2.8915, G_loss: -0.3559\n",
      "  Batch [620/1299] D_loss: -2.9314, G_loss: -1.7615\n",
      "  Batch [630/1299] D_loss: -1.4074, G_loss: -1.4001\n",
      "  Batch [640/1299] D_loss: -1.7677, G_loss: 3.7639\n",
      "  Batch [650/1299] D_loss: -2.4572, G_loss: 2.2722\n",
      "  Batch [660/1299] D_loss: -2.1230, G_loss: -0.7482\n",
      "  Batch [670/1299] D_loss: -2.9128, G_loss: 0.2468\n",
      "  Batch [680/1299] D_loss: -1.8156, G_loss: 0.4669\n",
      "  Batch [690/1299] D_loss: -3.6991, G_loss: 1.6322\n",
      "  Batch [700/1299] D_loss: -1.9935, G_loss: 0.1204\n",
      "  Batch [710/1299] D_loss: -2.2612, G_loss: -0.8251\n",
      "  Batch [720/1299] D_loss: -3.4485, G_loss: -0.4231\n",
      "  Batch [730/1299] D_loss: -2.8480, G_loss: 1.5240\n",
      "  Batch [740/1299] D_loss: -1.6003, G_loss: -1.3105\n",
      "  Batch [750/1299] D_loss: -2.1840, G_loss: 0.0613\n",
      "  Batch [760/1299] D_loss: -0.5740, G_loss: 0.0854\n",
      "  Batch [770/1299] D_loss: -1.6386, G_loss: -0.0088\n",
      "  Batch [780/1299] D_loss: -3.0651, G_loss: -1.4380\n",
      "  Batch [790/1299] D_loss: -2.1900, G_loss: -1.2883\n",
      "  Batch [800/1299] D_loss: -2.5509, G_loss: 0.8395\n",
      "  Batch [810/1299] D_loss: -1.1302, G_loss: 0.0323\n",
      "  Batch [820/1299] D_loss: -3.0481, G_loss: 0.7240\n",
      "  Batch [830/1299] D_loss: -1.0168, G_loss: 1.4273\n",
      "  Batch [840/1299] D_loss: -2.4065, G_loss: 1.1902\n",
      "  Batch [850/1299] D_loss: -2.5701, G_loss: -0.6268\n",
      "  Batch [860/1299] D_loss: -0.7234, G_loss: 1.4176\n",
      "  Batch [870/1299] D_loss: -3.9072, G_loss: 1.2381\n",
      "  Batch [880/1299] D_loss: -1.2813, G_loss: 0.8690\n",
      "  Batch [890/1299] D_loss: -0.4788, G_loss: -0.0362\n",
      "  Batch [900/1299] D_loss: -1.5817, G_loss: 1.0896\n",
      "  Batch [910/1299] D_loss: -2.5463, G_loss: 1.8519\n",
      "  Batch [920/1299] D_loss: -2.9470, G_loss: 2.9656\n",
      "  Batch [930/1299] D_loss: -1.6086, G_loss: 6.5353\n",
      "  Batch [940/1299] D_loss: -1.5392, G_loss: 5.7455\n",
      "  Batch [950/1299] D_loss: -0.7170, G_loss: 3.8929\n",
      "  Batch [960/1299] D_loss: -2.7197, G_loss: 2.0587\n",
      "  Batch [970/1299] D_loss: -1.3912, G_loss: 2.2802\n",
      "  Batch [980/1299] D_loss: -3.4181, G_loss: 2.4151\n",
      "  Batch [990/1299] D_loss: -2.1656, G_loss: 2.6193\n",
      "  Batch [1000/1299] D_loss: -1.0795, G_loss: 0.7645\n",
      "  Batch [1010/1299] D_loss: -2.0243, G_loss: 2.9791\n",
      "  Batch [1020/1299] D_loss: -3.2615, G_loss: 0.5195\n",
      "  Batch [1030/1299] D_loss: 0.0443, G_loss: -0.0048\n",
      "  Batch [1040/1299] D_loss: -1.8534, G_loss: 0.8485\n",
      "  Batch [1050/1299] D_loss: -1.4335, G_loss: 1.8575\n",
      "  Batch [1060/1299] D_loss: -1.1239, G_loss: 2.9559\n",
      "  Batch [1070/1299] D_loss: -2.5853, G_loss: 5.2127\n",
      "  Batch [1080/1299] D_loss: -2.9592, G_loss: 4.5682\n",
      "  Batch [1090/1299] D_loss: -2.1505, G_loss: 4.2047\n",
      "  Batch [1100/1299] D_loss: -2.4927, G_loss: 3.6424\n",
      "  Batch [1110/1299] D_loss: -2.4106, G_loss: 3.8980\n",
      "  Batch [1120/1299] D_loss: -3.6396, G_loss: 2.4614\n",
      "  Batch [1130/1299] D_loss: -1.5440, G_loss: 2.6692\n",
      "  Batch [1140/1299] D_loss: -0.4128, G_loss: 2.0045\n",
      "  Batch [1150/1299] D_loss: -1.4663, G_loss: 2.6247\n",
      "  Batch [1160/1299] D_loss: -2.3893, G_loss: 2.2172\n",
      "  Batch [1170/1299] D_loss: -2.3338, G_loss: 2.9942\n",
      "  Batch [1180/1299] D_loss: -2.2651, G_loss: 1.3831\n",
      "  Batch [1190/1299] D_loss: -0.9728, G_loss: 3.5299\n",
      "  Batch [1200/1299] D_loss: -1.2188, G_loss: 1.6140\n",
      "  Batch [1210/1299] D_loss: -1.9109, G_loss: -1.1092\n",
      "  Batch [1220/1299] D_loss: -1.9361, G_loss: -2.8917\n",
      "  Batch [1230/1299] D_loss: -3.3501, G_loss: 2.7248\n",
      "  Batch [1240/1299] D_loss: -2.4583, G_loss: -0.4021\n",
      "  Batch [1250/1299] D_loss: -2.2322, G_loss: -0.3950\n",
      "  Batch [1260/1299] D_loss: -1.8718, G_loss: -1.8466\n",
      "  Batch [1270/1299] D_loss: -1.3884, G_loss: 0.7134\n",
      "  Batch [1280/1299] D_loss: -1.8033, G_loss: 3.5333\n",
      "  Batch [1290/1299] D_loss: 0.0431, G_loss: 4.1342\n",
      "\n",
      "Epoch 49 Summary:\n",
      "  Average D_loss: -2.0310\n",
      "  Average G_loss: 1.0171\n",
      "\n",
      "Epoch [50/100]\n",
      "  Batch [0/1299] D_loss: -3.1289, G_loss: 5.2292\n",
      "  Batch [10/1299] D_loss: -3.2282, G_loss: 3.7365\n",
      "  Batch [20/1299] D_loss: -0.4016, G_loss: 3.5834\n",
      "  Batch [30/1299] D_loss: -2.6969, G_loss: 0.3696\n",
      "  Batch [40/1299] D_loss: -2.9534, G_loss: 0.5982\n",
      "  Batch [50/1299] D_loss: -1.9187, G_loss: 0.1500\n",
      "  Batch [60/1299] D_loss: -3.1644, G_loss: 2.5115\n",
      "  Batch [70/1299] D_loss: -3.3556, G_loss: -0.1111\n",
      "  Batch [80/1299] D_loss: -1.9205, G_loss: 2.4903\n",
      "  Batch [90/1299] D_loss: -3.3053, G_loss: 1.7485\n",
      "  Batch [100/1299] D_loss: -1.3628, G_loss: 2.4435\n",
      "  Batch [110/1299] D_loss: -1.9129, G_loss: 5.8896\n",
      "  Batch [120/1299] D_loss: -2.8970, G_loss: 5.2233\n",
      "  Batch [130/1299] D_loss: -3.1840, G_loss: 6.3064\n",
      "  Batch [140/1299] D_loss: -2.1887, G_loss: 3.6563\n",
      "  Batch [150/1299] D_loss: -2.0208, G_loss: 1.1007\n",
      "  Batch [160/1299] D_loss: -2.7458, G_loss: 0.9403\n",
      "  Batch [170/1299] D_loss: -3.0159, G_loss: 0.3385\n",
      "  Batch [180/1299] D_loss: -1.1447, G_loss: 1.9800\n",
      "  Batch [190/1299] D_loss: -3.1679, G_loss: 0.6127\n",
      "  Batch [200/1299] D_loss: -2.4206, G_loss: -2.1595\n",
      "  Batch [210/1299] D_loss: -2.5910, G_loss: 1.8588\n",
      "  Batch [220/1299] D_loss: -1.4293, G_loss: 3.6289\n",
      "  Batch [230/1299] D_loss: -1.9681, G_loss: 0.7242\n",
      "  Batch [240/1299] D_loss: -3.1256, G_loss: 3.0119\n",
      "  Batch [250/1299] D_loss: -1.9650, G_loss: 3.2148\n",
      "  Batch [260/1299] D_loss: -2.8997, G_loss: 2.0424\n",
      "  Batch [270/1299] D_loss: -3.4189, G_loss: 0.2627\n",
      "  Batch [280/1299] D_loss: -0.9464, G_loss: 2.0303\n",
      "  Batch [290/1299] D_loss: -4.9875, G_loss: 0.5153\n",
      "  Batch [300/1299] D_loss: -2.2873, G_loss: 1.2188\n",
      "  Batch [310/1299] D_loss: -2.0067, G_loss: 0.8733\n",
      "  Batch [320/1299] D_loss: -2.9454, G_loss: 0.6136\n",
      "  Batch [330/1299] D_loss: -1.8449, G_loss: -1.0326\n",
      "  Batch [340/1299] D_loss: -1.1828, G_loss: -2.5696\n",
      "  Batch [350/1299] D_loss: -3.1608, G_loss: -2.0335\n",
      "  Batch [360/1299] D_loss: -3.4677, G_loss: -3.4452\n",
      "  Batch [370/1299] D_loss: -1.5188, G_loss: -1.2756\n",
      "  Batch [380/1299] D_loss: -2.1965, G_loss: 0.2193\n",
      "  Batch [390/1299] D_loss: -1.2570, G_loss: 0.6276\n",
      "  Batch [400/1299] D_loss: -1.7583, G_loss: 1.0991\n",
      "  Batch [410/1299] D_loss: -2.2309, G_loss: 1.2629\n",
      "  Batch [420/1299] D_loss: -3.9117, G_loss: 0.8998\n",
      "  Batch [430/1299] D_loss: -3.1281, G_loss: -0.5511\n",
      "  Batch [440/1299] D_loss: -2.1997, G_loss: 0.4075\n",
      "  Batch [450/1299] D_loss: -2.9256, G_loss: 0.3025\n",
      "  Batch [460/1299] D_loss: -1.5878, G_loss: 0.3318\n",
      "  Batch [470/1299] D_loss: -2.4380, G_loss: 0.9671\n",
      "  Batch [480/1299] D_loss: -0.1625, G_loss: 1.9523\n",
      "  Batch [490/1299] D_loss: -2.7915, G_loss: 2.7705\n",
      "  Batch [500/1299] D_loss: -2.4314, G_loss: 2.6049\n",
      "  Batch [510/1299] D_loss: -2.3421, G_loss: 3.1788\n",
      "  Batch [520/1299] D_loss: -1.1340, G_loss: 1.7348\n",
      "  Batch [530/1299] D_loss: -1.3207, G_loss: 3.0088\n",
      "  Batch [540/1299] D_loss: -2.0703, G_loss: 3.0447\n",
      "  Batch [550/1299] D_loss: -2.5221, G_loss: 5.0742\n",
      "  Batch [560/1299] D_loss: -2.8053, G_loss: 4.2171\n",
      "  Batch [570/1299] D_loss: -1.1282, G_loss: 2.8394\n",
      "  Batch [580/1299] D_loss: -2.8695, G_loss: -0.0230\n",
      "  Batch [590/1299] D_loss: -1.4760, G_loss: -0.5741\n",
      "  Batch [600/1299] D_loss: -0.8993, G_loss: -1.3684\n",
      "  Batch [610/1299] D_loss: -1.9273, G_loss: -1.2870\n",
      "  Batch [620/1299] D_loss: -0.4350, G_loss: -3.1092\n",
      "  Batch [630/1299] D_loss: -2.7144, G_loss: -2.0898\n",
      "  Batch [640/1299] D_loss: -3.1767, G_loss: -1.9535\n",
      "  Batch [650/1299] D_loss: -3.1588, G_loss: -3.6391\n",
      "  Batch [660/1299] D_loss: -3.7506, G_loss: -2.5170\n",
      "  Batch [670/1299] D_loss: -1.5806, G_loss: -1.7787\n",
      "  Batch [680/1299] D_loss: -2.4503, G_loss: -1.7646\n",
      "  Batch [690/1299] D_loss: -2.5539, G_loss: -3.7304\n",
      "  Batch [700/1299] D_loss: -3.3490, G_loss: -1.1519\n",
      "  Batch [710/1299] D_loss: -3.2835, G_loss: -2.7922\n",
      "  Batch [720/1299] D_loss: -2.9781, G_loss: -2.8855\n",
      "  Batch [730/1299] D_loss: -3.6504, G_loss: -0.5704\n",
      "  Batch [740/1299] D_loss: -3.1488, G_loss: -0.1328\n",
      "  Batch [750/1299] D_loss: -2.1711, G_loss: 0.7664\n",
      "  Batch [760/1299] D_loss: -2.5815, G_loss: -0.3630\n",
      "  Batch [770/1299] D_loss: -3.2617, G_loss: -2.0649\n",
      "  Batch [780/1299] D_loss: -2.9418, G_loss: -0.5995\n",
      "  Batch [790/1299] D_loss: -2.5970, G_loss: 0.7432\n",
      "  Batch [800/1299] D_loss: -3.3793, G_loss: 0.7503\n",
      "  Batch [810/1299] D_loss: -1.7508, G_loss: 0.2191\n",
      "  Batch [820/1299] D_loss: -1.8562, G_loss: 0.7125\n",
      "  Batch [830/1299] D_loss: -1.5917, G_loss: 1.3745\n",
      "  Batch [840/1299] D_loss: -1.6030, G_loss: -0.7106\n",
      "  Batch [850/1299] D_loss: -3.5401, G_loss: -0.7012\n",
      "  Batch [860/1299] D_loss: -3.0135, G_loss: -1.2319\n",
      "  Batch [870/1299] D_loss: -2.6704, G_loss: 2.2931\n",
      "  Batch [880/1299] D_loss: -2.3997, G_loss: 3.8182\n",
      "  Batch [890/1299] D_loss: -0.2911, G_loss: 2.9298\n",
      "  Batch [900/1299] D_loss: -2.2968, G_loss: 3.3892\n",
      "  Batch [910/1299] D_loss: -2.1348, G_loss: 3.7201\n",
      "  Batch [920/1299] D_loss: -2.2881, G_loss: 2.1972\n",
      "  Batch [930/1299] D_loss: -2.2274, G_loss: 4.0325\n",
      "  Batch [940/1299] D_loss: -1.1417, G_loss: 0.6158\n",
      "  Batch [950/1299] D_loss: -1.6964, G_loss: 0.4627\n",
      "  Batch [960/1299] D_loss: -2.6302, G_loss: -0.2752\n",
      "  Batch [970/1299] D_loss: -1.4969, G_loss: 0.9255\n",
      "  Batch [980/1299] D_loss: -3.3927, G_loss: 2.8292\n",
      "  Batch [990/1299] D_loss: -1.2890, G_loss: 7.0051\n",
      "  Batch [1000/1299] D_loss: -3.4514, G_loss: 5.9583\n",
      "  Batch [1010/1299] D_loss: -1.7895, G_loss: 4.2261\n",
      "  Batch [1020/1299] D_loss: -2.7105, G_loss: 2.7420\n",
      "  Batch [1030/1299] D_loss: -1.8883, G_loss: 2.9044\n",
      "  Batch [1040/1299] D_loss: -2.6128, G_loss: 0.6572\n",
      "  Batch [1050/1299] D_loss: -0.7364, G_loss: 0.9068\n",
      "  Batch [1060/1299] D_loss: -4.1943, G_loss: 0.9871\n",
      "  Batch [1070/1299] D_loss: -2.9120, G_loss: 1.3226\n",
      "  Batch [1080/1299] D_loss: -2.8076, G_loss: 1.8285\n",
      "  Batch [1090/1299] D_loss: -3.1570, G_loss: 3.8850\n",
      "  Batch [1100/1299] D_loss: -2.9391, G_loss: 1.7390\n",
      "  Batch [1110/1299] D_loss: -1.9325, G_loss: 0.6369\n",
      "  Batch [1120/1299] D_loss: -3.2417, G_loss: 1.7323\n",
      "  Batch [1130/1299] D_loss: -1.6344, G_loss: 0.3826\n",
      "  Batch [1140/1299] D_loss: -2.9431, G_loss: 0.9042\n",
      "  Batch [1150/1299] D_loss: -0.7965, G_loss: 2.1424\n",
      "  Batch [1160/1299] D_loss: -1.2108, G_loss: 0.7369\n",
      "  Batch [1170/1299] D_loss: -3.5518, G_loss: -0.0575\n",
      "  Batch [1180/1299] D_loss: -1.7362, G_loss: 0.1709\n",
      "  Batch [1190/1299] D_loss: -2.5322, G_loss: 0.5273\n",
      "  Batch [1200/1299] D_loss: -2.6352, G_loss: 1.0787\n",
      "  Batch [1210/1299] D_loss: -2.8135, G_loss: 2.8120\n",
      "  Batch [1220/1299] D_loss: -2.0127, G_loss: 2.6647\n",
      "  Batch [1230/1299] D_loss: -3.6516, G_loss: 1.1922\n",
      "  Batch [1240/1299] D_loss: -2.2156, G_loss: -0.2858\n",
      "  Batch [1250/1299] D_loss: -2.0847, G_loss: -0.7926\n",
      "  Batch [1260/1299] D_loss: -3.2516, G_loss: -0.5352\n",
      "  Batch [1270/1299] D_loss: -2.5086, G_loss: -0.8642\n",
      "  Batch [1280/1299] D_loss: -1.6329, G_loss: -2.7338\n",
      "  Batch [1290/1299] D_loss: -1.6529, G_loss: 0.0453\n",
      "\n",
      "Epoch 50 Summary:\n",
      "  Average D_loss: -2.0015\n",
      "  Average G_loss: 1.0490\n",
      "\n",
      "Epoch [51/100]\n",
      "  Batch [0/1299] D_loss: -1.7321, G_loss: 0.8746\n",
      "  Batch [10/1299] D_loss: -1.9413, G_loss: -0.9809\n",
      "  Batch [20/1299] D_loss: -2.3011, G_loss: 0.7544\n",
      "  Batch [30/1299] D_loss: -1.7136, G_loss: -0.2950\n",
      "  Batch [40/1299] D_loss: -2.7874, G_loss: 2.9097\n",
      "  Batch [50/1299] D_loss: -3.6616, G_loss: 1.9956\n",
      "  Batch [60/1299] D_loss: -1.3948, G_loss: 2.3536\n",
      "  Batch [70/1299] D_loss: -2.1152, G_loss: 1.2207\n",
      "  Batch [80/1299] D_loss: -1.5482, G_loss: 0.3606\n",
      "  Batch [90/1299] D_loss: -2.9689, G_loss: -0.0392\n",
      "  Batch [100/1299] D_loss: -3.2403, G_loss: -0.5623\n",
      "  Batch [110/1299] D_loss: -2.5792, G_loss: -1.1833\n",
      "  Batch [120/1299] D_loss: -2.5597, G_loss: 0.1966\n",
      "  Batch [130/1299] D_loss: -2.4986, G_loss: 0.5247\n",
      "  Batch [140/1299] D_loss: -1.9706, G_loss: -0.2630\n",
      "  Batch [150/1299] D_loss: -3.3949, G_loss: -0.9102\n",
      "  Batch [160/1299] D_loss: -3.3444, G_loss: -0.9043\n",
      "  Batch [170/1299] D_loss: -3.1146, G_loss: 1.3911\n",
      "  Batch [180/1299] D_loss: -2.6625, G_loss: 0.6382\n",
      "  Batch [190/1299] D_loss: -1.4398, G_loss: 3.8033\n",
      "  Batch [200/1299] D_loss: -2.3364, G_loss: 0.7283\n",
      "  Batch [210/1299] D_loss: -3.2193, G_loss: -0.5101\n",
      "  Batch [220/1299] D_loss: -2.7910, G_loss: 1.1051\n",
      "  Batch [230/1299] D_loss: -2.6608, G_loss: 0.0985\n",
      "  Batch [240/1299] D_loss: -2.4296, G_loss: -1.0645\n",
      "  Batch [250/1299] D_loss: -3.3898, G_loss: -1.0790\n",
      "  Batch [260/1299] D_loss: -3.3964, G_loss: -1.5488\n",
      "  Batch [270/1299] D_loss: -3.0058, G_loss: 0.3165\n",
      "  Batch [280/1299] D_loss: 0.1082, G_loss: 0.1397\n",
      "  Batch [290/1299] D_loss: -2.2418, G_loss: 0.6275\n",
      "  Batch [300/1299] D_loss: -2.7266, G_loss: 1.7868\n",
      "  Batch [310/1299] D_loss: -1.2732, G_loss: 2.8485\n",
      "  Batch [320/1299] D_loss: -4.2597, G_loss: 3.5082\n",
      "  Batch [330/1299] D_loss: -1.5855, G_loss: 4.0627\n",
      "  Batch [340/1299] D_loss: -3.6689, G_loss: 0.8587\n",
      "  Batch [350/1299] D_loss: -2.5229, G_loss: 0.7492\n",
      "  Batch [360/1299] D_loss: -0.8249, G_loss: -1.8832\n",
      "  Batch [370/1299] D_loss: -1.5840, G_loss: -0.9191\n",
      "  Batch [380/1299] D_loss: -0.7630, G_loss: -1.5107\n",
      "  Batch [390/1299] D_loss: -1.2876, G_loss: -1.6662\n",
      "  Batch [400/1299] D_loss: -1.8899, G_loss: -0.1913\n",
      "  Batch [410/1299] D_loss: -2.9167, G_loss: -0.0729\n",
      "  Batch [420/1299] D_loss: -0.9418, G_loss: -1.3826\n",
      "  Batch [430/1299] D_loss: -3.5154, G_loss: 0.8802\n",
      "  Batch [440/1299] D_loss: -2.9205, G_loss: 0.6292\n",
      "  Batch [450/1299] D_loss: -0.2722, G_loss: 0.5907\n",
      "  Batch [460/1299] D_loss: -2.1968, G_loss: -1.3103\n",
      "  Batch [470/1299] D_loss: -2.1881, G_loss: 0.5588\n",
      "  Batch [480/1299] D_loss: -3.4994, G_loss: 0.0674\n",
      "  Batch [490/1299] D_loss: -3.1216, G_loss: 0.3641\n",
      "  Batch [500/1299] D_loss: -3.9333, G_loss: 0.3378\n",
      "  Batch [510/1299] D_loss: -1.1816, G_loss: 1.9111\n",
      "  Batch [520/1299] D_loss: -2.2210, G_loss: 4.0512\n",
      "  Batch [530/1299] D_loss: -2.4100, G_loss: 2.4742\n",
      "  Batch [540/1299] D_loss: -2.3522, G_loss: 4.4048\n",
      "  Batch [550/1299] D_loss: -1.4391, G_loss: 2.1241\n",
      "  Batch [560/1299] D_loss: -0.1582, G_loss: 2.2802\n",
      "  Batch [570/1299] D_loss: -2.6308, G_loss: -0.1678\n",
      "  Batch [580/1299] D_loss: -1.7665, G_loss: -1.5601\n",
      "  Batch [590/1299] D_loss: -2.9368, G_loss: -2.4411\n",
      "  Batch [600/1299] D_loss: -1.6920, G_loss: -2.6976\n",
      "  Batch [610/1299] D_loss: -2.0883, G_loss: -3.0047\n",
      "  Batch [620/1299] D_loss: -0.1152, G_loss: -1.5016\n",
      "  Batch [630/1299] D_loss: -2.3908, G_loss: -4.8473\n",
      "  Batch [640/1299] D_loss: -1.0711, G_loss: -1.6303\n",
      "  Batch [650/1299] D_loss: -4.0876, G_loss: -0.2839\n",
      "  Batch [660/1299] D_loss: -1.7927, G_loss: 0.7422\n",
      "  Batch [670/1299] D_loss: -1.0755, G_loss: 1.3114\n",
      "  Batch [680/1299] D_loss: -1.3087, G_loss: 0.6607\n",
      "  Batch [690/1299] D_loss: -2.7207, G_loss: 1.4854\n",
      "  Batch [700/1299] D_loss: -1.1190, G_loss: 0.8335\n",
      "  Batch [710/1299] D_loss: -1.4222, G_loss: 0.3874\n",
      "  Batch [720/1299] D_loss: -2.1573, G_loss: -0.7547\n",
      "  Batch [730/1299] D_loss: -4.2720, G_loss: -0.4933\n",
      "  Batch [740/1299] D_loss: -2.7991, G_loss: 1.0513\n",
      "  Batch [750/1299] D_loss: -1.2453, G_loss: -0.9650\n",
      "  Batch [760/1299] D_loss: -2.2255, G_loss: 1.9011\n",
      "  Batch [770/1299] D_loss: -2.7197, G_loss: 0.6481\n",
      "  Batch [780/1299] D_loss: -1.4626, G_loss: 0.7459\n",
      "  Batch [790/1299] D_loss: -2.2530, G_loss: 1.6596\n",
      "  Batch [800/1299] D_loss: -3.0737, G_loss: 1.5020\n",
      "  Batch [810/1299] D_loss: -2.6378, G_loss: 1.1081\n",
      "  Batch [820/1299] D_loss: -0.8781, G_loss: -0.1566\n",
      "  Batch [830/1299] D_loss: -1.6022, G_loss: -0.2415\n",
      "  Batch [840/1299] D_loss: -2.2078, G_loss: -1.6207\n",
      "  Batch [850/1299] D_loss: -2.3013, G_loss: 0.1979\n",
      "  Batch [860/1299] D_loss: -1.3666, G_loss: -1.2111\n",
      "  Batch [870/1299] D_loss: -2.2277, G_loss: -2.6770\n",
      "  Batch [880/1299] D_loss: -1.3458, G_loss: -0.5651\n",
      "  Batch [890/1299] D_loss: -1.5457, G_loss: 0.3013\n",
      "  Batch [900/1299] D_loss: -2.3935, G_loss: -3.0090\n",
      "  Batch [910/1299] D_loss: -3.1021, G_loss: -0.1173\n",
      "  Batch [920/1299] D_loss: -1.3767, G_loss: 1.2407\n",
      "  Batch [930/1299] D_loss: -2.6033, G_loss: 0.2148\n",
      "  Batch [940/1299] D_loss: -1.8380, G_loss: 1.3267\n",
      "  Batch [950/1299] D_loss: -1.2556, G_loss: 3.8706\n",
      "  Batch [960/1299] D_loss: -1.9551, G_loss: 3.0170\n",
      "  Batch [970/1299] D_loss: -2.4985, G_loss: 2.1107\n",
      "  Batch [980/1299] D_loss: -1.8434, G_loss: 2.6731\n",
      "  Batch [990/1299] D_loss: -2.8369, G_loss: 0.2563\n",
      "  Batch [1000/1299] D_loss: -1.6519, G_loss: 1.1850\n",
      "  Batch [1010/1299] D_loss: -3.0597, G_loss: 0.0360\n",
      "  Batch [1020/1299] D_loss: -1.6629, G_loss: 1.6689\n",
      "  Batch [1030/1299] D_loss: -1.4261, G_loss: 1.2801\n",
      "  Batch [1040/1299] D_loss: -1.3277, G_loss: 1.6299\n",
      "  Batch [1050/1299] D_loss: -0.4049, G_loss: 2.7509\n",
      "  Batch [1060/1299] D_loss: -3.4717, G_loss: 2.2593\n",
      "  Batch [1070/1299] D_loss: -1.1077, G_loss: 1.8498\n",
      "  Batch [1080/1299] D_loss: -1.4477, G_loss: 1.2465\n",
      "  Batch [1090/1299] D_loss: -2.0109, G_loss: 0.7850\n",
      "  Batch [1100/1299] D_loss: -2.5852, G_loss: -0.9716\n",
      "  Batch [1110/1299] D_loss: -2.6979, G_loss: -0.2881\n",
      "  Batch [1120/1299] D_loss: -2.7209, G_loss: -0.5211\n",
      "  Batch [1130/1299] D_loss: -1.7152, G_loss: 0.4045\n",
      "  Batch [1140/1299] D_loss: -2.9533, G_loss: 1.8028\n",
      "  Batch [1150/1299] D_loss: -3.2203, G_loss: -0.7440\n",
      "  Batch [1160/1299] D_loss: -2.4125, G_loss: -0.5170\n",
      "  Batch [1170/1299] D_loss: -2.1456, G_loss: 2.1132\n",
      "  Batch [1180/1299] D_loss: -0.7270, G_loss: 1.9936\n",
      "  Batch [1190/1299] D_loss: -3.9309, G_loss: 2.9875\n",
      "  Batch [1200/1299] D_loss: -2.2236, G_loss: 3.5040\n",
      "  Batch [1210/1299] D_loss: -1.0373, G_loss: 1.2577\n",
      "  Batch [1220/1299] D_loss: -1.0632, G_loss: 0.8066\n",
      "  Batch [1230/1299] D_loss: -2.9079, G_loss: -0.2350\n",
      "  Batch [1240/1299] D_loss: -2.7601, G_loss: 0.0771\n",
      "  Batch [1250/1299] D_loss: -2.6438, G_loss: 1.2613\n",
      "  Batch [1260/1299] D_loss: -2.5813, G_loss: 2.5033\n",
      "  Batch [1270/1299] D_loss: -0.9622, G_loss: 1.7622\n",
      "  Batch [1280/1299] D_loss: -3.7886, G_loss: 4.6971\n",
      "  Batch [1290/1299] D_loss: -1.8323, G_loss: 4.9210\n",
      "\n",
      "Epoch 51 Summary:\n",
      "  Average D_loss: -1.9942\n",
      "  Average G_loss: 0.6045\n",
      "\n",
      "Epoch [52/100]\n",
      "  Batch [0/1299] D_loss: -1.3889, G_loss: 3.6810\n",
      "  Batch [10/1299] D_loss: -3.4723, G_loss: 3.1105\n",
      "  Batch [20/1299] D_loss: -2.4948, G_loss: 5.7114\n",
      "  Batch [30/1299] D_loss: -3.1363, G_loss: 3.2048\n",
      "  Batch [40/1299] D_loss: -2.9634, G_loss: 3.5472\n",
      "  Batch [50/1299] D_loss: -2.0208, G_loss: 1.7076\n",
      "  Batch [60/1299] D_loss: -1.4770, G_loss: 3.8484\n",
      "  Batch [70/1299] D_loss: -2.1409, G_loss: 4.6398\n",
      "  Batch [80/1299] D_loss: -5.2037, G_loss: 2.8727\n",
      "  Batch [90/1299] D_loss: -1.9505, G_loss: 3.3338\n",
      "  Batch [100/1299] D_loss: -2.7139, G_loss: 1.5990\n",
      "  Batch [110/1299] D_loss: -1.7761, G_loss: 4.2915\n",
      "  Batch [120/1299] D_loss: -4.3956, G_loss: 3.0256\n",
      "  Batch [130/1299] D_loss: -0.6804, G_loss: 3.5247\n",
      "  Batch [140/1299] D_loss: -4.3210, G_loss: 1.6279\n",
      "  Batch [150/1299] D_loss: -2.1752, G_loss: -0.0077\n",
      "  Batch [160/1299] D_loss: -1.4731, G_loss: -0.0104\n",
      "  Batch [170/1299] D_loss: -3.0182, G_loss: -1.2025\n",
      "  Batch [180/1299] D_loss: -2.0604, G_loss: -0.1311\n",
      "  Batch [190/1299] D_loss: -2.6286, G_loss: 2.4663\n",
      "  Batch [200/1299] D_loss: -3.0473, G_loss: 2.8090\n",
      "  Batch [210/1299] D_loss: 0.1930, G_loss: 1.3501\n",
      "  Batch [220/1299] D_loss: -2.9123, G_loss: 0.9680\n",
      "  Batch [230/1299] D_loss: -2.0703, G_loss: 2.9645\n",
      "  Batch [240/1299] D_loss: -1.5022, G_loss: 6.2099\n",
      "  Batch [250/1299] D_loss: -1.4727, G_loss: 4.3390\n",
      "  Batch [260/1299] D_loss: -1.1835, G_loss: 3.5434\n",
      "  Batch [270/1299] D_loss: -3.7473, G_loss: 3.7097\n",
      "  Batch [280/1299] D_loss: -2.9934, G_loss: 5.1547\n",
      "  Batch [290/1299] D_loss: -1.5912, G_loss: 2.7421\n",
      "  Batch [300/1299] D_loss: -2.1701, G_loss: 3.2626\n",
      "  Batch [310/1299] D_loss: -2.8891, G_loss: 0.5749\n",
      "  Batch [320/1299] D_loss: -3.0670, G_loss: -0.4288\n",
      "  Batch [330/1299] D_loss: -1.3411, G_loss: -0.9127\n",
      "  Batch [340/1299] D_loss: -2.2223, G_loss: -3.0970\n",
      "  Batch [350/1299] D_loss: -1.1832, G_loss: -1.0754\n",
      "  Batch [360/1299] D_loss: -1.4551, G_loss: 0.1364\n",
      "  Batch [370/1299] D_loss: -1.9769, G_loss: 1.3161\n",
      "  Batch [380/1299] D_loss: -2.4383, G_loss: 0.5136\n",
      "  Batch [390/1299] D_loss: -1.6717, G_loss: 1.9569\n",
      "  Batch [400/1299] D_loss: -0.5671, G_loss: 1.5414\n",
      "  Batch [410/1299] D_loss: -0.8359, G_loss: 1.2670\n",
      "  Batch [420/1299] D_loss: -0.7415, G_loss: 0.7211\n",
      "  Batch [430/1299] D_loss: -2.0285, G_loss: 0.3702\n",
      "  Batch [440/1299] D_loss: -3.6736, G_loss: 0.8419\n",
      "  Batch [450/1299] D_loss: -2.1091, G_loss: 1.1349\n",
      "  Batch [460/1299] D_loss: -1.6026, G_loss: -0.4985\n",
      "  Batch [470/1299] D_loss: -1.5255, G_loss: 1.3792\n",
      "  Batch [480/1299] D_loss: -1.1847, G_loss: -1.5424\n",
      "  Batch [490/1299] D_loss: -1.9316, G_loss: -0.5958\n",
      "  Batch [500/1299] D_loss: -1.5603, G_loss: 2.6434\n",
      "  Batch [510/1299] D_loss: -2.6796, G_loss: 1.9289\n",
      "  Batch [520/1299] D_loss: -1.4018, G_loss: 1.7334\n",
      "  Batch [530/1299] D_loss: -1.9439, G_loss: 3.2619\n",
      "  Batch [540/1299] D_loss: -2.3909, G_loss: 3.4477\n",
      "  Batch [550/1299] D_loss: -1.9281, G_loss: 0.0875\n",
      "  Batch [560/1299] D_loss: -3.4992, G_loss: 0.6447\n",
      "  Batch [570/1299] D_loss: -4.1441, G_loss: 0.1203\n",
      "  Batch [580/1299] D_loss: -2.4617, G_loss: -0.3343\n",
      "  Batch [590/1299] D_loss: -1.6046, G_loss: -0.3291\n",
      "  Batch [600/1299] D_loss: -2.0138, G_loss: -1.7416\n",
      "  Batch [610/1299] D_loss: -2.9026, G_loss: -2.3891\n",
      "  Batch [620/1299] D_loss: -3.5938, G_loss: -0.5516\n",
      "  Batch [630/1299] D_loss: -3.3838, G_loss: -0.1922\n",
      "  Batch [640/1299] D_loss: -1.2780, G_loss: -0.6144\n",
      "  Batch [650/1299] D_loss: -3.6335, G_loss: 0.0950\n",
      "  Batch [660/1299] D_loss: -1.5226, G_loss: -1.0600\n",
      "  Batch [670/1299] D_loss: -3.0695, G_loss: -0.1236\n",
      "  Batch [680/1299] D_loss: -2.3666, G_loss: -0.8152\n",
      "  Batch [690/1299] D_loss: -0.9875, G_loss: -1.7084\n",
      "  Batch [700/1299] D_loss: -2.9010, G_loss: -2.1608\n",
      "  Batch [710/1299] D_loss: -3.6100, G_loss: 1.2043\n",
      "  Batch [720/1299] D_loss: -0.3650, G_loss: -0.3996\n",
      "  Batch [730/1299] D_loss: -1.1589, G_loss: -0.3433\n",
      "  Batch [740/1299] D_loss: -0.6096, G_loss: 0.0115\n",
      "  Batch [750/1299] D_loss: -3.1991, G_loss: 1.5667\n",
      "  Batch [760/1299] D_loss: -2.1791, G_loss: 1.2012\n",
      "  Batch [770/1299] D_loss: -3.4286, G_loss: 0.5127\n",
      "  Batch [780/1299] D_loss: -0.3990, G_loss: -0.8302\n",
      "  Batch [790/1299] D_loss: -3.2323, G_loss: -0.9708\n",
      "  Batch [800/1299] D_loss: -1.8490, G_loss: 0.0807\n",
      "  Batch [810/1299] D_loss: -3.6850, G_loss: 0.3790\n",
      "  Batch [820/1299] D_loss: -2.0405, G_loss: -0.0210\n",
      "  Batch [830/1299] D_loss: -1.8077, G_loss: 1.4315\n",
      "  Batch [840/1299] D_loss: -0.2330, G_loss: 0.3350\n",
      "  Batch [850/1299] D_loss: -1.9884, G_loss: 3.4307\n",
      "  Batch [860/1299] D_loss: -1.7556, G_loss: 5.6891\n",
      "  Batch [870/1299] D_loss: -2.0646, G_loss: 6.9357\n",
      "  Batch [880/1299] D_loss: -2.8410, G_loss: 3.7204\n",
      "  Batch [890/1299] D_loss: -3.3862, G_loss: 3.9373\n",
      "  Batch [900/1299] D_loss: -2.5538, G_loss: 0.5310\n",
      "  Batch [910/1299] D_loss: -2.0746, G_loss: -1.1563\n",
      "  Batch [920/1299] D_loss: -3.0745, G_loss: 3.1596\n",
      "  Batch [930/1299] D_loss: 0.3187, G_loss: 0.1026\n",
      "  Batch [940/1299] D_loss: -4.3234, G_loss: 0.5740\n",
      "  Batch [950/1299] D_loss: -1.6131, G_loss: 0.3622\n",
      "  Batch [960/1299] D_loss: -2.5956, G_loss: 2.9930\n",
      "  Batch [970/1299] D_loss: -2.4782, G_loss: 2.0207\n",
      "  Batch [980/1299] D_loss: -0.3572, G_loss: 2.4588\n",
      "  Batch [990/1299] D_loss: -0.3825, G_loss: 0.9678\n",
      "  Batch [1000/1299] D_loss: -1.6491, G_loss: 0.7596\n",
      "  Batch [1010/1299] D_loss: -0.6732, G_loss: -1.3948\n",
      "  Batch [1020/1299] D_loss: -3.0939, G_loss: -0.0661\n",
      "  Batch [1030/1299] D_loss: -1.7506, G_loss: 3.2247\n",
      "  Batch [1040/1299] D_loss: -4.2748, G_loss: 1.9782\n",
      "  Batch [1050/1299] D_loss: -0.8431, G_loss: 2.4301\n",
      "  Batch [1060/1299] D_loss: -3.2897, G_loss: 3.4004\n",
      "  Batch [1070/1299] D_loss: -0.9976, G_loss: 4.0542\n",
      "  Batch [1080/1299] D_loss: -1.1983, G_loss: 5.0654\n",
      "  Batch [1090/1299] D_loss: -0.5697, G_loss: 6.2279\n",
      "  Batch [1100/1299] D_loss: -1.0031, G_loss: 7.3582\n",
      "  Batch [1110/1299] D_loss: -3.0880, G_loss: 6.1577\n",
      "  Batch [1120/1299] D_loss: -2.5559, G_loss: 3.3738\n",
      "  Batch [1130/1299] D_loss: -2.6356, G_loss: 2.8611\n",
      "  Batch [1140/1299] D_loss: -1.4275, G_loss: 3.3700\n",
      "  Batch [1150/1299] D_loss: -3.3614, G_loss: 2.8476\n",
      "  Batch [1160/1299] D_loss: -2.4725, G_loss: 1.4455\n",
      "  Batch [1170/1299] D_loss: -2.2906, G_loss: 0.3229\n",
      "  Batch [1180/1299] D_loss: -2.8202, G_loss: -2.6303\n",
      "  Batch [1190/1299] D_loss: -3.2450, G_loss: -2.4458\n",
      "  Batch [1200/1299] D_loss: -1.0286, G_loss: 0.9190\n",
      "  Batch [1210/1299] D_loss: -1.4325, G_loss: -2.1200\n",
      "  Batch [1220/1299] D_loss: -2.5579, G_loss: 0.5645\n",
      "  Batch [1230/1299] D_loss: -1.6430, G_loss: -3.1077\n",
      "  Batch [1240/1299] D_loss: -1.6790, G_loss: -1.6285\n",
      "  Batch [1250/1299] D_loss: -0.2244, G_loss: 3.0526\n",
      "  Batch [1260/1299] D_loss: -2.6973, G_loss: 0.1262\n",
      "  Batch [1270/1299] D_loss: -2.0215, G_loss: 0.3620\n",
      "  Batch [1280/1299] D_loss: -2.7496, G_loss: -0.4846\n",
      "  Batch [1290/1299] D_loss: -1.5585, G_loss: -0.3347\n",
      "\n",
      "Epoch 52 Summary:\n",
      "  Average D_loss: -1.9983\n",
      "  Average G_loss: 1.4063\n",
      "\n",
      "Epoch [53/100]\n",
      "  Batch [0/1299] D_loss: -1.6614, G_loss: -2.3835\n",
      "  Batch [10/1299] D_loss: -0.8686, G_loss: -0.2296\n",
      "  Batch [20/1299] D_loss: -1.3680, G_loss: 0.0950\n",
      "  Batch [30/1299] D_loss: -3.2158, G_loss: 1.9054\n",
      "  Batch [40/1299] D_loss: -2.3033, G_loss: -0.2650\n",
      "  Batch [50/1299] D_loss: -2.3268, G_loss: -0.0240\n",
      "  Batch [60/1299] D_loss: -2.3393, G_loss: -0.2576\n",
      "  Batch [70/1299] D_loss: -1.6726, G_loss: -1.0320\n",
      "  Batch [80/1299] D_loss: -2.3750, G_loss: -0.1098\n",
      "  Batch [90/1299] D_loss: -3.6242, G_loss: 1.5010\n",
      "  Batch [100/1299] D_loss: -2.0176, G_loss: -0.9457\n",
      "  Batch [110/1299] D_loss: -0.9420, G_loss: -0.5568\n",
      "  Batch [120/1299] D_loss: -2.8164, G_loss: -0.6671\n",
      "  Batch [130/1299] D_loss: -2.8841, G_loss: -3.6753\n",
      "  Batch [140/1299] D_loss: -4.5925, G_loss: -3.3617\n",
      "  Batch [150/1299] D_loss: -2.2314, G_loss: -4.1960\n",
      "  Batch [160/1299] D_loss: -1.8472, G_loss: -2.7213\n",
      "  Batch [170/1299] D_loss: -2.9461, G_loss: -2.7907\n",
      "  Batch [180/1299] D_loss: -1.5646, G_loss: 0.0633\n",
      "  Batch [190/1299] D_loss: 0.0476, G_loss: -0.3189\n",
      "  Batch [200/1299] D_loss: -1.9306, G_loss: -0.9007\n",
      "  Batch [210/1299] D_loss: -2.0516, G_loss: 1.0903\n",
      "  Batch [220/1299] D_loss: -1.3354, G_loss: 2.0395\n",
      "  Batch [230/1299] D_loss: -3.9817, G_loss: 3.2671\n",
      "  Batch [240/1299] D_loss: -0.3841, G_loss: 0.7608\n",
      "  Batch [250/1299] D_loss: -2.7445, G_loss: 1.0566\n",
      "  Batch [260/1299] D_loss: -3.2007, G_loss: -0.2235\n",
      "  Batch [270/1299] D_loss: -2.4625, G_loss: -1.3864\n",
      "  Batch [280/1299] D_loss: -2.4560, G_loss: 0.0421\n",
      "  Batch [290/1299] D_loss: -1.8754, G_loss: -0.7894\n",
      "  Batch [300/1299] D_loss: -2.2938, G_loss: -1.4665\n",
      "  Batch [310/1299] D_loss: -1.3096, G_loss: -1.6174\n",
      "  Batch [320/1299] D_loss: -2.3765, G_loss: -0.3101\n",
      "  Batch [330/1299] D_loss: -3.2884, G_loss: -1.6889\n",
      "  Batch [340/1299] D_loss: -1.2023, G_loss: -1.1520\n",
      "  Batch [350/1299] D_loss: -0.5885, G_loss: -0.5768\n",
      "  Batch [360/1299] D_loss: -1.3685, G_loss: -1.5592\n",
      "  Batch [370/1299] D_loss: -3.7837, G_loss: 1.5449\n",
      "  Batch [380/1299] D_loss: -1.6715, G_loss: 0.7326\n",
      "  Batch [390/1299] D_loss: -2.1188, G_loss: 1.6220\n",
      "  Batch [400/1299] D_loss: -3.9675, G_loss: 2.0865\n",
      "  Batch [410/1299] D_loss: -2.7016, G_loss: 1.2158\n",
      "  Batch [420/1299] D_loss: -1.7030, G_loss: -0.5693\n",
      "  Batch [430/1299] D_loss: -2.4252, G_loss: 0.8290\n",
      "  Batch [440/1299] D_loss: -2.5928, G_loss: -0.1503\n",
      "  Batch [450/1299] D_loss: -4.0507, G_loss: 0.4820\n",
      "  Batch [460/1299] D_loss: -1.7632, G_loss: -0.2632\n",
      "  Batch [470/1299] D_loss: -2.5096, G_loss: -2.2356\n",
      "  Batch [480/1299] D_loss: -1.4211, G_loss: -0.5791\n",
      "  Batch [490/1299] D_loss: -1.8940, G_loss: -0.6509\n",
      "  Batch [500/1299] D_loss: -2.2740, G_loss: -3.0775\n",
      "  Batch [510/1299] D_loss: -0.6684, G_loss: -2.5638\n",
      "  Batch [520/1299] D_loss: -2.6981, G_loss: -4.6235\n",
      "  Batch [530/1299] D_loss: -1.7742, G_loss: -2.6613\n",
      "  Batch [540/1299] D_loss: -0.8683, G_loss: -1.5754\n",
      "  Batch [550/1299] D_loss: -2.0952, G_loss: -0.1184\n",
      "  Batch [560/1299] D_loss: -0.9508, G_loss: -0.0785\n",
      "  Batch [570/1299] D_loss: -1.8815, G_loss: 0.5574\n",
      "  Batch [580/1299] D_loss: -2.7987, G_loss: 2.5218\n",
      "  Batch [590/1299] D_loss: -3.8805, G_loss: 4.2851\n",
      "  Batch [600/1299] D_loss: -1.0563, G_loss: 3.0798\n",
      "  Batch [610/1299] D_loss: -1.9586, G_loss: 2.2649\n",
      "  Batch [620/1299] D_loss: -1.3082, G_loss: 0.9916\n",
      "  Batch [630/1299] D_loss: -2.7651, G_loss: 2.8230\n",
      "  Batch [640/1299] D_loss: -3.9344, G_loss: 1.4211\n",
      "  Batch [650/1299] D_loss: -0.6264, G_loss: 1.0407\n",
      "  Batch [660/1299] D_loss: -2.4225, G_loss: 3.9235\n",
      "  Batch [670/1299] D_loss: -2.5660, G_loss: 3.9900\n",
      "  Batch [680/1299] D_loss: -0.9698, G_loss: 5.0159\n",
      "  Batch [690/1299] D_loss: -0.6500, G_loss: 4.7070\n",
      "  Batch [700/1299] D_loss: -2.5685, G_loss: 4.0933\n",
      "  Batch [710/1299] D_loss: -2.6162, G_loss: 1.7953\n",
      "  Batch [720/1299] D_loss: -2.9395, G_loss: 1.5753\n",
      "  Batch [730/1299] D_loss: -3.6541, G_loss: -0.2210\n",
      "  Batch [740/1299] D_loss: -2.7074, G_loss: 1.7070\n",
      "  Batch [750/1299] D_loss: -2.0141, G_loss: 0.9564\n",
      "  Batch [760/1299] D_loss: -2.0805, G_loss: 2.7727\n",
      "  Batch [770/1299] D_loss: -2.0234, G_loss: 0.6199\n",
      "  Batch [780/1299] D_loss: -1.4041, G_loss: 1.6480\n",
      "  Batch [790/1299] D_loss: -1.7121, G_loss: 2.0095\n",
      "  Batch [800/1299] D_loss: -1.8641, G_loss: 2.5863\n",
      "  Batch [810/1299] D_loss: -2.8341, G_loss: -0.1574\n",
      "  Batch [820/1299] D_loss: -0.2931, G_loss: -0.7021\n",
      "  Batch [830/1299] D_loss: -1.3806, G_loss: -1.8793\n",
      "  Batch [840/1299] D_loss: -3.8983, G_loss: 0.8875\n",
      "  Batch [850/1299] D_loss: -1.7709, G_loss: 0.6205\n",
      "  Batch [860/1299] D_loss: -1.7491, G_loss: 3.2884\n",
      "  Batch [870/1299] D_loss: -2.6512, G_loss: 2.3032\n",
      "  Batch [880/1299] D_loss: -4.1442, G_loss: 0.9250\n",
      "  Batch [890/1299] D_loss: -1.0263, G_loss: -0.8781\n",
      "  Batch [900/1299] D_loss: -1.5807, G_loss: 0.8216\n",
      "  Batch [910/1299] D_loss: -2.1454, G_loss: -0.3696\n",
      "  Batch [920/1299] D_loss: -0.2648, G_loss: -0.4118\n",
      "  Batch [930/1299] D_loss: -1.5281, G_loss: -0.3168\n",
      "  Batch [940/1299] D_loss: -2.6880, G_loss: -1.7572\n",
      "  Batch [950/1299] D_loss: -3.6699, G_loss: -2.0663\n",
      "  Batch [960/1299] D_loss: -3.6726, G_loss: -1.7542\n",
      "  Batch [970/1299] D_loss: -2.8965, G_loss: 0.4491\n",
      "  Batch [980/1299] D_loss: -3.1044, G_loss: 1.0358\n",
      "  Batch [990/1299] D_loss: -1.9997, G_loss: 0.4519\n",
      "  Batch [1000/1299] D_loss: -3.3006, G_loss: 2.9978\n",
      "  Batch [1010/1299] D_loss: -1.5227, G_loss: 0.6070\n",
      "  Batch [1020/1299] D_loss: -3.8674, G_loss: 2.3898\n",
      "  Batch [1030/1299] D_loss: -1.2110, G_loss: 2.9439\n",
      "  Batch [1040/1299] D_loss: -2.9634, G_loss: 2.5212\n",
      "  Batch [1050/1299] D_loss: -0.7224, G_loss: 1.0274\n",
      "  Batch [1060/1299] D_loss: -2.2709, G_loss: 0.5020\n",
      "  Batch [1070/1299] D_loss: -1.8194, G_loss: 0.3944\n",
      "  Batch [1080/1299] D_loss: -2.9564, G_loss: -0.1996\n",
      "  Batch [1090/1299] D_loss: -3.0704, G_loss: 0.2957\n",
      "  Batch [1100/1299] D_loss: -2.8880, G_loss: 0.7411\n",
      "  Batch [1110/1299] D_loss: -1.7307, G_loss: 5.4120\n",
      "  Batch [1120/1299] D_loss: -2.9784, G_loss: 3.6101\n",
      "  Batch [1130/1299] D_loss: -0.0991, G_loss: 3.0109\n",
      "  Batch [1140/1299] D_loss: -2.2598, G_loss: 1.3163\n",
      "  Batch [1150/1299] D_loss: -0.7915, G_loss: -0.6604\n",
      "  Batch [1160/1299] D_loss: -2.7243, G_loss: -1.5057\n",
      "  Batch [1170/1299] D_loss: -2.1209, G_loss: -1.1442\n",
      "  Batch [1180/1299] D_loss: -2.7666, G_loss: -1.8462\n",
      "  Batch [1190/1299] D_loss: -1.5719, G_loss: -2.8536\n",
      "  Batch [1200/1299] D_loss: -2.7884, G_loss: -0.4047\n",
      "  Batch [1210/1299] D_loss: -3.3149, G_loss: -1.3697\n",
      "  Batch [1220/1299] D_loss: -1.3258, G_loss: -1.6025\n",
      "  Batch [1230/1299] D_loss: -3.0282, G_loss: -4.3879\n",
      "  Batch [1240/1299] D_loss: -0.6062, G_loss: -2.2019\n",
      "  Batch [1250/1299] D_loss: -3.2102, G_loss: 1.2547\n",
      "  Batch [1260/1299] D_loss: -0.1720, G_loss: 1.3128\n",
      "  Batch [1270/1299] D_loss: -1.3888, G_loss: 1.8786\n",
      "  Batch [1280/1299] D_loss: -3.4706, G_loss: 0.5847\n",
      "  Batch [1290/1299] D_loss: -0.7586, G_loss: 2.1268\n",
      "\n",
      "Epoch 53 Summary:\n",
      "  Average D_loss: -1.9736\n",
      "  Average G_loss: 0.2765\n",
      "\n",
      "Epoch [54/100]\n",
      "  Batch [0/1299] D_loss: -2.4521, G_loss: 2.3265\n",
      "  Batch [10/1299] D_loss: -1.3696, G_loss: 4.8926\n",
      "  Batch [20/1299] D_loss: -1.5342, G_loss: 3.4054\n",
      "  Batch [30/1299] D_loss: -1.4059, G_loss: 4.0924\n",
      "  Batch [40/1299] D_loss: -2.7587, G_loss: 4.1579\n",
      "  Batch [50/1299] D_loss: -3.8154, G_loss: 2.3572\n",
      "  Batch [60/1299] D_loss: -1.3538, G_loss: 0.2462\n",
      "  Batch [70/1299] D_loss: -1.7930, G_loss: 0.8949\n",
      "  Batch [80/1299] D_loss: -4.9082, G_loss: 2.0428\n",
      "  Batch [90/1299] D_loss: -2.5798, G_loss: 2.7457\n",
      "  Batch [100/1299] D_loss: -2.1577, G_loss: 1.7314\n",
      "  Batch [110/1299] D_loss: -1.2122, G_loss: 2.4973\n",
      "  Batch [120/1299] D_loss: -1.7172, G_loss: 2.9806\n",
      "  Batch [130/1299] D_loss: -2.2456, G_loss: 3.5635\n",
      "  Batch [140/1299] D_loss: -1.0151, G_loss: 0.6848\n",
      "  Batch [150/1299] D_loss: -3.1311, G_loss: 0.8811\n",
      "  Batch [160/1299] D_loss: -3.2249, G_loss: 1.9924\n",
      "  Batch [170/1299] D_loss: -1.6756, G_loss: 2.5736\n",
      "  Batch [180/1299] D_loss: -2.8364, G_loss: 1.5152\n",
      "  Batch [190/1299] D_loss: -2.2285, G_loss: 2.0915\n",
      "  Batch [200/1299] D_loss: -0.1967, G_loss: 0.9750\n",
      "  Batch [210/1299] D_loss: -4.3944, G_loss: 1.3219\n",
      "  Batch [220/1299] D_loss: -4.1717, G_loss: -1.5931\n",
      "  Batch [230/1299] D_loss: -1.4299, G_loss: -0.3148\n",
      "  Batch [240/1299] D_loss: -2.5431, G_loss: 0.6331\n",
      "  Batch [250/1299] D_loss: -3.2697, G_loss: -1.8815\n",
      "  Batch [260/1299] D_loss: -2.5577, G_loss: -1.8362\n",
      "  Batch [270/1299] D_loss: -2.3307, G_loss: -2.4445\n",
      "  Batch [280/1299] D_loss: -2.3768, G_loss: -1.2687\n",
      "  Batch [290/1299] D_loss: -3.7584, G_loss: 0.6809\n",
      "  Batch [300/1299] D_loss: -1.2580, G_loss: 0.0614\n",
      "  Batch [310/1299] D_loss: -1.0437, G_loss: -0.2670\n",
      "  Batch [320/1299] D_loss: -4.3029, G_loss: 1.5593\n",
      "  Batch [330/1299] D_loss: -0.9543, G_loss: 2.1512\n",
      "  Batch [340/1299] D_loss: -2.6819, G_loss: 2.3519\n",
      "  Batch [350/1299] D_loss: -2.1099, G_loss: -0.1192\n",
      "  Batch [360/1299] D_loss: -3.0750, G_loss: -0.6165\n",
      "  Batch [370/1299] D_loss: -4.4193, G_loss: 0.6215\n",
      "  Batch [380/1299] D_loss: -1.2548, G_loss: 1.0685\n",
      "  Batch [390/1299] D_loss: -2.6382, G_loss: 1.5587\n",
      "  Batch [400/1299] D_loss: -3.0294, G_loss: 3.6245\n",
      "  Batch [410/1299] D_loss: -1.1970, G_loss: 2.8305\n",
      "  Batch [420/1299] D_loss: -1.7031, G_loss: 4.5257\n",
      "  Batch [430/1299] D_loss: -2.5781, G_loss: 5.0461\n",
      "  Batch [440/1299] D_loss: -0.9998, G_loss: 2.9474\n",
      "  Batch [450/1299] D_loss: -2.4897, G_loss: 4.6092\n",
      "  Batch [460/1299] D_loss: -3.4882, G_loss: 2.8383\n",
      "  Batch [470/1299] D_loss: -1.3847, G_loss: 3.0162\n",
      "  Batch [480/1299] D_loss: -3.4222, G_loss: 0.6106\n",
      "  Batch [490/1299] D_loss: -1.7021, G_loss: -1.3940\n",
      "  Batch [500/1299] D_loss: -1.9372, G_loss: -1.5670\n",
      "  Batch [510/1299] D_loss: -4.1488, G_loss: 0.1187\n",
      "  Batch [520/1299] D_loss: -1.8431, G_loss: 0.7163\n",
      "  Batch [530/1299] D_loss: -2.1688, G_loss: -2.0785\n",
      "  Batch [540/1299] D_loss: -2.4181, G_loss: -0.5079\n",
      "  Batch [550/1299] D_loss: -0.7085, G_loss: -1.4687\n",
      "  Batch [560/1299] D_loss: -3.8128, G_loss: -0.3912\n",
      "  Batch [570/1299] D_loss: -1.9743, G_loss: 0.0164\n",
      "  Batch [580/1299] D_loss: -2.3537, G_loss: 0.1702\n",
      "  Batch [590/1299] D_loss: -0.8166, G_loss: 1.1527\n",
      "  Batch [600/1299] D_loss: -2.7744, G_loss: 0.2718\n",
      "  Batch [610/1299] D_loss: -0.3871, G_loss: 2.8633\n",
      "  Batch [620/1299] D_loss: -1.8713, G_loss: 2.7290\n",
      "  Batch [630/1299] D_loss: -2.2799, G_loss: 2.8123\n",
      "  Batch [640/1299] D_loss: -2.6709, G_loss: -0.6448\n",
      "  Batch [650/1299] D_loss: -1.2581, G_loss: 1.9647\n",
      "  Batch [660/1299] D_loss: -0.6901, G_loss: 0.4444\n",
      "  Batch [670/1299] D_loss: -2.2622, G_loss: 0.2604\n",
      "  Batch [680/1299] D_loss: -2.9464, G_loss: 1.4286\n",
      "  Batch [690/1299] D_loss: -1.3920, G_loss: 5.9311\n",
      "  Batch [700/1299] D_loss: -1.9471, G_loss: 2.4397\n",
      "  Batch [710/1299] D_loss: -2.5760, G_loss: 4.6410\n",
      "  Batch [720/1299] D_loss: -4.0329, G_loss: 3.6681\n",
      "  Batch [730/1299] D_loss: -1.5655, G_loss: 1.8809\n",
      "  Batch [740/1299] D_loss: -1.0843, G_loss: 3.4839\n",
      "  Batch [750/1299] D_loss: 0.8725, G_loss: 1.5458\n",
      "  Batch [760/1299] D_loss: -0.3280, G_loss: 2.5857\n",
      "  Batch [770/1299] D_loss: -1.1360, G_loss: 1.4514\n",
      "  Batch [780/1299] D_loss: -0.6825, G_loss: -1.1444\n",
      "  Batch [790/1299] D_loss: -2.3456, G_loss: 0.1920\n",
      "  Batch [800/1299] D_loss: -2.2729, G_loss: -1.6527\n",
      "  Batch [810/1299] D_loss: -3.1638, G_loss: 0.9934\n",
      "  Batch [820/1299] D_loss: -1.7486, G_loss: 1.3805\n",
      "  Batch [830/1299] D_loss: -1.8505, G_loss: -0.5992\n",
      "  Batch [840/1299] D_loss: -4.0975, G_loss: -1.3372\n",
      "  Batch [850/1299] D_loss: -1.1421, G_loss: -3.8849\n",
      "  Batch [860/1299] D_loss: -1.8944, G_loss: -2.4979\n",
      "  Batch [870/1299] D_loss: -2.9341, G_loss: 0.3137\n",
      "  Batch [880/1299] D_loss: -1.4534, G_loss: 0.7211\n",
      "  Batch [890/1299] D_loss: -1.1104, G_loss: -0.5348\n",
      "  Batch [900/1299] D_loss: -4.2652, G_loss: 1.3116\n",
      "  Batch [910/1299] D_loss: -1.1249, G_loss: 2.3570\n",
      "  Batch [920/1299] D_loss: -1.6513, G_loss: 1.9794\n",
      "  Batch [930/1299] D_loss: -1.2943, G_loss: -0.6817\n",
      "  Batch [940/1299] D_loss: -2.4135, G_loss: -1.2400\n",
      "  Batch [950/1299] D_loss: -0.7195, G_loss: -0.0591\n",
      "  Batch [960/1299] D_loss: -2.2672, G_loss: 0.5327\n",
      "  Batch [970/1299] D_loss: -3.9337, G_loss: -3.5575\n",
      "  Batch [980/1299] D_loss: -2.9142, G_loss: 1.0252\n",
      "  Batch [990/1299] D_loss: -2.0415, G_loss: 0.8064\n",
      "  Batch [1000/1299] D_loss: -1.1084, G_loss: 3.3515\n",
      "  Batch [1010/1299] D_loss: -2.5558, G_loss: 2.1195\n",
      "  Batch [1020/1299] D_loss: -2.6909, G_loss: 1.8819\n",
      "  Batch [1030/1299] D_loss: -1.1242, G_loss: 1.4746\n",
      "  Batch [1040/1299] D_loss: -2.0639, G_loss: 1.8659\n",
      "  Batch [1050/1299] D_loss: -1.8591, G_loss: 2.6509\n",
      "  Batch [1060/1299] D_loss: -1.1191, G_loss: 1.3094\n",
      "  Batch [1070/1299] D_loss: -2.5868, G_loss: 3.4701\n",
      "  Batch [1080/1299] D_loss: 0.2235, G_loss: 3.0109\n",
      "  Batch [1090/1299] D_loss: -2.6689, G_loss: 2.1710\n",
      "  Batch [1100/1299] D_loss: -2.0317, G_loss: 0.0217\n",
      "  Batch [1110/1299] D_loss: -5.0986, G_loss: -1.9585\n",
      "  Batch [1120/1299] D_loss: -1.4715, G_loss: -1.4961\n",
      "  Batch [1130/1299] D_loss: -0.0939, G_loss: -0.3352\n",
      "  Batch [1140/1299] D_loss: -2.3399, G_loss: -2.4894\n",
      "  Batch [1150/1299] D_loss: 0.2266, G_loss: -0.1002\n",
      "  Batch [1160/1299] D_loss: -1.7083, G_loss: -0.1908\n",
      "  Batch [1170/1299] D_loss: -1.2800, G_loss: 1.4595\n",
      "  Batch [1180/1299] D_loss: -0.9425, G_loss: 0.4316\n",
      "  Batch [1190/1299] D_loss: -2.9722, G_loss: 2.8755\n",
      "  Batch [1200/1299] D_loss: -2.0061, G_loss: 3.9359\n",
      "  Batch [1210/1299] D_loss: -2.0459, G_loss: 3.1723\n",
      "  Batch [1220/1299] D_loss: -1.1724, G_loss: 3.6727\n",
      "  Batch [1230/1299] D_loss: -1.3959, G_loss: 3.9141\n",
      "  Batch [1240/1299] D_loss: -2.3982, G_loss: 0.8332\n",
      "  Batch [1250/1299] D_loss: -0.8015, G_loss: -1.1286\n",
      "  Batch [1260/1299] D_loss: -2.8863, G_loss: -1.2709\n",
      "  Batch [1270/1299] D_loss: -2.8684, G_loss: -1.7229\n",
      "  Batch [1280/1299] D_loss: -3.1068, G_loss: -2.7188\n",
      "  Batch [1290/1299] D_loss: -1.9350, G_loss: 0.2789\n",
      "\n",
      "Epoch 54 Summary:\n",
      "  Average D_loss: -1.9542\n",
      "  Average G_loss: 1.0750\n",
      "\n",
      "Epoch [55/100]\n",
      "  Batch [0/1299] D_loss: -1.6373, G_loss: -1.4881\n",
      "  Batch [10/1299] D_loss: -1.6346, G_loss: -1.0591\n",
      "  Batch [20/1299] D_loss: -3.2401, G_loss: 1.4742\n",
      "  Batch [30/1299] D_loss: -2.2117, G_loss: 3.2223\n",
      "  Batch [40/1299] D_loss: -2.7095, G_loss: 2.0259\n",
      "  Batch [50/1299] D_loss: -1.5781, G_loss: 1.7394\n",
      "  Batch [60/1299] D_loss: -1.7857, G_loss: 0.4373\n",
      "  Batch [70/1299] D_loss: -2.2981, G_loss: -0.1485\n",
      "  Batch [80/1299] D_loss: -3.2644, G_loss: -0.8062\n",
      "  Batch [90/1299] D_loss: -1.1380, G_loss: -0.6742\n",
      "  Batch [100/1299] D_loss: -0.9062, G_loss: -1.3509\n",
      "  Batch [110/1299] D_loss: -3.1192, G_loss: -1.2910\n",
      "  Batch [120/1299] D_loss: -2.7208, G_loss: 2.9777\n",
      "  Batch [130/1299] D_loss: -2.1454, G_loss: 3.1236\n",
      "  Batch [140/1299] D_loss: -2.7796, G_loss: 1.5180\n",
      "  Batch [150/1299] D_loss: -1.8808, G_loss: 2.6419\n",
      "  Batch [160/1299] D_loss: -0.9087, G_loss: 1.8685\n",
      "  Batch [170/1299] D_loss: -2.1126, G_loss: 2.7000\n",
      "  Batch [180/1299] D_loss: -1.3169, G_loss: 2.0996\n",
      "  Batch [190/1299] D_loss: -1.1210, G_loss: -0.1368\n",
      "  Batch [200/1299] D_loss: -0.9799, G_loss: -1.7620\n",
      "  Batch [210/1299] D_loss: -1.8324, G_loss: -1.8518\n",
      "  Batch [220/1299] D_loss: -3.4385, G_loss: 0.6756\n",
      "  Batch [230/1299] D_loss: -1.4458, G_loss: -0.9201\n",
      "  Batch [240/1299] D_loss: -0.9108, G_loss: -0.0757\n",
      "  Batch [250/1299] D_loss: -2.1826, G_loss: -0.3218\n",
      "  Batch [260/1299] D_loss: -2.3287, G_loss: -0.5785\n",
      "  Batch [270/1299] D_loss: -1.3409, G_loss: -0.4894\n",
      "  Batch [280/1299] D_loss: -2.0153, G_loss: -1.0270\n",
      "  Batch [290/1299] D_loss: -2.3666, G_loss: -0.2884\n",
      "  Batch [300/1299] D_loss: -1.2322, G_loss: 2.2078\n",
      "  Batch [310/1299] D_loss: -1.4201, G_loss: 0.9291\n",
      "  Batch [320/1299] D_loss: -1.3573, G_loss: 0.8976\n",
      "  Batch [330/1299] D_loss: -3.2740, G_loss: 0.7332\n",
      "  Batch [340/1299] D_loss: -2.4379, G_loss: 3.0059\n",
      "  Batch [350/1299] D_loss: -2.3009, G_loss: 3.2716\n",
      "  Batch [360/1299] D_loss: -3.0067, G_loss: 1.6414\n",
      "  Batch [370/1299] D_loss: -1.3337, G_loss: 2.4017\n",
      "  Batch [380/1299] D_loss: -2.2172, G_loss: 0.1136\n",
      "  Batch [390/1299] D_loss: -2.0771, G_loss: -0.7115\n",
      "  Batch [400/1299] D_loss: -2.3450, G_loss: -1.5947\n",
      "  Batch [410/1299] D_loss: -2.0967, G_loss: -2.9561\n",
      "  Batch [420/1299] D_loss: -2.1829, G_loss: -1.9586\n",
      "  Batch [430/1299] D_loss: -2.7175, G_loss: 0.0325\n",
      "  Batch [440/1299] D_loss: -2.8568, G_loss: 1.3150\n",
      "  Batch [450/1299] D_loss: -2.2227, G_loss: 1.8944\n",
      "  Batch [460/1299] D_loss: -3.5713, G_loss: 2.6684\n",
      "  Batch [470/1299] D_loss: -1.4624, G_loss: 2.7842\n",
      "  Batch [480/1299] D_loss: -2.3088, G_loss: 3.3314\n",
      "  Batch [490/1299] D_loss: -3.3351, G_loss: 3.3182\n",
      "  Batch [500/1299] D_loss: -3.3792, G_loss: 1.3909\n",
      "  Batch [510/1299] D_loss: -3.4046, G_loss: 1.9625\n",
      "  Batch [520/1299] D_loss: -2.9146, G_loss: 4.1496\n",
      "  Batch [530/1299] D_loss: -2.3218, G_loss: 4.1483\n",
      "  Batch [540/1299] D_loss: -3.4680, G_loss: 2.4943\n",
      "  Batch [550/1299] D_loss: -3.4794, G_loss: 2.9145\n",
      "  Batch [560/1299] D_loss: -2.9319, G_loss: 2.2700\n",
      "  Batch [570/1299] D_loss: -1.6807, G_loss: -0.2198\n",
      "  Batch [580/1299] D_loss: -2.6028, G_loss: -0.7835\n",
      "  Batch [590/1299] D_loss: -3.0376, G_loss: -2.4126\n",
      "  Batch [600/1299] D_loss: -2.8334, G_loss: -0.5279\n",
      "  Batch [610/1299] D_loss: -6.6736, G_loss: -2.0370\n",
      "  Batch [620/1299] D_loss: -2.0455, G_loss: 0.4591\n",
      "  Batch [630/1299] D_loss: -1.8954, G_loss: 1.2812\n",
      "  Batch [640/1299] D_loss: -2.3589, G_loss: -0.2508\n",
      "  Batch [650/1299] D_loss: -1.9413, G_loss: 1.3069\n",
      "  Batch [660/1299] D_loss: -1.1028, G_loss: -1.0801\n",
      "  Batch [670/1299] D_loss: -1.3055, G_loss: 0.7243\n",
      "  Batch [680/1299] D_loss: -1.8550, G_loss: -1.2198\n",
      "  Batch [690/1299] D_loss: -1.1784, G_loss: 2.4163\n",
      "  Batch [700/1299] D_loss: -1.3930, G_loss: -0.6544\n",
      "  Batch [710/1299] D_loss: -1.7711, G_loss: -0.9213\n",
      "  Batch [720/1299] D_loss: -0.1275, G_loss: 0.2093\n",
      "  Batch [730/1299] D_loss: -1.6639, G_loss: -1.3713\n",
      "  Batch [740/1299] D_loss: -2.5295, G_loss: 0.2826\n",
      "  Batch [750/1299] D_loss: -0.7564, G_loss: -1.4707\n",
      "  Batch [760/1299] D_loss: -3.9751, G_loss: -1.1974\n",
      "  Batch [770/1299] D_loss: -1.6711, G_loss: -1.5335\n",
      "  Batch [780/1299] D_loss: -1.4194, G_loss: -0.1314\n",
      "  Batch [790/1299] D_loss: -3.6622, G_loss: 1.3461\n",
      "  Batch [800/1299] D_loss: -0.6853, G_loss: 0.1747\n",
      "  Batch [810/1299] D_loss: -1.6458, G_loss: 2.6139\n",
      "  Batch [820/1299] D_loss: -2.5282, G_loss: 0.4130\n",
      "  Batch [830/1299] D_loss: -0.6883, G_loss: -1.8503\n",
      "  Batch [840/1299] D_loss: -2.1020, G_loss: -0.4154\n",
      "  Batch [850/1299] D_loss: -1.6572, G_loss: -3.5716\n",
      "  Batch [860/1299] D_loss: -2.2266, G_loss: 0.5706\n",
      "  Batch [870/1299] D_loss: -3.3175, G_loss: 0.5780\n",
      "  Batch [880/1299] D_loss: -1.0873, G_loss: 0.7205\n",
      "  Batch [890/1299] D_loss: -1.1823, G_loss: 0.6807\n",
      "  Batch [900/1299] D_loss: -2.8627, G_loss: -0.0966\n",
      "  Batch [910/1299] D_loss: -1.5375, G_loss: 1.0335\n",
      "  Batch [920/1299] D_loss: -2.3343, G_loss: -0.7297\n",
      "  Batch [930/1299] D_loss: -0.4977, G_loss: -1.5042\n",
      "  Batch [940/1299] D_loss: -2.4261, G_loss: -3.2216\n",
      "  Batch [950/1299] D_loss: -5.2836, G_loss: -3.9131\n",
      "  Batch [960/1299] D_loss: -3.1937, G_loss: -2.0729\n",
      "  Batch [970/1299] D_loss: -0.4639, G_loss: -2.4505\n",
      "  Batch [980/1299] D_loss: -2.4061, G_loss: -0.5401\n",
      "  Batch [990/1299] D_loss: -1.6451, G_loss: 2.6029\n",
      "  Batch [1000/1299] D_loss: -2.4643, G_loss: -0.7690\n",
      "  Batch [1010/1299] D_loss: -3.4533, G_loss: 2.1984\n",
      "  Batch [1020/1299] D_loss: -0.0722, G_loss: 3.0173\n",
      "  Batch [1030/1299] D_loss: -2.4427, G_loss: 1.6287\n",
      "  Batch [1040/1299] D_loss: -2.5388, G_loss: 3.4074\n",
      "  Batch [1050/1299] D_loss: -0.8910, G_loss: 1.8003\n",
      "  Batch [1060/1299] D_loss: -1.3844, G_loss: 1.1143\n",
      "  Batch [1070/1299] D_loss: -2.0998, G_loss: -1.2830\n",
      "  Batch [1080/1299] D_loss: -2.6473, G_loss: -1.3514\n",
      "  Batch [1090/1299] D_loss: -1.0805, G_loss: 0.0611\n",
      "  Batch [1100/1299] D_loss: -1.7649, G_loss: 1.4635\n",
      "  Batch [1110/1299] D_loss: -1.9241, G_loss: -2.3414\n",
      "  Batch [1120/1299] D_loss: -1.7451, G_loss: -0.2848\n",
      "  Batch [1130/1299] D_loss: -3.3897, G_loss: -0.0724\n",
      "  Batch [1140/1299] D_loss: -2.8539, G_loss: -1.8404\n",
      "  Batch [1150/1299] D_loss: -3.3272, G_loss: 2.1138\n",
      "  Batch [1160/1299] D_loss: -3.3483, G_loss: 1.5647\n",
      "  Batch [1170/1299] D_loss: -1.8804, G_loss: 2.6336\n",
      "  Batch [1180/1299] D_loss: -1.7995, G_loss: 1.3884\n",
      "  Batch [1190/1299] D_loss: -1.4758, G_loss: 0.9579\n",
      "  Batch [1200/1299] D_loss: -3.2017, G_loss: 0.1081\n",
      "  Batch [1210/1299] D_loss: -0.1908, G_loss: -2.1331\n",
      "  Batch [1220/1299] D_loss: -2.8882, G_loss: -0.8720\n",
      "  Batch [1230/1299] D_loss: -4.0676, G_loss: 0.2940\n",
      "  Batch [1240/1299] D_loss: -1.8312, G_loss: 1.5472\n",
      "  Batch [1250/1299] D_loss: -0.9916, G_loss: 2.1472\n",
      "  Batch [1260/1299] D_loss: -2.2481, G_loss: 3.8097\n",
      "  Batch [1270/1299] D_loss: -3.7798, G_loss: 2.1730\n",
      "  Batch [1280/1299] D_loss: -1.0015, G_loss: 1.5537\n",
      "  Batch [1290/1299] D_loss: -0.1430, G_loss: 2.8307\n",
      "\n",
      "Epoch 55 Summary:\n",
      "  Average D_loss: -1.9484\n",
      "  Average G_loss: 0.4829\n",
      "\n",
      "Epoch [56/100]\n",
      "  Batch [0/1299] D_loss: -1.6309, G_loss: 4.9700\n",
      "  Batch [10/1299] D_loss: -2.7935, G_loss: 1.3220\n",
      "  Batch [20/1299] D_loss: -1.3732, G_loss: 0.5802\n",
      "  Batch [30/1299] D_loss: -2.6003, G_loss: -0.2547\n",
      "  Batch [40/1299] D_loss: -3.6099, G_loss: 0.5367\n",
      "  Batch [50/1299] D_loss: -3.7185, G_loss: -3.4913\n",
      "  Batch [60/1299] D_loss: -2.3401, G_loss: -1.9208\n",
      "  Batch [70/1299] D_loss: -2.4802, G_loss: -2.0766\n",
      "  Batch [80/1299] D_loss: -2.2309, G_loss: -2.4440\n",
      "  Batch [90/1299] D_loss: -1.6746, G_loss: 0.2893\n",
      "  Batch [100/1299] D_loss: -0.9460, G_loss: 2.3995\n",
      "  Batch [110/1299] D_loss: -1.6662, G_loss: 2.8955\n",
      "  Batch [120/1299] D_loss: -3.2144, G_loss: 1.8175\n",
      "  Batch [130/1299] D_loss: -2.6545, G_loss: 5.8724\n",
      "  Batch [140/1299] D_loss: -2.8486, G_loss: 3.4179\n",
      "  Batch [150/1299] D_loss: 0.0977, G_loss: 2.2375\n",
      "  Batch [160/1299] D_loss: -2.2639, G_loss: 2.2708\n",
      "  Batch [170/1299] D_loss: -4.0426, G_loss: 3.6180\n",
      "  Batch [180/1299] D_loss: -2.2374, G_loss: 1.9731\n",
      "  Batch [190/1299] D_loss: -2.5111, G_loss: 2.5760\n",
      "  Batch [200/1299] D_loss: -0.3045, G_loss: 1.3729\n",
      "  Batch [210/1299] D_loss: -2.4700, G_loss: 1.2632\n",
      "  Batch [220/1299] D_loss: -0.6179, G_loss: 0.8504\n",
      "  Batch [230/1299] D_loss: -1.5544, G_loss: 2.0226\n",
      "  Batch [240/1299] D_loss: -4.6156, G_loss: -0.3137\n",
      "  Batch [250/1299] D_loss: -1.8435, G_loss: -0.2548\n",
      "  Batch [260/1299] D_loss: -1.6952, G_loss: -0.6957\n",
      "  Batch [270/1299] D_loss: -2.6922, G_loss: -3.8402\n",
      "  Batch [280/1299] D_loss: -1.2863, G_loss: -1.6185\n",
      "  Batch [290/1299] D_loss: -4.4535, G_loss: -0.7174\n",
      "  Batch [300/1299] D_loss: -1.2596, G_loss: -0.7533\n",
      "  Batch [310/1299] D_loss: -4.4737, G_loss: -0.3180\n",
      "  Batch [320/1299] D_loss: -0.6556, G_loss: 2.3716\n",
      "  Batch [330/1299] D_loss: -2.9929, G_loss: 3.4789\n",
      "  Batch [340/1299] D_loss: -1.7502, G_loss: 0.8456\n",
      "  Batch [350/1299] D_loss: -1.3894, G_loss: 1.8656\n",
      "  Batch [360/1299] D_loss: -0.6194, G_loss: 1.1048\n",
      "  Batch [370/1299] D_loss: -0.9216, G_loss: 0.5265\n",
      "  Batch [380/1299] D_loss: -4.1827, G_loss: -0.1576\n",
      "  Batch [390/1299] D_loss: -3.4101, G_loss: -0.2957\n",
      "  Batch [400/1299] D_loss: -2.4725, G_loss: 0.9242\n",
      "  Batch [410/1299] D_loss: -3.0994, G_loss: 2.2983\n",
      "  Batch [420/1299] D_loss: -2.0834, G_loss: 3.2990\n",
      "  Batch [430/1299] D_loss: 0.4048, G_loss: 2.3572\n",
      "  Batch [440/1299] D_loss: 0.5546, G_loss: 1.9195\n",
      "  Batch [450/1299] D_loss: -0.9114, G_loss: 1.4142\n",
      "  Batch [460/1299] D_loss: -3.2245, G_loss: 1.2347\n",
      "  Batch [470/1299] D_loss: -1.8020, G_loss: -1.8214\n",
      "  Batch [480/1299] D_loss: -2.5831, G_loss: -2.8592\n",
      "  Batch [490/1299] D_loss: -3.2315, G_loss: -4.7742\n",
      "  Batch [500/1299] D_loss: -0.9324, G_loss: -4.4157\n",
      "  Batch [510/1299] D_loss: -1.7062, G_loss: -2.7458\n",
      "  Batch [520/1299] D_loss: -1.3690, G_loss: -0.8865\n",
      "  Batch [530/1299] D_loss: -2.0587, G_loss: 0.7514\n",
      "  Batch [540/1299] D_loss: -2.6624, G_loss: 2.4112\n",
      "  Batch [550/1299] D_loss: -2.9624, G_loss: 1.2984\n",
      "  Batch [560/1299] D_loss: -2.2283, G_loss: 2.1115\n",
      "  Batch [570/1299] D_loss: -1.9832, G_loss: 1.3365\n",
      "  Batch [580/1299] D_loss: -2.1965, G_loss: 3.3271\n",
      "  Batch [590/1299] D_loss: -3.2626, G_loss: 2.3640\n",
      "  Batch [600/1299] D_loss: -1.8306, G_loss: 2.7673\n",
      "  Batch [610/1299] D_loss: -3.3805, G_loss: 1.4806\n",
      "  Batch [620/1299] D_loss: -2.7475, G_loss: 0.6180\n",
      "  Batch [630/1299] D_loss: -1.5428, G_loss: 1.1911\n",
      "  Batch [640/1299] D_loss: -2.0479, G_loss: -0.2698\n",
      "  Batch [650/1299] D_loss: -2.4971, G_loss: -3.3035\n",
      "  Batch [660/1299] D_loss: -1.8301, G_loss: -1.5537\n",
      "  Batch [670/1299] D_loss: 0.4276, G_loss: 0.8108\n",
      "  Batch [680/1299] D_loss: -1.5801, G_loss: 2.8025\n",
      "  Batch [690/1299] D_loss: -1.3505, G_loss: 2.7817\n",
      "  Batch [700/1299] D_loss: -0.6171, G_loss: 4.3107\n",
      "  Batch [710/1299] D_loss: -1.7423, G_loss: 8.3237\n",
      "  Batch [720/1299] D_loss: -2.3897, G_loss: 8.1158\n",
      "  Batch [730/1299] D_loss: -3.9946, G_loss: 7.4384\n",
      "  Batch [740/1299] D_loss: -2.8494, G_loss: 5.1961\n",
      "  Batch [750/1299] D_loss: -0.9515, G_loss: 0.9601\n",
      "  Batch [760/1299] D_loss: -2.9014, G_loss: 1.7870\n",
      "  Batch [770/1299] D_loss: -0.7909, G_loss: 0.2663\n",
      "  Batch [780/1299] D_loss: -1.8552, G_loss: 1.2318\n",
      "  Batch [790/1299] D_loss: -0.7645, G_loss: -3.6380\n",
      "  Batch [800/1299] D_loss: -0.2135, G_loss: -3.1140\n",
      "  Batch [810/1299] D_loss: -4.2276, G_loss: -3.0810\n",
      "  Batch [820/1299] D_loss: -2.9324, G_loss: -3.9001\n",
      "  Batch [830/1299] D_loss: -2.1618, G_loss: -3.0028\n",
      "  Batch [840/1299] D_loss: -2.1664, G_loss: -4.7071\n",
      "  Batch [850/1299] D_loss: -2.5297, G_loss: -3.1202\n",
      "  Batch [860/1299] D_loss: -1.8098, G_loss: 0.1268\n",
      "  Batch [870/1299] D_loss: -2.2738, G_loss: -0.5228\n",
      "  Batch [880/1299] D_loss: 0.1535, G_loss: 0.8522\n",
      "  Batch [890/1299] D_loss: -2.0983, G_loss: 2.3741\n",
      "  Batch [900/1299] D_loss: -3.9958, G_loss: 2.0270\n",
      "  Batch [910/1299] D_loss: -3.7454, G_loss: 4.1549\n",
      "  Batch [920/1299] D_loss: -2.4397, G_loss: 2.2270\n",
      "  Batch [930/1299] D_loss: -2.1870, G_loss: -1.8800\n",
      "  Batch [940/1299] D_loss: -3.1995, G_loss: -1.1070\n",
      "  Batch [950/1299] D_loss: -2.5269, G_loss: -3.3140\n",
      "  Batch [960/1299] D_loss: -3.6032, G_loss: -1.1284\n",
      "  Batch [970/1299] D_loss: -3.7655, G_loss: 1.6848\n",
      "  Batch [980/1299] D_loss: -3.4899, G_loss: 1.2846\n",
      "  Batch [990/1299] D_loss: -1.0546, G_loss: 2.6398\n",
      "  Batch [1000/1299] D_loss: -4.2620, G_loss: 2.3997\n",
      "  Batch [1010/1299] D_loss: -4.2928, G_loss: 1.2026\n",
      "  Batch [1020/1299] D_loss: -1.4204, G_loss: 0.7231\n",
      "  Batch [1030/1299] D_loss: -4.8946, G_loss: 1.3549\n",
      "  Batch [1040/1299] D_loss: -1.9746, G_loss: 0.9829\n",
      "  Batch [1050/1299] D_loss: -1.7130, G_loss: 0.0766\n",
      "  Batch [1060/1299] D_loss: -2.1021, G_loss: -0.6614\n",
      "  Batch [1070/1299] D_loss: -2.8067, G_loss: -1.2651\n",
      "  Batch [1080/1299] D_loss: -3.2488, G_loss: -1.5225\n",
      "  Batch [1090/1299] D_loss: -0.6525, G_loss: 0.0285\n",
      "  Batch [1100/1299] D_loss: -1.9850, G_loss: 1.8708\n",
      "  Batch [1110/1299] D_loss: -1.9271, G_loss: -2.1244\n",
      "  Batch [1120/1299] D_loss: -2.8297, G_loss: 0.2589\n",
      "  Batch [1130/1299] D_loss: -1.1612, G_loss: -1.8099\n",
      "  Batch [1140/1299] D_loss: -0.5364, G_loss: -2.0934\n",
      "  Batch [1150/1299] D_loss: -2.8100, G_loss: -3.4923\n",
      "  Batch [1160/1299] D_loss: -1.1779, G_loss: -4.6046\n",
      "  Batch [1170/1299] D_loss: -1.8522, G_loss: -4.9872\n",
      "  Batch [1180/1299] D_loss: -2.2267, G_loss: -2.9380\n",
      "  Batch [1190/1299] D_loss: -3.3852, G_loss: -0.9344\n",
      "  Batch [1200/1299] D_loss: -1.6543, G_loss: -0.3430\n",
      "  Batch [1210/1299] D_loss: -0.0641, G_loss: -1.2771\n",
      "  Batch [1220/1299] D_loss: -1.6933, G_loss: -1.8572\n",
      "  Batch [1230/1299] D_loss: -1.6144, G_loss: 1.6318\n",
      "  Batch [1240/1299] D_loss: -2.8733, G_loss: 2.4717\n",
      "  Batch [1250/1299] D_loss: -1.0019, G_loss: 1.0269\n",
      "  Batch [1260/1299] D_loss: -2.0208, G_loss: 1.0862\n",
      "  Batch [1270/1299] D_loss: 0.0667, G_loss: -1.8174\n",
      "  Batch [1280/1299] D_loss: -2.9958, G_loss: -2.8187\n",
      "  Batch [1290/1299] D_loss: -1.2352, G_loss: 0.7819\n",
      "\n",
      "Epoch 56 Summary:\n",
      "  Average D_loss: -1.9614\n",
      "  Average G_loss: 0.3769\n",
      "\n",
      "Epoch [57/100]\n",
      "  Batch [0/1299] D_loss: -2.0113, G_loss: 3.8182\n",
      "  Batch [10/1299] D_loss: -3.1287, G_loss: 3.6957\n",
      "  Batch [20/1299] D_loss: -1.3662, G_loss: 2.9942\n",
      "  Batch [30/1299] D_loss: -2.3841, G_loss: 0.5525\n",
      "  Batch [40/1299] D_loss: -2.6890, G_loss: 1.0032\n",
      "  Batch [50/1299] D_loss: -2.8653, G_loss: -0.3624\n",
      "  Batch [60/1299] D_loss: -4.4744, G_loss: -1.8463\n",
      "  Batch [70/1299] D_loss: -3.7707, G_loss: -0.8907\n",
      "  Batch [80/1299] D_loss: -0.4634, G_loss: -2.7881\n",
      "  Batch [90/1299] D_loss: -2.1344, G_loss: -2.8801\n",
      "  Batch [100/1299] D_loss: -2.4480, G_loss: -2.6219\n",
      "  Batch [110/1299] D_loss: -1.2602, G_loss: -1.0853\n",
      "  Batch [120/1299] D_loss: -2.2549, G_loss: 0.7745\n",
      "  Batch [130/1299] D_loss: -1.7153, G_loss: 0.0788\n",
      "  Batch [140/1299] D_loss: -1.6687, G_loss: 0.3812\n",
      "  Batch [150/1299] D_loss: -1.9074, G_loss: 2.1179\n",
      "  Batch [160/1299] D_loss: -2.0317, G_loss: 1.7268\n",
      "  Batch [170/1299] D_loss: -2.3489, G_loss: 2.2778\n",
      "  Batch [180/1299] D_loss: -2.1695, G_loss: 3.7302\n",
      "  Batch [190/1299] D_loss: -3.0080, G_loss: 3.6114\n",
      "  Batch [200/1299] D_loss: -2.0741, G_loss: 3.9794\n",
      "  Batch [210/1299] D_loss: -3.3382, G_loss: 1.4782\n",
      "  Batch [220/1299] D_loss: -2.6414, G_loss: 0.8458\n",
      "  Batch [230/1299] D_loss: -2.9620, G_loss: -0.9055\n",
      "  Batch [240/1299] D_loss: -0.9894, G_loss: -1.0318\n",
      "  Batch [250/1299] D_loss: -1.1552, G_loss: -2.3055\n",
      "  Batch [260/1299] D_loss: -1.6024, G_loss: 1.1502\n",
      "  Batch [270/1299] D_loss: -0.9618, G_loss: 0.1605\n",
      "  Batch [280/1299] D_loss: -3.2011, G_loss: -0.9397\n",
      "  Batch [290/1299] D_loss: -2.2950, G_loss: 2.3692\n",
      "  Batch [300/1299] D_loss: -2.3896, G_loss: 3.3845\n",
      "  Batch [310/1299] D_loss: -1.2417, G_loss: 2.8296\n",
      "  Batch [320/1299] D_loss: -0.2461, G_loss: 1.1935\n",
      "  Batch [330/1299] D_loss: -2.3119, G_loss: 1.4490\n",
      "  Batch [340/1299] D_loss: -1.3208, G_loss: -0.5172\n",
      "  Batch [350/1299] D_loss: -3.0536, G_loss: 0.5722\n",
      "  Batch [360/1299] D_loss: -2.8688, G_loss: -0.4445\n",
      "  Batch [370/1299] D_loss: -1.9423, G_loss: -2.6919\n",
      "  Batch [380/1299] D_loss: -2.8910, G_loss: -0.7537\n",
      "  Batch [390/1299] D_loss: -1.6137, G_loss: -0.9185\n",
      "  Batch [400/1299] D_loss: -2.1601, G_loss: 1.8491\n",
      "  Batch [410/1299] D_loss: -1.3636, G_loss: 1.2870\n",
      "  Batch [420/1299] D_loss: -2.6127, G_loss: 3.6835\n",
      "  Batch [430/1299] D_loss: -2.4783, G_loss: 3.0866\n",
      "  Batch [440/1299] D_loss: -1.8193, G_loss: 2.1370\n",
      "  Batch [450/1299] D_loss: -4.3494, G_loss: 3.1951\n",
      "  Batch [460/1299] D_loss: -4.1538, G_loss: 5.2913\n",
      "  Batch [470/1299] D_loss: 0.1143, G_loss: 2.0944\n",
      "  Batch [480/1299] D_loss: -2.5046, G_loss: 3.4956\n",
      "  Batch [490/1299] D_loss: -1.7012, G_loss: 0.9246\n",
      "  Batch [500/1299] D_loss: -0.3789, G_loss: 0.6126\n",
      "  Batch [510/1299] D_loss: -2.3555, G_loss: 2.3312\n",
      "  Batch [520/1299] D_loss: -2.6154, G_loss: 2.3203\n",
      "  Batch [530/1299] D_loss: -2.1564, G_loss: 3.0535\n",
      "  Batch [540/1299] D_loss: -3.2078, G_loss: 3.1684\n",
      "  Batch [550/1299] D_loss: -2.2150, G_loss: 3.1756\n",
      "  Batch [560/1299] D_loss: -3.5498, G_loss: 2.3333\n",
      "  Batch [570/1299] D_loss: -3.4533, G_loss: 2.1202\n",
      "  Batch [580/1299] D_loss: -3.2629, G_loss: 1.5798\n",
      "  Batch [590/1299] D_loss: -1.9085, G_loss: -1.4864\n",
      "  Batch [600/1299] D_loss: -2.0866, G_loss: 0.4778\n",
      "  Batch [610/1299] D_loss: -1.7323, G_loss: 0.2413\n",
      "  Batch [620/1299] D_loss: -1.9549, G_loss: 1.7664\n",
      "  Batch [630/1299] D_loss: -2.6605, G_loss: 1.3673\n",
      "  Batch [640/1299] D_loss: -2.0393, G_loss: -0.1791\n",
      "  Batch [650/1299] D_loss: -1.9211, G_loss: -0.7813\n",
      "  Batch [660/1299] D_loss: -1.8489, G_loss: -0.4569\n",
      "  Batch [670/1299] D_loss: -0.6126, G_loss: 0.6736\n",
      "  Batch [680/1299] D_loss: -3.3812, G_loss: 1.3934\n",
      "  Batch [690/1299] D_loss: -0.5461, G_loss: -0.7514\n",
      "  Batch [700/1299] D_loss: -1.3818, G_loss: -3.7428\n",
      "  Batch [710/1299] D_loss: -1.2450, G_loss: -0.6065\n",
      "  Batch [720/1299] D_loss: -1.7993, G_loss: 1.1113\n",
      "  Batch [730/1299] D_loss: -2.7340, G_loss: 0.1020\n",
      "  Batch [740/1299] D_loss: -1.4885, G_loss: 4.3603\n",
      "  Batch [750/1299] D_loss: -1.7228, G_loss: 0.9332\n",
      "  Batch [760/1299] D_loss: -2.2356, G_loss: 1.8624\n",
      "  Batch [770/1299] D_loss: -1.8401, G_loss: 1.2195\n",
      "  Batch [780/1299] D_loss: -0.8925, G_loss: 3.0431\n",
      "  Batch [790/1299] D_loss: -1.1055, G_loss: 1.9073\n",
      "  Batch [800/1299] D_loss: -1.6856, G_loss: 4.0380\n",
      "  Batch [810/1299] D_loss: -3.2376, G_loss: 0.1567\n",
      "  Batch [820/1299] D_loss: -1.8827, G_loss: -2.2255\n",
      "  Batch [830/1299] D_loss: -2.0634, G_loss: -6.1762\n",
      "  Batch [840/1299] D_loss: -2.3561, G_loss: -1.0420\n",
      "  Batch [850/1299] D_loss: -2.2488, G_loss: -1.0396\n",
      "  Batch [860/1299] D_loss: -1.3709, G_loss: -1.9126\n",
      "  Batch [870/1299] D_loss: -2.1581, G_loss: -0.6155\n",
      "  Batch [880/1299] D_loss: -4.3059, G_loss: -1.2088\n",
      "  Batch [890/1299] D_loss: -1.8730, G_loss: 0.0396\n",
      "  Batch [900/1299] D_loss: -1.7156, G_loss: 3.2354\n",
      "  Batch [910/1299] D_loss: -0.6479, G_loss: 6.4602\n",
      "  Batch [920/1299] D_loss: -1.5838, G_loss: 4.5847\n",
      "  Batch [930/1299] D_loss: -2.2953, G_loss: 2.2165\n",
      "  Batch [940/1299] D_loss: -2.1519, G_loss: 3.3333\n",
      "  Batch [950/1299] D_loss: -2.3567, G_loss: 2.5285\n",
      "  Batch [960/1299] D_loss: -1.7823, G_loss: -0.5212\n",
      "  Batch [970/1299] D_loss: -1.8019, G_loss: -0.1438\n",
      "  Batch [980/1299] D_loss: -3.8736, G_loss: 3.5688\n",
      "  Batch [990/1299] D_loss: -3.6059, G_loss: 2.6734\n",
      "  Batch [1000/1299] D_loss: -3.6910, G_loss: 1.8215\n",
      "  Batch [1010/1299] D_loss: -4.0714, G_loss: 0.3292\n",
      "  Batch [1020/1299] D_loss: -1.7467, G_loss: 5.6564\n",
      "  Batch [1030/1299] D_loss: -1.8852, G_loss: 2.9780\n",
      "  Batch [1040/1299] D_loss: 0.5347, G_loss: 0.9125\n",
      "  Batch [1050/1299] D_loss: -4.8344, G_loss: 2.0966\n",
      "  Batch [1060/1299] D_loss: -2.6883, G_loss: 0.4185\n",
      "  Batch [1070/1299] D_loss: -2.6799, G_loss: 1.3528\n",
      "  Batch [1080/1299] D_loss: -1.9658, G_loss: -1.7997\n",
      "  Batch [1090/1299] D_loss: -1.6609, G_loss: -1.3589\n",
      "  Batch [1100/1299] D_loss: -3.2626, G_loss: 1.4849\n",
      "  Batch [1110/1299] D_loss: -3.0466, G_loss: 1.2841\n",
      "  Batch [1120/1299] D_loss: -3.2392, G_loss: 0.3234\n",
      "  Batch [1130/1299] D_loss: -1.3033, G_loss: 1.2420\n",
      "  Batch [1140/1299] D_loss: -2.4270, G_loss: 1.2175\n",
      "  Batch [1150/1299] D_loss: -2.2294, G_loss: -0.1454\n",
      "  Batch [1160/1299] D_loss: -3.3109, G_loss: 1.2138\n",
      "  Batch [1170/1299] D_loss: -1.5423, G_loss: 0.7664\n",
      "  Batch [1180/1299] D_loss: -1.4716, G_loss: 3.9570\n",
      "  Batch [1190/1299] D_loss: -1.6310, G_loss: 2.8505\n",
      "  Batch [1200/1299] D_loss: -3.2917, G_loss: 1.9605\n",
      "  Batch [1210/1299] D_loss: -3.3486, G_loss: 1.0205\n",
      "  Batch [1220/1299] D_loss: -2.8222, G_loss: 1.1207\n",
      "  Batch [1230/1299] D_loss: -2.3629, G_loss: -0.3957\n",
      "  Batch [1240/1299] D_loss: -4.5985, G_loss: -4.5997\n",
      "  Batch [1250/1299] D_loss: -3.0113, G_loss: -2.2691\n",
      "  Batch [1260/1299] D_loss: -1.9638, G_loss: -1.3428\n",
      "  Batch [1270/1299] D_loss: -2.3287, G_loss: -1.4514\n",
      "  Batch [1280/1299] D_loss: -2.1500, G_loss: -0.3435\n",
      "  Batch [1290/1299] D_loss: -2.6847, G_loss: 0.2827\n",
      "\n",
      "Epoch 57 Summary:\n",
      "  Average D_loss: -1.9749\n",
      "  Average G_loss: 0.8435\n",
      "\n",
      "Epoch [58/100]\n",
      "  Batch [0/1299] D_loss: -1.2986, G_loss: 1.3451\n",
      "  Batch [10/1299] D_loss: -2.4945, G_loss: 0.6592\n",
      "  Batch [20/1299] D_loss: -2.8040, G_loss: 1.4385\n",
      "  Batch [30/1299] D_loss: -0.1206, G_loss: -0.5447\n",
      "  Batch [40/1299] D_loss: -0.1073, G_loss: -3.1245\n",
      "  Batch [50/1299] D_loss: -0.7108, G_loss: -6.6250\n",
      "  Batch [60/1299] D_loss: -3.9667, G_loss: -6.1262\n",
      "  Batch [70/1299] D_loss: -2.7961, G_loss: -5.5873\n",
      "  Batch [80/1299] D_loss: -0.9528, G_loss: -4.3468\n",
      "  Batch [90/1299] D_loss: -2.0468, G_loss: -3.9599\n",
      "  Batch [100/1299] D_loss: -3.6433, G_loss: -1.9031\n",
      "  Batch [110/1299] D_loss: -4.2319, G_loss: -0.8649\n",
      "  Batch [120/1299] D_loss: -1.8399, G_loss: 0.5618\n",
      "  Batch [130/1299] D_loss: -2.8020, G_loss: 1.8114\n",
      "  Batch [140/1299] D_loss: -0.7535, G_loss: 0.3710\n",
      "  Batch [150/1299] D_loss: -3.4687, G_loss: 1.3962\n",
      "  Batch [160/1299] D_loss: -0.2323, G_loss: 0.4790\n",
      "  Batch [170/1299] D_loss: -1.3528, G_loss: -0.2958\n",
      "  Batch [180/1299] D_loss: -1.5143, G_loss: 0.6352\n",
      "  Batch [190/1299] D_loss: -2.0723, G_loss: 1.4824\n",
      "  Batch [200/1299] D_loss: -2.1458, G_loss: 0.9908\n",
      "  Batch [210/1299] D_loss: -4.0622, G_loss: 1.5524\n",
      "  Batch [220/1299] D_loss: -1.4883, G_loss: 2.7453\n",
      "  Batch [230/1299] D_loss: -0.8853, G_loss: 1.2672\n",
      "  Batch [240/1299] D_loss: -1.4391, G_loss: 0.3438\n",
      "  Batch [250/1299] D_loss: -1.0021, G_loss: -1.2077\n",
      "  Batch [260/1299] D_loss: -0.9695, G_loss: -1.4900\n",
      "  Batch [270/1299] D_loss: -2.2629, G_loss: -1.9925\n",
      "  Batch [280/1299] D_loss: -0.7117, G_loss: -2.2603\n",
      "  Batch [290/1299] D_loss: -0.4039, G_loss: -2.2674\n",
      "  Batch [300/1299] D_loss: -1.6053, G_loss: -1.2888\n",
      "  Batch [310/1299] D_loss: -2.2242, G_loss: -0.6696\n",
      "  Batch [320/1299] D_loss: -1.1245, G_loss: -2.1825\n",
      "  Batch [330/1299] D_loss: -1.7634, G_loss: -0.2597\n",
      "  Batch [340/1299] D_loss: -3.3340, G_loss: 0.3195\n",
      "  Batch [350/1299] D_loss: -1.6693, G_loss: 4.8108\n",
      "  Batch [360/1299] D_loss: -1.5805, G_loss: 2.8289\n",
      "  Batch [370/1299] D_loss: -1.5287, G_loss: 4.0747\n",
      "  Batch [380/1299] D_loss: -2.6443, G_loss: 2.8919\n",
      "  Batch [390/1299] D_loss: -4.2991, G_loss: 2.5697\n",
      "  Batch [400/1299] D_loss: -3.1696, G_loss: 4.8960\n",
      "  Batch [410/1299] D_loss: -2.7095, G_loss: 1.9984\n",
      "  Batch [420/1299] D_loss: -2.8923, G_loss: 1.5743\n",
      "  Batch [430/1299] D_loss: -1.6047, G_loss: 2.2303\n",
      "  Batch [440/1299] D_loss: -3.3974, G_loss: 0.8108\n",
      "  Batch [450/1299] D_loss: -1.9215, G_loss: 0.9327\n",
      "  Batch [460/1299] D_loss: -3.2881, G_loss: 0.2674\n",
      "  Batch [470/1299] D_loss: -0.5428, G_loss: 0.1054\n",
      "  Batch [480/1299] D_loss: -2.9892, G_loss: -0.0653\n",
      "  Batch [490/1299] D_loss: -0.4907, G_loss: 1.4885\n",
      "  Batch [500/1299] D_loss: -1.0774, G_loss: 2.4933\n",
      "  Batch [510/1299] D_loss: -3.0170, G_loss: 0.3699\n",
      "  Batch [520/1299] D_loss: -3.0728, G_loss: 2.0089\n",
      "  Batch [530/1299] D_loss: -1.1835, G_loss: 1.5310\n",
      "  Batch [540/1299] D_loss: -0.4745, G_loss: 2.4539\n",
      "  Batch [550/1299] D_loss: -1.6366, G_loss: 3.7560\n",
      "  Batch [560/1299] D_loss: -2.1860, G_loss: 0.9577\n",
      "  Batch [570/1299] D_loss: -2.6108, G_loss: 0.7153\n",
      "  Batch [580/1299] D_loss: -0.1471, G_loss: 0.9790\n",
      "  Batch [590/1299] D_loss: -2.0959, G_loss: 1.0329\n",
      "  Batch [600/1299] D_loss: -1.3828, G_loss: 1.7585\n",
      "  Batch [610/1299] D_loss: -0.9586, G_loss: 0.9197\n",
      "  Batch [620/1299] D_loss: -4.0820, G_loss: 1.3061\n",
      "  Batch [630/1299] D_loss: -0.9522, G_loss: 2.4511\n",
      "  Batch [640/1299] D_loss: -3.8963, G_loss: 2.0565\n",
      "  Batch [650/1299] D_loss: -1.6997, G_loss: -0.2843\n",
      "  Batch [660/1299] D_loss: -1.8809, G_loss: 1.3936\n",
      "  Batch [670/1299] D_loss: -1.0487, G_loss: 0.5287\n",
      "  Batch [680/1299] D_loss: -2.7005, G_loss: 0.1773\n",
      "  Batch [690/1299] D_loss: -2.4284, G_loss: -0.8331\n",
      "  Batch [700/1299] D_loss: -0.4432, G_loss: -0.8155\n",
      "  Batch [710/1299] D_loss: -0.9937, G_loss: 2.3327\n",
      "  Batch [720/1299] D_loss: -1.9319, G_loss: 1.5577\n",
      "  Batch [730/1299] D_loss: -2.2633, G_loss: 2.8925\n",
      "  Batch [740/1299] D_loss: -3.8593, G_loss: 2.8262\n",
      "  Batch [750/1299] D_loss: -4.5279, G_loss: 3.0910\n",
      "  Batch [760/1299] D_loss: -3.3120, G_loss: 2.7839\n",
      "  Batch [770/1299] D_loss: -2.5661, G_loss: 2.6754\n",
      "  Batch [780/1299] D_loss: -3.2218, G_loss: 2.7513\n",
      "  Batch [790/1299] D_loss: -2.1606, G_loss: 0.6857\n",
      "  Batch [800/1299] D_loss: -4.4146, G_loss: -1.3009\n",
      "  Batch [810/1299] D_loss: -0.6227, G_loss: -0.5172\n",
      "  Batch [820/1299] D_loss: -0.5581, G_loss: -0.2526\n",
      "  Batch [830/1299] D_loss: -1.1014, G_loss: -0.3369\n",
      "  Batch [840/1299] D_loss: -2.9952, G_loss: 0.1320\n",
      "  Batch [850/1299] D_loss: -4.9549, G_loss: 3.5940\n",
      "  Batch [860/1299] D_loss: -1.1366, G_loss: 0.0815\n",
      "  Batch [870/1299] D_loss: -1.8134, G_loss: -1.2567\n",
      "  Batch [880/1299] D_loss: 0.4043, G_loss: 1.7443\n",
      "  Batch [890/1299] D_loss: -0.1451, G_loss: 0.8526\n",
      "  Batch [900/1299] D_loss: -2.0613, G_loss: 2.3210\n",
      "  Batch [910/1299] D_loss: -2.5463, G_loss: 1.8055\n",
      "  Batch [920/1299] D_loss: -1.8119, G_loss: 1.5012\n",
      "  Batch [930/1299] D_loss: -4.6672, G_loss: 1.2084\n",
      "  Batch [940/1299] D_loss: -0.4250, G_loss: 1.0227\n",
      "  Batch [950/1299] D_loss: -2.6800, G_loss: 2.5431\n",
      "  Batch [960/1299] D_loss: 0.2149, G_loss: 2.7926\n",
      "  Batch [970/1299] D_loss: -4.0144, G_loss: 4.2713\n",
      "  Batch [980/1299] D_loss: -1.2454, G_loss: 1.2102\n",
      "  Batch [990/1299] D_loss: -2.0113, G_loss: 1.2558\n",
      "  Batch [1000/1299] D_loss: -2.9787, G_loss: -1.4178\n",
      "  Batch [1010/1299] D_loss: -2.7051, G_loss: 1.2063\n",
      "  Batch [1020/1299] D_loss: -3.8334, G_loss: 0.6028\n",
      "  Batch [1030/1299] D_loss: -2.6986, G_loss: 3.5545\n",
      "  Batch [1040/1299] D_loss: -1.7039, G_loss: 1.8937\n",
      "  Batch [1050/1299] D_loss: -2.2014, G_loss: 4.0695\n",
      "  Batch [1060/1299] D_loss: -1.5339, G_loss: 5.2321\n",
      "  Batch [1070/1299] D_loss: -2.5355, G_loss: 0.7771\n",
      "  Batch [1080/1299] D_loss: -1.8264, G_loss: 0.5767\n",
      "  Batch [1090/1299] D_loss: -1.0546, G_loss: -1.1270\n",
      "  Batch [1100/1299] D_loss: -1.8391, G_loss: -4.3887\n",
      "  Batch [1110/1299] D_loss: -2.9367, G_loss: -4.0260\n",
      "  Batch [1120/1299] D_loss: -4.3956, G_loss: -0.7744\n",
      "  Batch [1130/1299] D_loss: -2.4543, G_loss: 0.4937\n",
      "  Batch [1140/1299] D_loss: -2.0685, G_loss: -0.8685\n",
      "  Batch [1150/1299] D_loss: -3.3057, G_loss: 1.6983\n",
      "  Batch [1160/1299] D_loss: -2.8463, G_loss: 2.9349\n",
      "  Batch [1170/1299] D_loss: -2.5288, G_loss: 2.1126\n",
      "  Batch [1180/1299] D_loss: -2.2241, G_loss: 8.2625\n",
      "  Batch [1190/1299] D_loss: -2.9802, G_loss: 5.5800\n",
      "  Batch [1200/1299] D_loss: 0.5756, G_loss: 5.4566\n",
      "  Batch [1210/1299] D_loss: -0.8245, G_loss: 3.7268\n",
      "  Batch [1220/1299] D_loss: -1.7832, G_loss: 3.2735\n",
      "  Batch [1230/1299] D_loss: -1.9399, G_loss: 3.0817\n",
      "  Batch [1240/1299] D_loss: -1.2011, G_loss: 3.1300\n",
      "  Batch [1250/1299] D_loss: -3.5600, G_loss: 3.3288\n",
      "  Batch [1260/1299] D_loss: -4.3085, G_loss: 3.7654\n",
      "  Batch [1270/1299] D_loss: -2.2037, G_loss: 0.2667\n",
      "  Batch [1280/1299] D_loss: -2.8625, G_loss: 1.5859\n",
      "  Batch [1290/1299] D_loss: -1.5787, G_loss: 2.0562\n",
      "\n",
      "Epoch 58 Summary:\n",
      "  Average D_loss: -1.9488\n",
      "  Average G_loss: 0.9788\n",
      "\n",
      "Epoch [59/100]\n",
      "  Batch [0/1299] D_loss: -1.9300, G_loss: 2.0538\n",
      "  Batch [10/1299] D_loss: -4.0196, G_loss: -2.6501\n",
      "  Batch [20/1299] D_loss: -3.1294, G_loss: -3.4943\n",
      "  Batch [30/1299] D_loss: -1.9440, G_loss: 2.1761\n",
      "  Batch [40/1299] D_loss: -1.4200, G_loss: 2.6357\n",
      "  Batch [50/1299] D_loss: -1.3819, G_loss: 1.2020\n",
      "  Batch [60/1299] D_loss: -1.0453, G_loss: 4.0283\n",
      "  Batch [70/1299] D_loss: -3.0921, G_loss: 7.2905\n",
      "  Batch [80/1299] D_loss: -2.5210, G_loss: 2.9264\n",
      "  Batch [90/1299] D_loss: -1.5761, G_loss: 2.9990\n",
      "  Batch [100/1299] D_loss: -1.2302, G_loss: 1.2382\n",
      "  Batch [110/1299] D_loss: -1.7469, G_loss: 1.4805\n",
      "  Batch [120/1299] D_loss: -2.3946, G_loss: 2.8235\n",
      "  Batch [130/1299] D_loss: -4.3860, G_loss: 3.0045\n",
      "  Batch [140/1299] D_loss: -1.1809, G_loss: 3.4759\n",
      "  Batch [150/1299] D_loss: -1.6673, G_loss: 3.6878\n",
      "  Batch [160/1299] D_loss: -0.9273, G_loss: 0.9962\n",
      "  Batch [170/1299] D_loss: -1.1884, G_loss: 0.2287\n",
      "  Batch [180/1299] D_loss: -2.1074, G_loss: -0.6540\n",
      "  Batch [190/1299] D_loss: -0.9899, G_loss: -1.1498\n",
      "  Batch [200/1299] D_loss: -1.3088, G_loss: -0.1737\n",
      "  Batch [210/1299] D_loss: -1.6871, G_loss: 2.0624\n",
      "  Batch [220/1299] D_loss: -3.8245, G_loss: 5.8021\n",
      "  Batch [230/1299] D_loss: -3.4017, G_loss: 3.5985\n",
      "  Batch [240/1299] D_loss: -4.8479, G_loss: 2.4148\n",
      "  Batch [250/1299] D_loss: -1.0307, G_loss: 0.6001\n",
      "  Batch [260/1299] D_loss: -3.2667, G_loss: -0.0119\n",
      "  Batch [270/1299] D_loss: -1.0356, G_loss: 1.0746\n",
      "  Batch [280/1299] D_loss: -1.0030, G_loss: 1.1674\n",
      "  Batch [290/1299] D_loss: -1.0088, G_loss: -0.7932\n",
      "  Batch [300/1299] D_loss: -1.3852, G_loss: -1.0961\n",
      "  Batch [310/1299] D_loss: -2.4406, G_loss: -1.7948\n",
      "  Batch [320/1299] D_loss: -2.1685, G_loss: 1.4067\n",
      "  Batch [330/1299] D_loss: -2.0354, G_loss: 1.5049\n",
      "  Batch [340/1299] D_loss: -2.5211, G_loss: 1.0809\n",
      "  Batch [350/1299] D_loss: -1.5306, G_loss: 0.6927\n",
      "  Batch [360/1299] D_loss: -2.1784, G_loss: -0.7087\n",
      "  Batch [370/1299] D_loss: -2.4097, G_loss: -0.8800\n",
      "  Batch [380/1299] D_loss: -2.3696, G_loss: -0.0559\n",
      "  Batch [390/1299] D_loss: -2.0771, G_loss: -0.6211\n",
      "  Batch [400/1299] D_loss: -2.0054, G_loss: -1.6016\n",
      "  Batch [410/1299] D_loss: -2.0553, G_loss: -1.1852\n",
      "  Batch [420/1299] D_loss: -2.9567, G_loss: -2.0475\n",
      "  Batch [430/1299] D_loss: -2.4031, G_loss: 0.9215\n",
      "  Batch [440/1299] D_loss: -1.7430, G_loss: 2.6264\n",
      "  Batch [450/1299] D_loss: -1.9256, G_loss: 3.0924\n",
      "  Batch [460/1299] D_loss: -2.9510, G_loss: 2.3983\n",
      "  Batch [470/1299] D_loss: -2.4198, G_loss: 1.1420\n",
      "  Batch [480/1299] D_loss: -2.5247, G_loss: 3.7139\n",
      "  Batch [490/1299] D_loss: -1.9579, G_loss: 1.5192\n",
      "  Batch [500/1299] D_loss: -4.0773, G_loss: 0.2434\n",
      "  Batch [510/1299] D_loss: -1.5346, G_loss: 1.4984\n",
      "  Batch [520/1299] D_loss: -0.8070, G_loss: -0.2457\n",
      "  Batch [530/1299] D_loss: -3.9440, G_loss: -1.6497\n",
      "  Batch [540/1299] D_loss: -1.8853, G_loss: -2.9719\n",
      "  Batch [550/1299] D_loss: -1.9450, G_loss: -1.5892\n",
      "  Batch [560/1299] D_loss: -1.6010, G_loss: -2.3463\n",
      "  Batch [570/1299] D_loss: -2.1018, G_loss: -0.7572\n",
      "  Batch [580/1299] D_loss: -3.9375, G_loss: -0.3724\n",
      "  Batch [590/1299] D_loss: -2.1963, G_loss: -2.0401\n",
      "  Batch [600/1299] D_loss: -2.8764, G_loss: 0.1037\n",
      "  Batch [610/1299] D_loss: -2.8550, G_loss: 1.3116\n",
      "  Batch [620/1299] D_loss: -3.2778, G_loss: 1.9170\n",
      "  Batch [630/1299] D_loss: -2.7774, G_loss: 2.9386\n",
      "  Batch [640/1299] D_loss: -2.1881, G_loss: 1.6422\n",
      "  Batch [650/1299] D_loss: -2.2591, G_loss: 1.1106\n",
      "  Batch [660/1299] D_loss: -2.9115, G_loss: -0.7095\n",
      "  Batch [670/1299] D_loss: -2.2571, G_loss: -1.5734\n",
      "  Batch [680/1299] D_loss: -3.1439, G_loss: -2.6695\n",
      "  Batch [690/1299] D_loss: -0.7882, G_loss: -2.2783\n",
      "  Batch [700/1299] D_loss: -2.9635, G_loss: -1.8757\n",
      "  Batch [710/1299] D_loss: -2.8611, G_loss: -3.2022\n",
      "  Batch [720/1299] D_loss: -1.4696, G_loss: -1.4279\n",
      "  Batch [730/1299] D_loss: -2.5063, G_loss: -2.9402\n",
      "  Batch [740/1299] D_loss: -2.4517, G_loss: -3.1483\n",
      "  Batch [750/1299] D_loss: -2.8794, G_loss: -0.7856\n",
      "  Batch [760/1299] D_loss: -1.6192, G_loss: -1.7151\n",
      "  Batch [770/1299] D_loss: -3.2507, G_loss: -2.0154\n",
      "  Batch [780/1299] D_loss: -1.5920, G_loss: 0.3942\n",
      "  Batch [790/1299] D_loss: -1.2489, G_loss: -0.2211\n",
      "  Batch [800/1299] D_loss: -2.8239, G_loss: 1.2684\n",
      "  Batch [810/1299] D_loss: -1.0150, G_loss: 2.1360\n",
      "  Batch [820/1299] D_loss: -1.7311, G_loss: 2.9324\n",
      "  Batch [830/1299] D_loss: -1.4915, G_loss: 1.5922\n",
      "  Batch [840/1299] D_loss: -3.0520, G_loss: -0.9120\n",
      "  Batch [850/1299] D_loss: -4.5290, G_loss: -1.3652\n",
      "  Batch [860/1299] D_loss: -2.2823, G_loss: 0.9328\n",
      "  Batch [870/1299] D_loss: -2.1059, G_loss: -2.2669\n",
      "  Batch [880/1299] D_loss: -2.7045, G_loss: -4.5246\n",
      "  Batch [890/1299] D_loss: -2.5085, G_loss: -4.2641\n",
      "  Batch [900/1299] D_loss: -3.0260, G_loss: -5.5163\n",
      "  Batch [910/1299] D_loss: -1.5776, G_loss: -1.5671\n",
      "  Batch [920/1299] D_loss: -1.1394, G_loss: -1.7980\n",
      "  Batch [930/1299] D_loss: -1.6666, G_loss: 0.7276\n",
      "  Batch [940/1299] D_loss: -1.9902, G_loss: 3.6044\n",
      "  Batch [950/1299] D_loss: -2.1826, G_loss: 5.5172\n",
      "  Batch [960/1299] D_loss: -3.1245, G_loss: 6.3580\n",
      "  Batch [970/1299] D_loss: -3.0210, G_loss: 5.9089\n",
      "  Batch [980/1299] D_loss: -3.3510, G_loss: 5.4663\n",
      "  Batch [990/1299] D_loss: -1.2433, G_loss: 1.2880\n",
      "  Batch [1000/1299] D_loss: -0.5937, G_loss: 0.6161\n",
      "  Batch [1010/1299] D_loss: -2.3296, G_loss: -0.8739\n",
      "  Batch [1020/1299] D_loss: -4.5392, G_loss: -2.6758\n",
      "  Batch [1030/1299] D_loss: -2.5174, G_loss: -2.0309\n",
      "  Batch [1040/1299] D_loss: -1.3912, G_loss: -1.0748\n",
      "  Batch [1050/1299] D_loss: -2.3237, G_loss: 1.0989\n",
      "  Batch [1060/1299] D_loss: -2.4053, G_loss: -0.9249\n",
      "  Batch [1070/1299] D_loss: -1.4051, G_loss: -1.2170\n",
      "  Batch [1080/1299] D_loss: -1.7175, G_loss: 0.1762\n",
      "  Batch [1090/1299] D_loss: -2.8909, G_loss: 2.6360\n",
      "  Batch [1100/1299] D_loss: -4.5390, G_loss: 1.8752\n",
      "  Batch [1110/1299] D_loss: -2.1465, G_loss: 1.4415\n",
      "  Batch [1120/1299] D_loss: -2.6925, G_loss: 2.7166\n",
      "  Batch [1130/1299] D_loss: -3.3097, G_loss: 3.0363\n",
      "  Batch [1140/1299] D_loss: -1.8068, G_loss: 1.9128\n",
      "  Batch [1150/1299] D_loss: -2.5530, G_loss: 1.7944\n",
      "  Batch [1160/1299] D_loss: -0.6622, G_loss: 1.0007\n",
      "  Batch [1170/1299] D_loss: -2.4029, G_loss: -3.2364\n",
      "  Batch [1180/1299] D_loss: -0.1095, G_loss: -2.6092\n",
      "  Batch [1190/1299] D_loss: -2.5879, G_loss: -2.9903\n",
      "  Batch [1200/1299] D_loss: -0.7417, G_loss: -3.8359\n",
      "  Batch [1210/1299] D_loss: -1.8212, G_loss: -2.5253\n",
      "  Batch [1220/1299] D_loss: -1.9399, G_loss: -3.2330\n",
      "  Batch [1230/1299] D_loss: -2.0314, G_loss: 0.7103\n",
      "  Batch [1240/1299] D_loss: -5.3310, G_loss: -0.8456\n",
      "  Batch [1250/1299] D_loss: -3.4516, G_loss: 1.8549\n",
      "  Batch [1260/1299] D_loss: -2.8427, G_loss: 2.4809\n",
      "  Batch [1270/1299] D_loss: -2.7982, G_loss: 2.3616\n",
      "  Batch [1280/1299] D_loss: -2.7153, G_loss: 3.4416\n",
      "  Batch [1290/1299] D_loss: -2.8743, G_loss: 3.4367\n",
      "\n",
      "Epoch 59 Summary:\n",
      "  Average D_loss: -1.9460\n",
      "  Average G_loss: 0.5232\n",
      "\n",
      "Epoch [60/100]\n",
      "  Batch [0/1299] D_loss: -4.7826, G_loss: 3.5598\n",
      "  Batch [10/1299] D_loss: -2.6205, G_loss: -0.6324\n",
      "  Batch [20/1299] D_loss: -2.8431, G_loss: 0.0789\n",
      "  Batch [30/1299] D_loss: -0.9703, G_loss: 1.0964\n",
      "  Batch [40/1299] D_loss: -0.0151, G_loss: 0.9635\n",
      "  Batch [50/1299] D_loss: -1.8224, G_loss: 2.1625\n",
      "  Batch [60/1299] D_loss: -2.4711, G_loss: 1.2159\n",
      "  Batch [70/1299] D_loss: -1.8480, G_loss: 2.2893\n",
      "  Batch [80/1299] D_loss: -3.9229, G_loss: 3.4688\n",
      "  Batch [90/1299] D_loss: -1.3984, G_loss: 5.4466\n",
      "  Batch [100/1299] D_loss: -2.1023, G_loss: 4.6348\n",
      "  Batch [110/1299] D_loss: -3.1376, G_loss: 2.1963\n",
      "  Batch [120/1299] D_loss: -2.5680, G_loss: 5.0983\n",
      "  Batch [130/1299] D_loss: -3.0399, G_loss: 3.8920\n",
      "  Batch [140/1299] D_loss: -4.3456, G_loss: 4.0686\n",
      "  Batch [150/1299] D_loss: -0.8547, G_loss: 3.5458\n",
      "  Batch [160/1299] D_loss: -2.5814, G_loss: 3.2571\n",
      "  Batch [170/1299] D_loss: -3.8030, G_loss: 0.6530\n",
      "  Batch [180/1299] D_loss: -0.5065, G_loss: -0.9203\n",
      "  Batch [190/1299] D_loss: -2.3139, G_loss: 1.8648\n",
      "  Batch [200/1299] D_loss: -4.4098, G_loss: 0.7544\n",
      "  Batch [210/1299] D_loss: -2.5132, G_loss: 1.1251\n",
      "  Batch [220/1299] D_loss: -2.8676, G_loss: 0.8053\n",
      "  Batch [230/1299] D_loss: -3.7931, G_loss: -3.1649\n",
      "  Batch [240/1299] D_loss: -2.6397, G_loss: -1.9282\n",
      "  Batch [250/1299] D_loss: -0.2953, G_loss: -2.7321\n",
      "  Batch [260/1299] D_loss: -0.9821, G_loss: -3.4362\n",
      "  Batch [270/1299] D_loss: -0.4565, G_loss: -4.2340\n",
      "  Batch [280/1299] D_loss: -0.9028, G_loss: -7.2456\n",
      "  Batch [290/1299] D_loss: -2.9831, G_loss: -2.5637\n",
      "  Batch [300/1299] D_loss: -1.3286, G_loss: -3.4155\n",
      "  Batch [310/1299] D_loss: -2.5877, G_loss: -2.1640\n",
      "  Batch [320/1299] D_loss: -1.2238, G_loss: 0.8329\n",
      "  Batch [330/1299] D_loss: -1.7705, G_loss: 1.7616\n",
      "  Batch [340/1299] D_loss: -0.4384, G_loss: 3.5708\n",
      "  Batch [350/1299] D_loss: -2.5858, G_loss: 1.2226\n",
      "  Batch [360/1299] D_loss: -2.4303, G_loss: -1.6203\n",
      "  Batch [370/1299] D_loss: -1.3341, G_loss: -3.0812\n",
      "  Batch [380/1299] D_loss: -0.8787, G_loss: -2.0868\n",
      "  Batch [390/1299] D_loss: -1.5758, G_loss: -2.7065\n",
      "  Batch [400/1299] D_loss: -1.6340, G_loss: -4.0456\n",
      "  Batch [410/1299] D_loss: -2.2580, G_loss: 0.4020\n",
      "  Batch [420/1299] D_loss: -2.3490, G_loss: 0.2162\n",
      "  Batch [430/1299] D_loss: -1.7209, G_loss: -0.5457\n",
      "  Batch [440/1299] D_loss: -1.9199, G_loss: 1.0382\n",
      "  Batch [450/1299] D_loss: -1.0037, G_loss: 4.9948\n",
      "  Batch [460/1299] D_loss: -3.2228, G_loss: 1.6462\n",
      "  Batch [470/1299] D_loss: -2.3288, G_loss: 0.1228\n",
      "  Batch [480/1299] D_loss: -1.3568, G_loss: 0.1247\n",
      "  Batch [490/1299] D_loss: -4.2434, G_loss: 0.8856\n",
      "  Batch [500/1299] D_loss: -1.8265, G_loss: 1.5876\n",
      "  Batch [510/1299] D_loss: -0.7536, G_loss: -1.1935\n",
      "  Batch [520/1299] D_loss: -0.1665, G_loss: -2.6841\n",
      "  Batch [530/1299] D_loss: -1.1346, G_loss: -5.0316\n",
      "  Batch [540/1299] D_loss: -1.9682, G_loss: -3.8979\n",
      "  Batch [550/1299] D_loss: -1.9629, G_loss: -4.9853\n",
      "  Batch [560/1299] D_loss: -2.9709, G_loss: -4.2872\n",
      "  Batch [570/1299] D_loss: -1.7256, G_loss: -4.6401\n",
      "  Batch [580/1299] D_loss: -2.7182, G_loss: -4.4741\n",
      "  Batch [590/1299] D_loss: -1.9922, G_loss: -0.8762\n",
      "  Batch [600/1299] D_loss: -1.4741, G_loss: -1.0680\n",
      "  Batch [610/1299] D_loss: -2.1012, G_loss: 0.7902\n",
      "  Batch [620/1299] D_loss: -1.5474, G_loss: 2.4147\n",
      "  Batch [630/1299] D_loss: -1.5984, G_loss: 7.6088\n",
      "  Batch [640/1299] D_loss: 0.0994, G_loss: 4.2310\n",
      "  Batch [650/1299] D_loss: -2.7764, G_loss: 4.0155\n",
      "  Batch [660/1299] D_loss: -1.8924, G_loss: 4.0167\n",
      "  Batch [670/1299] D_loss: -0.4173, G_loss: 1.6185\n",
      "  Batch [680/1299] D_loss: -2.2080, G_loss: 4.2533\n",
      "  Batch [690/1299] D_loss: -4.1072, G_loss: 2.9634\n",
      "  Batch [700/1299] D_loss: -0.1892, G_loss: 4.2537\n",
      "  Batch [710/1299] D_loss: -1.4862, G_loss: 4.4057\n",
      "  Batch [720/1299] D_loss: -3.7343, G_loss: 2.4016\n",
      "  Batch [730/1299] D_loss: -3.6678, G_loss: 2.4493\n",
      "  Batch [740/1299] D_loss: -3.1016, G_loss: 1.5095\n",
      "  Batch [750/1299] D_loss: -1.1370, G_loss: 2.1890\n",
      "  Batch [760/1299] D_loss: -1.6692, G_loss: 2.7550\n",
      "  Batch [770/1299] D_loss: -2.2850, G_loss: 2.1310\n",
      "  Batch [780/1299] D_loss: -0.0058, G_loss: 5.2844\n",
      "  Batch [790/1299] D_loss: -1.4531, G_loss: 2.1092\n",
      "  Batch [800/1299] D_loss: 1.0858, G_loss: 2.3847\n",
      "  Batch [810/1299] D_loss: -2.7181, G_loss: 3.6219\n",
      "  Batch [820/1299] D_loss: -5.2303, G_loss: 4.3553\n",
      "  Batch [830/1299] D_loss: -2.7964, G_loss: 5.7405\n",
      "  Batch [840/1299] D_loss: -1.5609, G_loss: 4.5309\n",
      "  Batch [850/1299] D_loss: -2.3682, G_loss: 3.3479\n",
      "  Batch [860/1299] D_loss: -1.4296, G_loss: 4.4384\n",
      "  Batch [870/1299] D_loss: -0.9354, G_loss: 0.9301\n",
      "  Batch [880/1299] D_loss: -3.2129, G_loss: -0.1185\n",
      "  Batch [890/1299] D_loss: -2.6286, G_loss: 1.9613\n",
      "  Batch [900/1299] D_loss: -1.8332, G_loss: 0.8896\n",
      "  Batch [910/1299] D_loss: -2.7986, G_loss: -1.3272\n",
      "  Batch [920/1299] D_loss: -3.8002, G_loss: 0.1359\n",
      "  Batch [930/1299] D_loss: -1.4529, G_loss: -1.7892\n",
      "  Batch [940/1299] D_loss: -2.7874, G_loss: -0.6564\n",
      "  Batch [950/1299] D_loss: -0.7128, G_loss: 1.7653\n",
      "  Batch [960/1299] D_loss: -0.7425, G_loss: 1.6657\n",
      "  Batch [970/1299] D_loss: -0.5295, G_loss: 4.2873\n",
      "  Batch [980/1299] D_loss: -3.5678, G_loss: 0.2743\n",
      "  Batch [990/1299] D_loss: -1.3558, G_loss: -1.6252\n",
      "  Batch [1000/1299] D_loss: 0.1913, G_loss: 0.5955\n",
      "  Batch [1010/1299] D_loss: -1.6646, G_loss: -1.7376\n",
      "  Batch [1020/1299] D_loss: -1.2670, G_loss: 1.4548\n",
      "  Batch [1030/1299] D_loss: -2.4745, G_loss: -2.7050\n",
      "  Batch [1040/1299] D_loss: -2.9082, G_loss: -0.1662\n",
      "  Batch [1050/1299] D_loss: -2.8188, G_loss: -0.2382\n",
      "  Batch [1060/1299] D_loss: -2.7921, G_loss: 0.1540\n",
      "  Batch [1070/1299] D_loss: -2.0201, G_loss: 2.1963\n",
      "  Batch [1080/1299] D_loss: 0.1306, G_loss: 2.3311\n",
      "  Batch [1090/1299] D_loss: -2.4913, G_loss: 2.3823\n",
      "  Batch [1100/1299] D_loss: -1.1486, G_loss: 1.7375\n",
      "  Batch [1110/1299] D_loss: -2.9783, G_loss: 0.5774\n",
      "  Batch [1120/1299] D_loss: -0.3433, G_loss: 3.6129\n",
      "  Batch [1130/1299] D_loss: -2.1582, G_loss: 2.1632\n",
      "  Batch [1140/1299] D_loss: 0.4424, G_loss: 0.8498\n",
      "  Batch [1150/1299] D_loss: -3.0287, G_loss: -1.5188\n",
      "  Batch [1160/1299] D_loss: -2.4602, G_loss: 0.4443\n",
      "  Batch [1170/1299] D_loss: -0.3958, G_loss: -0.1020\n",
      "  Batch [1180/1299] D_loss: -3.0681, G_loss: -0.1736\n",
      "  Batch [1190/1299] D_loss: -3.0852, G_loss: -1.5041\n",
      "  Batch [1200/1299] D_loss: -1.2478, G_loss: 0.0517\n",
      "  Batch [1210/1299] D_loss: -0.9339, G_loss: 0.7664\n",
      "  Batch [1220/1299] D_loss: -2.1268, G_loss: 1.4001\n",
      "  Batch [1230/1299] D_loss: -2.0362, G_loss: 2.9232\n",
      "  Batch [1240/1299] D_loss: -1.1660, G_loss: 4.1649\n",
      "  Batch [1250/1299] D_loss: -0.6500, G_loss: 6.0502\n",
      "  Batch [1260/1299] D_loss: -2.3454, G_loss: 4.2383\n",
      "  Batch [1270/1299] D_loss: -3.2922, G_loss: 6.4211\n",
      "  Batch [1280/1299] D_loss: -2.7945, G_loss: 4.6051\n",
      "  Batch [1290/1299] D_loss: -2.5546, G_loss: 2.3246\n",
      "\n",
      "Epoch 60 Summary:\n",
      "  Average D_loss: -1.9361\n",
      "  Average G_loss: 0.9832\n",
      "\n",
      "Epoch [61/100]\n",
      "  Batch [0/1299] D_loss: -1.5805, G_loss: 3.5923\n",
      "  Batch [10/1299] D_loss: -0.8994, G_loss: 0.7544\n",
      "  Batch [20/1299] D_loss: -2.8572, G_loss: -1.8213\n",
      "  Batch [30/1299] D_loss: -0.8089, G_loss: -1.0225\n",
      "  Batch [40/1299] D_loss: -3.2520, G_loss: -1.1014\n",
      "  Batch [50/1299] D_loss: -2.4232, G_loss: -1.2210\n",
      "  Batch [60/1299] D_loss: -2.6207, G_loss: -1.0388\n",
      "  Batch [70/1299] D_loss: -0.2358, G_loss: 0.2536\n",
      "  Batch [80/1299] D_loss: -1.0238, G_loss: 0.4480\n",
      "  Batch [90/1299] D_loss: -3.3451, G_loss: 2.8956\n",
      "  Batch [100/1299] D_loss: -2.7218, G_loss: 1.0256\n",
      "  Batch [110/1299] D_loss: -1.4988, G_loss: 0.5781\n",
      "  Batch [120/1299] D_loss: -1.7968, G_loss: 1.6749\n",
      "  Batch [130/1299] D_loss: -1.1997, G_loss: -0.2622\n",
      "  Batch [140/1299] D_loss: -1.9994, G_loss: -0.9471\n",
      "  Batch [150/1299] D_loss: -2.7534, G_loss: -0.7467\n",
      "  Batch [160/1299] D_loss: -1.7825, G_loss: -0.6430\n",
      "  Batch [170/1299] D_loss: -3.3322, G_loss: 0.9599\n",
      "  Batch [180/1299] D_loss: -3.4077, G_loss: 0.8263\n",
      "  Batch [190/1299] D_loss: -2.5183, G_loss: 2.0645\n",
      "  Batch [200/1299] D_loss: -1.1938, G_loss: 1.6767\n",
      "  Batch [210/1299] D_loss: -3.0272, G_loss: 3.4485\n",
      "  Batch [220/1299] D_loss: -1.7988, G_loss: 3.5776\n",
      "  Batch [230/1299] D_loss: -1.7277, G_loss: 2.6453\n",
      "  Batch [240/1299] D_loss: -2.5411, G_loss: 1.4871\n",
      "  Batch [250/1299] D_loss: -1.7634, G_loss: 1.0229\n",
      "  Batch [260/1299] D_loss: -0.8062, G_loss: -0.2449\n",
      "  Batch [270/1299] D_loss: -1.9726, G_loss: 0.2657\n",
      "  Batch [280/1299] D_loss: -0.9229, G_loss: 1.0706\n",
      "  Batch [290/1299] D_loss: -2.5791, G_loss: -0.0556\n",
      "  Batch [300/1299] D_loss: -0.6115, G_loss: -3.0703\n",
      "  Batch [310/1299] D_loss: -1.8056, G_loss: -0.9756\n",
      "  Batch [320/1299] D_loss: -3.3330, G_loss: 0.8961\n",
      "  Batch [330/1299] D_loss: -2.3709, G_loss: -1.3420\n",
      "  Batch [340/1299] D_loss: -2.5583, G_loss: -1.2297\n",
      "  Batch [350/1299] D_loss: -2.7116, G_loss: -2.6871\n",
      "  Batch [360/1299] D_loss: -1.3346, G_loss: 0.1705\n",
      "  Batch [370/1299] D_loss: -1.0394, G_loss: -0.7711\n",
      "  Batch [380/1299] D_loss: -1.6353, G_loss: -1.8231\n",
      "  Batch [390/1299] D_loss: -2.6305, G_loss: -0.5307\n",
      "  Batch [400/1299] D_loss: -1.7321, G_loss: 1.6985\n",
      "  Batch [410/1299] D_loss: -1.2566, G_loss: 4.7520\n",
      "  Batch [420/1299] D_loss: -2.1867, G_loss: 4.0098\n",
      "  Batch [430/1299] D_loss: -1.2280, G_loss: 2.1225\n",
      "  Batch [440/1299] D_loss: -2.3459, G_loss: 3.0076\n",
      "  Batch [450/1299] D_loss: -1.2973, G_loss: 3.0507\n",
      "  Batch [460/1299] D_loss: -1.6585, G_loss: 2.8882\n",
      "  Batch [470/1299] D_loss: -3.2998, G_loss: 2.1403\n",
      "  Batch [480/1299] D_loss: -3.1339, G_loss: 3.9868\n",
      "  Batch [490/1299] D_loss: -2.4647, G_loss: 2.3730\n",
      "  Batch [500/1299] D_loss: -4.0486, G_loss: 3.3193\n",
      "  Batch [510/1299] D_loss: -0.9382, G_loss: 3.4199\n",
      "  Batch [520/1299] D_loss: -2.8772, G_loss: 4.0294\n",
      "  Batch [530/1299] D_loss: -2.4748, G_loss: 4.5465\n",
      "  Batch [540/1299] D_loss: -3.8335, G_loss: 3.1661\n",
      "  Batch [550/1299] D_loss: -2.5214, G_loss: 3.8110\n",
      "  Batch [560/1299] D_loss: -1.9902, G_loss: 4.2660\n",
      "  Batch [570/1299] D_loss: -1.5490, G_loss: 7.1029\n",
      "  Batch [580/1299] D_loss: -1.2046, G_loss: 4.0650\n",
      "  Batch [590/1299] D_loss: -2.2123, G_loss: 3.2935\n",
      "  Batch [600/1299] D_loss: -2.4319, G_loss: 3.0083\n",
      "  Batch [610/1299] D_loss: -2.1116, G_loss: 2.1010\n",
      "  Batch [620/1299] D_loss: -1.8039, G_loss: 1.0432\n",
      "  Batch [630/1299] D_loss: -2.3086, G_loss: 1.6302\n",
      "  Batch [640/1299] D_loss: -3.5692, G_loss: -0.5498\n",
      "  Batch [650/1299] D_loss: -2.3279, G_loss: -1.9468\n",
      "  Batch [660/1299] D_loss: -3.1192, G_loss: -2.6856\n",
      "  Batch [670/1299] D_loss: -1.4127, G_loss: -3.5488\n",
      "  Batch [680/1299] D_loss: -0.6988, G_loss: -1.6218\n",
      "  Batch [690/1299] D_loss: -1.7434, G_loss: -1.7171\n",
      "  Batch [700/1299] D_loss: -2.3728, G_loss: -0.9290\n",
      "  Batch [710/1299] D_loss: -2.4829, G_loss: -2.1556\n",
      "  Batch [720/1299] D_loss: -1.9670, G_loss: -2.6789\n",
      "  Batch [730/1299] D_loss: -3.0287, G_loss: 0.3675\n",
      "  Batch [740/1299] D_loss: -1.9381, G_loss: 0.3214\n",
      "  Batch [750/1299] D_loss: -1.2521, G_loss: -0.8376\n",
      "  Batch [760/1299] D_loss: -1.4536, G_loss: 0.9445\n",
      "  Batch [770/1299] D_loss: -0.7618, G_loss: -0.7453\n",
      "  Batch [780/1299] D_loss: -2.0558, G_loss: 4.5734\n",
      "  Batch [790/1299] D_loss: -1.6564, G_loss: 3.6417\n",
      "  Batch [800/1299] D_loss: -0.8391, G_loss: 5.1219\n",
      "  Batch [810/1299] D_loss: -3.2211, G_loss: 4.7944\n",
      "  Batch [820/1299] D_loss: -2.3902, G_loss: 3.1912\n",
      "  Batch [830/1299] D_loss: -2.9564, G_loss: 4.3701\n",
      "  Batch [840/1299] D_loss: -1.7446, G_loss: 5.0517\n",
      "  Batch [850/1299] D_loss: -1.8006, G_loss: 3.7973\n",
      "  Batch [860/1299] D_loss: -1.8386, G_loss: 3.1662\n",
      "  Batch [870/1299] D_loss: -1.0276, G_loss: 4.8778\n",
      "  Batch [880/1299] D_loss: -3.6272, G_loss: 2.4966\n",
      "  Batch [890/1299] D_loss: -1.2174, G_loss: 2.6147\n",
      "  Batch [900/1299] D_loss: -2.0188, G_loss: 1.6655\n",
      "  Batch [910/1299] D_loss: -2.5864, G_loss: 3.0766\n",
      "  Batch [920/1299] D_loss: -2.5292, G_loss: 2.3681\n",
      "  Batch [930/1299] D_loss: -1.3633, G_loss: 2.7880\n",
      "  Batch [940/1299] D_loss: -2.5110, G_loss: 1.9398\n",
      "  Batch [950/1299] D_loss: 0.0736, G_loss: 1.8972\n",
      "  Batch [960/1299] D_loss: -1.4202, G_loss: 2.4828\n",
      "  Batch [970/1299] D_loss: -3.1997, G_loss: 3.1889\n",
      "  Batch [980/1299] D_loss: -1.2204, G_loss: 3.1637\n",
      "  Batch [990/1299] D_loss: -2.8180, G_loss: 3.8549\n",
      "  Batch [1000/1299] D_loss: -2.0305, G_loss: 4.1042\n",
      "  Batch [1010/1299] D_loss: -1.8977, G_loss: 1.5289\n",
      "  Batch [1020/1299] D_loss: -1.6861, G_loss: 2.8826\n",
      "  Batch [1030/1299] D_loss: -2.3655, G_loss: 3.3671\n",
      "  Batch [1040/1299] D_loss: -2.6028, G_loss: 5.6299\n",
      "  Batch [1050/1299] D_loss: -4.8964, G_loss: 6.4890\n",
      "  Batch [1060/1299] D_loss: -1.1814, G_loss: 3.5683\n",
      "  Batch [1070/1299] D_loss: -2.3420, G_loss: 3.6281\n",
      "  Batch [1080/1299] D_loss: -2.8079, G_loss: 3.2500\n",
      "  Batch [1090/1299] D_loss: -1.4909, G_loss: 1.2364\n",
      "  Batch [1100/1299] D_loss: -1.4536, G_loss: -0.0825\n",
      "  Batch [1110/1299] D_loss: -3.1275, G_loss: 1.5747\n",
      "  Batch [1120/1299] D_loss: -1.0587, G_loss: -1.9150\n",
      "  Batch [1130/1299] D_loss: -2.1533, G_loss: -1.8647\n",
      "  Batch [1140/1299] D_loss: -2.7070, G_loss: -2.3832\n",
      "  Batch [1150/1299] D_loss: -1.3788, G_loss: -2.6872\n",
      "  Batch [1160/1299] D_loss: -1.8396, G_loss: -1.7880\n",
      "  Batch [1170/1299] D_loss: -2.8181, G_loss: 3.4085\n",
      "  Batch [1180/1299] D_loss: -2.3447, G_loss: 3.0983\n",
      "  Batch [1190/1299] D_loss: -2.2855, G_loss: 1.5779\n",
      "  Batch [1200/1299] D_loss: -3.0365, G_loss: -0.8526\n",
      "  Batch [1210/1299] D_loss: -2.2327, G_loss: 0.5671\n",
      "  Batch [1220/1299] D_loss: -1.5752, G_loss: 1.3482\n",
      "  Batch [1230/1299] D_loss: -2.4202, G_loss: 2.8862\n",
      "  Batch [1240/1299] D_loss: -3.2389, G_loss: 4.3773\n",
      "  Batch [1250/1299] D_loss: -2.4769, G_loss: 5.7078\n",
      "  Batch [1260/1299] D_loss: -2.9354, G_loss: 3.5707\n",
      "  Batch [1270/1299] D_loss: -1.6296, G_loss: 4.0505\n",
      "  Batch [1280/1299] D_loss: -2.3526, G_loss: 3.8672\n",
      "  Batch [1290/1299] D_loss: -1.2612, G_loss: 1.5278\n",
      "\n",
      "Epoch 61 Summary:\n",
      "  Average D_loss: -1.9612\n",
      "  Average G_loss: 1.5315\n",
      "\n",
      "Models saved at epoch 61:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_150654_dataset+cell_type/generator_20250121_150654_dataset+cell_type_epoch_61.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_150654_dataset+cell_type/discriminator_20250121_150654_dataset+cell_type_epoch_61.pt\n",
      "\n",
      "Epoch [62/100]\n",
      "  Batch [0/1299] D_loss: -2.4329, G_loss: -0.9755\n",
      "  Batch [10/1299] D_loss: -1.9801, G_loss: -3.3055\n",
      "  Batch [20/1299] D_loss: -2.7973, G_loss: -3.8140\n",
      "  Batch [30/1299] D_loss: -2.5619, G_loss: -3.0117\n",
      "  Batch [40/1299] D_loss: -0.4759, G_loss: -2.6543\n",
      "  Batch [50/1299] D_loss: -1.8449, G_loss: -2.1581\n",
      "  Batch [60/1299] D_loss: -2.2819, G_loss: -1.0645\n",
      "  Batch [70/1299] D_loss: -2.2974, G_loss: 0.6766\n",
      "  Batch [80/1299] D_loss: -2.7457, G_loss: 3.3190\n",
      "  Batch [90/1299] D_loss: -2.8114, G_loss: 2.9664\n",
      "  Batch [100/1299] D_loss: -2.6262, G_loss: 1.6455\n",
      "  Batch [110/1299] D_loss: -2.0644, G_loss: 1.5469\n",
      "  Batch [120/1299] D_loss: -2.0847, G_loss: 1.9466\n",
      "  Batch [130/1299] D_loss: -2.9365, G_loss: 1.8559\n",
      "  Batch [140/1299] D_loss: -2.8842, G_loss: 0.4457\n",
      "  Batch [150/1299] D_loss: -2.1015, G_loss: 0.5762\n",
      "  Batch [160/1299] D_loss: -2.8217, G_loss: -0.6654\n",
      "  Batch [170/1299] D_loss: -1.6565, G_loss: -0.4890\n",
      "  Batch [180/1299] D_loss: -1.5779, G_loss: -0.4735\n",
      "  Batch [190/1299] D_loss: -3.0550, G_loss: -1.2735\n",
      "  Batch [200/1299] D_loss: -1.9060, G_loss: -0.7122\n",
      "  Batch [210/1299] D_loss: 0.3639, G_loss: 0.1397\n",
      "  Batch [220/1299] D_loss: -1.8837, G_loss: -0.0107\n",
      "  Batch [230/1299] D_loss: -1.2089, G_loss: -0.8921\n",
      "  Batch [240/1299] D_loss: -1.2089, G_loss: 0.4871\n",
      "  Batch [250/1299] D_loss: -2.8944, G_loss: -1.7401\n",
      "  Batch [260/1299] D_loss: -0.6439, G_loss: -2.7233\n",
      "  Batch [270/1299] D_loss: -1.5016, G_loss: -4.8054\n",
      "  Batch [280/1299] D_loss: -0.6450, G_loss: -3.5403\n",
      "  Batch [290/1299] D_loss: -1.7707, G_loss: -1.6637\n",
      "  Batch [300/1299] D_loss: -1.6893, G_loss: -1.7198\n",
      "  Batch [310/1299] D_loss: -1.6720, G_loss: 0.3326\n",
      "  Batch [320/1299] D_loss: -1.3648, G_loss: -0.8102\n",
      "  Batch [330/1299] D_loss: -2.0513, G_loss: 1.5172\n",
      "  Batch [340/1299] D_loss: -2.3564, G_loss: 1.8526\n",
      "  Batch [350/1299] D_loss: -2.0580, G_loss: 1.6049\n",
      "  Batch [360/1299] D_loss: -2.4892, G_loss: 1.6719\n",
      "  Batch [370/1299] D_loss: -2.1239, G_loss: 1.2544\n",
      "  Batch [380/1299] D_loss: -3.4731, G_loss: 0.3351\n",
      "  Batch [390/1299] D_loss: -2.0442, G_loss: 1.7083\n",
      "  Batch [400/1299] D_loss: -1.9095, G_loss: 1.2013\n",
      "  Batch [410/1299] D_loss: -3.2862, G_loss: 0.3577\n",
      "  Batch [420/1299] D_loss: -2.4398, G_loss: 1.2809\n",
      "  Batch [430/1299] D_loss: -2.1989, G_loss: -0.4109\n",
      "  Batch [440/1299] D_loss: -2.1666, G_loss: -1.5041\n",
      "  Batch [450/1299] D_loss: -2.4459, G_loss: 1.0532\n",
      "  Batch [460/1299] D_loss: -2.5632, G_loss: -0.8571\n",
      "  Batch [470/1299] D_loss: -1.7721, G_loss: -0.2801\n",
      "  Batch [480/1299] D_loss: -1.7613, G_loss: -0.1564\n",
      "  Batch [490/1299] D_loss: -2.0403, G_loss: -0.3788\n",
      "  Batch [500/1299] D_loss: -2.9651, G_loss: -0.0335\n",
      "  Batch [510/1299] D_loss: -1.6361, G_loss: 0.6242\n",
      "  Batch [520/1299] D_loss: -2.1026, G_loss: -0.1739\n",
      "  Batch [530/1299] D_loss: -1.0213, G_loss: -0.1887\n",
      "  Batch [540/1299] D_loss: -3.0314, G_loss: -0.6735\n",
      "  Batch [550/1299] D_loss: -2.0542, G_loss: -1.1107\n",
      "  Batch [560/1299] D_loss: -2.0006, G_loss: 1.2268\n",
      "  Batch [570/1299] D_loss: -2.1965, G_loss: 2.7100\n",
      "  Batch [580/1299] D_loss: -2.8344, G_loss: 7.6782\n",
      "  Batch [590/1299] D_loss: -2.8710, G_loss: 5.1114\n",
      "  Batch [600/1299] D_loss: -2.3467, G_loss: 6.5597\n",
      "  Batch [610/1299] D_loss: -2.7720, G_loss: 3.8092\n",
      "  Batch [620/1299] D_loss: -2.4566, G_loss: 2.6906\n",
      "  Batch [630/1299] D_loss: -2.5526, G_loss: 0.4689\n",
      "  Batch [640/1299] D_loss: -1.3146, G_loss: 1.1663\n",
      "  Batch [650/1299] D_loss: -1.5775, G_loss: 0.0422\n",
      "  Batch [660/1299] D_loss: -2.4050, G_loss: -1.9474\n",
      "  Batch [670/1299] D_loss: -1.7400, G_loss: -1.2799\n",
      "  Batch [680/1299] D_loss: -1.3459, G_loss: -1.3371\n",
      "  Batch [690/1299] D_loss: -1.7026, G_loss: -1.1120\n",
      "  Batch [700/1299] D_loss: -0.7628, G_loss: -2.5630\n",
      "  Batch [710/1299] D_loss: -2.2957, G_loss: -1.9744\n",
      "  Batch [720/1299] D_loss: -2.0655, G_loss: -1.3970\n",
      "  Batch [730/1299] D_loss: -1.9643, G_loss: -0.6386\n",
      "  Batch [740/1299] D_loss: -2.1657, G_loss: 0.1739\n",
      "  Batch [750/1299] D_loss: -2.7092, G_loss: 1.4743\n",
      "  Batch [760/1299] D_loss: -2.4172, G_loss: 0.9835\n",
      "  Batch [770/1299] D_loss: -3.5621, G_loss: -0.0617\n",
      "  Batch [780/1299] D_loss: -1.8737, G_loss: 0.3059\n",
      "  Batch [790/1299] D_loss: -2.9659, G_loss: 0.6389\n",
      "  Batch [800/1299] D_loss: -1.4550, G_loss: -1.1084\n",
      "  Batch [810/1299] D_loss: -3.3471, G_loss: -0.3757\n",
      "  Batch [820/1299] D_loss: -1.9927, G_loss: 0.9010\n",
      "  Batch [830/1299] D_loss: -3.6984, G_loss: -2.7817\n",
      "  Batch [840/1299] D_loss: -1.6050, G_loss: -2.5671\n",
      "  Batch [850/1299] D_loss: -2.2473, G_loss: -2.9019\n",
      "  Batch [860/1299] D_loss: -2.4036, G_loss: -3.5054\n",
      "  Batch [870/1299] D_loss: -1.8665, G_loss: -3.1201\n",
      "  Batch [880/1299] D_loss: -1.9970, G_loss: -1.0582\n",
      "  Batch [890/1299] D_loss: -2.7373, G_loss: -0.9656\n",
      "  Batch [900/1299] D_loss: -2.7083, G_loss: -1.2004\n",
      "  Batch [910/1299] D_loss: -2.8703, G_loss: -4.1488\n",
      "  Batch [920/1299] D_loss: -0.6284, G_loss: 0.5044\n",
      "  Batch [930/1299] D_loss: -1.8555, G_loss: 1.8249\n",
      "  Batch [940/1299] D_loss: -1.5934, G_loss: 1.1120\n",
      "  Batch [950/1299] D_loss: -3.8374, G_loss: 1.9333\n",
      "  Batch [960/1299] D_loss: -1.8809, G_loss: 0.9869\n",
      "  Batch [970/1299] D_loss: -1.3746, G_loss: 1.4765\n",
      "  Batch [980/1299] D_loss: -1.5343, G_loss: 1.9374\n",
      "  Batch [990/1299] D_loss: -2.2880, G_loss: 2.1088\n",
      "  Batch [1000/1299] D_loss: -2.0415, G_loss: 2.3230\n",
      "  Batch [1010/1299] D_loss: -3.0403, G_loss: 2.6016\n",
      "  Batch [1020/1299] D_loss: -1.9994, G_loss: 2.2667\n",
      "  Batch [1030/1299] D_loss: -1.0716, G_loss: -0.6268\n",
      "  Batch [1040/1299] D_loss: -3.7083, G_loss: 1.3862\n",
      "  Batch [1050/1299] D_loss: -1.9043, G_loss: 0.9123\n",
      "  Batch [1060/1299] D_loss: -1.4043, G_loss: 0.7205\n",
      "  Batch [1070/1299] D_loss: -1.0552, G_loss: 0.3046\n",
      "  Batch [1080/1299] D_loss: -1.8750, G_loss: 2.5720\n",
      "  Batch [1090/1299] D_loss: -0.9486, G_loss: 3.8992\n",
      "  Batch [1100/1299] D_loss: -1.5759, G_loss: 5.1965\n",
      "  Batch [1110/1299] D_loss: -2.1391, G_loss: 4.6130\n",
      "  Batch [1120/1299] D_loss: -2.2979, G_loss: 3.1386\n",
      "  Batch [1130/1299] D_loss: -1.4869, G_loss: 5.1560\n",
      "  Batch [1140/1299] D_loss: -3.0643, G_loss: 0.5476\n",
      "  Batch [1150/1299] D_loss: -1.9047, G_loss: 1.5461\n",
      "  Batch [1160/1299] D_loss: -2.5228, G_loss: 0.7322\n",
      "  Batch [1170/1299] D_loss: -2.8375, G_loss: 1.6817\n",
      "  Batch [1180/1299] D_loss: -4.5800, G_loss: 5.3966\n",
      "  Batch [1190/1299] D_loss: -2.5644, G_loss: 4.3629\n",
      "  Batch [1200/1299] D_loss: -2.9057, G_loss: 5.6705\n",
      "  Batch [1210/1299] D_loss: -3.3333, G_loss: 3.1239\n",
      "  Batch [1220/1299] D_loss: -0.5551, G_loss: 2.5720\n",
      "  Batch [1230/1299] D_loss: -1.3486, G_loss: 2.9823\n",
      "  Batch [1240/1299] D_loss: -1.7956, G_loss: 4.3614\n",
      "  Batch [1250/1299] D_loss: -0.8769, G_loss: 3.1622\n",
      "  Batch [1260/1299] D_loss: -2.9205, G_loss: 2.7112\n",
      "  Batch [1270/1299] D_loss: -2.2551, G_loss: 1.7494\n",
      "  Batch [1280/1299] D_loss: -2.9507, G_loss: 1.8270\n",
      "  Batch [1290/1299] D_loss: -2.4209, G_loss: 1.0032\n",
      "\n",
      "Epoch 62 Summary:\n",
      "  Average D_loss: -1.9567\n",
      "  Average G_loss: 0.5826\n",
      "\n",
      "Epoch [63/100]\n",
      "  Batch [0/1299] D_loss: -2.4546, G_loss: 1.4004\n",
      "  Batch [10/1299] D_loss: -2.1776, G_loss: 1.1971\n",
      "  Batch [20/1299] D_loss: -2.3710, G_loss: 1.7345\n",
      "  Batch [30/1299] D_loss: -2.8492, G_loss: 3.3623\n",
      "  Batch [40/1299] D_loss: -2.6244, G_loss: 3.2689\n",
      "  Batch [50/1299] D_loss: -1.9992, G_loss: 5.0855\n",
      "  Batch [60/1299] D_loss: -2.2793, G_loss: 3.9157\n",
      "  Batch [70/1299] D_loss: -3.2805, G_loss: 4.0187\n",
      "  Batch [80/1299] D_loss: -1.5973, G_loss: 1.8313\n",
      "  Batch [90/1299] D_loss: -1.1817, G_loss: 1.1091\n",
      "  Batch [100/1299] D_loss: -2.4215, G_loss: 2.2351\n",
      "  Batch [110/1299] D_loss: -2.0819, G_loss: 1.3863\n",
      "  Batch [120/1299] D_loss: -0.9579, G_loss: -0.3683\n",
      "  Batch [130/1299] D_loss: -2.1883, G_loss: -0.1309\n",
      "  Batch [140/1299] D_loss: -1.6555, G_loss: 0.6482\n",
      "  Batch [150/1299] D_loss: -1.1831, G_loss: 0.9831\n",
      "  Batch [160/1299] D_loss: -2.7940, G_loss: 1.4042\n",
      "  Batch [170/1299] D_loss: -2.7513, G_loss: 2.1047\n",
      "  Batch [180/1299] D_loss: -0.8548, G_loss: 4.4811\n",
      "  Batch [190/1299] D_loss: -2.3239, G_loss: 6.1581\n",
      "  Batch [200/1299] D_loss: -2.1452, G_loss: 5.9781\n",
      "  Batch [210/1299] D_loss: -0.6892, G_loss: 3.7893\n",
      "  Batch [220/1299] D_loss: -1.1632, G_loss: 2.7761\n",
      "  Batch [230/1299] D_loss: -2.6306, G_loss: 3.4648\n",
      "  Batch [240/1299] D_loss: -2.8761, G_loss: 2.8812\n",
      "  Batch [250/1299] D_loss: -3.0414, G_loss: 2.0363\n",
      "  Batch [260/1299] D_loss: -2.7744, G_loss: 2.9232\n",
      "  Batch [270/1299] D_loss: -1.5510, G_loss: 3.0738\n",
      "  Batch [280/1299] D_loss: -1.9444, G_loss: 2.3678\n",
      "  Batch [290/1299] D_loss: -1.7686, G_loss: 0.1661\n",
      "  Batch [300/1299] D_loss: -1.5072, G_loss: 3.4257\n",
      "  Batch [310/1299] D_loss: -1.9299, G_loss: 4.3680\n",
      "  Batch [320/1299] D_loss: -2.3320, G_loss: 2.0375\n",
      "  Batch [330/1299] D_loss: -0.8527, G_loss: 1.3810\n",
      "  Batch [340/1299] D_loss: -3.0063, G_loss: 0.5144\n",
      "  Batch [350/1299] D_loss: -3.4592, G_loss: -0.8042\n",
      "  Batch [360/1299] D_loss: -1.4218, G_loss: -2.4004\n",
      "  Batch [370/1299] D_loss: -1.2137, G_loss: -1.7411\n",
      "  Batch [380/1299] D_loss: -3.5406, G_loss: -2.5262\n",
      "  Batch [390/1299] D_loss: -2.1136, G_loss: -1.3977\n",
      "  Batch [400/1299] D_loss: -2.3547, G_loss: 0.0020\n",
      "  Batch [410/1299] D_loss: -2.4102, G_loss: 1.5994\n",
      "  Batch [420/1299] D_loss: -1.6680, G_loss: 2.1814\n",
      "  Batch [430/1299] D_loss: -3.1735, G_loss: 3.2673\n",
      "  Batch [440/1299] D_loss: -1.3164, G_loss: 4.5191\n",
      "  Batch [450/1299] D_loss: -1.5352, G_loss: 3.3298\n",
      "  Batch [460/1299] D_loss: -0.6651, G_loss: 3.4266\n",
      "  Batch [470/1299] D_loss: -1.9369, G_loss: 2.1044\n",
      "  Batch [480/1299] D_loss: -1.5666, G_loss: 2.7342\n",
      "  Batch [490/1299] D_loss: -2.7617, G_loss: 1.8205\n",
      "  Batch [500/1299] D_loss: -4.8593, G_loss: 2.1241\n",
      "  Batch [510/1299] D_loss: -1.9786, G_loss: 0.1726\n",
      "  Batch [520/1299] D_loss: -1.9194, G_loss: -0.0586\n",
      "  Batch [530/1299] D_loss: -1.5411, G_loss: -0.0806\n",
      "  Batch [540/1299] D_loss: -1.5278, G_loss: -1.7647\n",
      "  Batch [550/1299] D_loss: -3.4269, G_loss: -0.1486\n",
      "  Batch [560/1299] D_loss: -3.3505, G_loss: 1.5121\n",
      "  Batch [570/1299] D_loss: -0.9210, G_loss: 0.8458\n",
      "  Batch [580/1299] D_loss: -3.1050, G_loss: 1.4174\n",
      "  Batch [590/1299] D_loss: -1.3805, G_loss: 1.4641\n",
      "  Batch [600/1299] D_loss: -2.4991, G_loss: 0.4347\n",
      "  Batch [610/1299] D_loss: -2.2456, G_loss: -0.1601\n",
      "  Batch [620/1299] D_loss: -3.1911, G_loss: 0.9113\n",
      "  Batch [630/1299] D_loss: -2.5825, G_loss: 3.1597\n",
      "  Batch [640/1299] D_loss: -0.8988, G_loss: 1.6913\n",
      "  Batch [650/1299] D_loss: -2.6491, G_loss: 3.0644\n",
      "  Batch [660/1299] D_loss: -2.6724, G_loss: 4.5951\n",
      "  Batch [670/1299] D_loss: -1.6744, G_loss: 2.6775\n",
      "  Batch [680/1299] D_loss: -2.5656, G_loss: 4.4207\n",
      "  Batch [690/1299] D_loss: -3.0759, G_loss: 3.6442\n",
      "  Batch [700/1299] D_loss: -2.4355, G_loss: 1.7909\n",
      "  Batch [710/1299] D_loss: -2.0588, G_loss: 1.4073\n",
      "  Batch [720/1299] D_loss: -2.6086, G_loss: -0.3422\n",
      "  Batch [730/1299] D_loss: -2.5062, G_loss: 1.8564\n",
      "  Batch [740/1299] D_loss: -2.7277, G_loss: 1.7015\n",
      "  Batch [750/1299] D_loss: -2.0541, G_loss: 4.4175\n",
      "  Batch [760/1299] D_loss: -2.5034, G_loss: 5.6766\n",
      "  Batch [770/1299] D_loss: -4.1962, G_loss: 3.8646\n",
      "  Batch [780/1299] D_loss: -2.6251, G_loss: 1.9551\n",
      "  Batch [790/1299] D_loss: -4.2149, G_loss: 3.4888\n",
      "  Batch [800/1299] D_loss: -2.9312, G_loss: 4.1536\n",
      "  Batch [810/1299] D_loss: -1.6533, G_loss: 1.0774\n",
      "  Batch [820/1299] D_loss: -2.1936, G_loss: -1.4216\n",
      "  Batch [830/1299] D_loss: -4.4728, G_loss: 1.3601\n",
      "  Batch [840/1299] D_loss: -0.6048, G_loss: 1.3289\n",
      "  Batch [850/1299] D_loss: -2.8123, G_loss: 0.8276\n",
      "  Batch [860/1299] D_loss: -3.4446, G_loss: 1.1682\n",
      "  Batch [870/1299] D_loss: -1.6571, G_loss: 0.0694\n",
      "  Batch [880/1299] D_loss: -2.5173, G_loss: 0.5100\n",
      "  Batch [890/1299] D_loss: -2.9677, G_loss: 2.6998\n",
      "  Batch [900/1299] D_loss: -1.2284, G_loss: 0.1480\n",
      "  Batch [910/1299] D_loss: -2.3273, G_loss: 1.5286\n",
      "  Batch [920/1299] D_loss: -1.0138, G_loss: 6.3824\n",
      "  Batch [930/1299] D_loss: -1.5079, G_loss: 3.5681\n",
      "  Batch [940/1299] D_loss: -3.1634, G_loss: 2.9480\n",
      "  Batch [950/1299] D_loss: -1.3053, G_loss: 1.4300\n",
      "  Batch [960/1299] D_loss: -2.1535, G_loss: 0.1140\n",
      "  Batch [970/1299] D_loss: -2.4528, G_loss: 4.0638\n",
      "  Batch [980/1299] D_loss: -2.5016, G_loss: 2.8372\n",
      "  Batch [990/1299] D_loss: -1.8685, G_loss: 2.8359\n",
      "  Batch [1000/1299] D_loss: -3.2431, G_loss: 2.0162\n",
      "  Batch [1010/1299] D_loss: -0.8710, G_loss: 4.8306\n",
      "  Batch [1020/1299] D_loss: -2.0457, G_loss: 3.0795\n",
      "  Batch [1030/1299] D_loss: -1.3113, G_loss: 6.4059\n",
      "  Batch [1040/1299] D_loss: -1.5181, G_loss: 2.1950\n",
      "  Batch [1050/1299] D_loss: -1.3590, G_loss: -0.0934\n",
      "  Batch [1060/1299] D_loss: -2.6649, G_loss: -1.8593\n",
      "  Batch [1070/1299] D_loss: -2.8148, G_loss: -1.6189\n",
      "  Batch [1080/1299] D_loss: -2.3883, G_loss: -3.2533\n",
      "  Batch [1090/1299] D_loss: -2.7279, G_loss: -1.8389\n",
      "  Batch [1100/1299] D_loss: -2.7395, G_loss: -2.6039\n",
      "  Batch [1110/1299] D_loss: -1.6149, G_loss: -2.2227\n",
      "  Batch [1120/1299] D_loss: -0.7472, G_loss: -1.2010\n",
      "  Batch [1130/1299] D_loss: -1.6503, G_loss: 1.0131\n",
      "  Batch [1140/1299] D_loss: -3.4170, G_loss: 1.3520\n",
      "  Batch [1150/1299] D_loss: -3.0265, G_loss: 2.6803\n",
      "  Batch [1160/1299] D_loss: -3.7324, G_loss: 1.1657\n",
      "  Batch [1170/1299] D_loss: -2.5396, G_loss: 1.7379\n",
      "  Batch [1180/1299] D_loss: -1.7461, G_loss: 1.9470\n",
      "  Batch [1190/1299] D_loss: -1.9672, G_loss: 1.5477\n",
      "  Batch [1200/1299] D_loss: -0.2319, G_loss: -1.7734\n",
      "  Batch [1210/1299] D_loss: -1.6133, G_loss: -2.2167\n",
      "  Batch [1220/1299] D_loss: -3.3148, G_loss: -3.6854\n",
      "  Batch [1230/1299] D_loss: -2.3679, G_loss: -0.0440\n",
      "  Batch [1240/1299] D_loss: -2.3068, G_loss: 0.6316\n",
      "  Batch [1250/1299] D_loss: -2.8183, G_loss: 0.2326\n",
      "  Batch [1260/1299] D_loss: -2.5635, G_loss: 2.6191\n",
      "  Batch [1270/1299] D_loss: -2.2499, G_loss: 6.1483\n",
      "  Batch [1280/1299] D_loss: -2.0363, G_loss: 5.0914\n",
      "  Batch [1290/1299] D_loss: -3.0762, G_loss: 5.8431\n",
      "\n",
      "Epoch 63 Summary:\n",
      "  Average D_loss: -2.0105\n",
      "  Average G_loss: 1.7337\n",
      "\n",
      "Epoch [64/100]\n",
      "  Batch [0/1299] D_loss: -2.6532, G_loss: 5.1728\n",
      "  Batch [10/1299] D_loss: -3.7507, G_loss: 2.9523\n",
      "  Batch [20/1299] D_loss: -1.9221, G_loss: 3.1625\n",
      "  Batch [30/1299] D_loss: -1.9509, G_loss: 3.0347\n",
      "  Batch [40/1299] D_loss: -2.5162, G_loss: 4.4821\n",
      "  Batch [50/1299] D_loss: -1.2425, G_loss: 1.6181\n",
      "  Batch [60/1299] D_loss: -0.8816, G_loss: 2.8580\n",
      "  Batch [70/1299] D_loss: -0.8157, G_loss: 1.5154\n",
      "  Batch [80/1299] D_loss: -2.6435, G_loss: 2.1682\n",
      "  Batch [90/1299] D_loss: -0.9053, G_loss: 2.1983\n",
      "  Batch [100/1299] D_loss: -1.4010, G_loss: 0.5874\n",
      "  Batch [110/1299] D_loss: -1.2261, G_loss: 2.1403\n",
      "  Batch [120/1299] D_loss: -2.3849, G_loss: 4.0353\n",
      "  Batch [130/1299] D_loss: -2.5095, G_loss: 0.8711\n",
      "  Batch [140/1299] D_loss: -2.1611, G_loss: 1.9239\n",
      "  Batch [150/1299] D_loss: -2.0460, G_loss: 0.0899\n",
      "  Batch [160/1299] D_loss: -1.9544, G_loss: 3.7218\n",
      "  Batch [170/1299] D_loss: -2.1399, G_loss: 2.7931\n",
      "  Batch [180/1299] D_loss: -3.9452, G_loss: 3.0223\n",
      "  Batch [190/1299] D_loss: -1.9913, G_loss: 4.5219\n",
      "  Batch [200/1299] D_loss: -2.0041, G_loss: 1.4805\n",
      "  Batch [210/1299] D_loss: -2.8332, G_loss: 0.4847\n",
      "  Batch [220/1299] D_loss: -3.7453, G_loss: 0.6977\n",
      "  Batch [230/1299] D_loss: -1.3621, G_loss: 1.0790\n",
      "  Batch [240/1299] D_loss: -0.9578, G_loss: 1.3008\n",
      "  Batch [250/1299] D_loss: -3.0699, G_loss: 3.4361\n",
      "  Batch [260/1299] D_loss: -4.5091, G_loss: 2.0287\n",
      "  Batch [270/1299] D_loss: -2.9498, G_loss: 5.0928\n",
      "  Batch [280/1299] D_loss: -2.8178, G_loss: 3.7525\n",
      "  Batch [290/1299] D_loss: -1.4556, G_loss: 1.7608\n",
      "  Batch [300/1299] D_loss: -1.5884, G_loss: 3.4457\n",
      "  Batch [310/1299] D_loss: -1.1597, G_loss: 3.4452\n",
      "  Batch [320/1299] D_loss: -0.5102, G_loss: 0.6473\n",
      "  Batch [330/1299] D_loss: -2.7621, G_loss: 1.7817\n",
      "  Batch [340/1299] D_loss: -1.5774, G_loss: -0.6520\n",
      "  Batch [350/1299] D_loss: -2.7072, G_loss: -3.5174\n",
      "  Batch [360/1299] D_loss: -0.8223, G_loss: -4.2675\n",
      "  Batch [370/1299] D_loss: -2.8638, G_loss: -5.8261\n",
      "  Batch [380/1299] D_loss: -0.7348, G_loss: -4.8179\n",
      "  Batch [390/1299] D_loss: -1.1106, G_loss: -5.6379\n",
      "  Batch [400/1299] D_loss: -0.3318, G_loss: -3.4936\n",
      "  Batch [410/1299] D_loss: -1.5762, G_loss: -2.0623\n",
      "  Batch [420/1299] D_loss: -2.9235, G_loss: -0.9574\n",
      "  Batch [430/1299] D_loss: -2.5194, G_loss: 1.4107\n",
      "  Batch [440/1299] D_loss: -2.6166, G_loss: 2.1968\n",
      "  Batch [450/1299] D_loss: -1.5361, G_loss: 0.8953\n",
      "  Batch [460/1299] D_loss: -3.9216, G_loss: 4.3487\n",
      "  Batch [470/1299] D_loss: -1.7441, G_loss: 3.5572\n",
      "  Batch [480/1299] D_loss: -1.3790, G_loss: 4.3548\n",
      "  Batch [490/1299] D_loss: -1.1801, G_loss: 2.3266\n",
      "  Batch [500/1299] D_loss: -3.2160, G_loss: 2.2355\n",
      "  Batch [510/1299] D_loss: -3.2426, G_loss: 5.2992\n",
      "  Batch [520/1299] D_loss: -1.9936, G_loss: 4.2938\n",
      "  Batch [530/1299] D_loss: -2.6178, G_loss: 3.2969\n",
      "  Batch [540/1299] D_loss: -2.9094, G_loss: 3.0030\n",
      "  Batch [550/1299] D_loss: -1.0412, G_loss: 2.4312\n",
      "  Batch [560/1299] D_loss: -2.1657, G_loss: 1.5147\n",
      "  Batch [570/1299] D_loss: -1.8804, G_loss: 0.9799\n",
      "  Batch [580/1299] D_loss: -1.2723, G_loss: 2.3313\n",
      "  Batch [590/1299] D_loss: -2.1139, G_loss: 0.4449\n",
      "  Batch [600/1299] D_loss: -1.2002, G_loss: -1.1834\n",
      "  Batch [610/1299] D_loss: -2.0311, G_loss: -1.2820\n",
      "  Batch [620/1299] D_loss: -2.9790, G_loss: -2.4607\n",
      "  Batch [630/1299] D_loss: -1.1280, G_loss: -2.0862\n",
      "  Batch [640/1299] D_loss: -1.6994, G_loss: -0.9000\n",
      "  Batch [650/1299] D_loss: -2.5981, G_loss: 0.1799\n",
      "  Batch [660/1299] D_loss: -2.3392, G_loss: -0.7098\n",
      "  Batch [670/1299] D_loss: -3.4143, G_loss: 2.3049\n",
      "  Batch [680/1299] D_loss: -2.3236, G_loss: 2.5713\n",
      "  Batch [690/1299] D_loss: -1.2398, G_loss: 4.2906\n",
      "  Batch [700/1299] D_loss: -1.4891, G_loss: 3.1795\n",
      "  Batch [710/1299] D_loss: -2.2650, G_loss: 3.2251\n",
      "  Batch [720/1299] D_loss: -1.8996, G_loss: 1.7654\n",
      "  Batch [730/1299] D_loss: -2.0559, G_loss: 0.1038\n",
      "  Batch [740/1299] D_loss: -1.7354, G_loss: 1.8622\n",
      "  Batch [750/1299] D_loss: -2.5749, G_loss: -0.4439\n",
      "  Batch [760/1299] D_loss: -2.5898, G_loss: -1.2061\n",
      "  Batch [770/1299] D_loss: -3.8597, G_loss: -2.7424\n",
      "  Batch [780/1299] D_loss: -1.6543, G_loss: -1.8901\n",
      "  Batch [790/1299] D_loss: -3.0330, G_loss: -1.0258\n",
      "  Batch [800/1299] D_loss: -3.3314, G_loss: 0.2118\n",
      "  Batch [810/1299] D_loss: -2.0022, G_loss: 0.6788\n",
      "  Batch [820/1299] D_loss: -3.0886, G_loss: 1.3868\n",
      "  Batch [830/1299] D_loss: -1.0159, G_loss: 4.0539\n",
      "  Batch [840/1299] D_loss: -1.6133, G_loss: 6.0103\n",
      "  Batch [850/1299] D_loss: -0.4482, G_loss: 3.7068\n",
      "  Batch [860/1299] D_loss: -2.5243, G_loss: 2.7466\n",
      "  Batch [870/1299] D_loss: -1.9562, G_loss: 2.2648\n",
      "  Batch [880/1299] D_loss: -1.4034, G_loss: 0.0122\n",
      "  Batch [890/1299] D_loss: -2.6006, G_loss: -1.2587\n",
      "  Batch [900/1299] D_loss: -3.1017, G_loss: -4.7870\n",
      "  Batch [910/1299] D_loss: -2.9260, G_loss: -6.2482\n",
      "  Batch [920/1299] D_loss: -2.5253, G_loss: -3.4992\n",
      "  Batch [930/1299] D_loss: -1.7936, G_loss: -2.6705\n",
      "  Batch [940/1299] D_loss: -2.4051, G_loss: -2.3743\n",
      "  Batch [950/1299] D_loss: -2.5670, G_loss: 0.6771\n",
      "  Batch [960/1299] D_loss: -1.2431, G_loss: -0.2356\n",
      "  Batch [970/1299] D_loss: -1.8517, G_loss: 0.7618\n",
      "  Batch [980/1299] D_loss: -0.7083, G_loss: 1.0912\n",
      "  Batch [990/1299] D_loss: -2.2570, G_loss: 4.1655\n",
      "  Batch [1000/1299] D_loss: -2.3608, G_loss: 1.4267\n",
      "  Batch [1010/1299] D_loss: -2.9024, G_loss: 1.0163\n",
      "  Batch [1020/1299] D_loss: -0.4698, G_loss: -0.5188\n",
      "  Batch [1030/1299] D_loss: -1.0945, G_loss: -2.3713\n",
      "  Batch [1040/1299] D_loss: -1.6086, G_loss: -3.2965\n",
      "  Batch [1050/1299] D_loss: -3.1246, G_loss: -3.4703\n",
      "  Batch [1060/1299] D_loss: -1.2110, G_loss: -3.3888\n",
      "  Batch [1070/1299] D_loss: -0.9267, G_loss: -1.4044\n",
      "  Batch [1080/1299] D_loss: -2.1269, G_loss: -0.8358\n",
      "  Batch [1090/1299] D_loss: -3.1668, G_loss: -1.6077\n",
      "  Batch [1100/1299] D_loss: -1.3147, G_loss: 2.1652\n",
      "  Batch [1110/1299] D_loss: -1.7578, G_loss: 2.0331\n",
      "  Batch [1120/1299] D_loss: -0.3765, G_loss: 3.4232\n",
      "  Batch [1130/1299] D_loss: -1.6996, G_loss: 2.0666\n",
      "  Batch [1140/1299] D_loss: -0.7920, G_loss: 4.7523\n",
      "  Batch [1150/1299] D_loss: -2.0473, G_loss: 1.8790\n",
      "  Batch [1160/1299] D_loss: -3.8549, G_loss: 3.0067\n",
      "  Batch [1170/1299] D_loss: -3.1679, G_loss: 3.2308\n",
      "  Batch [1180/1299] D_loss: -2.1715, G_loss: 2.8014\n",
      "  Batch [1190/1299] D_loss: -2.2745, G_loss: 2.6705\n",
      "  Batch [1200/1299] D_loss: -1.2221, G_loss: 3.1100\n",
      "  Batch [1210/1299] D_loss: -0.4663, G_loss: 2.0658\n",
      "  Batch [1220/1299] D_loss: -1.7676, G_loss: 1.3571\n",
      "  Batch [1230/1299] D_loss: -1.9632, G_loss: 1.5228\n",
      "  Batch [1240/1299] D_loss: -4.2369, G_loss: 1.9198\n",
      "  Batch [1250/1299] D_loss: -2.6217, G_loss: 2.6056\n",
      "  Batch [1260/1299] D_loss: -2.4954, G_loss: 2.3662\n",
      "  Batch [1270/1299] D_loss: -2.1172, G_loss: 2.3321\n",
      "  Batch [1280/1299] D_loss: -2.5286, G_loss: 3.0955\n",
      "  Batch [1290/1299] D_loss: -2.3617, G_loss: 2.1304\n",
      "\n",
      "Epoch 64 Summary:\n",
      "  Average D_loss: -1.9690\n",
      "  Average G_loss: 1.1239\n",
      "\n",
      "Epoch [65/100]\n",
      "  Batch [0/1299] D_loss: -3.0703, G_loss: 1.2248\n",
      "  Batch [10/1299] D_loss: -2.0933, G_loss: -0.1905\n",
      "  Batch [20/1299] D_loss: -2.3487, G_loss: -1.0195\n",
      "  Batch [30/1299] D_loss: -2.2087, G_loss: -2.5630\n",
      "  Batch [40/1299] D_loss: -2.1759, G_loss: -2.3149\n",
      "  Batch [50/1299] D_loss: -2.3683, G_loss: -2.9981\n",
      "  Batch [60/1299] D_loss: -3.4823, G_loss: -3.9620\n",
      "  Batch [70/1299] D_loss: -1.9194, G_loss: -1.6847\n",
      "  Batch [80/1299] D_loss: -2.0614, G_loss: -1.2318\n",
      "  Batch [90/1299] D_loss: -1.5857, G_loss: -0.7461\n",
      "  Batch [100/1299] D_loss: -2.7264, G_loss: 0.7646\n",
      "  Batch [110/1299] D_loss: -1.6495, G_loss: 0.6477\n",
      "  Batch [120/1299] D_loss: -2.1592, G_loss: 1.2962\n",
      "  Batch [130/1299] D_loss: -2.0328, G_loss: 1.0735\n",
      "  Batch [140/1299] D_loss: -2.6908, G_loss: -1.3959\n",
      "  Batch [150/1299] D_loss: -2.2272, G_loss: -0.2567\n",
      "  Batch [160/1299] D_loss: -2.1134, G_loss: 1.5804\n",
      "  Batch [170/1299] D_loss: -1.6636, G_loss: 2.7317\n",
      "  Batch [180/1299] D_loss: -0.9293, G_loss: 1.6841\n",
      "  Batch [190/1299] D_loss: -2.3143, G_loss: -1.1443\n",
      "  Batch [200/1299] D_loss: -3.9000, G_loss: 0.8493\n",
      "  Batch [210/1299] D_loss: -2.4088, G_loss: -0.5720\n",
      "  Batch [220/1299] D_loss: -2.6001, G_loss: 1.1668\n",
      "  Batch [230/1299] D_loss: -2.2225, G_loss: 2.1990\n",
      "  Batch [240/1299] D_loss: -0.8569, G_loss: 0.1172\n",
      "  Batch [250/1299] D_loss: -4.2653, G_loss: 0.1903\n",
      "  Batch [260/1299] D_loss: -1.0287, G_loss: 0.3222\n",
      "  Batch [270/1299] D_loss: -3.7038, G_loss: -0.5554\n",
      "  Batch [280/1299] D_loss: -2.8242, G_loss: 1.6375\n",
      "  Batch [290/1299] D_loss: -3.0728, G_loss: 1.4761\n",
      "  Batch [300/1299] D_loss: -2.2572, G_loss: 2.4751\n",
      "  Batch [310/1299] D_loss: -1.9759, G_loss: 1.7016\n",
      "  Batch [320/1299] D_loss: -2.7632, G_loss: 2.2670\n",
      "  Batch [330/1299] D_loss: -2.3449, G_loss: -0.1535\n",
      "  Batch [340/1299] D_loss: -3.5546, G_loss: -0.3944\n",
      "  Batch [350/1299] D_loss: -3.4756, G_loss: 0.3690\n",
      "  Batch [360/1299] D_loss: -2.7729, G_loss: 1.4086\n",
      "  Batch [370/1299] D_loss: -1.9979, G_loss: 0.9432\n",
      "  Batch [380/1299] D_loss: -2.6678, G_loss: 1.0615\n",
      "  Batch [390/1299] D_loss: -1.0907, G_loss: 4.0421\n",
      "  Batch [400/1299] D_loss: -1.7616, G_loss: 3.9950\n",
      "  Batch [410/1299] D_loss: -1.1005, G_loss: 3.6206\n",
      "  Batch [420/1299] D_loss: -1.4554, G_loss: 4.7453\n",
      "  Batch [430/1299] D_loss: -2.0462, G_loss: 3.4656\n",
      "  Batch [440/1299] D_loss: -2.1387, G_loss: 3.3008\n",
      "  Batch [450/1299] D_loss: -2.6818, G_loss: 1.8061\n",
      "  Batch [460/1299] D_loss: -2.0329, G_loss: -0.9101\n",
      "  Batch [470/1299] D_loss: -2.1544, G_loss: -1.9414\n",
      "  Batch [480/1299] D_loss: -3.3014, G_loss: -3.0141\n",
      "  Batch [490/1299] D_loss: -0.6570, G_loss: -3.4703\n",
      "  Batch [500/1299] D_loss: -1.3427, G_loss: -4.5842\n",
      "  Batch [510/1299] D_loss: -1.8255, G_loss: -4.3495\n",
      "  Batch [520/1299] D_loss: -1.9942, G_loss: -3.9693\n",
      "  Batch [530/1299] D_loss: -3.2028, G_loss: -3.9412\n",
      "  Batch [540/1299] D_loss: -2.1192, G_loss: -2.5164\n",
      "  Batch [550/1299] D_loss: -0.8994, G_loss: -3.8097\n",
      "  Batch [560/1299] D_loss: -3.7232, G_loss: -0.3307\n",
      "  Batch [570/1299] D_loss: -1.4831, G_loss: -0.0035\n",
      "  Batch [580/1299] D_loss: -1.7307, G_loss: 0.4724\n",
      "  Batch [590/1299] D_loss: -3.3057, G_loss: 1.5838\n",
      "  Batch [600/1299] D_loss: -2.5140, G_loss: 3.1673\n",
      "  Batch [610/1299] D_loss: -2.9587, G_loss: 3.8340\n",
      "  Batch [620/1299] D_loss: -1.3116, G_loss: 1.5386\n",
      "  Batch [630/1299] D_loss: -2.4418, G_loss: 1.0295\n",
      "  Batch [640/1299] D_loss: -1.6490, G_loss: 2.0079\n",
      "  Batch [650/1299] D_loss: -1.1535, G_loss: 6.2778\n",
      "  Batch [660/1299] D_loss: -1.2312, G_loss: 4.3992\n",
      "  Batch [670/1299] D_loss: -2.3786, G_loss: 3.5486\n",
      "  Batch [680/1299] D_loss: -0.7460, G_loss: 6.0667\n",
      "  Batch [690/1299] D_loss: -2.5923, G_loss: 2.5002\n",
      "  Batch [700/1299] D_loss: -2.9324, G_loss: 0.3378\n",
      "  Batch [710/1299] D_loss: -2.0129, G_loss: 1.6968\n",
      "  Batch [720/1299] D_loss: -2.2914, G_loss: -1.6177\n",
      "  Batch [730/1299] D_loss: -3.1107, G_loss: -0.3345\n",
      "  Batch [740/1299] D_loss: -2.2530, G_loss: 0.2982\n",
      "  Batch [750/1299] D_loss: -3.1490, G_loss: 0.2001\n",
      "  Batch [760/1299] D_loss: -3.8261, G_loss: 1.2302\n",
      "  Batch [770/1299] D_loss: -3.1559, G_loss: 2.4726\n",
      "  Batch [780/1299] D_loss: -1.9650, G_loss: 3.7349\n",
      "  Batch [790/1299] D_loss: -1.4843, G_loss: 5.0108\n",
      "  Batch [800/1299] D_loss: -3.5737, G_loss: 3.9007\n",
      "  Batch [810/1299] D_loss: -3.3530, G_loss: 2.7378\n",
      "  Batch [820/1299] D_loss: 0.5757, G_loss: 2.4804\n",
      "  Batch [830/1299] D_loss: -3.6696, G_loss: -0.1494\n",
      "  Batch [840/1299] D_loss: -0.7573, G_loss: -3.1523\n",
      "  Batch [850/1299] D_loss: -1.0393, G_loss: -3.6687\n",
      "  Batch [860/1299] D_loss: -0.8906, G_loss: -1.8634\n",
      "  Batch [870/1299] D_loss: -2.2782, G_loss: -2.2588\n",
      "  Batch [880/1299] D_loss: -1.8124, G_loss: -1.3777\n",
      "  Batch [890/1299] D_loss: -1.9691, G_loss: 0.2667\n",
      "  Batch [900/1299] D_loss: -1.6754, G_loss: 1.1114\n",
      "  Batch [910/1299] D_loss: -1.6262, G_loss: 3.6004\n",
      "  Batch [920/1299] D_loss: -0.8677, G_loss: 6.4219\n",
      "  Batch [930/1299] D_loss: -1.7234, G_loss: 6.8543\n",
      "  Batch [940/1299] D_loss: -3.2633, G_loss: 4.8981\n",
      "  Batch [950/1299] D_loss: -0.5476, G_loss: 3.8977\n",
      "  Batch [960/1299] D_loss: -3.1776, G_loss: 1.6033\n",
      "  Batch [970/1299] D_loss: -2.3205, G_loss: -0.0329\n",
      "  Batch [980/1299] D_loss: -1.0824, G_loss: -0.8472\n",
      "  Batch [990/1299] D_loss: -1.8319, G_loss: -0.9137\n",
      "  Batch [1000/1299] D_loss: -2.0010, G_loss: -1.5829\n",
      "  Batch [1010/1299] D_loss: -1.6169, G_loss: -1.4364\n",
      "  Batch [1020/1299] D_loss: -1.8768, G_loss: -0.8297\n",
      "  Batch [1030/1299] D_loss: -1.5743, G_loss: 0.4239\n",
      "  Batch [1040/1299] D_loss: -0.9440, G_loss: 0.4818\n",
      "  Batch [1050/1299] D_loss: -3.5281, G_loss: 1.4173\n",
      "  Batch [1060/1299] D_loss: -0.9856, G_loss: 2.8692\n",
      "  Batch [1070/1299] D_loss: -2.2962, G_loss: 4.9758\n",
      "  Batch [1080/1299] D_loss: -2.5537, G_loss: 3.0899\n",
      "  Batch [1090/1299] D_loss: -3.7931, G_loss: 1.7303\n",
      "  Batch [1100/1299] D_loss: -2.1986, G_loss: 3.1047\n",
      "  Batch [1110/1299] D_loss: -2.0895, G_loss: 3.9821\n",
      "  Batch [1120/1299] D_loss: -1.6537, G_loss: 0.1298\n",
      "  Batch [1130/1299] D_loss: -2.5395, G_loss: -0.4531\n",
      "  Batch [1140/1299] D_loss: -2.4261, G_loss: -0.3547\n",
      "  Batch [1150/1299] D_loss: -0.8805, G_loss: -1.3700\n",
      "  Batch [1160/1299] D_loss: -0.4378, G_loss: -3.3583\n",
      "  Batch [1170/1299] D_loss: -1.9545, G_loss: -3.6732\n",
      "  Batch [1180/1299] D_loss: -2.5233, G_loss: -2.6443\n",
      "  Batch [1190/1299] D_loss: -1.3472, G_loss: -2.0565\n",
      "  Batch [1200/1299] D_loss: -2.2580, G_loss: 1.1205\n",
      "  Batch [1210/1299] D_loss: -3.2291, G_loss: 3.1200\n",
      "  Batch [1220/1299] D_loss: -1.2437, G_loss: 4.1549\n",
      "  Batch [1230/1299] D_loss: -1.5762, G_loss: 2.2228\n",
      "  Batch [1240/1299] D_loss: -2.6866, G_loss: 3.1160\n",
      "  Batch [1250/1299] D_loss: -2.2673, G_loss: 2.2363\n",
      "  Batch [1260/1299] D_loss: -1.4784, G_loss: 2.4420\n",
      "  Batch [1270/1299] D_loss: -3.3310, G_loss: 0.4555\n",
      "  Batch [1280/1299] D_loss: -1.1712, G_loss: -1.4163\n",
      "  Batch [1290/1299] D_loss: -2.4299, G_loss: -0.2020\n",
      "\n",
      "Epoch 65 Summary:\n",
      "  Average D_loss: -2.0079\n",
      "  Average G_loss: 0.7575\n",
      "\n",
      "Epoch [66/100]\n",
      "  Batch [0/1299] D_loss: -2.3384, G_loss: 0.7845\n",
      "  Batch [10/1299] D_loss: -1.8247, G_loss: 1.6436\n",
      "  Batch [20/1299] D_loss: -2.4844, G_loss: 1.5764\n",
      "  Batch [30/1299] D_loss: -2.2074, G_loss: 2.8789\n",
      "  Batch [40/1299] D_loss: -3.2186, G_loss: 3.7651\n",
      "  Batch [50/1299] D_loss: -1.3990, G_loss: 3.1726\n",
      "  Batch [60/1299] D_loss: -2.1666, G_loss: 2.9904\n",
      "  Batch [70/1299] D_loss: -2.7308, G_loss: 3.8844\n",
      "  Batch [80/1299] D_loss: -1.6632, G_loss: 2.5738\n",
      "  Batch [90/1299] D_loss: -2.5985, G_loss: 3.6597\n",
      "  Batch [100/1299] D_loss: -1.5510, G_loss: 0.3045\n",
      "  Batch [110/1299] D_loss: -2.8528, G_loss: 0.1319\n",
      "  Batch [120/1299] D_loss: -1.8291, G_loss: -3.0229\n",
      "  Batch [130/1299] D_loss: -0.7360, G_loss: -3.4909\n",
      "  Batch [140/1299] D_loss: -2.9366, G_loss: -2.6532\n",
      "  Batch [150/1299] D_loss: -1.6708, G_loss: -2.1502\n",
      "  Batch [160/1299] D_loss: -1.2691, G_loss: -1.3565\n",
      "  Batch [170/1299] D_loss: -3.9462, G_loss: -0.7513\n",
      "  Batch [180/1299] D_loss: -2.7997, G_loss: 0.1308\n",
      "  Batch [190/1299] D_loss: -3.9097, G_loss: 1.5368\n",
      "  Batch [200/1299] D_loss: -0.8468, G_loss: 0.8029\n",
      "  Batch [210/1299] D_loss: -1.2661, G_loss: 3.0244\n",
      "  Batch [220/1299] D_loss: -1.6298, G_loss: 4.4101\n",
      "  Batch [230/1299] D_loss: -1.7708, G_loss: 2.3552\n",
      "  Batch [240/1299] D_loss: -2.2705, G_loss: 2.0622\n",
      "  Batch [250/1299] D_loss: -1.7727, G_loss: 2.6513\n",
      "  Batch [260/1299] D_loss: -1.7523, G_loss: 0.0539\n",
      "  Batch [270/1299] D_loss: -1.0220, G_loss: 0.7217\n",
      "  Batch [280/1299] D_loss: -3.2574, G_loss: 0.6490\n",
      "  Batch [290/1299] D_loss: -2.4786, G_loss: -0.9597\n",
      "  Batch [300/1299] D_loss: -1.2783, G_loss: 0.6738\n",
      "  Batch [310/1299] D_loss: -2.9297, G_loss: 0.2095\n",
      "  Batch [320/1299] D_loss: -1.4450, G_loss: 0.3150\n",
      "  Batch [330/1299] D_loss: -1.0989, G_loss: 0.2382\n",
      "  Batch [340/1299] D_loss: -1.4247, G_loss: 1.5124\n",
      "  Batch [350/1299] D_loss: -1.7887, G_loss: 0.7187\n",
      "  Batch [360/1299] D_loss: -1.5774, G_loss: 0.5980\n",
      "  Batch [370/1299] D_loss: -3.0398, G_loss: 2.5828\n",
      "  Batch [380/1299] D_loss: -2.2418, G_loss: 3.1794\n",
      "  Batch [390/1299] D_loss: -3.2297, G_loss: 3.3355\n",
      "  Batch [400/1299] D_loss: -1.9897, G_loss: 1.4750\n",
      "  Batch [410/1299] D_loss: -3.1099, G_loss: 0.4796\n",
      "  Batch [420/1299] D_loss: -1.1390, G_loss: 0.2279\n",
      "  Batch [430/1299] D_loss: -1.5936, G_loss: 1.3237\n",
      "  Batch [440/1299] D_loss: -2.3066, G_loss: -0.4760\n",
      "  Batch [450/1299] D_loss: -2.7547, G_loss: -1.7357\n",
      "  Batch [460/1299] D_loss: -3.2780, G_loss: -2.1862\n",
      "  Batch [470/1299] D_loss: -2.0525, G_loss: -2.4599\n",
      "  Batch [480/1299] D_loss: -1.4148, G_loss: -2.7724\n",
      "  Batch [490/1299] D_loss: -0.9746, G_loss: -2.4450\n",
      "  Batch [500/1299] D_loss: -1.3572, G_loss: -4.6554\n",
      "  Batch [510/1299] D_loss: -0.4720, G_loss: -2.7215\n",
      "  Batch [520/1299] D_loss: -1.9327, G_loss: -2.1335\n",
      "  Batch [530/1299] D_loss: -1.5293, G_loss: -1.8596\n",
      "  Batch [540/1299] D_loss: -1.6049, G_loss: -0.6688\n",
      "  Batch [550/1299] D_loss: -2.3413, G_loss: -0.4109\n",
      "  Batch [560/1299] D_loss: -2.5242, G_loss: 1.3551\n",
      "  Batch [570/1299] D_loss: -2.4127, G_loss: 0.9999\n",
      "  Batch [580/1299] D_loss: -1.5181, G_loss: 4.5437\n",
      "  Batch [590/1299] D_loss: -2.0458, G_loss: 4.5282\n",
      "  Batch [600/1299] D_loss: -2.6352, G_loss: 4.7191\n",
      "  Batch [610/1299] D_loss: -2.0162, G_loss: 4.1957\n",
      "  Batch [620/1299] D_loss: -2.1292, G_loss: 3.4114\n",
      "  Batch [630/1299] D_loss: -3.6484, G_loss: 2.4097\n",
      "  Batch [640/1299] D_loss: -0.7988, G_loss: 3.8750\n",
      "  Batch [650/1299] D_loss: -2.4327, G_loss: 1.7139\n",
      "  Batch [660/1299] D_loss: -1.2383, G_loss: 0.6068\n",
      "  Batch [670/1299] D_loss: -1.9930, G_loss: -1.0205\n",
      "  Batch [680/1299] D_loss: -0.8724, G_loss: -0.5131\n",
      "  Batch [690/1299] D_loss: -1.4560, G_loss: -0.6781\n",
      "  Batch [700/1299] D_loss: -2.9188, G_loss: 0.1845\n",
      "  Batch [710/1299] D_loss: -2.8441, G_loss: 1.1102\n",
      "  Batch [720/1299] D_loss: -1.3059, G_loss: 0.3152\n",
      "  Batch [730/1299] D_loss: -2.2315, G_loss: 2.4738\n",
      "  Batch [740/1299] D_loss: -2.7819, G_loss: 3.0714\n",
      "  Batch [750/1299] D_loss: -2.1141, G_loss: 2.4119\n",
      "  Batch [760/1299] D_loss: -1.1469, G_loss: 0.1798\n",
      "  Batch [770/1299] D_loss: -1.5434, G_loss: 2.0057\n",
      "  Batch [780/1299] D_loss: -2.0868, G_loss: 1.5505\n",
      "  Batch [790/1299] D_loss: -2.6303, G_loss: 0.9443\n",
      "  Batch [800/1299] D_loss: -3.4404, G_loss: 2.0741\n",
      "  Batch [810/1299] D_loss: -2.3544, G_loss: 0.3327\n",
      "  Batch [820/1299] D_loss: -2.6801, G_loss: 0.9255\n",
      "  Batch [830/1299] D_loss: -2.5998, G_loss: 0.7605\n",
      "  Batch [840/1299] D_loss: -1.8030, G_loss: 1.9580\n",
      "  Batch [850/1299] D_loss: -2.6168, G_loss: 1.7307\n",
      "  Batch [860/1299] D_loss: -2.4824, G_loss: 3.5516\n",
      "  Batch [870/1299] D_loss: -2.4998, G_loss: 3.2173\n",
      "  Batch [880/1299] D_loss: -3.0392, G_loss: 2.1686\n",
      "  Batch [890/1299] D_loss: -2.6851, G_loss: 1.3535\n",
      "  Batch [900/1299] D_loss: -3.4731, G_loss: -0.5115\n",
      "  Batch [910/1299] D_loss: -2.0997, G_loss: -2.8110\n",
      "  Batch [920/1299] D_loss: -1.3669, G_loss: -5.4170\n",
      "  Batch [930/1299] D_loss: -2.7680, G_loss: -4.7987\n",
      "  Batch [940/1299] D_loss: -1.6701, G_loss: -4.8115\n",
      "  Batch [950/1299] D_loss: -1.0888, G_loss: -1.8595\n",
      "  Batch [960/1299] D_loss: -1.6841, G_loss: -0.0368\n",
      "  Batch [970/1299] D_loss: -1.2378, G_loss: 0.8827\n",
      "  Batch [980/1299] D_loss: -1.8720, G_loss: 1.2936\n",
      "  Batch [990/1299] D_loss: -1.5396, G_loss: 1.5949\n",
      "  Batch [1000/1299] D_loss: -2.4306, G_loss: 0.3048\n",
      "  Batch [1010/1299] D_loss: -2.4957, G_loss: 1.3734\n",
      "  Batch [1020/1299] D_loss: -2.5635, G_loss: 3.0270\n",
      "  Batch [1030/1299] D_loss: -1.6409, G_loss: 0.1740\n",
      "  Batch [1040/1299] D_loss: -1.2260, G_loss: 1.5349\n",
      "  Batch [1050/1299] D_loss: -2.2699, G_loss: 0.4563\n",
      "  Batch [1060/1299] D_loss: -2.8424, G_loss: 2.3246\n",
      "  Batch [1070/1299] D_loss: -1.2915, G_loss: 3.4155\n",
      "  Batch [1080/1299] D_loss: -1.2973, G_loss: 4.1969\n",
      "  Batch [1090/1299] D_loss: -2.0415, G_loss: 2.1123\n",
      "  Batch [1100/1299] D_loss: -2.2446, G_loss: 1.7929\n",
      "  Batch [1110/1299] D_loss: -1.6871, G_loss: 0.6111\n",
      "  Batch [1120/1299] D_loss: -0.5431, G_loss: -0.3553\n",
      "  Batch [1130/1299] D_loss: -1.8153, G_loss: 0.0709\n",
      "  Batch [1140/1299] D_loss: -2.0506, G_loss: 0.1758\n",
      "  Batch [1150/1299] D_loss: -2.7255, G_loss: 0.6162\n",
      "  Batch [1160/1299] D_loss: -2.6405, G_loss: 2.0534\n",
      "  Batch [1170/1299] D_loss: -2.4491, G_loss: 0.4957\n",
      "  Batch [1180/1299] D_loss: -1.4584, G_loss: 1.7306\n",
      "  Batch [1190/1299] D_loss: -2.0435, G_loss: 2.6220\n",
      "  Batch [1200/1299] D_loss: -2.2887, G_loss: -0.5374\n",
      "  Batch [1210/1299] D_loss: -0.8892, G_loss: 0.4814\n",
      "  Batch [1220/1299] D_loss: -1.7768, G_loss: -0.7158\n",
      "  Batch [1230/1299] D_loss: -1.7566, G_loss: -1.1280\n",
      "  Batch [1240/1299] D_loss: -2.2886, G_loss: -0.1100\n",
      "  Batch [1250/1299] D_loss: -2.3485, G_loss: -0.4010\n",
      "  Batch [1260/1299] D_loss: -0.4264, G_loss: 0.3282\n",
      "  Batch [1270/1299] D_loss: -2.0985, G_loss: 2.0848\n",
      "  Batch [1280/1299] D_loss: -3.8184, G_loss: 1.9670\n",
      "  Batch [1290/1299] D_loss: -2.1064, G_loss: 2.6925\n",
      "\n",
      "Epoch 66 Summary:\n",
      "  Average D_loss: -2.0137\n",
      "  Average G_loss: 0.7947\n",
      "\n",
      "Epoch [67/100]\n",
      "  Batch [0/1299] D_loss: -3.1046, G_loss: 1.9065\n",
      "  Batch [10/1299] D_loss: -2.3725, G_loss: 0.7852\n",
      "  Batch [20/1299] D_loss: -1.4088, G_loss: 2.4282\n",
      "  Batch [30/1299] D_loss: -2.7744, G_loss: 2.9589\n",
      "  Batch [40/1299] D_loss: -1.5782, G_loss: 1.7407\n",
      "  Batch [50/1299] D_loss: -2.0632, G_loss: 2.0741\n",
      "  Batch [60/1299] D_loss: -1.2580, G_loss: 1.8418\n",
      "  Batch [70/1299] D_loss: -2.1203, G_loss: 1.7625\n",
      "  Batch [80/1299] D_loss: -1.3887, G_loss: 0.5633\n",
      "  Batch [90/1299] D_loss: -3.3645, G_loss: -0.9054\n",
      "  Batch [100/1299] D_loss: -0.4407, G_loss: -3.2055\n",
      "  Batch [110/1299] D_loss: -3.2364, G_loss: -5.7866\n",
      "  Batch [120/1299] D_loss: -3.5703, G_loss: -5.2369\n",
      "  Batch [130/1299] D_loss: -3.1445, G_loss: -6.8994\n",
      "  Batch [140/1299] D_loss: -2.2771, G_loss: -3.0409\n",
      "  Batch [150/1299] D_loss: -4.0128, G_loss: -0.5014\n",
      "  Batch [160/1299] D_loss: -1.5208, G_loss: 0.1142\n",
      "  Batch [170/1299] D_loss: -0.6755, G_loss: 3.9579\n",
      "  Batch [180/1299] D_loss: -2.7404, G_loss: 4.8401\n",
      "  Batch [190/1299] D_loss: -2.8687, G_loss: 2.8324\n",
      "  Batch [200/1299] D_loss: -1.2681, G_loss: 4.0286\n",
      "  Batch [210/1299] D_loss: -2.0682, G_loss: 3.8317\n",
      "  Batch [220/1299] D_loss: -2.4586, G_loss: 4.7007\n",
      "  Batch [230/1299] D_loss: -3.7114, G_loss: 1.1389\n",
      "  Batch [240/1299] D_loss: -1.3955, G_loss: 2.0929\n",
      "  Batch [250/1299] D_loss: -1.1124, G_loss: 1.3266\n",
      "  Batch [260/1299] D_loss: -3.0533, G_loss: -0.3572\n",
      "  Batch [270/1299] D_loss: -1.9226, G_loss: -0.5917\n",
      "  Batch [280/1299] D_loss: -2.6296, G_loss: -2.2336\n",
      "  Batch [290/1299] D_loss: -0.6628, G_loss: -1.0131\n",
      "  Batch [300/1299] D_loss: -3.2133, G_loss: -1.6629\n",
      "  Batch [310/1299] D_loss: -2.5478, G_loss: -1.2107\n",
      "  Batch [320/1299] D_loss: -2.0805, G_loss: -1.1899\n",
      "  Batch [330/1299] D_loss: -3.5653, G_loss: 1.8040\n",
      "  Batch [340/1299] D_loss: -3.4129, G_loss: 2.3859\n",
      "  Batch [350/1299] D_loss: -2.9737, G_loss: 3.8630\n",
      "  Batch [360/1299] D_loss: -3.6011, G_loss: 3.5899\n",
      "  Batch [370/1299] D_loss: -1.3455, G_loss: 1.5419\n",
      "  Batch [380/1299] D_loss: -3.0173, G_loss: 1.2110\n",
      "  Batch [390/1299] D_loss: -2.4640, G_loss: 0.4285\n",
      "  Batch [400/1299] D_loss: -2.0958, G_loss: -1.0211\n",
      "  Batch [410/1299] D_loss: -1.1392, G_loss: -1.1004\n",
      "  Batch [420/1299] D_loss: -3.1923, G_loss: -3.0659\n",
      "  Batch [430/1299] D_loss: -1.5422, G_loss: -3.3573\n",
      "  Batch [440/1299] D_loss: -2.9416, G_loss: -3.1791\n",
      "  Batch [450/1299] D_loss: -1.4623, G_loss: -1.9508\n",
      "  Batch [460/1299] D_loss: -1.9020, G_loss: -1.8256\n",
      "  Batch [470/1299] D_loss: -2.5502, G_loss: 1.2034\n",
      "  Batch [480/1299] D_loss: -2.1937, G_loss: 4.3511\n",
      "  Batch [490/1299] D_loss: -2.3347, G_loss: 4.7891\n",
      "  Batch [500/1299] D_loss: -2.4834, G_loss: 5.9419\n",
      "  Batch [510/1299] D_loss: -1.0343, G_loss: 3.4097\n",
      "  Batch [520/1299] D_loss: -2.7635, G_loss: 2.8756\n",
      "  Batch [530/1299] D_loss: -1.2557, G_loss: 1.6314\n",
      "  Batch [540/1299] D_loss: -1.7728, G_loss: -0.1596\n",
      "  Batch [550/1299] D_loss: -2.4078, G_loss: -1.3288\n",
      "  Batch [560/1299] D_loss: -2.4155, G_loss: -2.1142\n",
      "  Batch [570/1299] D_loss: -2.5764, G_loss: -2.5646\n",
      "  Batch [580/1299] D_loss: -2.1309, G_loss: -1.0825\n",
      "  Batch [590/1299] D_loss: -2.5520, G_loss: -1.5420\n",
      "  Batch [600/1299] D_loss: -2.2257, G_loss: -0.8767\n",
      "  Batch [610/1299] D_loss: -2.6161, G_loss: -0.0954\n",
      "  Batch [620/1299] D_loss: -2.4357, G_loss: 0.9320\n",
      "  Batch [630/1299] D_loss: -1.0839, G_loss: 2.5731\n",
      "  Batch [640/1299] D_loss: -2.4613, G_loss: 0.6505\n",
      "  Batch [650/1299] D_loss: -3.3151, G_loss: -0.0604\n",
      "  Batch [660/1299] D_loss: -1.3242, G_loss: 0.3630\n",
      "  Batch [670/1299] D_loss: -2.3305, G_loss: 0.0060\n",
      "  Batch [680/1299] D_loss: -2.8715, G_loss: -1.7087\n",
      "  Batch [690/1299] D_loss: -2.7205, G_loss: -2.7781\n",
      "  Batch [700/1299] D_loss: -2.2161, G_loss: 0.4158\n",
      "  Batch [710/1299] D_loss: -2.3713, G_loss: -1.9415\n",
      "  Batch [720/1299] D_loss: -1.8440, G_loss: 0.0015\n",
      "  Batch [730/1299] D_loss: -2.5322, G_loss: 0.2482\n",
      "  Batch [740/1299] D_loss: -2.6370, G_loss: 2.9446\n",
      "  Batch [750/1299] D_loss: -1.7069, G_loss: 3.9261\n",
      "  Batch [760/1299] D_loss: -2.9121, G_loss: 6.7679\n",
      "  Batch [770/1299] D_loss: -2.4523, G_loss: 6.2321\n",
      "  Batch [780/1299] D_loss: -1.4518, G_loss: 3.8535\n",
      "  Batch [790/1299] D_loss: -2.1797, G_loss: 4.7400\n",
      "  Batch [800/1299] D_loss: -2.1366, G_loss: 4.3264\n",
      "  Batch [810/1299] D_loss: -2.7663, G_loss: 3.0661\n",
      "  Batch [820/1299] D_loss: -2.6535, G_loss: 3.1344\n",
      "  Batch [830/1299] D_loss: -2.2308, G_loss: 1.7414\n",
      "  Batch [840/1299] D_loss: -1.8166, G_loss: 3.7509\n",
      "  Batch [850/1299] D_loss: -2.9725, G_loss: 2.1302\n",
      "  Batch [860/1299] D_loss: -2.1167, G_loss: 1.5530\n",
      "  Batch [870/1299] D_loss: -1.9040, G_loss: 2.7331\n",
      "  Batch [880/1299] D_loss: -2.7989, G_loss: 2.2314\n",
      "  Batch [890/1299] D_loss: -3.1518, G_loss: 4.7840\n",
      "  Batch [900/1299] D_loss: -3.0034, G_loss: 4.6781\n",
      "  Batch [910/1299] D_loss: -1.1804, G_loss: 3.3387\n",
      "  Batch [920/1299] D_loss: -1.9825, G_loss: 3.1801\n",
      "  Batch [930/1299] D_loss: -2.5628, G_loss: 4.2154\n",
      "  Batch [940/1299] D_loss: -1.3402, G_loss: 3.1529\n",
      "  Batch [950/1299] D_loss: -2.9887, G_loss: 3.6025\n",
      "  Batch [960/1299] D_loss: -1.3834, G_loss: 1.7501\n",
      "  Batch [970/1299] D_loss: -2.0137, G_loss: 2.9817\n",
      "  Batch [980/1299] D_loss: -1.8345, G_loss: 2.9659\n",
      "  Batch [990/1299] D_loss: -2.9360, G_loss: 2.7245\n",
      "  Batch [1000/1299] D_loss: -2.3962, G_loss: 2.2673\n",
      "  Batch [1010/1299] D_loss: -2.3173, G_loss: 2.7622\n",
      "  Batch [1020/1299] D_loss: -1.5756, G_loss: 3.0752\n",
      "  Batch [1030/1299] D_loss: -1.0919, G_loss: 2.8691\n",
      "  Batch [1040/1299] D_loss: -3.0290, G_loss: 2.8468\n",
      "  Batch [1050/1299] D_loss: -0.9420, G_loss: 2.7196\n",
      "  Batch [1060/1299] D_loss: -3.0673, G_loss: 1.5682\n",
      "  Batch [1070/1299] D_loss: -2.7452, G_loss: 2.0056\n",
      "  Batch [1080/1299] D_loss: -1.1279, G_loss: 2.5186\n",
      "  Batch [1090/1299] D_loss: -0.4381, G_loss: 3.5430\n",
      "  Batch [1100/1299] D_loss: -3.0099, G_loss: 1.7912\n",
      "  Batch [1110/1299] D_loss: -1.3083, G_loss: 2.6397\n",
      "  Batch [1120/1299] D_loss: -1.9586, G_loss: 2.1348\n",
      "  Batch [1130/1299] D_loss: -2.5811, G_loss: 2.6019\n",
      "  Batch [1140/1299] D_loss: -3.7524, G_loss: 1.5773\n",
      "  Batch [1150/1299] D_loss: -2.6204, G_loss: 2.5596\n",
      "  Batch [1160/1299] D_loss: -1.6894, G_loss: 2.9017\n",
      "  Batch [1170/1299] D_loss: -1.6729, G_loss: 3.5578\n",
      "  Batch [1180/1299] D_loss: -1.2688, G_loss: 2.6350\n",
      "  Batch [1190/1299] D_loss: -2.9100, G_loss: 2.8029\n",
      "  Batch [1200/1299] D_loss: -2.7404, G_loss: 2.3699\n",
      "  Batch [1210/1299] D_loss: -2.2167, G_loss: 3.9344\n",
      "  Batch [1220/1299] D_loss: -1.6454, G_loss: 3.1372\n",
      "  Batch [1230/1299] D_loss: -2.0646, G_loss: 2.5528\n",
      "  Batch [1240/1299] D_loss: -1.5503, G_loss: 3.8641\n",
      "  Batch [1250/1299] D_loss: -2.9197, G_loss: 5.3129\n",
      "  Batch [1260/1299] D_loss: -2.9027, G_loss: 2.5785\n",
      "  Batch [1270/1299] D_loss: -3.3541, G_loss: 3.1206\n",
      "  Batch [1280/1299] D_loss: -2.1649, G_loss: 0.1537\n",
      "  Batch [1290/1299] D_loss: -1.5436, G_loss: -1.0972\n",
      "\n",
      "Epoch 67 Summary:\n",
      "  Average D_loss: -2.0167\n",
      "  Average G_loss: 1.3911\n",
      "\n",
      "Epoch [68/100]\n",
      "  Batch [0/1299] D_loss: -3.2927, G_loss: -1.3328\n",
      "  Batch [10/1299] D_loss: -1.3554, G_loss: -0.2242\n",
      "  Batch [20/1299] D_loss: -3.2424, G_loss: -0.3220\n",
      "  Batch [30/1299] D_loss: -1.9546, G_loss: 1.9048\n",
      "  Batch [40/1299] D_loss: -1.6627, G_loss: 2.8605\n",
      "  Batch [50/1299] D_loss: -1.8743, G_loss: 4.3261\n",
      "  Batch [60/1299] D_loss: -2.5319, G_loss: 5.4520\n",
      "  Batch [70/1299] D_loss: -0.4024, G_loss: 3.7744\n",
      "  Batch [80/1299] D_loss: -2.2551, G_loss: 4.8470\n",
      "  Batch [90/1299] D_loss: -1.2618, G_loss: 2.8834\n",
      "  Batch [100/1299] D_loss: -0.6775, G_loss: 2.6940\n",
      "  Batch [110/1299] D_loss: -2.2279, G_loss: 2.7750\n",
      "  Batch [120/1299] D_loss: -2.5036, G_loss: 2.0994\n",
      "  Batch [130/1299] D_loss: -1.8856, G_loss: 2.7108\n",
      "  Batch [140/1299] D_loss: -3.3954, G_loss: 2.9582\n",
      "  Batch [150/1299] D_loss: -2.6470, G_loss: 2.0751\n",
      "  Batch [160/1299] D_loss: -2.7181, G_loss: 1.9473\n",
      "  Batch [170/1299] D_loss: -2.5786, G_loss: 1.9845\n",
      "  Batch [180/1299] D_loss: -1.9902, G_loss: 3.1404\n",
      "  Batch [190/1299] D_loss: -2.3088, G_loss: 1.3462\n",
      "  Batch [200/1299] D_loss: -3.0462, G_loss: 1.4571\n",
      "  Batch [210/1299] D_loss: -1.5998, G_loss: -0.1082\n",
      "  Batch [220/1299] D_loss: -1.9736, G_loss: 0.0500\n",
      "  Batch [230/1299] D_loss: -2.2582, G_loss: 1.5252\n",
      "  Batch [240/1299] D_loss: -1.4560, G_loss: 0.8118\n",
      "  Batch [250/1299] D_loss: -1.6896, G_loss: 2.1450\n",
      "  Batch [260/1299] D_loss: -2.5198, G_loss: 3.0806\n",
      "  Batch [270/1299] D_loss: -2.2859, G_loss: 3.4191\n",
      "  Batch [280/1299] D_loss: -1.8346, G_loss: 3.3224\n",
      "  Batch [290/1299] D_loss: -2.1754, G_loss: 2.7978\n",
      "  Batch [300/1299] D_loss: -3.0358, G_loss: 2.9108\n",
      "  Batch [310/1299] D_loss: -3.0807, G_loss: 0.9975\n",
      "  Batch [320/1299] D_loss: -2.0851, G_loss: 1.3444\n",
      "  Batch [330/1299] D_loss: -3.5671, G_loss: -0.7059\n",
      "  Batch [340/1299] D_loss: -1.9619, G_loss: 0.8990\n",
      "  Batch [350/1299] D_loss: -1.1022, G_loss: 1.8068\n",
      "  Batch [360/1299] D_loss: -2.5780, G_loss: 2.1464\n",
      "  Batch [370/1299] D_loss: -2.0212, G_loss: 1.5637\n",
      "  Batch [380/1299] D_loss: -1.8430, G_loss: 0.7677\n",
      "  Batch [390/1299] D_loss: -2.5955, G_loss: 0.0131\n",
      "  Batch [400/1299] D_loss: -2.5077, G_loss: 2.6292\n",
      "  Batch [410/1299] D_loss: -2.7960, G_loss: 1.7290\n",
      "  Batch [420/1299] D_loss: -1.3210, G_loss: 1.2549\n",
      "  Batch [430/1299] D_loss: -2.1439, G_loss: 2.4188\n",
      "  Batch [440/1299] D_loss: -1.6083, G_loss: 3.2008\n",
      "  Batch [450/1299] D_loss: -2.3589, G_loss: 3.0136\n",
      "  Batch [460/1299] D_loss: -2.0584, G_loss: 2.8855\n",
      "  Batch [470/1299] D_loss: -1.8340, G_loss: 3.0968\n",
      "  Batch [480/1299] D_loss: -2.1419, G_loss: 1.9110\n",
      "  Batch [490/1299] D_loss: -1.6070, G_loss: 1.5407\n",
      "  Batch [500/1299] D_loss: -2.0872, G_loss: -0.1871\n",
      "  Batch [510/1299] D_loss: -3.2849, G_loss: 0.9288\n",
      "  Batch [520/1299] D_loss: -1.7102, G_loss: 2.7654\n",
      "  Batch [530/1299] D_loss: -0.9805, G_loss: 1.7453\n",
      "  Batch [540/1299] D_loss: -3.3717, G_loss: 1.5297\n",
      "  Batch [550/1299] D_loss: -1.9455, G_loss: 0.4259\n",
      "  Batch [560/1299] D_loss: -2.3979, G_loss: 0.5070\n",
      "  Batch [570/1299] D_loss: -3.5196, G_loss: 1.0565\n",
      "  Batch [580/1299] D_loss: -2.0891, G_loss: 0.7716\n",
      "  Batch [590/1299] D_loss: -2.6981, G_loss: 1.0768\n",
      "  Batch [600/1299] D_loss: -1.4046, G_loss: 1.3187\n",
      "  Batch [610/1299] D_loss: -2.2250, G_loss: 3.3048\n",
      "  Batch [620/1299] D_loss: -2.4711, G_loss: 4.8983\n",
      "  Batch [630/1299] D_loss: -1.1116, G_loss: 4.1402\n",
      "  Batch [640/1299] D_loss: -1.5785, G_loss: 3.0665\n",
      "  Batch [650/1299] D_loss: -1.8764, G_loss: 4.5170\n",
      "  Batch [660/1299] D_loss: -2.6025, G_loss: 3.7092\n",
      "  Batch [670/1299] D_loss: -0.6603, G_loss: 2.8561\n",
      "  Batch [680/1299] D_loss: -3.2205, G_loss: 0.4671\n",
      "  Batch [690/1299] D_loss: -2.4616, G_loss: -0.2000\n",
      "  Batch [700/1299] D_loss: -3.2911, G_loss: -1.4189\n",
      "  Batch [710/1299] D_loss: -2.9774, G_loss: -2.1696\n",
      "  Batch [720/1299] D_loss: -1.2294, G_loss: -1.6667\n",
      "  Batch [730/1299] D_loss: -2.4049, G_loss: -1.3794\n",
      "  Batch [740/1299] D_loss: -1.7235, G_loss: -0.9886\n",
      "  Batch [750/1299] D_loss: -2.0612, G_loss: 0.5359\n",
      "  Batch [760/1299] D_loss: -2.1481, G_loss: 2.6592\n",
      "  Batch [770/1299] D_loss: -2.2578, G_loss: 5.0651\n",
      "  Batch [780/1299] D_loss: -2.1186, G_loss: 4.5831\n",
      "  Batch [790/1299] D_loss: -2.2744, G_loss: 2.5745\n",
      "  Batch [800/1299] D_loss: -1.6835, G_loss: 1.0172\n",
      "  Batch [810/1299] D_loss: -2.0600, G_loss: 1.9372\n",
      "  Batch [820/1299] D_loss: -0.7697, G_loss: 0.3279\n",
      "  Batch [830/1299] D_loss: -2.4800, G_loss: -0.6611\n",
      "  Batch [840/1299] D_loss: -2.6060, G_loss: -1.7304\n",
      "  Batch [850/1299] D_loss: -2.1201, G_loss: -4.6587\n",
      "  Batch [860/1299] D_loss: -1.7392, G_loss: -6.1880\n",
      "  Batch [870/1299] D_loss: -2.3348, G_loss: -6.5774\n",
      "  Batch [880/1299] D_loss: -3.2855, G_loss: -4.8812\n",
      "  Batch [890/1299] D_loss: -1.5098, G_loss: -4.9125\n",
      "  Batch [900/1299] D_loss: -1.8123, G_loss: -3.5252\n",
      "  Batch [910/1299] D_loss: -1.7606, G_loss: -0.6465\n",
      "  Batch [920/1299] D_loss: -1.0234, G_loss: -1.9694\n",
      "  Batch [930/1299] D_loss: -2.4957, G_loss: -1.4838\n",
      "  Batch [940/1299] D_loss: -1.8329, G_loss: 0.9509\n",
      "  Batch [950/1299] D_loss: -2.5580, G_loss: 2.2250\n",
      "  Batch [960/1299] D_loss: -2.8896, G_loss: 3.2427\n",
      "  Batch [970/1299] D_loss: -2.0559, G_loss: 5.6035\n",
      "  Batch [980/1299] D_loss: -2.6978, G_loss: 3.5271\n",
      "  Batch [990/1299] D_loss: -2.2040, G_loss: 2.2726\n",
      "  Batch [1000/1299] D_loss: -1.7884, G_loss: 2.0896\n",
      "  Batch [1010/1299] D_loss: -2.2549, G_loss: 3.3678\n",
      "  Batch [1020/1299] D_loss: -2.4974, G_loss: 1.8720\n",
      "  Batch [1030/1299] D_loss: -1.0568, G_loss: 1.0014\n",
      "  Batch [1040/1299] D_loss: -2.5113, G_loss: 1.0324\n",
      "  Batch [1050/1299] D_loss: -2.0764, G_loss: 1.4337\n",
      "  Batch [1060/1299] D_loss: -2.7011, G_loss: 2.5161\n",
      "  Batch [1070/1299] D_loss: -2.4330, G_loss: 3.6792\n",
      "  Batch [1080/1299] D_loss: -1.7934, G_loss: 2.8399\n",
      "  Batch [1090/1299] D_loss: -2.2418, G_loss: 2.6414\n",
      "  Batch [1100/1299] D_loss: -1.4824, G_loss: 2.3255\n",
      "  Batch [1110/1299] D_loss: -2.9901, G_loss: 0.2337\n",
      "  Batch [1120/1299] D_loss: -3.4322, G_loss: -0.8749\n",
      "  Batch [1130/1299] D_loss: -2.5386, G_loss: -1.8805\n",
      "  Batch [1140/1299] D_loss: -2.5188, G_loss: -3.3492\n",
      "  Batch [1150/1299] D_loss: -1.8339, G_loss: -3.4410\n",
      "  Batch [1160/1299] D_loss: -2.4100, G_loss: -2.6304\n",
      "  Batch [1170/1299] D_loss: -2.4655, G_loss: -0.9402\n",
      "  Batch [1180/1299] D_loss: -2.3585, G_loss: 0.3539\n",
      "  Batch [1190/1299] D_loss: -2.3878, G_loss: 1.7014\n",
      "  Batch [1200/1299] D_loss: -2.8465, G_loss: 2.2909\n",
      "  Batch [1210/1299] D_loss: -3.5777, G_loss: 2.3539\n",
      "  Batch [1220/1299] D_loss: -1.6960, G_loss: 2.6323\n",
      "  Batch [1230/1299] D_loss: -1.4414, G_loss: 3.3504\n",
      "  Batch [1240/1299] D_loss: -2.2558, G_loss: 2.8110\n",
      "  Batch [1250/1299] D_loss: -1.2793, G_loss: 1.8570\n",
      "  Batch [1260/1299] D_loss: -1.8373, G_loss: 0.0057\n",
      "  Batch [1270/1299] D_loss: -2.8539, G_loss: -0.5862\n",
      "  Batch [1280/1299] D_loss: -1.6105, G_loss: -1.5381\n",
      "  Batch [1290/1299] D_loss: -3.0334, G_loss: -0.0682\n",
      "\n",
      "Epoch 68 Summary:\n",
      "  Average D_loss: -2.0086\n",
      "  Average G_loss: 1.1850\n",
      "\n",
      "Epoch [69/100]\n",
      "  Batch [0/1299] D_loss: -2.2682, G_loss: -2.3430\n",
      "  Batch [10/1299] D_loss: -2.8728, G_loss: -1.9675\n",
      "  Batch [20/1299] D_loss: -2.4705, G_loss: -1.9442\n",
      "  Batch [30/1299] D_loss: -1.8178, G_loss: -1.9364\n",
      "  Batch [40/1299] D_loss: -1.7084, G_loss: -0.5999\n",
      "  Batch [50/1299] D_loss: -3.5641, G_loss: 0.2977\n",
      "  Batch [60/1299] D_loss: -2.6986, G_loss: 0.8344\n",
      "  Batch [70/1299] D_loss: -2.4862, G_loss: 2.8322\n",
      "  Batch [80/1299] D_loss: -1.8490, G_loss: 2.8822\n",
      "  Batch [90/1299] D_loss: -2.3100, G_loss: 3.6100\n",
      "  Batch [100/1299] D_loss: -2.8812, G_loss: 3.6582\n",
      "  Batch [110/1299] D_loss: -2.4220, G_loss: 2.7269\n",
      "  Batch [120/1299] D_loss: -1.7686, G_loss: 1.9747\n",
      "  Batch [130/1299] D_loss: -1.6074, G_loss: 0.5545\n",
      "  Batch [140/1299] D_loss: -2.0968, G_loss: 1.3272\n",
      "  Batch [150/1299] D_loss: -1.6540, G_loss: 1.7160\n",
      "  Batch [160/1299] D_loss: -2.6937, G_loss: -0.7786\n",
      "  Batch [170/1299] D_loss: -2.6995, G_loss: -1.3312\n",
      "  Batch [180/1299] D_loss: -2.0129, G_loss: -2.8284\n",
      "  Batch [190/1299] D_loss: -2.3787, G_loss: -4.9166\n",
      "  Batch [200/1299] D_loss: -1.6900, G_loss: -1.7540\n",
      "  Batch [210/1299] D_loss: -1.9454, G_loss: -1.4885\n",
      "  Batch [220/1299] D_loss: -1.9493, G_loss: 0.2568\n",
      "  Batch [230/1299] D_loss: -2.6605, G_loss: -1.9302\n",
      "  Batch [240/1299] D_loss: -2.3566, G_loss: 1.4790\n",
      "  Batch [250/1299] D_loss: -1.7203, G_loss: 3.1856\n",
      "  Batch [260/1299] D_loss: -1.8926, G_loss: 5.6885\n",
      "  Batch [270/1299] D_loss: -1.8864, G_loss: 2.4804\n",
      "  Batch [280/1299] D_loss: -1.6466, G_loss: 2.8162\n",
      "  Batch [290/1299] D_loss: -1.1740, G_loss: 1.1170\n",
      "  Batch [300/1299] D_loss: -2.5033, G_loss: 2.9229\n",
      "  Batch [310/1299] D_loss: -2.9081, G_loss: 0.5749\n",
      "  Batch [320/1299] D_loss: -2.4164, G_loss: -0.5101\n",
      "  Batch [330/1299] D_loss: -2.2394, G_loss: -0.2824\n",
      "  Batch [340/1299] D_loss: -2.1487, G_loss: -0.6810\n",
      "  Batch [350/1299] D_loss: -0.9245, G_loss: 1.8104\n",
      "  Batch [360/1299] D_loss: -2.2996, G_loss: 2.0341\n",
      "  Batch [370/1299] D_loss: -2.9632, G_loss: 1.8618\n",
      "  Batch [380/1299] D_loss: -2.2918, G_loss: 1.4030\n",
      "  Batch [390/1299] D_loss: -2.5674, G_loss: -0.4480\n",
      "  Batch [400/1299] D_loss: -2.3390, G_loss: -0.1218\n",
      "  Batch [410/1299] D_loss: -1.3118, G_loss: 1.1478\n",
      "  Batch [420/1299] D_loss: -2.4898, G_loss: 0.8599\n",
      "  Batch [430/1299] D_loss: -2.6427, G_loss: 2.3284\n",
      "  Batch [440/1299] D_loss: -2.3636, G_loss: 2.4039\n",
      "  Batch [450/1299] D_loss: -2.6523, G_loss: 1.3207\n",
      "  Batch [460/1299] D_loss: -1.8695, G_loss: 0.1916\n",
      "  Batch [470/1299] D_loss: -2.9764, G_loss: 1.7794\n",
      "  Batch [480/1299] D_loss: -2.3306, G_loss: 4.6270\n",
      "  Batch [490/1299] D_loss: -2.2871, G_loss: 2.4190\n",
      "  Batch [500/1299] D_loss: -2.6879, G_loss: 2.2720\n",
      "  Batch [510/1299] D_loss: -3.5735, G_loss: 0.4733\n",
      "  Batch [520/1299] D_loss: -2.2258, G_loss: 0.6134\n",
      "  Batch [530/1299] D_loss: -2.7330, G_loss: 1.5198\n",
      "  Batch [540/1299] D_loss: -2.8483, G_loss: 2.0515\n",
      "  Batch [550/1299] D_loss: -1.7162, G_loss: 0.8986\n",
      "  Batch [560/1299] D_loss: -3.6583, G_loss: -0.1176\n",
      "  Batch [570/1299] D_loss: -1.8802, G_loss: 0.2230\n",
      "  Batch [580/1299] D_loss: -2.0182, G_loss: 1.1573\n",
      "  Batch [590/1299] D_loss: -1.9381, G_loss: 1.1192\n",
      "  Batch [600/1299] D_loss: -2.1455, G_loss: -0.2969\n",
      "  Batch [610/1299] D_loss: -2.6558, G_loss: 0.3484\n",
      "  Batch [620/1299] D_loss: -1.8046, G_loss: 2.1597\n",
      "  Batch [630/1299] D_loss: -1.6527, G_loss: 0.7871\n",
      "  Batch [640/1299] D_loss: -1.6003, G_loss: 0.3084\n",
      "  Batch [650/1299] D_loss: -1.0765, G_loss: -2.0512\n",
      "  Batch [660/1299] D_loss: -1.4037, G_loss: -1.0642\n",
      "  Batch [670/1299] D_loss: -2.8247, G_loss: -0.2363\n",
      "  Batch [680/1299] D_loss: -3.1175, G_loss: 0.9538\n",
      "  Batch [690/1299] D_loss: -3.1530, G_loss: 3.4496\n",
      "  Batch [700/1299] D_loss: -2.0482, G_loss: 4.5809\n",
      "  Batch [710/1299] D_loss: -2.0737, G_loss: 3.9165\n",
      "  Batch [720/1299] D_loss: -1.4299, G_loss: 2.2741\n",
      "  Batch [730/1299] D_loss: -1.9636, G_loss: 4.1508\n",
      "  Batch [740/1299] D_loss: -2.9230, G_loss: 0.0379\n",
      "  Batch [750/1299] D_loss: -2.9186, G_loss: -1.3606\n",
      "  Batch [760/1299] D_loss: -2.6130, G_loss: 0.1597\n",
      "  Batch [770/1299] D_loss: -2.1639, G_loss: -0.0111\n",
      "  Batch [780/1299] D_loss: -1.0416, G_loss: -1.9883\n",
      "  Batch [790/1299] D_loss: -3.4658, G_loss: -1.0719\n",
      "  Batch [800/1299] D_loss: -0.9478, G_loss: -1.1886\n",
      "  Batch [810/1299] D_loss: -1.2861, G_loss: 0.8938\n",
      "  Batch [820/1299] D_loss: -2.6446, G_loss: 2.2887\n",
      "  Batch [830/1299] D_loss: -3.3424, G_loss: 4.4710\n",
      "  Batch [840/1299] D_loss: -3.4664, G_loss: 2.1101\n",
      "  Batch [850/1299] D_loss: -1.9793, G_loss: 2.7726\n",
      "  Batch [860/1299] D_loss: -3.2168, G_loss: 3.4613\n",
      "  Batch [870/1299] D_loss: -2.2548, G_loss: 2.5985\n",
      "  Batch [880/1299] D_loss: -2.3994, G_loss: -0.2339\n",
      "  Batch [890/1299] D_loss: -1.7480, G_loss: -1.4917\n",
      "  Batch [900/1299] D_loss: -0.7924, G_loss: -0.6642\n",
      "  Batch [910/1299] D_loss: -1.7491, G_loss: -2.6718\n",
      "  Batch [920/1299] D_loss: -1.7406, G_loss: -3.6871\n",
      "  Batch [930/1299] D_loss: -3.4521, G_loss: -4.8396\n",
      "  Batch [940/1299] D_loss: -3.1159, G_loss: -2.3223\n",
      "  Batch [950/1299] D_loss: -3.2648, G_loss: -0.1936\n",
      "  Batch [960/1299] D_loss: -2.8319, G_loss: 2.4390\n",
      "  Batch [970/1299] D_loss: -1.8943, G_loss: 3.9482\n",
      "  Batch [980/1299] D_loss: -2.6019, G_loss: 5.2041\n",
      "  Batch [990/1299] D_loss: -1.0877, G_loss: 5.6248\n",
      "  Batch [1000/1299] D_loss: -2.9951, G_loss: 4.4075\n",
      "  Batch [1010/1299] D_loss: -2.4890, G_loss: 2.2615\n",
      "  Batch [1020/1299] D_loss: -1.0362, G_loss: 4.0933\n",
      "  Batch [1030/1299] D_loss: -1.0971, G_loss: 1.0520\n",
      "  Batch [1040/1299] D_loss: -2.6067, G_loss: -1.1616\n",
      "  Batch [1050/1299] D_loss: -1.8426, G_loss: 0.0457\n",
      "  Batch [1060/1299] D_loss: -2.4880, G_loss: -0.3380\n",
      "  Batch [1070/1299] D_loss: -2.3857, G_loss: -2.1626\n",
      "  Batch [1080/1299] D_loss: -2.2847, G_loss: -2.2686\n",
      "  Batch [1090/1299] D_loss: -2.9356, G_loss: -3.2412\n",
      "  Batch [1100/1299] D_loss: -2.7406, G_loss: -2.9383\n",
      "  Batch [1110/1299] D_loss: -2.2260, G_loss: -1.3873\n",
      "  Batch [1120/1299] D_loss: -2.2947, G_loss: -0.3935\n",
      "  Batch [1130/1299] D_loss: -1.3795, G_loss: 2.8977\n",
      "  Batch [1140/1299] D_loss: -1.2790, G_loss: 3.9138\n",
      "  Batch [1150/1299] D_loss: -2.7613, G_loss: 4.0763\n",
      "  Batch [1160/1299] D_loss: -3.2809, G_loss: 4.0552\n",
      "  Batch [1170/1299] D_loss: -1.9471, G_loss: 1.9337\n",
      "  Batch [1180/1299] D_loss: -1.5267, G_loss: 4.3825\n",
      "  Batch [1190/1299] D_loss: -1.5178, G_loss: 1.5723\n",
      "  Batch [1200/1299] D_loss: -1.7975, G_loss: 1.9489\n",
      "  Batch [1210/1299] D_loss: -2.2209, G_loss: 1.0412\n",
      "  Batch [1220/1299] D_loss: -1.4558, G_loss: -0.2838\n",
      "  Batch [1230/1299] D_loss: -2.3707, G_loss: -0.7871\n",
      "  Batch [1240/1299] D_loss: -2.2250, G_loss: -0.0082\n",
      "  Batch [1250/1299] D_loss: -2.5620, G_loss: 0.0222\n",
      "  Batch [1260/1299] D_loss: -1.7031, G_loss: 2.0599\n",
      "  Batch [1270/1299] D_loss: -2.4276, G_loss: 3.0621\n",
      "  Batch [1280/1299] D_loss: -2.8937, G_loss: 4.2068\n",
      "  Batch [1290/1299] D_loss: -3.4337, G_loss: 4.7898\n",
      "\n",
      "Epoch 69 Summary:\n",
      "  Average D_loss: -2.0002\n",
      "  Average G_loss: 0.9446\n",
      "\n",
      "Epoch [70/100]\n",
      "  Batch [0/1299] D_loss: -2.0675, G_loss: 2.9047\n",
      "  Batch [10/1299] D_loss: -2.7189, G_loss: -0.2503\n",
      "  Batch [20/1299] D_loss: -3.0859, G_loss: -0.9223\n",
      "  Batch [30/1299] D_loss: -2.3159, G_loss: -1.4810\n",
      "  Batch [40/1299] D_loss: -2.4164, G_loss: -4.1761\n",
      "  Batch [50/1299] D_loss: -3.0242, G_loss: -3.5313\n",
      "  Batch [60/1299] D_loss: -1.1377, G_loss: -4.1309\n",
      "  Batch [70/1299] D_loss: -2.4033, G_loss: -2.7600\n",
      "  Batch [80/1299] D_loss: -1.8734, G_loss: -0.8111\n",
      "  Batch [90/1299] D_loss: -1.5546, G_loss: -0.8985\n",
      "  Batch [100/1299] D_loss: -2.1272, G_loss: 3.7239\n",
      "  Batch [110/1299] D_loss: -1.6888, G_loss: 1.8263\n",
      "  Batch [120/1299] D_loss: -3.0199, G_loss: 2.3040\n",
      "  Batch [130/1299] D_loss: -2.3554, G_loss: 3.9513\n",
      "  Batch [140/1299] D_loss: -2.3142, G_loss: 1.1130\n",
      "  Batch [150/1299] D_loss: -2.3983, G_loss: 0.7467\n",
      "  Batch [160/1299] D_loss: -2.2557, G_loss: -1.5663\n",
      "  Batch [170/1299] D_loss: -2.6029, G_loss: -0.8370\n",
      "  Batch [180/1299] D_loss: -2.4948, G_loss: -0.6457\n",
      "  Batch [190/1299] D_loss: -0.6354, G_loss: -1.8431\n",
      "  Batch [200/1299] D_loss: -1.8979, G_loss: -2.0496\n",
      "  Batch [210/1299] D_loss: -1.6976, G_loss: -1.5097\n",
      "  Batch [220/1299] D_loss: -0.5633, G_loss: -0.3027\n",
      "  Batch [230/1299] D_loss: -2.1431, G_loss: 1.1422\n",
      "  Batch [240/1299] D_loss: -1.3664, G_loss: 2.4023\n",
      "  Batch [250/1299] D_loss: -1.3015, G_loss: 3.4102\n",
      "  Batch [260/1299] D_loss: -1.8293, G_loss: 3.0027\n",
      "  Batch [270/1299] D_loss: -1.7354, G_loss: 4.9552\n",
      "  Batch [280/1299] D_loss: -2.0941, G_loss: 4.1980\n",
      "  Batch [290/1299] D_loss: -2.0412, G_loss: 4.2423\n",
      "  Batch [300/1299] D_loss: -2.1242, G_loss: 5.1608\n",
      "  Batch [310/1299] D_loss: -2.4714, G_loss: 1.0400\n",
      "  Batch [320/1299] D_loss: -2.0053, G_loss: 1.2082\n",
      "  Batch [330/1299] D_loss: -2.3743, G_loss: 0.6155\n",
      "  Batch [340/1299] D_loss: -2.4222, G_loss: -1.9716\n",
      "  Batch [350/1299] D_loss: -2.8232, G_loss: 0.1232\n",
      "  Batch [360/1299] D_loss: -2.0451, G_loss: -0.6095\n",
      "  Batch [370/1299] D_loss: -2.0437, G_loss: 0.2013\n",
      "  Batch [380/1299] D_loss: -1.8155, G_loss: 1.5141\n",
      "  Batch [390/1299] D_loss: -2.3914, G_loss: 2.1412\n",
      "  Batch [400/1299] D_loss: -1.8385, G_loss: 1.9641\n",
      "  Batch [410/1299] D_loss: -1.3494, G_loss: 1.1226\n",
      "  Batch [420/1299] D_loss: -2.7406, G_loss: -0.0788\n",
      "  Batch [430/1299] D_loss: -2.8424, G_loss: -0.8866\n",
      "  Batch [440/1299] D_loss: -1.5303, G_loss: 0.1618\n",
      "  Batch [450/1299] D_loss: -3.4032, G_loss: 1.5214\n",
      "  Batch [460/1299] D_loss: -3.0005, G_loss: 0.4446\n",
      "  Batch [470/1299] D_loss: -2.0474, G_loss: -1.7808\n",
      "  Batch [480/1299] D_loss: -1.8322, G_loss: -2.6221\n",
      "  Batch [490/1299] D_loss: -0.7932, G_loss: -2.5050\n",
      "  Batch [500/1299] D_loss: -1.4172, G_loss: -1.8795\n",
      "  Batch [510/1299] D_loss: -1.8793, G_loss: -1.9154\n",
      "  Batch [520/1299] D_loss: -2.1933, G_loss: -1.5723\n",
      "  Batch [530/1299] D_loss: -1.7874, G_loss: -1.3071\n",
      "  Batch [540/1299] D_loss: -2.3744, G_loss: -0.2533\n",
      "  Batch [550/1299] D_loss: -2.2469, G_loss: -0.6637\n",
      "  Batch [560/1299] D_loss: -4.2062, G_loss: 0.4306\n",
      "  Batch [570/1299] D_loss: -2.9377, G_loss: 2.5918\n",
      "  Batch [580/1299] D_loss: -1.4003, G_loss: 1.6452\n",
      "  Batch [590/1299] D_loss: -3.5605, G_loss: 1.1440\n",
      "  Batch [600/1299] D_loss: -0.3193, G_loss: 3.8907\n",
      "  Batch [610/1299] D_loss: -1.5187, G_loss: 1.8814\n",
      "  Batch [620/1299] D_loss: -3.4710, G_loss: 4.2217\n",
      "  Batch [630/1299] D_loss: -2.9262, G_loss: 3.3153\n",
      "  Batch [640/1299] D_loss: -2.3725, G_loss: 2.6393\n",
      "  Batch [650/1299] D_loss: -1.4367, G_loss: 1.4864\n",
      "  Batch [660/1299] D_loss: -2.2404, G_loss: 2.5835\n",
      "  Batch [670/1299] D_loss: -3.3014, G_loss: 0.2854\n",
      "  Batch [680/1299] D_loss: -2.6758, G_loss: -1.2853\n",
      "  Batch [690/1299] D_loss: -0.3775, G_loss: -1.0163\n",
      "  Batch [700/1299] D_loss: -1.8980, G_loss: 1.2577\n",
      "  Batch [710/1299] D_loss: -1.4878, G_loss: 0.3740\n",
      "  Batch [720/1299] D_loss: -1.8195, G_loss: 1.1685\n",
      "  Batch [730/1299] D_loss: -2.0021, G_loss: 2.7321\n",
      "  Batch [740/1299] D_loss: -1.6121, G_loss: 3.7135\n",
      "  Batch [750/1299] D_loss: -3.7993, G_loss: 2.5056\n",
      "  Batch [760/1299] D_loss: -2.6055, G_loss: 2.2024\n",
      "  Batch [770/1299] D_loss: -2.6172, G_loss: 1.6602\n",
      "  Batch [780/1299] D_loss: -3.2723, G_loss: 0.9393\n",
      "  Batch [790/1299] D_loss: -1.5672, G_loss: -0.8071\n",
      "  Batch [800/1299] D_loss: -1.6102, G_loss: -0.1462\n",
      "  Batch [810/1299] D_loss: -2.2977, G_loss: -1.1169\n",
      "  Batch [820/1299] D_loss: -0.7826, G_loss: -0.0581\n",
      "  Batch [830/1299] D_loss: -1.3436, G_loss: -2.1115\n",
      "  Batch [840/1299] D_loss: -1.7979, G_loss: -2.9678\n",
      "  Batch [850/1299] D_loss: -1.9013, G_loss: -2.1742\n",
      "  Batch [860/1299] D_loss: -3.3711, G_loss: -0.4909\n",
      "  Batch [870/1299] D_loss: -2.3622, G_loss: -0.6753\n",
      "  Batch [880/1299] D_loss: -2.5008, G_loss: 2.4969\n",
      "  Batch [890/1299] D_loss: -2.4885, G_loss: 4.6555\n",
      "  Batch [900/1299] D_loss: -1.6760, G_loss: 5.1691\n",
      "  Batch [910/1299] D_loss: -0.6135, G_loss: 6.7810\n",
      "  Batch [920/1299] D_loss: -4.2594, G_loss: 5.8005\n",
      "  Batch [930/1299] D_loss: -1.2945, G_loss: 3.4790\n",
      "  Batch [940/1299] D_loss: -2.0071, G_loss: 2.9761\n",
      "  Batch [950/1299] D_loss: -2.2277, G_loss: 2.5806\n",
      "  Batch [960/1299] D_loss: -0.6890, G_loss: 2.7824\n",
      "  Batch [970/1299] D_loss: -0.8950, G_loss: 3.6800\n",
      "  Batch [980/1299] D_loss: -0.8074, G_loss: 2.5610\n",
      "  Batch [990/1299] D_loss: -1.7086, G_loss: 2.8876\n",
      "  Batch [1000/1299] D_loss: -4.1589, G_loss: 2.3720\n",
      "  Batch [1010/1299] D_loss: -2.1654, G_loss: 2.7409\n",
      "  Batch [1020/1299] D_loss: -0.6524, G_loss: 1.7980\n",
      "  Batch [1030/1299] D_loss: -1.9047, G_loss: 1.1157\n",
      "  Batch [1040/1299] D_loss: -2.2518, G_loss: 3.1720\n",
      "  Batch [1050/1299] D_loss: -0.3570, G_loss: 2.0066\n",
      "  Batch [1060/1299] D_loss: -1.5993, G_loss: 0.1265\n",
      "  Batch [1070/1299] D_loss: -1.6639, G_loss: 0.0296\n",
      "  Batch [1080/1299] D_loss: -2.8568, G_loss: 0.7061\n",
      "  Batch [1090/1299] D_loss: -1.7328, G_loss: -0.7269\n",
      "  Batch [1100/1299] D_loss: -3.3385, G_loss: -2.8659\n",
      "  Batch [1110/1299] D_loss: -3.2567, G_loss: -4.6236\n",
      "  Batch [1120/1299] D_loss: -1.4511, G_loss: -5.5656\n",
      "  Batch [1130/1299] D_loss: -2.7648, G_loss: -5.0438\n",
      "  Batch [1140/1299] D_loss: -2.1856, G_loss: -5.0671\n",
      "  Batch [1150/1299] D_loss: -0.9631, G_loss: -3.8290\n",
      "  Batch [1160/1299] D_loss: -2.5656, G_loss: -1.5639\n",
      "  Batch [1170/1299] D_loss: -3.2287, G_loss: -1.6212\n",
      "  Batch [1180/1299] D_loss: -2.9366, G_loss: 0.0929\n",
      "  Batch [1190/1299] D_loss: -1.6311, G_loss: 1.1423\n",
      "  Batch [1200/1299] D_loss: -1.9256, G_loss: 4.3118\n",
      "  Batch [1210/1299] D_loss: -0.1620, G_loss: 3.7454\n",
      "  Batch [1220/1299] D_loss: -1.6414, G_loss: 5.1746\n",
      "  Batch [1230/1299] D_loss: -1.1265, G_loss: 2.8563\n",
      "  Batch [1240/1299] D_loss: -2.3616, G_loss: 2.0461\n",
      "  Batch [1250/1299] D_loss: -3.2143, G_loss: 2.6386\n",
      "  Batch [1260/1299] D_loss: -4.3143, G_loss: 2.4499\n",
      "  Batch [1270/1299] D_loss: -3.5620, G_loss: -0.3953\n",
      "  Batch [1280/1299] D_loss: -1.8279, G_loss: -1.7628\n",
      "  Batch [1290/1299] D_loss: -1.5509, G_loss: -0.4697\n",
      "\n",
      "Epoch 70 Summary:\n",
      "  Average D_loss: -2.0094\n",
      "  Average G_loss: 0.7645\n",
      "\n",
      "Epoch [71/100]\n",
      "  Batch [0/1299] D_loss: -0.4989, G_loss: 1.0028\n",
      "  Batch [10/1299] D_loss: -2.0317, G_loss: 3.1803\n",
      "  Batch [20/1299] D_loss: -1.8238, G_loss: 3.7741\n",
      "  Batch [30/1299] D_loss: -0.9092, G_loss: 1.5603\n",
      "  Batch [40/1299] D_loss: -2.9904, G_loss: 1.7369\n",
      "  Batch [50/1299] D_loss: -1.6082, G_loss: 2.5856\n",
      "  Batch [60/1299] D_loss: -3.0114, G_loss: 3.4219\n",
      "  Batch [70/1299] D_loss: -4.8279, G_loss: 1.2676\n",
      "  Batch [80/1299] D_loss: 0.0222, G_loss: 3.4328\n",
      "  Batch [90/1299] D_loss: -3.3661, G_loss: 2.2341\n",
      "  Batch [100/1299] D_loss: -0.3356, G_loss: -0.7028\n",
      "  Batch [110/1299] D_loss: -1.7313, G_loss: -1.2468\n",
      "  Batch [120/1299] D_loss: -2.5103, G_loss: -1.5076\n",
      "  Batch [130/1299] D_loss: -3.6245, G_loss: -1.4573\n",
      "  Batch [140/1299] D_loss: -0.4606, G_loss: -0.4140\n",
      "  Batch [150/1299] D_loss: -0.0586, G_loss: 1.3897\n",
      "  Batch [160/1299] D_loss: -3.4796, G_loss: 2.4163\n",
      "  Batch [170/1299] D_loss: -3.1714, G_loss: 3.8848\n",
      "  Batch [180/1299] D_loss: -2.2007, G_loss: 2.0548\n",
      "  Batch [190/1299] D_loss: -2.9258, G_loss: 3.6433\n",
      "  Batch [200/1299] D_loss: -2.1331, G_loss: 3.4833\n",
      "  Batch [210/1299] D_loss: -3.6171, G_loss: 1.4986\n",
      "  Batch [220/1299] D_loss: -2.2441, G_loss: 0.8221\n",
      "  Batch [230/1299] D_loss: -1.7548, G_loss: 0.8846\n",
      "  Batch [240/1299] D_loss: -1.0906, G_loss: 0.1382\n",
      "  Batch [250/1299] D_loss: -1.0558, G_loss: 1.4758\n",
      "  Batch [260/1299] D_loss: -0.6340, G_loss: 1.5662\n",
      "  Batch [270/1299] D_loss: -1.4038, G_loss: 4.6887\n",
      "  Batch [280/1299] D_loss: -1.3490, G_loss: 3.8104\n",
      "  Batch [290/1299] D_loss: -2.1983, G_loss: 8.3035\n",
      "  Batch [300/1299] D_loss: -0.6748, G_loss: 7.2995\n",
      "  Batch [310/1299] D_loss: -2.6541, G_loss: 4.6092\n",
      "  Batch [320/1299] D_loss: -1.9833, G_loss: 3.0637\n",
      "  Batch [330/1299] D_loss: -3.0824, G_loss: 2.5997\n",
      "  Batch [340/1299] D_loss: -1.0464, G_loss: 2.0362\n",
      "  Batch [350/1299] D_loss: -3.2889, G_loss: 2.0982\n",
      "  Batch [360/1299] D_loss: -1.9809, G_loss: 0.5008\n",
      "  Batch [370/1299] D_loss: -2.9388, G_loss: -0.9877\n",
      "  Batch [380/1299] D_loss: -3.0621, G_loss: -0.4457\n",
      "  Batch [390/1299] D_loss: -1.3886, G_loss: -1.0679\n",
      "  Batch [400/1299] D_loss: -3.7009, G_loss: -1.4129\n",
      "  Batch [410/1299] D_loss: -3.6215, G_loss: -2.0138\n",
      "  Batch [420/1299] D_loss: -1.4290, G_loss: -0.7681\n",
      "  Batch [430/1299] D_loss: -2.7603, G_loss: 0.7733\n",
      "  Batch [440/1299] D_loss: -1.9079, G_loss: 2.7954\n",
      "  Batch [450/1299] D_loss: -3.0160, G_loss: 5.5307\n",
      "  Batch [460/1299] D_loss: -2.6325, G_loss: 3.6564\n",
      "  Batch [470/1299] D_loss: -2.6196, G_loss: 6.2448\n",
      "  Batch [480/1299] D_loss: -1.7085, G_loss: 3.1739\n",
      "  Batch [490/1299] D_loss: -1.5967, G_loss: 4.6879\n",
      "  Batch [500/1299] D_loss: -1.1330, G_loss: 0.6325\n",
      "  Batch [510/1299] D_loss: -1.6154, G_loss: 1.4233\n",
      "  Batch [520/1299] D_loss: -1.8621, G_loss: -0.1743\n",
      "  Batch [530/1299] D_loss: -2.2658, G_loss: 0.7661\n",
      "  Batch [540/1299] D_loss: -0.7423, G_loss: 1.7970\n",
      "  Batch [550/1299] D_loss: -2.8897, G_loss: 2.5549\n",
      "  Batch [560/1299] D_loss: -1.6535, G_loss: 4.8755\n",
      "  Batch [570/1299] D_loss: -1.6925, G_loss: 7.2878\n",
      "  Batch [580/1299] D_loss: -3.1151, G_loss: 8.6531\n",
      "  Batch [590/1299] D_loss: -2.0998, G_loss: 9.4033\n",
      "  Batch [600/1299] D_loss: -3.3061, G_loss: 8.8547\n",
      "  Batch [610/1299] D_loss: -2.1261, G_loss: 6.5763\n",
      "  Batch [620/1299] D_loss: -2.0967, G_loss: 3.7519\n",
      "  Batch [630/1299] D_loss: -2.7968, G_loss: 1.4142\n",
      "  Batch [640/1299] D_loss: -1.0502, G_loss: 2.3266\n",
      "  Batch [650/1299] D_loss: -2.6787, G_loss: 1.5146\n",
      "  Batch [660/1299] D_loss: -2.3327, G_loss: 0.4222\n",
      "  Batch [670/1299] D_loss: -1.7030, G_loss: 0.9567\n",
      "  Batch [680/1299] D_loss: -1.8919, G_loss: 1.8342\n",
      "  Batch [690/1299] D_loss: -1.7891, G_loss: 0.2384\n",
      "  Batch [700/1299] D_loss: -2.6550, G_loss: 2.1519\n",
      "  Batch [710/1299] D_loss: -2.3089, G_loss: 1.1148\n",
      "  Batch [720/1299] D_loss: -3.1450, G_loss: 1.5963\n",
      "  Batch [730/1299] D_loss: -0.3171, G_loss: 1.7604\n",
      "  Batch [740/1299] D_loss: -4.1596, G_loss: 4.0979\n",
      "  Batch [750/1299] D_loss: -0.0643, G_loss: 1.6248\n",
      "  Batch [760/1299] D_loss: -2.0515, G_loss: 0.4250\n",
      "  Batch [770/1299] D_loss: -0.7056, G_loss: 3.4105\n",
      "  Batch [780/1299] D_loss: -2.6974, G_loss: 1.9244\n",
      "  Batch [790/1299] D_loss: -2.1270, G_loss: 0.6553\n",
      "  Batch [800/1299] D_loss: -1.7150, G_loss: 0.6668\n",
      "  Batch [810/1299] D_loss: -1.6195, G_loss: 1.5386\n",
      "  Batch [820/1299] D_loss: -4.7688, G_loss: -0.8692\n",
      "  Batch [830/1299] D_loss: -4.7090, G_loss: -3.4183\n",
      "  Batch [840/1299] D_loss: -2.2565, G_loss: -3.8222\n",
      "  Batch [850/1299] D_loss: -1.6205, G_loss: -0.8308\n",
      "  Batch [860/1299] D_loss: -2.4667, G_loss: -5.1770\n",
      "  Batch [870/1299] D_loss: -3.3246, G_loss: -3.7178\n",
      "  Batch [880/1299] D_loss: -2.5085, G_loss: -0.9776\n",
      "  Batch [890/1299] D_loss: -2.8330, G_loss: 0.6480\n",
      "  Batch [900/1299] D_loss: -3.7333, G_loss: 1.6238\n",
      "  Batch [910/1299] D_loss: -3.4925, G_loss: 4.7699\n",
      "  Batch [920/1299] D_loss: -2.0282, G_loss: 4.2205\n",
      "  Batch [930/1299] D_loss: -0.8938, G_loss: 2.1016\n",
      "  Batch [940/1299] D_loss: -2.0418, G_loss: 2.3958\n",
      "  Batch [950/1299] D_loss: -1.7659, G_loss: 4.8410\n",
      "  Batch [960/1299] D_loss: -1.4282, G_loss: 5.1289\n",
      "  Batch [970/1299] D_loss: -2.9763, G_loss: 6.0171\n",
      "  Batch [980/1299] D_loss: -2.4737, G_loss: 7.5285\n",
      "  Batch [990/1299] D_loss: -3.6047, G_loss: 8.9726\n",
      "  Batch [1000/1299] D_loss: -2.2569, G_loss: 7.1341\n",
      "  Batch [1010/1299] D_loss: -4.2912, G_loss: 7.2942\n",
      "  Batch [1020/1299] D_loss: -2.8105, G_loss: 4.6509\n",
      "  Batch [1030/1299] D_loss: -3.7163, G_loss: 1.2456\n",
      "  Batch [1040/1299] D_loss: -3.3110, G_loss: 2.7825\n",
      "  Batch [1050/1299] D_loss: -1.8746, G_loss: 2.5158\n",
      "  Batch [1060/1299] D_loss: -2.5586, G_loss: 0.9737\n",
      "  Batch [1070/1299] D_loss: -4.2951, G_loss: 0.8397\n",
      "  Batch [1080/1299] D_loss: -3.9001, G_loss: -1.1251\n",
      "  Batch [1090/1299] D_loss: -2.2139, G_loss: -3.5237\n",
      "  Batch [1100/1299] D_loss: -1.5159, G_loss: -2.7065\n",
      "  Batch [1110/1299] D_loss: -4.4764, G_loss: -3.9434\n",
      "  Batch [1120/1299] D_loss: -1.1034, G_loss: -5.0917\n",
      "  Batch [1130/1299] D_loss: -3.3649, G_loss: -4.6162\n",
      "  Batch [1140/1299] D_loss: -1.8180, G_loss: -3.0944\n",
      "  Batch [1150/1299] D_loss: -2.5936, G_loss: -1.9380\n",
      "  Batch [1160/1299] D_loss: -0.9874, G_loss: -1.2207\n",
      "  Batch [1170/1299] D_loss: -1.6459, G_loss: 1.7523\n",
      "  Batch [1180/1299] D_loss: -3.3173, G_loss: 4.3318\n",
      "  Batch [1190/1299] D_loss: -2.0705, G_loss: 6.8274\n",
      "  Batch [1200/1299] D_loss: -1.0907, G_loss: 6.0181\n",
      "  Batch [1210/1299] D_loss: -3.8028, G_loss: 6.7029\n",
      "  Batch [1220/1299] D_loss: -2.9479, G_loss: 4.2568\n",
      "  Batch [1230/1299] D_loss: -2.3214, G_loss: 5.4017\n",
      "  Batch [1240/1299] D_loss: -2.7183, G_loss: 2.8287\n",
      "  Batch [1250/1299] D_loss: -1.3509, G_loss: 2.5276\n",
      "  Batch [1260/1299] D_loss: -2.8629, G_loss: 5.0922\n",
      "  Batch [1270/1299] D_loss: -1.8950, G_loss: 2.9786\n",
      "  Batch [1280/1299] D_loss: -3.7288, G_loss: 2.2048\n",
      "  Batch [1290/1299] D_loss: -2.9475, G_loss: 3.0810\n",
      "\n",
      "Epoch 71 Summary:\n",
      "  Average D_loss: -2.0158\n",
      "  Average G_loss: 2.0353\n",
      "\n",
      "Epoch [72/100]\n",
      "  Batch [0/1299] D_loss: -2.3931, G_loss: 3.2492\n",
      "  Batch [10/1299] D_loss: -3.1657, G_loss: 2.6510\n",
      "  Batch [20/1299] D_loss: -2.8348, G_loss: 1.6371\n",
      "  Batch [30/1299] D_loss: -2.7010, G_loss: 1.6864\n",
      "  Batch [40/1299] D_loss: -1.9810, G_loss: 2.9265\n",
      "  Batch [50/1299] D_loss: -0.3296, G_loss: 1.0213\n",
      "  Batch [60/1299] D_loss: -1.4991, G_loss: 2.0488\n",
      "  Batch [70/1299] D_loss: -0.9574, G_loss: 1.7652\n",
      "  Batch [80/1299] D_loss: -2.5750, G_loss: 1.5371\n",
      "  Batch [90/1299] D_loss: -1.1501, G_loss: 1.4597\n",
      "  Batch [100/1299] D_loss: -2.1584, G_loss: -1.0696\n",
      "  Batch [110/1299] D_loss: -1.4889, G_loss: -3.6416\n",
      "  Batch [120/1299] D_loss: -3.6380, G_loss: -3.8995\n",
      "  Batch [130/1299] D_loss: -1.9100, G_loss: -4.9204\n",
      "  Batch [140/1299] D_loss: -3.8828, G_loss: -6.1340\n",
      "  Batch [150/1299] D_loss: -1.2394, G_loss: -8.7121\n",
      "  Batch [160/1299] D_loss: -1.5243, G_loss: -5.2027\n",
      "  Batch [170/1299] D_loss: -2.7541, G_loss: -8.0757\n",
      "  Batch [180/1299] D_loss: -1.9318, G_loss: -6.4363\n",
      "  Batch [190/1299] D_loss: -0.7593, G_loss: -4.0680\n",
      "  Batch [200/1299] D_loss: -3.9565, G_loss: 0.9293\n",
      "  Batch [210/1299] D_loss: -2.5018, G_loss: 1.2683\n",
      "  Batch [220/1299] D_loss: -2.1770, G_loss: 3.1286\n",
      "  Batch [230/1299] D_loss: -2.1186, G_loss: 5.4466\n",
      "  Batch [240/1299] D_loss: -1.5085, G_loss: 6.2455\n",
      "  Batch [250/1299] D_loss: -0.0306, G_loss: 5.1541\n",
      "  Batch [260/1299] D_loss: -3.7336, G_loss: 7.6756\n",
      "  Batch [270/1299] D_loss: -1.7479, G_loss: 6.2794\n",
      "  Batch [280/1299] D_loss: -3.2318, G_loss: 2.5895\n",
      "  Batch [290/1299] D_loss: -2.9612, G_loss: 2.8437\n",
      "  Batch [300/1299] D_loss: -3.1460, G_loss: -0.8152\n",
      "  Batch [310/1299] D_loss: -3.4186, G_loss: -2.0227\n",
      "  Batch [320/1299] D_loss: -1.8033, G_loss: -0.8690\n",
      "  Batch [330/1299] D_loss: -1.0671, G_loss: -2.1402\n",
      "  Batch [340/1299] D_loss: -0.9861, G_loss: -2.2218\n",
      "  Batch [350/1299] D_loss: -1.0780, G_loss: -3.6739\n",
      "  Batch [360/1299] D_loss: -2.4311, G_loss: -2.5944\n",
      "  Batch [370/1299] D_loss: -2.7205, G_loss: -0.7073\n",
      "  Batch [380/1299] D_loss: -1.9009, G_loss: 0.0615\n",
      "  Batch [390/1299] D_loss: -4.3033, G_loss: 4.1809\n",
      "  Batch [400/1299] D_loss: -0.3077, G_loss: 4.8542\n",
      "  Batch [410/1299] D_loss: -2.8725, G_loss: 5.3565\n",
      "  Batch [420/1299] D_loss: -2.8771, G_loss: 6.0170\n",
      "  Batch [430/1299] D_loss: -1.4373, G_loss: 4.5507\n",
      "  Batch [440/1299] D_loss: -2.8522, G_loss: 1.4679\n",
      "  Batch [450/1299] D_loss: -2.4876, G_loss: 2.9191\n",
      "  Batch [460/1299] D_loss: -3.7217, G_loss: 1.5251\n",
      "  Batch [470/1299] D_loss: -2.6249, G_loss: -0.2045\n",
      "  Batch [480/1299] D_loss: -1.7251, G_loss: 0.9810\n",
      "  Batch [490/1299] D_loss: -3.7791, G_loss: -1.3806\n",
      "  Batch [500/1299] D_loss: -3.7147, G_loss: -0.6635\n",
      "  Batch [510/1299] D_loss: -0.6721, G_loss: 0.2838\n",
      "  Batch [520/1299] D_loss: -1.3808, G_loss: 0.9950\n",
      "  Batch [530/1299] D_loss: -1.7913, G_loss: 2.3763\n",
      "  Batch [540/1299] D_loss: -1.5320, G_loss: 2.9053\n",
      "  Batch [550/1299] D_loss: -2.1798, G_loss: 0.1317\n",
      "  Batch [560/1299] D_loss: -2.9307, G_loss: 0.2037\n",
      "  Batch [570/1299] D_loss: -3.0207, G_loss: 1.3371\n",
      "  Batch [580/1299] D_loss: -2.8593, G_loss: -0.6913\n",
      "  Batch [590/1299] D_loss: -1.3855, G_loss: 0.2507\n",
      "  Batch [600/1299] D_loss: -2.6651, G_loss: -1.5676\n",
      "  Batch [610/1299] D_loss: -3.4421, G_loss: 0.6556\n",
      "  Batch [620/1299] D_loss: -2.2810, G_loss: 0.6461\n",
      "  Batch [630/1299] D_loss: -3.8859, G_loss: 1.9413\n",
      "  Batch [640/1299] D_loss: -5.1136, G_loss: 6.3862\n",
      "  Batch [650/1299] D_loss: -4.0339, G_loss: 6.9791\n",
      "  Batch [660/1299] D_loss: -2.1483, G_loss: 6.9677\n",
      "  Batch [670/1299] D_loss: -3.0279, G_loss: 4.0696\n",
      "  Batch [680/1299] D_loss: -2.0194, G_loss: 4.4500\n",
      "  Batch [690/1299] D_loss: -0.6157, G_loss: 4.0050\n",
      "  Batch [700/1299] D_loss: -3.3026, G_loss: 1.9794\n",
      "  Batch [710/1299] D_loss: -3.1550, G_loss: 2.6908\n",
      "  Batch [720/1299] D_loss: -2.0357, G_loss: 1.8999\n",
      "  Batch [730/1299] D_loss: -3.6224, G_loss: 0.6619\n",
      "  Batch [740/1299] D_loss: -1.0716, G_loss: 2.8532\n",
      "  Batch [750/1299] D_loss: -2.0857, G_loss: -1.2385\n",
      "  Batch [760/1299] D_loss: -2.1434, G_loss: 1.1661\n",
      "  Batch [770/1299] D_loss: -1.9292, G_loss: 0.2805\n",
      "  Batch [780/1299] D_loss: -2.0954, G_loss: 0.1822\n",
      "  Batch [790/1299] D_loss: -2.9453, G_loss: -0.9299\n",
      "  Batch [800/1299] D_loss: -1.9715, G_loss: 0.0650\n",
      "  Batch [810/1299] D_loss: -2.3591, G_loss: -0.1678\n",
      "  Batch [820/1299] D_loss: -0.7844, G_loss: 1.0771\n",
      "  Batch [830/1299] D_loss: -1.7032, G_loss: -1.5101\n",
      "  Batch [840/1299] D_loss: -3.0158, G_loss: 0.4632\n",
      "  Batch [850/1299] D_loss: -2.2516, G_loss: 0.9165\n",
      "  Batch [860/1299] D_loss: -2.1247, G_loss: 2.2623\n",
      "  Batch [870/1299] D_loss: -0.8181, G_loss: -0.1932\n",
      "  Batch [880/1299] D_loss: -2.2686, G_loss: 1.8256\n",
      "  Batch [890/1299] D_loss: -3.5722, G_loss: -0.0918\n",
      "  Batch [900/1299] D_loss: -3.1874, G_loss: 0.9118\n",
      "  Batch [910/1299] D_loss: -2.1049, G_loss: -1.0473\n",
      "  Batch [920/1299] D_loss: -2.0502, G_loss: -3.2412\n",
      "  Batch [930/1299] D_loss: -4.1571, G_loss: -4.5030\n",
      "  Batch [940/1299] D_loss: -2.2667, G_loss: -3.0564\n",
      "  Batch [950/1299] D_loss: -2.2427, G_loss: -4.0564\n",
      "  Batch [960/1299] D_loss: -1.1934, G_loss: -3.7894\n",
      "  Batch [970/1299] D_loss: -0.9684, G_loss: -3.0459\n",
      "  Batch [980/1299] D_loss: -2.2067, G_loss: -2.2652\n",
      "  Batch [990/1299] D_loss: -2.6961, G_loss: 0.6252\n",
      "  Batch [1000/1299] D_loss: -0.4395, G_loss: 4.7282\n",
      "  Batch [1010/1299] D_loss: -2.6956, G_loss: 7.1115\n",
      "  Batch [1020/1299] D_loss: -1.5241, G_loss: 8.8891\n",
      "  Batch [1030/1299] D_loss: -3.5536, G_loss: 8.2094\n",
      "  Batch [1040/1299] D_loss: -2.2289, G_loss: 6.5813\n",
      "  Batch [1050/1299] D_loss: -2.2942, G_loss: 4.8694\n",
      "  Batch [1060/1299] D_loss: -3.2359, G_loss: 3.7292\n",
      "  Batch [1070/1299] D_loss: -2.9374, G_loss: 4.4253\n",
      "  Batch [1080/1299] D_loss: 0.3414, G_loss: 3.2003\n",
      "  Batch [1090/1299] D_loss: -3.3736, G_loss: 4.1034\n",
      "  Batch [1100/1299] D_loss: -2.5288, G_loss: 4.3336\n",
      "  Batch [1110/1299] D_loss: -2.5608, G_loss: 1.0176\n",
      "  Batch [1120/1299] D_loss: -2.9905, G_loss: 1.7926\n",
      "  Batch [1130/1299] D_loss: -1.6599, G_loss: 1.9395\n",
      "  Batch [1140/1299] D_loss: -2.9010, G_loss: 4.9722\n",
      "  Batch [1150/1299] D_loss: -0.9993, G_loss: 4.0971\n",
      "  Batch [1160/1299] D_loss: -2.1420, G_loss: 4.7003\n",
      "  Batch [1170/1299] D_loss: -2.7543, G_loss: 3.0843\n",
      "  Batch [1180/1299] D_loss: -1.0183, G_loss: 3.7046\n",
      "  Batch [1190/1299] D_loss: -2.6867, G_loss: 2.8697\n",
      "  Batch [1200/1299] D_loss: -2.3105, G_loss: 2.0378\n",
      "  Batch [1210/1299] D_loss: -2.4255, G_loss: 1.6993\n",
      "  Batch [1220/1299] D_loss: -2.4153, G_loss: 1.7837\n",
      "  Batch [1230/1299] D_loss: -2.2791, G_loss: 3.3598\n",
      "  Batch [1240/1299] D_loss: -1.9890, G_loss: 3.1352\n",
      "  Batch [1250/1299] D_loss: -1.4013, G_loss: 2.6821\n",
      "  Batch [1260/1299] D_loss: -2.4791, G_loss: 3.3743\n",
      "  Batch [1270/1299] D_loss: -2.5613, G_loss: 3.5760\n",
      "  Batch [1280/1299] D_loss: -1.7260, G_loss: 4.3283\n",
      "  Batch [1290/1299] D_loss: -1.9907, G_loss: 4.2743\n",
      "\n",
      "Epoch 72 Summary:\n",
      "  Average D_loss: -1.9995\n",
      "  Average G_loss: 1.2905\n",
      "\n",
      "Epoch [73/100]\n",
      "  Batch [0/1299] D_loss: -1.9879, G_loss: 3.9475\n",
      "  Batch [10/1299] D_loss: -3.5480, G_loss: 4.6911\n",
      "  Batch [20/1299] D_loss: -1.4349, G_loss: 0.7970\n",
      "  Batch [30/1299] D_loss: -1.3272, G_loss: 1.0444\n",
      "  Batch [40/1299] D_loss: -1.5670, G_loss: 0.5475\n",
      "  Batch [50/1299] D_loss: -1.9919, G_loss: -0.2966\n",
      "  Batch [60/1299] D_loss: -3.1019, G_loss: -3.5170\n",
      "  Batch [70/1299] D_loss: -2.7254, G_loss: -5.2171\n",
      "  Batch [80/1299] D_loss: -2.5688, G_loss: -5.0930\n",
      "  Batch [90/1299] D_loss: -1.8699, G_loss: -4.5342\n",
      "  Batch [100/1299] D_loss: -2.6944, G_loss: -5.4777\n",
      "  Batch [110/1299] D_loss: -0.9207, G_loss: -2.6766\n",
      "  Batch [120/1299] D_loss: -1.7862, G_loss: -3.9110\n",
      "  Batch [130/1299] D_loss: -3.3465, G_loss: -2.2802\n",
      "  Batch [140/1299] D_loss: -2.2855, G_loss: 2.2849\n",
      "  Batch [150/1299] D_loss: -2.3912, G_loss: 3.3586\n",
      "  Batch [160/1299] D_loss: -0.9644, G_loss: 3.7590\n",
      "  Batch [170/1299] D_loss: -0.5504, G_loss: 3.5937\n",
      "  Batch [180/1299] D_loss: -2.6029, G_loss: 3.0067\n",
      "  Batch [190/1299] D_loss: -1.9120, G_loss: 4.6041\n",
      "  Batch [200/1299] D_loss: -2.3824, G_loss: 4.2704\n",
      "  Batch [210/1299] D_loss: -3.1569, G_loss: 4.8291\n",
      "  Batch [220/1299] D_loss: -1.1464, G_loss: 1.1144\n",
      "  Batch [230/1299] D_loss: -1.6849, G_loss: 0.3043\n",
      "  Batch [240/1299] D_loss: -1.5371, G_loss: -0.3097\n",
      "  Batch [250/1299] D_loss: -2.4402, G_loss: 1.7330\n",
      "  Batch [260/1299] D_loss: -0.8044, G_loss: -0.0762\n",
      "  Batch [270/1299] D_loss: -2.9316, G_loss: -0.0333\n",
      "  Batch [280/1299] D_loss: -2.9962, G_loss: 0.8954\n",
      "  Batch [290/1299] D_loss: -2.4794, G_loss: 1.3723\n",
      "  Batch [300/1299] D_loss: -2.1854, G_loss: 1.0869\n",
      "  Batch [310/1299] D_loss: -0.8402, G_loss: 0.5292\n",
      "  Batch [320/1299] D_loss: -1.9904, G_loss: 2.3409\n",
      "  Batch [330/1299] D_loss: -2.3611, G_loss: 2.2032\n",
      "  Batch [340/1299] D_loss: -1.0608, G_loss: 2.3374\n",
      "  Batch [350/1299] D_loss: -3.0641, G_loss: 1.2660\n",
      "  Batch [360/1299] D_loss: -0.2845, G_loss: 3.3544\n",
      "  Batch [370/1299] D_loss: -2.3896, G_loss: 1.7486\n",
      "  Batch [380/1299] D_loss: -0.3513, G_loss: 1.1769\n",
      "  Batch [390/1299] D_loss: -2.2720, G_loss: -1.1851\n",
      "  Batch [400/1299] D_loss: -3.1567, G_loss: -1.5705\n",
      "  Batch [410/1299] D_loss: -1.6050, G_loss: 0.0897\n",
      "  Batch [420/1299] D_loss: -2.1126, G_loss: -0.0795\n",
      "  Batch [430/1299] D_loss: -1.5867, G_loss: -0.2509\n",
      "  Batch [440/1299] D_loss: -1.6784, G_loss: -0.6775\n",
      "  Batch [450/1299] D_loss: -2.5010, G_loss: -0.4576\n",
      "  Batch [460/1299] D_loss: -1.8191, G_loss: 0.3319\n",
      "  Batch [470/1299] D_loss: -1.3712, G_loss: 1.9469\n",
      "  Batch [480/1299] D_loss: -0.9892, G_loss: 2.3050\n",
      "  Batch [490/1299] D_loss: -3.7252, G_loss: 6.1769\n",
      "  Batch [500/1299] D_loss: -1.7124, G_loss: 8.0106\n",
      "  Batch [510/1299] D_loss: -2.6647, G_loss: 7.3054\n",
      "  Batch [520/1299] D_loss: -3.2010, G_loss: 3.0732\n",
      "  Batch [530/1299] D_loss: -1.5528, G_loss: 3.2593\n",
      "  Batch [540/1299] D_loss: -2.1731, G_loss: 2.2198\n",
      "  Batch [550/1299] D_loss: -1.6855, G_loss: 0.6597\n",
      "  Batch [560/1299] D_loss: -2.5511, G_loss: 0.9650\n",
      "  Batch [570/1299] D_loss: -3.6952, G_loss: 0.5750\n",
      "  Batch [580/1299] D_loss: -0.7829, G_loss: -0.5196\n",
      "  Batch [590/1299] D_loss: -1.5439, G_loss: 0.8738\n",
      "  Batch [600/1299] D_loss: -1.6998, G_loss: 0.6985\n",
      "  Batch [610/1299] D_loss: -1.1925, G_loss: -0.1906\n",
      "  Batch [620/1299] D_loss: -1.9295, G_loss: 0.2082\n",
      "  Batch [630/1299] D_loss: -3.3727, G_loss: -1.2973\n",
      "  Batch [640/1299] D_loss: -2.6271, G_loss: -1.1829\n",
      "  Batch [650/1299] D_loss: -2.0473, G_loss: 0.2043\n",
      "  Batch [660/1299] D_loss: -1.9577, G_loss: 0.8874\n",
      "  Batch [670/1299] D_loss: -1.9075, G_loss: 3.3826\n",
      "  Batch [680/1299] D_loss: -2.0159, G_loss: 4.4724\n",
      "  Batch [690/1299] D_loss: -2.0659, G_loss: 4.5403\n",
      "  Batch [700/1299] D_loss: -1.5238, G_loss: 5.1107\n",
      "  Batch [710/1299] D_loss: -3.0749, G_loss: 4.9797\n",
      "  Batch [720/1299] D_loss: -2.2687, G_loss: 2.0636\n",
      "  Batch [730/1299] D_loss: -2.2585, G_loss: 2.4735\n",
      "  Batch [740/1299] D_loss: -1.5237, G_loss: 0.4588\n",
      "  Batch [750/1299] D_loss: -4.4562, G_loss: -1.7293\n",
      "  Batch [760/1299] D_loss: -3.1815, G_loss: -2.5149\n",
      "  Batch [770/1299] D_loss: -0.7776, G_loss: -1.9881\n",
      "  Batch [780/1299] D_loss: -2.3718, G_loss: -2.5504\n",
      "  Batch [790/1299] D_loss: -3.0939, G_loss: -3.0268\n",
      "  Batch [800/1299] D_loss: -0.7327, G_loss: -3.0719\n",
      "  Batch [810/1299] D_loss: -2.4795, G_loss: -4.3518\n",
      "  Batch [820/1299] D_loss: -2.0843, G_loss: -4.7736\n",
      "  Batch [830/1299] D_loss: -1.9333, G_loss: -2.5696\n",
      "  Batch [840/1299] D_loss: -3.2756, G_loss: -2.0038\n",
      "  Batch [850/1299] D_loss: -2.6267, G_loss: -0.3059\n",
      "  Batch [860/1299] D_loss: -1.4688, G_loss: 3.4435\n",
      "  Batch [870/1299] D_loss: -0.7607, G_loss: 4.3958\n",
      "  Batch [880/1299] D_loss: -3.1140, G_loss: 7.2146\n",
      "  Batch [890/1299] D_loss: -2.0697, G_loss: 6.1611\n",
      "  Batch [900/1299] D_loss: -2.3731, G_loss: 6.4007\n",
      "  Batch [910/1299] D_loss: -1.4750, G_loss: 5.6100\n",
      "  Batch [920/1299] D_loss: -2.0279, G_loss: 3.1017\n",
      "  Batch [930/1299] D_loss: -0.7108, G_loss: 1.7084\n",
      "  Batch [940/1299] D_loss: -3.0568, G_loss: 2.1856\n",
      "  Batch [950/1299] D_loss: -3.6038, G_loss: 2.6489\n",
      "  Batch [960/1299] D_loss: -2.9076, G_loss: 1.2110\n",
      "  Batch [970/1299] D_loss: -1.5785, G_loss: 3.1113\n",
      "  Batch [980/1299] D_loss: -0.9594, G_loss: 3.4506\n",
      "  Batch [990/1299] D_loss: -1.4085, G_loss: 4.0573\n",
      "  Batch [1000/1299] D_loss: -2.1883, G_loss: 5.1517\n",
      "  Batch [1010/1299] D_loss: -1.5418, G_loss: 2.7805\n",
      "  Batch [1020/1299] D_loss: -2.9451, G_loss: 2.3083\n",
      "  Batch [1030/1299] D_loss: -1.9763, G_loss: 0.8516\n",
      "  Batch [1040/1299] D_loss: -1.6130, G_loss: -0.8782\n",
      "  Batch [1050/1299] D_loss: -2.8588, G_loss: -2.6347\n",
      "  Batch [1060/1299] D_loss: -1.5833, G_loss: -2.1786\n",
      "  Batch [1070/1299] D_loss: -2.7041, G_loss: -3.3409\n",
      "  Batch [1080/1299] D_loss: -1.5183, G_loss: -3.5699\n",
      "  Batch [1090/1299] D_loss: -0.8322, G_loss: -4.1566\n",
      "  Batch [1100/1299] D_loss: -3.7968, G_loss: -5.1584\n",
      "  Batch [1110/1299] D_loss: -2.7045, G_loss: -4.3049\n",
      "  Batch [1120/1299] D_loss: -2.3666, G_loss: -4.7440\n",
      "  Batch [1130/1299] D_loss: -1.4570, G_loss: -3.1882\n",
      "  Batch [1140/1299] D_loss: -0.9962, G_loss: -2.0159\n",
      "  Batch [1150/1299] D_loss: -3.7436, G_loss: 0.5928\n",
      "  Batch [1160/1299] D_loss: -1.5837, G_loss: 5.6683\n",
      "  Batch [1170/1299] D_loss: -1.9102, G_loss: 5.3164\n",
      "  Batch [1180/1299] D_loss: -2.3190, G_loss: 6.6225\n",
      "  Batch [1190/1299] D_loss: -1.8724, G_loss: 5.8085\n",
      "  Batch [1200/1299] D_loss: -1.1964, G_loss: 7.4493\n",
      "  Batch [1210/1299] D_loss: -1.7447, G_loss: 6.6713\n",
      "  Batch [1220/1299] D_loss: -1.6909, G_loss: 4.4403\n",
      "  Batch [1230/1299] D_loss: -1.4252, G_loss: 2.6515\n",
      "  Batch [1240/1299] D_loss: -2.4306, G_loss: 1.4712\n",
      "  Batch [1250/1299] D_loss: -1.8478, G_loss: 1.6278\n",
      "  Batch [1260/1299] D_loss: -2.5520, G_loss: -1.7719\n",
      "  Batch [1270/1299] D_loss: -3.7352, G_loss: -1.8620\n",
      "  Batch [1280/1299] D_loss: -1.4055, G_loss: -0.7420\n",
      "  Batch [1290/1299] D_loss: -3.1182, G_loss: -1.2980\n",
      "\n",
      "Epoch 73 Summary:\n",
      "  Average D_loss: -2.0035\n",
      "  Average G_loss: 0.9920\n",
      "\n",
      "Epoch [74/100]\n",
      "  Batch [0/1299] D_loss: -2.1584, G_loss: -0.6772\n",
      "  Batch [10/1299] D_loss: -3.2560, G_loss: -0.6475\n",
      "  Batch [20/1299] D_loss: -1.1112, G_loss: 0.4521\n",
      "  Batch [30/1299] D_loss: -1.1465, G_loss: 2.1472\n",
      "  Batch [40/1299] D_loss: -2.5714, G_loss: 2.5449\n",
      "  Batch [50/1299] D_loss: -1.9265, G_loss: 4.0075\n",
      "  Batch [60/1299] D_loss: -2.0601, G_loss: 4.2771\n",
      "  Batch [70/1299] D_loss: -2.9586, G_loss: 5.1768\n",
      "  Batch [80/1299] D_loss: -2.2459, G_loss: 2.5158\n",
      "  Batch [90/1299] D_loss: -3.2525, G_loss: 3.3086\n",
      "  Batch [100/1299] D_loss: -2.7848, G_loss: 2.4260\n",
      "  Batch [110/1299] D_loss: -1.9835, G_loss: 1.0995\n",
      "  Batch [120/1299] D_loss: -2.3928, G_loss: -3.7351\n",
      "  Batch [130/1299] D_loss: -1.3000, G_loss: -2.0020\n",
      "  Batch [140/1299] D_loss: -1.5504, G_loss: -2.6952\n",
      "  Batch [150/1299] D_loss: -4.4132, G_loss: -2.5950\n",
      "  Batch [160/1299] D_loss: -2.4271, G_loss: -6.3911\n",
      "  Batch [170/1299] D_loss: -2.7790, G_loss: -4.3243\n",
      "  Batch [180/1299] D_loss: -1.9276, G_loss: -4.0392\n",
      "  Batch [190/1299] D_loss: -2.2204, G_loss: -2.4849\n",
      "  Batch [200/1299] D_loss: -1.7362, G_loss: 0.1958\n",
      "  Batch [210/1299] D_loss: -4.0952, G_loss: 0.8352\n",
      "  Batch [220/1299] D_loss: -1.1972, G_loss: 2.9488\n",
      "  Batch [230/1299] D_loss: -1.1669, G_loss: 2.5374\n",
      "  Batch [240/1299] D_loss: -1.9660, G_loss: 3.1173\n",
      "  Batch [250/1299] D_loss: -2.0657, G_loss: 2.9303\n",
      "  Batch [260/1299] D_loss: -1.1664, G_loss: 3.4803\n",
      "  Batch [270/1299] D_loss: -3.3494, G_loss: 2.1615\n",
      "  Batch [280/1299] D_loss: -4.3312, G_loss: 2.0108\n",
      "  Batch [290/1299] D_loss: -1.5899, G_loss: 0.6223\n",
      "  Batch [300/1299] D_loss: -2.1115, G_loss: -0.2816\n",
      "  Batch [310/1299] D_loss: -1.8919, G_loss: -2.7234\n",
      "  Batch [320/1299] D_loss: -3.1856, G_loss: -2.0651\n",
      "  Batch [330/1299] D_loss: -1.4168, G_loss: -4.3556\n",
      "  Batch [340/1299] D_loss: -1.7599, G_loss: -4.9346\n",
      "  Batch [350/1299] D_loss: -3.4923, G_loss: -4.8599\n",
      "  Batch [360/1299] D_loss: -0.8977, G_loss: -5.8862\n",
      "  Batch [370/1299] D_loss: -2.3446, G_loss: -4.0681\n",
      "  Batch [380/1299] D_loss: -2.2225, G_loss: -1.4963\n",
      "  Batch [390/1299] D_loss: -2.2678, G_loss: -1.7512\n",
      "  Batch [400/1299] D_loss: -3.4701, G_loss: -0.5053\n",
      "  Batch [410/1299] D_loss: -2.4283, G_loss: 1.5140\n",
      "  Batch [420/1299] D_loss: -1.1283, G_loss: 3.8939\n",
      "  Batch [430/1299] D_loss: -2.6082, G_loss: 5.2847\n",
      "  Batch [440/1299] D_loss: -1.0667, G_loss: 7.7351\n",
      "  Batch [450/1299] D_loss: -1.6547, G_loss: 8.3529\n",
      "  Batch [460/1299] D_loss: -2.0961, G_loss: 8.5280\n",
      "  Batch [470/1299] D_loss: -3.5092, G_loss: 5.1043\n",
      "  Batch [480/1299] D_loss: -1.7133, G_loss: 2.0035\n",
      "  Batch [490/1299] D_loss: -1.7631, G_loss: 1.0612\n",
      "  Batch [500/1299] D_loss: -1.7255, G_loss: 0.3103\n",
      "  Batch [510/1299] D_loss: -2.8352, G_loss: 0.6774\n",
      "  Batch [520/1299] D_loss: -1.4723, G_loss: 0.2021\n",
      "  Batch [530/1299] D_loss: -2.5269, G_loss: -0.5512\n",
      "  Batch [540/1299] D_loss: -1.3836, G_loss: 1.2925\n",
      "  Batch [550/1299] D_loss: -1.6121, G_loss: 1.4397\n",
      "  Batch [560/1299] D_loss: -2.5107, G_loss: 3.1073\n",
      "  Batch [570/1299] D_loss: -1.4665, G_loss: 4.3721\n",
      "  Batch [580/1299] D_loss: -2.9689, G_loss: 5.1676\n",
      "  Batch [590/1299] D_loss: -1.5956, G_loss: 6.3250\n",
      "  Batch [600/1299] D_loss: -2.8177, G_loss: 7.0617\n",
      "  Batch [610/1299] D_loss: -3.3985, G_loss: 5.7181\n",
      "  Batch [620/1299] D_loss: -3.2172, G_loss: 4.6757\n",
      "  Batch [630/1299] D_loss: -2.9849, G_loss: 6.7107\n",
      "  Batch [640/1299] D_loss: -1.4640, G_loss: 2.8436\n",
      "  Batch [650/1299] D_loss: -0.4714, G_loss: 3.2602\n",
      "  Batch [660/1299] D_loss: -1.8283, G_loss: 2.7868\n",
      "  Batch [670/1299] D_loss: -1.5557, G_loss: 6.0702\n",
      "  Batch [680/1299] D_loss: -2.7343, G_loss: 3.3405\n",
      "  Batch [690/1299] D_loss: -1.9928, G_loss: 2.0074\n",
      "  Batch [700/1299] D_loss: -2.0764, G_loss: -2.1650\n",
      "  Batch [710/1299] D_loss: -3.5840, G_loss: -1.4113\n",
      "  Batch [720/1299] D_loss: -3.3538, G_loss: -3.8254\n",
      "  Batch [730/1299] D_loss: -2.3911, G_loss: -4.7118\n",
      "  Batch [740/1299] D_loss: -0.7718, G_loss: -4.2461\n",
      "  Batch [750/1299] D_loss: -2.1249, G_loss: -5.7678\n",
      "  Batch [760/1299] D_loss: -0.8666, G_loss: -4.6404\n",
      "  Batch [770/1299] D_loss: -3.2938, G_loss: -3.1375\n",
      "  Batch [780/1299] D_loss: -2.0688, G_loss: -2.0159\n",
      "  Batch [790/1299] D_loss: -2.1850, G_loss: -0.3326\n",
      "  Batch [800/1299] D_loss: -3.3582, G_loss: 1.3580\n",
      "  Batch [810/1299] D_loss: -2.0510, G_loss: 2.4465\n",
      "  Batch [820/1299] D_loss: -3.2144, G_loss: 2.3202\n",
      "  Batch [830/1299] D_loss: -2.0263, G_loss: 3.8863\n",
      "  Batch [840/1299] D_loss: -2.2164, G_loss: 3.1924\n",
      "  Batch [850/1299] D_loss: -3.8244, G_loss: 2.4146\n",
      "  Batch [860/1299] D_loss: -2.9640, G_loss: 2.6595\n",
      "  Batch [870/1299] D_loss: -1.9325, G_loss: 1.7304\n",
      "  Batch [880/1299] D_loss: -2.8233, G_loss: 1.9361\n",
      "  Batch [890/1299] D_loss: -3.6219, G_loss: -0.7278\n",
      "  Batch [900/1299] D_loss: -1.6432, G_loss: 0.2798\n",
      "  Batch [910/1299] D_loss: -3.0002, G_loss: 0.0452\n",
      "  Batch [920/1299] D_loss: -0.7797, G_loss: -0.5988\n",
      "  Batch [930/1299] D_loss: -1.7127, G_loss: 1.6756\n",
      "  Batch [940/1299] D_loss: -3.0093, G_loss: 3.4328\n",
      "  Batch [950/1299] D_loss: -2.3432, G_loss: 3.5668\n",
      "  Batch [960/1299] D_loss: -0.4873, G_loss: 7.2114\n",
      "  Batch [970/1299] D_loss: -1.2262, G_loss: 8.7423\n",
      "  Batch [980/1299] D_loss: -2.0633, G_loss: 7.8257\n",
      "  Batch [990/1299] D_loss: -3.2939, G_loss: 6.2275\n",
      "  Batch [1000/1299] D_loss: -4.3596, G_loss: 7.0491\n",
      "  Batch [1010/1299] D_loss: -3.3999, G_loss: 3.1175\n",
      "  Batch [1020/1299] D_loss: -2.9825, G_loss: 0.3503\n",
      "  Batch [1030/1299] D_loss: -1.6473, G_loss: -1.1263\n",
      "  Batch [1040/1299] D_loss: -1.4922, G_loss: -1.3583\n",
      "  Batch [1050/1299] D_loss: -2.6349, G_loss: -1.3600\n",
      "  Batch [1060/1299] D_loss: -1.5608, G_loss: -2.5663\n",
      "  Batch [1070/1299] D_loss: -1.4146, G_loss: -1.0575\n",
      "  Batch [1080/1299] D_loss: -1.6683, G_loss: -0.0647\n",
      "  Batch [1090/1299] D_loss: -1.9024, G_loss: -0.3894\n",
      "  Batch [1100/1299] D_loss: -0.6794, G_loss: 0.4171\n",
      "  Batch [1110/1299] D_loss: -2.4755, G_loss: 1.0877\n",
      "  Batch [1120/1299] D_loss: -0.7956, G_loss: 2.9927\n",
      "  Batch [1130/1299] D_loss: -1.6391, G_loss: 3.1249\n",
      "  Batch [1140/1299] D_loss: -1.8537, G_loss: 5.0935\n",
      "  Batch [1150/1299] D_loss: -1.7862, G_loss: 3.3757\n",
      "  Batch [1160/1299] D_loss: -1.1754, G_loss: 2.5498\n",
      "  Batch [1170/1299] D_loss: -3.0097, G_loss: 4.6834\n",
      "  Batch [1180/1299] D_loss: -1.7094, G_loss: 3.2199\n",
      "  Batch [1190/1299] D_loss: -1.8900, G_loss: 2.7554\n",
      "  Batch [1200/1299] D_loss: -1.5186, G_loss: 1.0208\n",
      "  Batch [1210/1299] D_loss: -1.5814, G_loss: -2.5448\n",
      "  Batch [1220/1299] D_loss: -1.3537, G_loss: -0.5139\n",
      "  Batch [1230/1299] D_loss: -1.2060, G_loss: -1.8019\n",
      "  Batch [1240/1299] D_loss: -1.9993, G_loss: -3.0379\n",
      "  Batch [1250/1299] D_loss: -3.7158, G_loss: -4.3099\n",
      "  Batch [1260/1299] D_loss: -1.1445, G_loss: -2.0066\n",
      "  Batch [1270/1299] D_loss: -2.0712, G_loss: -1.3453\n",
      "  Batch [1280/1299] D_loss: -2.3555, G_loss: -1.1289\n",
      "  Batch [1290/1299] D_loss: -2.3894, G_loss: -1.1489\n",
      "\n",
      "Epoch 74 Summary:\n",
      "  Average D_loss: -2.0116\n",
      "  Average G_loss: 1.0430\n",
      "\n",
      "Epoch [75/100]\n",
      "  Batch [0/1299] D_loss: -2.7519, G_loss: -1.8371\n",
      "  Batch [10/1299] D_loss: -2.9441, G_loss: -0.4522\n",
      "  Batch [20/1299] D_loss: -3.2486, G_loss: -2.4488\n",
      "  Batch [30/1299] D_loss: -3.6877, G_loss: -1.9964\n",
      "  Batch [40/1299] D_loss: -1.9663, G_loss: -2.1506\n",
      "  Batch [50/1299] D_loss: -3.5575, G_loss: -1.2574\n",
      "  Batch [60/1299] D_loss: -2.6917, G_loss: -2.5129\n",
      "  Batch [70/1299] D_loss: -2.0907, G_loss: -2.4643\n",
      "  Batch [80/1299] D_loss: -1.8270, G_loss: -0.7840\n",
      "  Batch [90/1299] D_loss: -1.5110, G_loss: -3.0035\n",
      "  Batch [100/1299] D_loss: -1.8366, G_loss: -1.5885\n",
      "  Batch [110/1299] D_loss: -3.1485, G_loss: -4.0162\n",
      "  Batch [120/1299] D_loss: -3.0495, G_loss: -4.8193\n",
      "  Batch [130/1299] D_loss: -1.6564, G_loss: -5.2300\n",
      "  Batch [140/1299] D_loss: -0.9683, G_loss: -2.3797\n",
      "  Batch [150/1299] D_loss: -3.8514, G_loss: -2.3002\n",
      "  Batch [160/1299] D_loss: -1.6205, G_loss: 0.2058\n",
      "  Batch [170/1299] D_loss: -4.3676, G_loss: 1.2172\n",
      "  Batch [180/1299] D_loss: -3.7724, G_loss: 3.5809\n",
      "  Batch [190/1299] D_loss: -2.9498, G_loss: 4.4262\n",
      "  Batch [200/1299] D_loss: -1.9378, G_loss: 4.6634\n",
      "  Batch [210/1299] D_loss: -1.4272, G_loss: 3.2530\n",
      "  Batch [220/1299] D_loss: -0.9573, G_loss: 3.7663\n",
      "  Batch [230/1299] D_loss: -0.9420, G_loss: 5.4687\n",
      "  Batch [240/1299] D_loss: -1.5272, G_loss: 6.3498\n",
      "  Batch [250/1299] D_loss: -2.3833, G_loss: 2.2174\n",
      "  Batch [260/1299] D_loss: -2.2282, G_loss: 6.1013\n",
      "  Batch [270/1299] D_loss: -2.9581, G_loss: 2.7824\n",
      "  Batch [280/1299] D_loss: -4.9507, G_loss: 3.5630\n",
      "  Batch [290/1299] D_loss: -2.3151, G_loss: 3.1396\n",
      "  Batch [300/1299] D_loss: -2.5750, G_loss: -2.4279\n",
      "  Batch [310/1299] D_loss: -2.9124, G_loss: -3.5521\n",
      "  Batch [320/1299] D_loss: -1.1874, G_loss: -3.1752\n",
      "  Batch [330/1299] D_loss: -3.6846, G_loss: -5.5410\n",
      "  Batch [340/1299] D_loss: -2.4858, G_loss: -6.7475\n",
      "  Batch [350/1299] D_loss: -1.7558, G_loss: -4.2729\n",
      "  Batch [360/1299] D_loss: -1.0670, G_loss: -6.1139\n",
      "  Batch [370/1299] D_loss: -2.6799, G_loss: -5.4538\n",
      "  Batch [380/1299] D_loss: -2.9137, G_loss: -2.8225\n",
      "  Batch [390/1299] D_loss: -1.1924, G_loss: 0.2478\n",
      "  Batch [400/1299] D_loss: -0.1152, G_loss: 2.4353\n",
      "  Batch [410/1299] D_loss: -3.4701, G_loss: 3.4011\n",
      "  Batch [420/1299] D_loss: -4.0561, G_loss: 4.0525\n",
      "  Batch [430/1299] D_loss: -2.8224, G_loss: 4.6221\n",
      "  Batch [440/1299] D_loss: -3.8443, G_loss: 2.6786\n",
      "  Batch [450/1299] D_loss: -1.6799, G_loss: 1.0218\n",
      "  Batch [460/1299] D_loss: -0.1953, G_loss: 0.0726\n",
      "  Batch [470/1299] D_loss: -3.6598, G_loss: -2.7049\n",
      "  Batch [480/1299] D_loss: -2.0071, G_loss: -1.0691\n",
      "  Batch [490/1299] D_loss: -3.7604, G_loss: -4.1573\n",
      "  Batch [500/1299] D_loss: -2.7157, G_loss: -4.5086\n",
      "  Batch [510/1299] D_loss: -0.9245, G_loss: -5.6097\n",
      "  Batch [520/1299] D_loss: -1.7887, G_loss: -3.3639\n",
      "  Batch [530/1299] D_loss: -3.0931, G_loss: -2.2204\n",
      "  Batch [540/1299] D_loss: -1.8721, G_loss: -4.3304\n",
      "  Batch [550/1299] D_loss: -1.4703, G_loss: -2.0405\n",
      "  Batch [560/1299] D_loss: -1.3044, G_loss: 2.4002\n",
      "  Batch [570/1299] D_loss: -2.3468, G_loss: 3.2266\n",
      "  Batch [580/1299] D_loss: -1.2346, G_loss: 6.0180\n",
      "  Batch [590/1299] D_loss: -2.5118, G_loss: 4.9646\n",
      "  Batch [600/1299] D_loss: -2.8817, G_loss: 5.7355\n",
      "  Batch [610/1299] D_loss: 0.0493, G_loss: 8.6334\n",
      "  Batch [620/1299] D_loss: -1.5834, G_loss: 6.0916\n",
      "  Batch [630/1299] D_loss: -1.6009, G_loss: 3.0745\n",
      "  Batch [640/1299] D_loss: -2.7418, G_loss: 3.3374\n",
      "  Batch [650/1299] D_loss: -0.7398, G_loss: 1.6017\n",
      "  Batch [660/1299] D_loss: -1.3471, G_loss: 0.0614\n",
      "  Batch [670/1299] D_loss: -1.6315, G_loss: -1.1907\n",
      "  Batch [680/1299] D_loss: -3.5133, G_loss: -0.0659\n",
      "  Batch [690/1299] D_loss: -2.1828, G_loss: -1.5263\n",
      "  Batch [700/1299] D_loss: -2.8196, G_loss: -1.5959\n",
      "  Batch [710/1299] D_loss: -2.4006, G_loss: -0.7927\n",
      "  Batch [720/1299] D_loss: -2.7015, G_loss: -1.8815\n",
      "  Batch [730/1299] D_loss: -1.9220, G_loss: -2.5401\n",
      "  Batch [740/1299] D_loss: -1.9683, G_loss: -3.1713\n",
      "  Batch [750/1299] D_loss: -3.3285, G_loss: -4.1441\n",
      "  Batch [760/1299] D_loss: -2.7155, G_loss: -3.8703\n",
      "  Batch [770/1299] D_loss: -2.1330, G_loss: -4.5817\n",
      "  Batch [780/1299] D_loss: -3.0505, G_loss: -0.9821\n",
      "  Batch [790/1299] D_loss: -0.6509, G_loss: -2.5277\n",
      "  Batch [800/1299] D_loss: -3.7228, G_loss: -2.3800\n",
      "  Batch [810/1299] D_loss: -0.6059, G_loss: -2.9127\n",
      "  Batch [820/1299] D_loss: -2.2860, G_loss: -0.7205\n",
      "  Batch [830/1299] D_loss: -2.2021, G_loss: -2.0722\n",
      "  Batch [840/1299] D_loss: -2.9312, G_loss: -2.3667\n",
      "  Batch [850/1299] D_loss: -3.5464, G_loss: -1.2205\n",
      "  Batch [860/1299] D_loss: -0.3682, G_loss: -0.2640\n",
      "  Batch [870/1299] D_loss: -2.2422, G_loss: 1.6238\n",
      "  Batch [880/1299] D_loss: 0.6875, G_loss: 3.1724\n",
      "  Batch [890/1299] D_loss: -0.4153, G_loss: 6.1754\n",
      "  Batch [900/1299] D_loss: -1.5603, G_loss: 6.3503\n",
      "  Batch [910/1299] D_loss: -0.8424, G_loss: 7.1043\n",
      "  Batch [920/1299] D_loss: -3.1499, G_loss: 5.6243\n",
      "  Batch [930/1299] D_loss: -1.5330, G_loss: 3.5157\n",
      "  Batch [940/1299] D_loss: -3.0490, G_loss: 1.0403\n",
      "  Batch [950/1299] D_loss: -1.9302, G_loss: 0.6429\n",
      "  Batch [960/1299] D_loss: -2.6329, G_loss: 0.2204\n",
      "  Batch [970/1299] D_loss: -1.2313, G_loss: -0.9627\n",
      "  Batch [980/1299] D_loss: -1.1539, G_loss: 0.7569\n",
      "  Batch [990/1299] D_loss: -2.0832, G_loss: 0.8694\n",
      "  Batch [1000/1299] D_loss: -1.7877, G_loss: 1.5903\n",
      "  Batch [1010/1299] D_loss: -2.8468, G_loss: 2.3467\n",
      "  Batch [1020/1299] D_loss: -1.3767, G_loss: 7.1426\n",
      "  Batch [1030/1299] D_loss: -1.5949, G_loss: 4.6625\n",
      "  Batch [1040/1299] D_loss: -0.6117, G_loss: 3.4669\n",
      "  Batch [1050/1299] D_loss: -2.6848, G_loss: 0.7988\n",
      "  Batch [1060/1299] D_loss: -2.9757, G_loss: 3.5944\n",
      "  Batch [1070/1299] D_loss: -3.0152, G_loss: 2.3939\n",
      "  Batch [1080/1299] D_loss: -1.6606, G_loss: 1.6516\n",
      "  Batch [1090/1299] D_loss: -3.5140, G_loss: -0.8044\n",
      "  Batch [1100/1299] D_loss: -3.0288, G_loss: -0.1781\n",
      "  Batch [1110/1299] D_loss: -1.8707, G_loss: 0.3396\n",
      "  Batch [1120/1299] D_loss: -0.5544, G_loss: -0.8748\n",
      "  Batch [1130/1299] D_loss: -1.6839, G_loss: 0.9860\n",
      "  Batch [1140/1299] D_loss: -1.7505, G_loss: 2.1166\n",
      "  Batch [1150/1299] D_loss: -1.6912, G_loss: 3.2004\n",
      "  Batch [1160/1299] D_loss: -2.8620, G_loss: 3.0417\n",
      "  Batch [1170/1299] D_loss: -1.2344, G_loss: 2.7657\n",
      "  Batch [1180/1299] D_loss: -3.2899, G_loss: 2.8481\n",
      "  Batch [1190/1299] D_loss: -1.7781, G_loss: 4.3951\n",
      "  Batch [1200/1299] D_loss: -3.1852, G_loss: 6.3650\n",
      "  Batch [1210/1299] D_loss: -1.2605, G_loss: 2.1233\n",
      "  Batch [1220/1299] D_loss: -0.4893, G_loss: 0.8761\n",
      "  Batch [1230/1299] D_loss: -2.3510, G_loss: -2.4901\n",
      "  Batch [1240/1299] D_loss: -1.1221, G_loss: -1.0539\n",
      "  Batch [1250/1299] D_loss: -2.8921, G_loss: -3.3556\n",
      "  Batch [1260/1299] D_loss: -2.2727, G_loss: -4.8896\n",
      "  Batch [1270/1299] D_loss: -1.4632, G_loss: -6.9871\n",
      "  Batch [1280/1299] D_loss: -1.5952, G_loss: -5.1174\n",
      "  Batch [1290/1299] D_loss: -2.0491, G_loss: -6.5527\n",
      "\n",
      "Epoch 75 Summary:\n",
      "  Average D_loss: -2.0006\n",
      "  Average G_loss: 0.2895\n",
      "\n",
      "Epoch [76/100]\n",
      "  Batch [0/1299] D_loss: -1.1172, G_loss: -2.8935\n",
      "  Batch [10/1299] D_loss: -1.9977, G_loss: -2.4206\n",
      "  Batch [20/1299] D_loss: -1.9897, G_loss: -1.9810\n",
      "  Batch [30/1299] D_loss: -2.6558, G_loss: 0.8949\n",
      "  Batch [40/1299] D_loss: -3.0605, G_loss: 2.9037\n",
      "  Batch [50/1299] D_loss: -2.1798, G_loss: 5.6537\n",
      "  Batch [60/1299] D_loss: -2.0088, G_loss: 5.8056\n",
      "  Batch [70/1299] D_loss: -2.3198, G_loss: 6.9654\n",
      "  Batch [80/1299] D_loss: -1.6028, G_loss: 5.0945\n",
      "  Batch [90/1299] D_loss: -1.7671, G_loss: 2.8139\n",
      "  Batch [100/1299] D_loss: -0.6700, G_loss: 1.5070\n",
      "  Batch [110/1299] D_loss: -1.9502, G_loss: 0.6588\n",
      "  Batch [120/1299] D_loss: -1.7467, G_loss: 0.9480\n",
      "  Batch [130/1299] D_loss: -1.6387, G_loss: 2.2886\n",
      "  Batch [140/1299] D_loss: -2.0687, G_loss: 0.8281\n",
      "  Batch [150/1299] D_loss: -1.2190, G_loss: 2.2935\n",
      "  Batch [160/1299] D_loss: -2.3848, G_loss: -0.1989\n",
      "  Batch [170/1299] D_loss: -2.3407, G_loss: -0.2891\n",
      "  Batch [180/1299] D_loss: -3.3617, G_loss: -0.3418\n",
      "  Batch [190/1299] D_loss: -2.9316, G_loss: 0.5389\n",
      "  Batch [200/1299] D_loss: -1.8609, G_loss: 3.0113\n",
      "  Batch [210/1299] D_loss: -1.2549, G_loss: 4.9487\n",
      "  Batch [220/1299] D_loss: -2.5439, G_loss: 5.1075\n",
      "  Batch [230/1299] D_loss: -2.0378, G_loss: 3.5314\n",
      "  Batch [240/1299] D_loss: -1.8492, G_loss: 2.4981\n",
      "  Batch [250/1299] D_loss: -2.1467, G_loss: 4.2515\n",
      "  Batch [260/1299] D_loss: -2.8032, G_loss: 3.4526\n",
      "  Batch [270/1299] D_loss: -1.5826, G_loss: 2.7881\n",
      "  Batch [280/1299] D_loss: -2.0618, G_loss: 2.5647\n",
      "  Batch [290/1299] D_loss: -1.8955, G_loss: 1.4982\n",
      "  Batch [300/1299] D_loss: -2.1810, G_loss: 2.0065\n",
      "  Batch [310/1299] D_loss: -2.6811, G_loss: 1.3750\n",
      "  Batch [320/1299] D_loss: -1.9433, G_loss: 1.6792\n",
      "  Batch [330/1299] D_loss: -1.3469, G_loss: 2.5819\n",
      "  Batch [340/1299] D_loss: -1.8398, G_loss: 1.0920\n",
      "  Batch [350/1299] D_loss: -2.0291, G_loss: 2.7108\n",
      "  Batch [360/1299] D_loss: -1.5244, G_loss: 4.1260\n",
      "  Batch [370/1299] D_loss: -0.2697, G_loss: 5.3795\n",
      "  Batch [380/1299] D_loss: -0.8936, G_loss: 4.7911\n",
      "  Batch [390/1299] D_loss: -2.8457, G_loss: 3.2838\n",
      "  Batch [400/1299] D_loss: -2.9338, G_loss: 4.4766\n",
      "  Batch [410/1299] D_loss: -1.2335, G_loss: 4.3778\n",
      "  Batch [420/1299] D_loss: -1.1261, G_loss: 4.0259\n",
      "  Batch [430/1299] D_loss: -2.4943, G_loss: 5.2589\n",
      "  Batch [440/1299] D_loss: -2.3546, G_loss: 3.3585\n",
      "  Batch [450/1299] D_loss: -2.9658, G_loss: 3.6341\n",
      "  Batch [460/1299] D_loss: -1.5687, G_loss: 2.1600\n",
      "  Batch [470/1299] D_loss: -3.0995, G_loss: 3.1905\n",
      "  Batch [480/1299] D_loss: -1.2696, G_loss: 4.1835\n",
      "  Batch [490/1299] D_loss: -2.9190, G_loss: 5.1476\n",
      "  Batch [500/1299] D_loss: -2.4084, G_loss: 5.0268\n",
      "  Batch [510/1299] D_loss: -2.0061, G_loss: 4.4378\n",
      "  Batch [520/1299] D_loss: -3.4049, G_loss: 4.3343\n",
      "  Batch [530/1299] D_loss: -1.5086, G_loss: 2.4092\n",
      "  Batch [540/1299] D_loss: -2.8578, G_loss: -2.5469\n",
      "  Batch [550/1299] D_loss: -2.0484, G_loss: -4.4941\n",
      "  Batch [560/1299] D_loss: -2.3354, G_loss: -6.4389\n",
      "  Batch [570/1299] D_loss: -2.7862, G_loss: -7.9977\n",
      "  Batch [580/1299] D_loss: -3.0021, G_loss: -10.1299\n",
      "  Batch [590/1299] D_loss: -1.6386, G_loss: -8.3076\n",
      "  Batch [600/1299] D_loss: -3.2600, G_loss: -7.3235\n",
      "  Batch [610/1299] D_loss: 0.1330, G_loss: -3.3849\n",
      "  Batch [620/1299] D_loss: -1.2788, G_loss: -3.3667\n",
      "  Batch [630/1299] D_loss: -2.2883, G_loss: -1.9276\n",
      "  Batch [640/1299] D_loss: -1.0471, G_loss: 1.9693\n",
      "  Batch [650/1299] D_loss: -2.5567, G_loss: 0.6583\n",
      "  Batch [660/1299] D_loss: -3.4894, G_loss: 1.0766\n",
      "  Batch [670/1299] D_loss: -3.2515, G_loss: 1.5717\n",
      "  Batch [680/1299] D_loss: -3.0815, G_loss: 2.2218\n",
      "  Batch [690/1299] D_loss: -3.1509, G_loss: 2.3972\n",
      "  Batch [700/1299] D_loss: -2.6578, G_loss: 2.0187\n",
      "  Batch [710/1299] D_loss: -1.4108, G_loss: 4.1169\n",
      "  Batch [720/1299] D_loss: -0.9980, G_loss: 5.4540\n",
      "  Batch [730/1299] D_loss: -2.0445, G_loss: 5.7943\n",
      "  Batch [740/1299] D_loss: -2.1600, G_loss: 7.4414\n",
      "  Batch [750/1299] D_loss: -2.0835, G_loss: 5.3818\n",
      "  Batch [760/1299] D_loss: -0.8956, G_loss: 3.6072\n",
      "  Batch [770/1299] D_loss: -1.7128, G_loss: 4.5639\n",
      "  Batch [780/1299] D_loss: -2.7883, G_loss: 1.4785\n",
      "  Batch [790/1299] D_loss: -1.4259, G_loss: 2.3593\n",
      "  Batch [800/1299] D_loss: -3.4641, G_loss: 2.9888\n",
      "  Batch [810/1299] D_loss: -1.9930, G_loss: 0.7422\n",
      "  Batch [820/1299] D_loss: -1.1610, G_loss: 2.0333\n",
      "  Batch [830/1299] D_loss: -3.2937, G_loss: 1.3065\n",
      "  Batch [840/1299] D_loss: -2.6850, G_loss: 1.5955\n",
      "  Batch [850/1299] D_loss: -2.4414, G_loss: 1.6767\n",
      "  Batch [860/1299] D_loss: -1.3338, G_loss: 0.1762\n",
      "  Batch [870/1299] D_loss: -2.0030, G_loss: -0.8526\n",
      "  Batch [880/1299] D_loss: -3.9503, G_loss: -2.4198\n",
      "  Batch [890/1299] D_loss: -2.0930, G_loss: -3.0403\n",
      "  Batch [900/1299] D_loss: -2.1420, G_loss: -3.5154\n",
      "  Batch [910/1299] D_loss: -3.5513, G_loss: -3.2108\n",
      "  Batch [920/1299] D_loss: -1.4513, G_loss: -4.3927\n",
      "  Batch [930/1299] D_loss: -3.1313, G_loss: -4.4156\n",
      "  Batch [940/1299] D_loss: -2.3982, G_loss: -5.0335\n",
      "  Batch [950/1299] D_loss: -0.5990, G_loss: -6.5475\n",
      "  Batch [960/1299] D_loss: -1.8416, G_loss: -5.2522\n",
      "  Batch [970/1299] D_loss: -2.3021, G_loss: -3.6983\n",
      "  Batch [980/1299] D_loss: -1.7365, G_loss: -4.0575\n",
      "  Batch [990/1299] D_loss: -0.6720, G_loss: -1.0479\n",
      "  Batch [1000/1299] D_loss: -2.5257, G_loss: -1.5766\n",
      "  Batch [1010/1299] D_loss: -2.1728, G_loss: 2.2993\n",
      "  Batch [1020/1299] D_loss: -2.0809, G_loss: 3.6128\n",
      "  Batch [1030/1299] D_loss: -1.6618, G_loss: 5.5448\n",
      "  Batch [1040/1299] D_loss: -2.5045, G_loss: 3.8640\n",
      "  Batch [1050/1299] D_loss: -3.1556, G_loss: 1.5869\n",
      "  Batch [1060/1299] D_loss: -1.3902, G_loss: 3.1488\n",
      "  Batch [1070/1299] D_loss: -2.1265, G_loss: 4.2864\n",
      "  Batch [1080/1299] D_loss: -1.7788, G_loss: 5.2866\n",
      "  Batch [1090/1299] D_loss: -2.3618, G_loss: 1.5160\n",
      "  Batch [1100/1299] D_loss: -2.7418, G_loss: 0.4476\n",
      "  Batch [1110/1299] D_loss: -2.0515, G_loss: -1.5381\n",
      "  Batch [1120/1299] D_loss: -3.2565, G_loss: -1.3468\n",
      "  Batch [1130/1299] D_loss: -1.6512, G_loss: -2.4158\n",
      "  Batch [1140/1299] D_loss: -3.7804, G_loss: -4.6018\n",
      "  Batch [1150/1299] D_loss: -2.1751, G_loss: -4.3412\n",
      "  Batch [1160/1299] D_loss: -0.7613, G_loss: -4.2203\n",
      "  Batch [1170/1299] D_loss: -1.0159, G_loss: -5.8707\n",
      "  Batch [1180/1299] D_loss: -2.9951, G_loss: -2.0257\n",
      "  Batch [1190/1299] D_loss: -3.1108, G_loss: -2.0954\n",
      "  Batch [1200/1299] D_loss: -1.7881, G_loss: 0.4209\n",
      "  Batch [1210/1299] D_loss: -3.6746, G_loss: 0.6843\n",
      "  Batch [1220/1299] D_loss: -3.3799, G_loss: 3.8295\n",
      "  Batch [1230/1299] D_loss: -1.3685, G_loss: 4.3699\n",
      "  Batch [1240/1299] D_loss: -2.1994, G_loss: 4.1109\n",
      "  Batch [1250/1299] D_loss: -1.1959, G_loss: 6.0318\n",
      "  Batch [1260/1299] D_loss: -1.1395, G_loss: 3.5862\n",
      "  Batch [1270/1299] D_loss: -1.8212, G_loss: 2.9676\n",
      "  Batch [1280/1299] D_loss: -0.6525, G_loss: 2.6101\n",
      "  Batch [1290/1299] D_loss: -1.7901, G_loss: 2.2886\n",
      "\n",
      "Epoch 76 Summary:\n",
      "  Average D_loss: -2.0050\n",
      "  Average G_loss: 1.0690\n",
      "\n",
      "Epoch [77/100]\n",
      "  Batch [0/1299] D_loss: -3.8075, G_loss: 2.2112\n",
      "  Batch [10/1299] D_loss: -1.6943, G_loss: 2.0053\n",
      "  Batch [20/1299] D_loss: -2.1820, G_loss: 3.8798\n",
      "  Batch [30/1299] D_loss: -1.5799, G_loss: 3.4170\n",
      "  Batch [40/1299] D_loss: -1.1741, G_loss: 2.0974\n",
      "  Batch [50/1299] D_loss: -0.6060, G_loss: 2.5912\n",
      "  Batch [60/1299] D_loss: -2.7862, G_loss: 2.8389\n",
      "  Batch [70/1299] D_loss: -4.2523, G_loss: 0.4916\n",
      "  Batch [80/1299] D_loss: -3.1884, G_loss: 1.5367\n",
      "  Batch [90/1299] D_loss: -1.7554, G_loss: 1.2166\n",
      "  Batch [100/1299] D_loss: -1.3556, G_loss: 1.7316\n",
      "  Batch [110/1299] D_loss: -2.3946, G_loss: 0.9695\n",
      "  Batch [120/1299] D_loss: -2.4432, G_loss: -0.6315\n",
      "  Batch [130/1299] D_loss: -1.8205, G_loss: -0.6116\n",
      "  Batch [140/1299] D_loss: -3.9294, G_loss: -0.4122\n",
      "  Batch [150/1299] D_loss: -3.0240, G_loss: -1.8550\n",
      "  Batch [160/1299] D_loss: -2.3978, G_loss: -3.0227\n",
      "  Batch [170/1299] D_loss: -1.4628, G_loss: -3.4470\n",
      "  Batch [180/1299] D_loss: -1.8746, G_loss: -6.1552\n",
      "  Batch [190/1299] D_loss: -2.6481, G_loss: -5.4447\n",
      "  Batch [200/1299] D_loss: -2.3420, G_loss: -6.7769\n",
      "  Batch [210/1299] D_loss: -1.1214, G_loss: -6.6658\n",
      "  Batch [220/1299] D_loss: -2.5266, G_loss: -4.5081\n",
      "  Batch [230/1299] D_loss: -1.0487, G_loss: -2.5287\n",
      "  Batch [240/1299] D_loss: -2.9196, G_loss: 0.0837\n",
      "  Batch [250/1299] D_loss: -4.3088, G_loss: -0.4601\n",
      "  Batch [260/1299] D_loss: -3.3359, G_loss: -1.3523\n",
      "  Batch [270/1299] D_loss: -3.3480, G_loss: 2.5300\n",
      "  Batch [280/1299] D_loss: -2.9561, G_loss: 3.6641\n",
      "  Batch [290/1299] D_loss: -3.3372, G_loss: 2.5369\n",
      "  Batch [300/1299] D_loss: -1.0401, G_loss: 1.8225\n",
      "  Batch [310/1299] D_loss: -1.1355, G_loss: 1.4643\n",
      "  Batch [320/1299] D_loss: -2.5383, G_loss: 2.0229\n",
      "  Batch [330/1299] D_loss: -2.7838, G_loss: 1.7176\n",
      "  Batch [340/1299] D_loss: -2.2011, G_loss: 1.1242\n",
      "  Batch [350/1299] D_loss: -3.0118, G_loss: 1.8097\n",
      "  Batch [360/1299] D_loss: 0.9469, G_loss: 4.5305\n",
      "  Batch [370/1299] D_loss: -1.7576, G_loss: 6.9268\n",
      "  Batch [380/1299] D_loss: 0.5905, G_loss: 8.3427\n",
      "  Batch [390/1299] D_loss: -2.4647, G_loss: 4.6481\n",
      "  Batch [400/1299] D_loss: -1.1130, G_loss: 7.6949\n",
      "  Batch [410/1299] D_loss: -2.7767, G_loss: 5.3269\n",
      "  Batch [420/1299] D_loss: -3.1805, G_loss: 5.7697\n",
      "  Batch [430/1299] D_loss: -3.6198, G_loss: 4.5758\n",
      "  Batch [440/1299] D_loss: -3.4631, G_loss: 5.7367\n",
      "  Batch [450/1299] D_loss: -2.0004, G_loss: 3.1188\n",
      "  Batch [460/1299] D_loss: -0.8717, G_loss: 1.5208\n",
      "  Batch [470/1299] D_loss: -3.8949, G_loss: 3.2069\n",
      "  Batch [480/1299] D_loss: -2.4692, G_loss: 0.9470\n",
      "  Batch [490/1299] D_loss: -2.4535, G_loss: 4.5277\n",
      "  Batch [500/1299] D_loss: -1.8995, G_loss: 3.2413\n",
      "  Batch [510/1299] D_loss: -2.1284, G_loss: 1.4363\n",
      "  Batch [520/1299] D_loss: -2.4404, G_loss: 1.9562\n",
      "  Batch [530/1299] D_loss: -3.3959, G_loss: 2.3424\n",
      "  Batch [540/1299] D_loss: -2.1486, G_loss: 2.2852\n",
      "  Batch [550/1299] D_loss: -1.5486, G_loss: 1.6061\n",
      "  Batch [560/1299] D_loss: -1.5763, G_loss: 1.5430\n",
      "  Batch [570/1299] D_loss: -1.4303, G_loss: -0.0699\n",
      "  Batch [580/1299] D_loss: -2.3410, G_loss: -2.0641\n",
      "  Batch [590/1299] D_loss: -1.3996, G_loss: -1.7209\n",
      "  Batch [600/1299] D_loss: -3.0338, G_loss: -4.3777\n",
      "  Batch [610/1299] D_loss: -0.6333, G_loss: -5.2110\n",
      "  Batch [620/1299] D_loss: -0.7994, G_loss: -8.9905\n",
      "  Batch [630/1299] D_loss: -0.8066, G_loss: -8.1811\n",
      "  Batch [640/1299] D_loss: -0.3943, G_loss: -5.1041\n",
      "  Batch [650/1299] D_loss: -3.0975, G_loss: -3.2307\n",
      "  Batch [660/1299] D_loss: -1.7993, G_loss: 2.4552\n",
      "  Batch [670/1299] D_loss: -2.4123, G_loss: 2.1840\n",
      "  Batch [680/1299] D_loss: -1.1561, G_loss: 3.6985\n",
      "  Batch [690/1299] D_loss: -2.6254, G_loss: 3.3607\n",
      "  Batch [700/1299] D_loss: -2.4267, G_loss: 1.5644\n",
      "  Batch [710/1299] D_loss: -2.4690, G_loss: 4.1952\n",
      "  Batch [720/1299] D_loss: -2.1740, G_loss: 5.3164\n",
      "  Batch [730/1299] D_loss: -2.3031, G_loss: 1.9974\n",
      "  Batch [740/1299] D_loss: -2.9299, G_loss: 3.0704\n",
      "  Batch [750/1299] D_loss: -1.7677, G_loss: 4.4793\n",
      "  Batch [760/1299] D_loss: -2.6732, G_loss: 5.4830\n",
      "  Batch [770/1299] D_loss: -3.5731, G_loss: 2.4399\n",
      "  Batch [780/1299] D_loss: -3.0999, G_loss: 2.3139\n",
      "  Batch [790/1299] D_loss: -1.1670, G_loss: 3.0746\n",
      "  Batch [800/1299] D_loss: -0.6129, G_loss: 2.9078\n",
      "  Batch [810/1299] D_loss: -0.3028, G_loss: 0.4533\n",
      "  Batch [820/1299] D_loss: -3.8690, G_loss: 0.6581\n",
      "  Batch [830/1299] D_loss: -2.8010, G_loss: 0.0379\n",
      "  Batch [840/1299] D_loss: -1.6336, G_loss: 0.6078\n",
      "  Batch [850/1299] D_loss: -3.4615, G_loss: 3.4885\n",
      "  Batch [860/1299] D_loss: -3.9007, G_loss: 1.4623\n",
      "  Batch [870/1299] D_loss: -0.1601, G_loss: 2.2879\n",
      "  Batch [880/1299] D_loss: -3.7890, G_loss: 0.5958\n",
      "  Batch [890/1299] D_loss: 0.0207, G_loss: 0.0313\n",
      "  Batch [900/1299] D_loss: -2.1068, G_loss: -1.8272\n",
      "  Batch [910/1299] D_loss: -1.7970, G_loss: -3.1398\n",
      "  Batch [920/1299] D_loss: -2.7265, G_loss: -3.0936\n",
      "  Batch [930/1299] D_loss: -3.2509, G_loss: -4.4155\n",
      "  Batch [940/1299] D_loss: -2.0162, G_loss: -5.6160\n",
      "  Batch [950/1299] D_loss: -1.1942, G_loss: -5.8853\n",
      "  Batch [960/1299] D_loss: -0.9004, G_loss: -8.5530\n",
      "  Batch [970/1299] D_loss: -0.8992, G_loss: -4.7342\n",
      "  Batch [980/1299] D_loss: -2.9449, G_loss: -1.6927\n",
      "  Batch [990/1299] D_loss: -3.7470, G_loss: -0.3616\n",
      "  Batch [1000/1299] D_loss: -2.2198, G_loss: 1.8950\n",
      "  Batch [1010/1299] D_loss: -3.2291, G_loss: 3.9496\n",
      "  Batch [1020/1299] D_loss: 0.4978, G_loss: 4.8189\n",
      "  Batch [1030/1299] D_loss: -2.7233, G_loss: 2.5920\n",
      "  Batch [1040/1299] D_loss: -2.2138, G_loss: 1.5313\n",
      "  Batch [1050/1299] D_loss: 0.1074, G_loss: 1.0849\n",
      "  Batch [1060/1299] D_loss: -3.7485, G_loss: -0.7761\n",
      "  Batch [1070/1299] D_loss: -2.4052, G_loss: -0.7425\n",
      "  Batch [1080/1299] D_loss: -1.0139, G_loss: -3.3982\n",
      "  Batch [1090/1299] D_loss: -1.7256, G_loss: -5.1969\n",
      "  Batch [1100/1299] D_loss: -1.1959, G_loss: -3.2577\n",
      "  Batch [1110/1299] D_loss: -0.5195, G_loss: -4.5988\n",
      "  Batch [1120/1299] D_loss: -2.0037, G_loss: -3.3040\n",
      "  Batch [1130/1299] D_loss: -2.6777, G_loss: -0.8891\n",
      "  Batch [1140/1299] D_loss: -2.3439, G_loss: -0.0337\n",
      "  Batch [1150/1299] D_loss: -1.5493, G_loss: 0.0758\n",
      "  Batch [1160/1299] D_loss: -1.5273, G_loss: 1.1861\n",
      "  Batch [1170/1299] D_loss: -2.1462, G_loss: 1.5404\n",
      "  Batch [1180/1299] D_loss: -0.8903, G_loss: 2.2035\n",
      "  Batch [1190/1299] D_loss: -3.1438, G_loss: 1.6545\n",
      "  Batch [1200/1299] D_loss: -1.3330, G_loss: 1.1351\n",
      "  Batch [1210/1299] D_loss: -2.7381, G_loss: -0.5785\n",
      "  Batch [1220/1299] D_loss: -1.8863, G_loss: 1.4151\n",
      "  Batch [1230/1299] D_loss: -2.7962, G_loss: 0.2356\n",
      "  Batch [1240/1299] D_loss: -1.9047, G_loss: -2.4417\n",
      "  Batch [1250/1299] D_loss: -1.6079, G_loss: -2.8678\n",
      "  Batch [1260/1299] D_loss: -2.4162, G_loss: -2.5469\n",
      "  Batch [1270/1299] D_loss: -2.1050, G_loss: -5.1882\n",
      "  Batch [1280/1299] D_loss: -2.4741, G_loss: -4.9862\n",
      "  Batch [1290/1299] D_loss: -2.0211, G_loss: -6.6070\n",
      "\n",
      "Epoch 77 Summary:\n",
      "  Average D_loss: -1.9909\n",
      "  Average G_loss: 0.1912\n",
      "\n",
      "Epoch [78/100]\n",
      "  Batch [0/1299] D_loss: -2.8929, G_loss: -5.4086\n",
      "  Batch [10/1299] D_loss: -1.0484, G_loss: -3.1540\n",
      "  Batch [20/1299] D_loss: -1.8653, G_loss: -1.6201\n",
      "  Batch [30/1299] D_loss: -1.8600, G_loss: 1.1098\n",
      "  Batch [40/1299] D_loss: -2.2272, G_loss: 3.2764\n",
      "  Batch [50/1299] D_loss: -2.6280, G_loss: 4.6740\n",
      "  Batch [60/1299] D_loss: -0.3833, G_loss: 3.8747\n",
      "  Batch [70/1299] D_loss: -2.3244, G_loss: 3.8917\n",
      "  Batch [80/1299] D_loss: -1.8542, G_loss: 3.5936\n",
      "  Batch [90/1299] D_loss: -2.9589, G_loss: 2.3015\n",
      "  Batch [100/1299] D_loss: -2.5608, G_loss: 1.8731\n",
      "  Batch [110/1299] D_loss: -2.6239, G_loss: 2.5486\n",
      "  Batch [120/1299] D_loss: -1.9213, G_loss: 2.2518\n",
      "  Batch [130/1299] D_loss: -1.7276, G_loss: -0.2694\n",
      "  Batch [140/1299] D_loss: -2.2357, G_loss: 0.7026\n",
      "  Batch [150/1299] D_loss: -1.7984, G_loss: 0.2598\n",
      "  Batch [160/1299] D_loss: -1.6045, G_loss: 1.4729\n",
      "  Batch [170/1299] D_loss: -1.8198, G_loss: 1.6454\n",
      "  Batch [180/1299] D_loss: -1.0568, G_loss: 1.1348\n",
      "  Batch [190/1299] D_loss: -2.6323, G_loss: 0.6497\n",
      "  Batch [200/1299] D_loss: -1.1655, G_loss: 0.0450\n",
      "  Batch [210/1299] D_loss: -1.4823, G_loss: 1.1434\n",
      "  Batch [220/1299] D_loss: -2.0155, G_loss: 2.0669\n",
      "  Batch [230/1299] D_loss: -3.2975, G_loss: 2.3438\n",
      "  Batch [240/1299] D_loss: -0.5440, G_loss: -0.4515\n",
      "  Batch [250/1299] D_loss: -2.2694, G_loss: -0.6268\n",
      "  Batch [260/1299] D_loss: -2.7159, G_loss: 0.1402\n",
      "  Batch [270/1299] D_loss: -2.4300, G_loss: 0.3855\n",
      "  Batch [280/1299] D_loss: -2.9567, G_loss: 0.9629\n",
      "  Batch [290/1299] D_loss: -2.4659, G_loss: 0.2676\n",
      "  Batch [300/1299] D_loss: -3.0828, G_loss: -3.3670\n",
      "  Batch [310/1299] D_loss: -1.8214, G_loss: -3.5697\n",
      "  Batch [320/1299] D_loss: -1.5953, G_loss: -3.5977\n",
      "  Batch [330/1299] D_loss: -2.2199, G_loss: -3.6379\n",
      "  Batch [340/1299] D_loss: -1.8575, G_loss: -3.0404\n",
      "  Batch [350/1299] D_loss: -2.1104, G_loss: -2.3733\n",
      "  Batch [360/1299] D_loss: -2.8299, G_loss: 1.1102\n",
      "  Batch [370/1299] D_loss: -1.6470, G_loss: 4.9367\n",
      "  Batch [380/1299] D_loss: -2.1998, G_loss: 3.9411\n",
      "  Batch [390/1299] D_loss: -2.6309, G_loss: 6.7746\n",
      "  Batch [400/1299] D_loss: -2.7838, G_loss: 4.2004\n",
      "  Batch [410/1299] D_loss: -2.2261, G_loss: 3.0795\n",
      "  Batch [420/1299] D_loss: -2.4155, G_loss: 2.8114\n",
      "  Batch [430/1299] D_loss: -1.4787, G_loss: 3.1555\n",
      "  Batch [440/1299] D_loss: -2.0837, G_loss: 2.7859\n",
      "  Batch [450/1299] D_loss: -2.2570, G_loss: 1.6848\n",
      "  Batch [460/1299] D_loss: -1.0780, G_loss: 3.9765\n",
      "  Batch [470/1299] D_loss: -1.7094, G_loss: 2.9064\n",
      "  Batch [480/1299] D_loss: -2.1368, G_loss: 1.1116\n",
      "  Batch [490/1299] D_loss: -2.6828, G_loss: 1.0580\n",
      "  Batch [500/1299] D_loss: -2.1067, G_loss: 0.6323\n",
      "  Batch [510/1299] D_loss: -2.2950, G_loss: 1.3347\n",
      "  Batch [520/1299] D_loss: -2.7712, G_loss: 1.3341\n",
      "  Batch [530/1299] D_loss: -2.6597, G_loss: 2.4090\n",
      "  Batch [540/1299] D_loss: -0.8788, G_loss: 2.4448\n",
      "  Batch [550/1299] D_loss: -2.0933, G_loss: 6.9330\n",
      "  Batch [560/1299] D_loss: -1.1144, G_loss: 4.5885\n",
      "  Batch [570/1299] D_loss: -3.4470, G_loss: 6.8316\n",
      "  Batch [580/1299] D_loss: -1.3478, G_loss: 4.5465\n",
      "  Batch [590/1299] D_loss: -1.7900, G_loss: 3.0407\n",
      "  Batch [600/1299] D_loss: -1.5918, G_loss: 1.1141\n",
      "  Batch [610/1299] D_loss: -2.6095, G_loss: 0.7777\n",
      "  Batch [620/1299] D_loss: -2.1695, G_loss: 1.4772\n",
      "  Batch [630/1299] D_loss: -1.8473, G_loss: -2.6723\n",
      "  Batch [640/1299] D_loss: -2.1856, G_loss: -0.8690\n",
      "  Batch [650/1299] D_loss: -3.2236, G_loss: -3.0997\n",
      "  Batch [660/1299] D_loss: -2.1439, G_loss: -1.9513\n",
      "  Batch [670/1299] D_loss: -3.2251, G_loss: -5.8699\n",
      "  Batch [680/1299] D_loss: -0.8007, G_loss: -7.8451\n",
      "  Batch [690/1299] D_loss: -0.8926, G_loss: -8.4821\n",
      "  Batch [700/1299] D_loss: -1.7641, G_loss: -7.3749\n",
      "  Batch [710/1299] D_loss: -1.2395, G_loss: -6.8612\n",
      "  Batch [720/1299] D_loss: -1.3506, G_loss: -2.8953\n",
      "  Batch [730/1299] D_loss: -2.1338, G_loss: -3.3327\n",
      "  Batch [740/1299] D_loss: -2.2996, G_loss: -2.7235\n",
      "  Batch [750/1299] D_loss: -1.9214, G_loss: -2.9891\n",
      "  Batch [760/1299] D_loss: -1.0175, G_loss: 0.2931\n",
      "  Batch [770/1299] D_loss: -1.2381, G_loss: -0.3690\n",
      "  Batch [780/1299] D_loss: -1.3466, G_loss: 2.7188\n",
      "  Batch [790/1299] D_loss: -1.3281, G_loss: 6.5051\n",
      "  Batch [800/1299] D_loss: -1.6094, G_loss: 7.0822\n",
      "  Batch [810/1299] D_loss: 0.2508, G_loss: 4.5629\n",
      "  Batch [820/1299] D_loss: -2.0675, G_loss: 4.6661\n",
      "  Batch [830/1299] D_loss: -2.3013, G_loss: 1.7953\n",
      "  Batch [840/1299] D_loss: -3.7636, G_loss: 2.4528\n",
      "  Batch [850/1299] D_loss: -2.6914, G_loss: 1.7950\n",
      "  Batch [860/1299] D_loss: -2.0126, G_loss: 1.8977\n",
      "  Batch [870/1299] D_loss: -2.3803, G_loss: 0.9909\n",
      "  Batch [880/1299] D_loss: -2.0620, G_loss: 2.8893\n",
      "  Batch [890/1299] D_loss: -2.8030, G_loss: 2.6699\n",
      "  Batch [900/1299] D_loss: -1.9620, G_loss: 1.6229\n",
      "  Batch [910/1299] D_loss: -1.0790, G_loss: 3.8461\n",
      "  Batch [920/1299] D_loss: -3.4122, G_loss: 1.5739\n",
      "  Batch [930/1299] D_loss: -2.0636, G_loss: -0.4598\n",
      "  Batch [940/1299] D_loss: -2.2834, G_loss: 0.1241\n",
      "  Batch [950/1299] D_loss: -2.4425, G_loss: -2.0890\n",
      "  Batch [960/1299] D_loss: -3.0135, G_loss: -4.7975\n",
      "  Batch [970/1299] D_loss: -1.3592, G_loss: -3.1166\n",
      "  Batch [980/1299] D_loss: -1.5830, G_loss: -3.7408\n",
      "  Batch [990/1299] D_loss: -0.6731, G_loss: -3.5918\n",
      "  Batch [1000/1299] D_loss: -2.2592, G_loss: -4.0835\n",
      "  Batch [1010/1299] D_loss: -2.0074, G_loss: -4.3346\n",
      "  Batch [1020/1299] D_loss: -1.6214, G_loss: -5.6149\n",
      "  Batch [1030/1299] D_loss: -1.6313, G_loss: -6.0512\n",
      "  Batch [1040/1299] D_loss: -1.5679, G_loss: -5.3636\n",
      "  Batch [1050/1299] D_loss: -3.7165, G_loss: -2.9987\n",
      "  Batch [1060/1299] D_loss: -2.2829, G_loss: -1.8511\n",
      "  Batch [1070/1299] D_loss: -2.2997, G_loss: -1.2364\n",
      "  Batch [1080/1299] D_loss: -2.0564, G_loss: 0.0435\n",
      "  Batch [1090/1299] D_loss: -2.3010, G_loss: 2.3257\n",
      "  Batch [1100/1299] D_loss: -2.3393, G_loss: 4.3737\n",
      "  Batch [1110/1299] D_loss: -3.1667, G_loss: 3.8883\n",
      "  Batch [1120/1299] D_loss: -2.4881, G_loss: 3.5679\n",
      "  Batch [1130/1299] D_loss: -2.5653, G_loss: 0.9081\n",
      "  Batch [1140/1299] D_loss: -2.2652, G_loss: 2.0185\n",
      "  Batch [1150/1299] D_loss: -1.3347, G_loss: 1.1112\n",
      "  Batch [1160/1299] D_loss: -2.2198, G_loss: -0.4513\n",
      "  Batch [1170/1299] D_loss: -1.5825, G_loss: 0.3841\n",
      "  Batch [1180/1299] D_loss: -1.5960, G_loss: -1.6976\n",
      "  Batch [1190/1299] D_loss: -2.4966, G_loss: -1.7084\n",
      "  Batch [1200/1299] D_loss: -2.4755, G_loss: -4.1531\n",
      "  Batch [1210/1299] D_loss: -2.0099, G_loss: -4.3871\n",
      "  Batch [1220/1299] D_loss: -1.2966, G_loss: -4.4653\n",
      "  Batch [1230/1299] D_loss: -2.5280, G_loss: -4.8686\n",
      "  Batch [1240/1299] D_loss: -1.6205, G_loss: -2.6282\n",
      "  Batch [1250/1299] D_loss: -1.8809, G_loss: -2.4619\n",
      "  Batch [1260/1299] D_loss: -1.8813, G_loss: -1.8045\n",
      "  Batch [1270/1299] D_loss: -2.8900, G_loss: -1.8163\n",
      "  Batch [1280/1299] D_loss: -1.9651, G_loss: -1.3197\n",
      "  Batch [1290/1299] D_loss: -1.8777, G_loss: 2.3304\n",
      "\n",
      "Epoch 78 Summary:\n",
      "  Average D_loss: -1.9822\n",
      "  Average G_loss: 0.1879\n",
      "\n",
      "Epoch [79/100]\n",
      "  Batch [0/1299] D_loss: -2.4490, G_loss: 3.7733\n",
      "  Batch [10/1299] D_loss: -1.7506, G_loss: 3.5369\n",
      "  Batch [20/1299] D_loss: -1.4496, G_loss: 2.9542\n",
      "  Batch [30/1299] D_loss: -2.3031, G_loss: 1.8967\n",
      "  Batch [40/1299] D_loss: -2.5050, G_loss: 1.9997\n",
      "  Batch [50/1299] D_loss: -2.5404, G_loss: 1.6995\n",
      "  Batch [60/1299] D_loss: -2.3712, G_loss: -0.4636\n",
      "  Batch [70/1299] D_loss: -1.6980, G_loss: -1.1024\n",
      "  Batch [80/1299] D_loss: -1.9619, G_loss: -1.1968\n",
      "  Batch [90/1299] D_loss: -1.4274, G_loss: -0.7176\n",
      "  Batch [100/1299] D_loss: -3.5088, G_loss: -2.5808\n",
      "  Batch [110/1299] D_loss: -1.4722, G_loss: -2.6004\n",
      "  Batch [120/1299] D_loss: -1.8477, G_loss: -2.8784\n",
      "  Batch [130/1299] D_loss: -1.2457, G_loss: -1.8247\n",
      "  Batch [140/1299] D_loss: -2.4192, G_loss: -2.0982\n",
      "  Batch [150/1299] D_loss: -0.3565, G_loss: -0.5434\n",
      "  Batch [160/1299] D_loss: -2.0909, G_loss: -0.5183\n",
      "  Batch [170/1299] D_loss: -2.0424, G_loss: 1.6426\n",
      "  Batch [180/1299] D_loss: -2.1162, G_loss: 2.0503\n",
      "  Batch [190/1299] D_loss: -1.4276, G_loss: 2.3058\n",
      "  Batch [200/1299] D_loss: -2.7903, G_loss: 3.2673\n",
      "  Batch [210/1299] D_loss: -2.2728, G_loss: 2.8286\n",
      "  Batch [220/1299] D_loss: -1.9083, G_loss: 2.0830\n",
      "  Batch [230/1299] D_loss: -2.8892, G_loss: 1.4077\n",
      "  Batch [240/1299] D_loss: -1.7789, G_loss: -0.7546\n",
      "  Batch [250/1299] D_loss: -1.3090, G_loss: -1.3689\n",
      "  Batch [260/1299] D_loss: -1.6597, G_loss: -2.7514\n",
      "  Batch [270/1299] D_loss: -1.7069, G_loss: -4.5616\n",
      "  Batch [280/1299] D_loss: -1.8264, G_loss: -4.7889\n",
      "  Batch [290/1299] D_loss: -2.9947, G_loss: -4.0366\n",
      "  Batch [300/1299] D_loss: -2.4997, G_loss: -3.8329\n",
      "  Batch [310/1299] D_loss: -2.1136, G_loss: -1.9074\n",
      "  Batch [320/1299] D_loss: -2.0405, G_loss: -1.6670\n",
      "  Batch [330/1299] D_loss: -1.8545, G_loss: -0.9620\n",
      "  Batch [340/1299] D_loss: -2.7453, G_loss: 1.2339\n",
      "  Batch [350/1299] D_loss: -2.1448, G_loss: 1.6411\n",
      "  Batch [360/1299] D_loss: -2.3561, G_loss: 1.9481\n",
      "  Batch [370/1299] D_loss: -2.4629, G_loss: 1.0156\n",
      "  Batch [380/1299] D_loss: -1.3802, G_loss: 2.4985\n",
      "  Batch [390/1299] D_loss: -2.5107, G_loss: 1.8323\n",
      "  Batch [400/1299] D_loss: -2.6345, G_loss: 1.6410\n",
      "  Batch [410/1299] D_loss: -2.3407, G_loss: 1.0212\n",
      "  Batch [420/1299] D_loss: -2.0535, G_loss: 0.8153\n",
      "  Batch [430/1299] D_loss: -1.7710, G_loss: 1.4652\n",
      "  Batch [440/1299] D_loss: -1.8593, G_loss: 0.4285\n",
      "  Batch [450/1299] D_loss: -1.5151, G_loss: 1.2945\n",
      "  Batch [460/1299] D_loss: -1.7992, G_loss: 1.2468\n",
      "  Batch [470/1299] D_loss: -1.9635, G_loss: 0.6453\n",
      "  Batch [480/1299] D_loss: -2.8002, G_loss: -0.3605\n",
      "  Batch [490/1299] D_loss: -2.8323, G_loss: 1.0958\n",
      "  Batch [500/1299] D_loss: -3.2259, G_loss: 1.0057\n",
      "  Batch [510/1299] D_loss: -1.7114, G_loss: -0.5389\n",
      "  Batch [520/1299] D_loss: -1.8164, G_loss: -1.7597\n",
      "  Batch [530/1299] D_loss: -1.8669, G_loss: -2.4516\n",
      "  Batch [540/1299] D_loss: -1.8301, G_loss: 0.3755\n",
      "  Batch [550/1299] D_loss: -2.2416, G_loss: -0.2408\n",
      "  Batch [560/1299] D_loss: -2.1877, G_loss: 0.0327\n",
      "  Batch [570/1299] D_loss: -1.7053, G_loss: 2.5676\n",
      "  Batch [580/1299] D_loss: -2.2967, G_loss: 3.3944\n",
      "  Batch [590/1299] D_loss: -2.4834, G_loss: 4.0286\n",
      "  Batch [600/1299] D_loss: -1.7202, G_loss: 3.9622\n",
      "  Batch [610/1299] D_loss: -2.0746, G_loss: 2.5611\n",
      "  Batch [620/1299] D_loss: -2.7015, G_loss: 1.6788\n",
      "  Batch [630/1299] D_loss: -2.5236, G_loss: -0.1509\n",
      "  Batch [640/1299] D_loss: -3.2701, G_loss: -0.9854\n",
      "  Batch [650/1299] D_loss: -2.0863, G_loss: -3.0712\n",
      "  Batch [660/1299] D_loss: -2.3301, G_loss: -3.3596\n",
      "  Batch [670/1299] D_loss: -2.1246, G_loss: -3.7095\n",
      "  Batch [680/1299] D_loss: -1.7163, G_loss: -2.9177\n",
      "  Batch [690/1299] D_loss: -1.8553, G_loss: -4.2812\n",
      "  Batch [700/1299] D_loss: -1.2202, G_loss: -3.1057\n",
      "  Batch [710/1299] D_loss: -1.6822, G_loss: -2.3640\n",
      "  Batch [720/1299] D_loss: -2.4647, G_loss: -1.3251\n",
      "  Batch [730/1299] D_loss: -2.0846, G_loss: -0.8321\n",
      "  Batch [740/1299] D_loss: -2.2405, G_loss: 0.3336\n",
      "  Batch [750/1299] D_loss: -1.9307, G_loss: -0.9638\n",
      "  Batch [760/1299] D_loss: -1.6249, G_loss: -1.3657\n",
      "  Batch [770/1299] D_loss: -2.2610, G_loss: -1.3083\n",
      "  Batch [780/1299] D_loss: -3.3224, G_loss: 1.4714\n",
      "  Batch [790/1299] D_loss: -2.7554, G_loss: 1.4365\n",
      "  Batch [800/1299] D_loss: -2.6381, G_loss: 1.6404\n",
      "  Batch [810/1299] D_loss: -2.0891, G_loss: 1.0633\n",
      "  Batch [820/1299] D_loss: -0.6454, G_loss: 0.4321\n",
      "  Batch [830/1299] D_loss: -2.5447, G_loss: 1.3928\n",
      "  Batch [840/1299] D_loss: -2.5469, G_loss: 0.1910\n",
      "  Batch [850/1299] D_loss: -1.7943, G_loss: -0.0079\n",
      "  Batch [860/1299] D_loss: -3.3492, G_loss: 0.1073\n",
      "  Batch [870/1299] D_loss: -1.8329, G_loss: 0.3788\n",
      "  Batch [880/1299] D_loss: -2.0367, G_loss: 0.4477\n",
      "  Batch [890/1299] D_loss: -2.2925, G_loss: 0.9614\n",
      "  Batch [900/1299] D_loss: -2.4615, G_loss: 1.9255\n",
      "  Batch [910/1299] D_loss: -2.7924, G_loss: 3.0180\n",
      "  Batch [920/1299] D_loss: -1.7074, G_loss: 1.4151\n",
      "  Batch [930/1299] D_loss: -2.1094, G_loss: 1.5621\n",
      "  Batch [940/1299] D_loss: -2.7655, G_loss: 0.9354\n",
      "  Batch [950/1299] D_loss: -3.1353, G_loss: -0.5559\n",
      "  Batch [960/1299] D_loss: -1.9167, G_loss: -2.5753\n",
      "  Batch [970/1299] D_loss: -1.8248, G_loss: -3.3103\n",
      "  Batch [980/1299] D_loss: -2.1260, G_loss: -3.4173\n",
      "  Batch [990/1299] D_loss: -1.7419, G_loss: -2.9931\n",
      "  Batch [1000/1299] D_loss: -2.2825, G_loss: -3.1849\n",
      "  Batch [1010/1299] D_loss: -1.7711, G_loss: -1.6431\n",
      "  Batch [1020/1299] D_loss: -2.1582, G_loss: -0.1366\n",
      "  Batch [1030/1299] D_loss: -3.0329, G_loss: 1.7057\n",
      "  Batch [1040/1299] D_loss: -2.3506, G_loss: 2.2422\n",
      "  Batch [1050/1299] D_loss: -2.2450, G_loss: 3.6251\n",
      "  Batch [1060/1299] D_loss: -1.2554, G_loss: 3.1269\n",
      "  Batch [1070/1299] D_loss: -2.1465, G_loss: 3.6663\n",
      "  Batch [1080/1299] D_loss: -2.8942, G_loss: 1.5888\n",
      "  Batch [1090/1299] D_loss: -2.0671, G_loss: 1.8499\n",
      "  Batch [1100/1299] D_loss: -2.0507, G_loss: 1.3660\n",
      "  Batch [1110/1299] D_loss: -2.9269, G_loss: 1.0485\n",
      "  Batch [1120/1299] D_loss: -1.5724, G_loss: 1.7466\n",
      "  Batch [1130/1299] D_loss: -1.9867, G_loss: 0.3688\n",
      "  Batch [1140/1299] D_loss: -2.2742, G_loss: 0.5386\n",
      "  Batch [1150/1299] D_loss: -2.1055, G_loss: 0.3477\n",
      "  Batch [1160/1299] D_loss: -2.0254, G_loss: 0.5977\n",
      "  Batch [1170/1299] D_loss: -2.3962, G_loss: 1.3280\n",
      "  Batch [1180/1299] D_loss: -2.3070, G_loss: 2.7083\n",
      "  Batch [1190/1299] D_loss: -2.2544, G_loss: 2.6395\n",
      "  Batch [1200/1299] D_loss: -1.7209, G_loss: 2.6664\n",
      "  Batch [1210/1299] D_loss: -1.3958, G_loss: 2.5378\n",
      "  Batch [1220/1299] D_loss: -2.2811, G_loss: 1.8524\n",
      "  Batch [1230/1299] D_loss: -1.9078, G_loss: 0.7561\n",
      "  Batch [1240/1299] D_loss: -1.9423, G_loss: 0.0811\n",
      "  Batch [1250/1299] D_loss: -1.8327, G_loss: 0.0436\n",
      "  Batch [1260/1299] D_loss: -2.1814, G_loss: -0.2596\n",
      "  Batch [1270/1299] D_loss: -2.2462, G_loss: -0.0625\n",
      "  Batch [1280/1299] D_loss: -2.2854, G_loss: -1.2408\n",
      "  Batch [1290/1299] D_loss: -2.1754, G_loss: -0.5064\n",
      "\n",
      "Epoch 79 Summary:\n",
      "  Average D_loss: -1.9581\n",
      "  Average G_loss: 0.1835\n",
      "\n",
      "Epoch [80/100]\n",
      "  Batch [0/1299] D_loss: -1.8623, G_loss: -2.2203\n",
      "  Batch [10/1299] D_loss: -2.4514, G_loss: -1.2074\n",
      "  Batch [20/1299] D_loss: -3.1547, G_loss: 0.4700\n",
      "  Batch [30/1299] D_loss: -1.7052, G_loss: 0.8420\n",
      "  Batch [40/1299] D_loss: -2.5938, G_loss: 3.5006\n",
      "  Batch [50/1299] D_loss: -2.1767, G_loss: 5.3052\n",
      "  Batch [60/1299] D_loss: -2.1094, G_loss: 4.2838\n",
      "  Batch [70/1299] D_loss: -1.7256, G_loss: 3.6928\n",
      "  Batch [80/1299] D_loss: -2.4112, G_loss: 4.0467\n",
      "  Batch [90/1299] D_loss: -2.7181, G_loss: 2.2517\n",
      "  Batch [100/1299] D_loss: -2.2307, G_loss: -0.5137\n",
      "  Batch [110/1299] D_loss: -0.8287, G_loss: -1.6073\n",
      "  Batch [120/1299] D_loss: -1.5875, G_loss: -1.7793\n",
      "  Batch [130/1299] D_loss: -1.5809, G_loss: -4.2191\n",
      "  Batch [140/1299] D_loss: -2.6842, G_loss: -3.8230\n",
      "  Batch [150/1299] D_loss: -1.6173, G_loss: -4.3460\n",
      "  Batch [160/1299] D_loss: -1.5506, G_loss: -4.4951\n",
      "  Batch [170/1299] D_loss: -2.2526, G_loss: -2.7698\n",
      "  Batch [180/1299] D_loss: -2.4510, G_loss: -1.6588\n",
      "  Batch [190/1299] D_loss: -3.0495, G_loss: 0.0460\n",
      "  Batch [200/1299] D_loss: -2.6730, G_loss: 1.3751\n",
      "  Batch [210/1299] D_loss: -2.5836, G_loss: 1.1957\n",
      "  Batch [220/1299] D_loss: -1.6435, G_loss: 3.4448\n",
      "  Batch [230/1299] D_loss: -1.8452, G_loss: 2.4740\n",
      "  Batch [240/1299] D_loss: -1.8006, G_loss: 0.2627\n",
      "  Batch [250/1299] D_loss: -2.1285, G_loss: 1.3106\n",
      "  Batch [260/1299] D_loss: -1.5299, G_loss: 2.5894\n",
      "  Batch [270/1299] D_loss: -1.1573, G_loss: 2.7286\n",
      "  Batch [280/1299] D_loss: -2.6757, G_loss: 1.7891\n",
      "  Batch [290/1299] D_loss: -2.3928, G_loss: 0.8326\n",
      "  Batch [300/1299] D_loss: -1.8825, G_loss: 0.7635\n",
      "  Batch [310/1299] D_loss: -2.6266, G_loss: 1.0210\n",
      "  Batch [320/1299] D_loss: -1.1863, G_loss: 0.4889\n",
      "  Batch [330/1299] D_loss: -1.5787, G_loss: -0.9326\n",
      "  Batch [340/1299] D_loss: -1.7414, G_loss: 1.0830\n",
      "  Batch [350/1299] D_loss: -2.5608, G_loss: 2.8548\n",
      "  Batch [360/1299] D_loss: -2.4466, G_loss: 2.6811\n",
      "  Batch [370/1299] D_loss: -1.6605, G_loss: 2.9378\n",
      "  Batch [380/1299] D_loss: -0.9904, G_loss: 0.9227\n",
      "  Batch [390/1299] D_loss: -2.6078, G_loss: 0.8437\n",
      "  Batch [400/1299] D_loss: -2.0696, G_loss: -2.0793\n",
      "  Batch [410/1299] D_loss: -1.3962, G_loss: -2.2447\n",
      "  Batch [420/1299] D_loss: -2.7016, G_loss: -2.6955\n",
      "  Batch [430/1299] D_loss: -2.0458, G_loss: -4.9049\n",
      "  Batch [440/1299] D_loss: -2.5683, G_loss: -3.9539\n",
      "  Batch [450/1299] D_loss: -2.9910, G_loss: -2.5413\n",
      "  Batch [460/1299] D_loss: -2.4352, G_loss: -3.4168\n",
      "  Batch [470/1299] D_loss: -2.0771, G_loss: -2.0493\n",
      "  Batch [480/1299] D_loss: -1.7079, G_loss: -0.7214\n",
      "  Batch [490/1299] D_loss: -2.5223, G_loss: 1.7444\n",
      "  Batch [500/1299] D_loss: -2.6821, G_loss: 2.1526\n",
      "  Batch [510/1299] D_loss: -1.6344, G_loss: 2.9972\n",
      "  Batch [520/1299] D_loss: -2.4906, G_loss: 2.1714\n",
      "  Batch [530/1299] D_loss: -2.0914, G_loss: 4.0074\n",
      "  Batch [540/1299] D_loss: -3.7113, G_loss: 3.6860\n",
      "  Batch [550/1299] D_loss: -2.2441, G_loss: 1.1292\n",
      "  Batch [560/1299] D_loss: -2.3631, G_loss: 1.6748\n",
      "  Batch [570/1299] D_loss: -2.1989, G_loss: -0.0354\n",
      "  Batch [580/1299] D_loss: -2.2793, G_loss: 0.4018\n",
      "  Batch [590/1299] D_loss: -2.9048, G_loss: 1.0128\n",
      "  Batch [600/1299] D_loss: -2.6431, G_loss: 0.7784\n",
      "  Batch [610/1299] D_loss: -1.1700, G_loss: -0.9882\n",
      "  Batch [620/1299] D_loss: -2.5024, G_loss: -1.1933\n",
      "  Batch [630/1299] D_loss: -2.0892, G_loss: -0.8438\n",
      "  Batch [640/1299] D_loss: -1.1004, G_loss: -0.5034\n",
      "  Batch [650/1299] D_loss: -2.1136, G_loss: 1.8678\n",
      "  Batch [660/1299] D_loss: -2.6907, G_loss: 3.1632\n",
      "  Batch [670/1299] D_loss: -1.9497, G_loss: 4.5112\n",
      "  Batch [680/1299] D_loss: -3.0177, G_loss: 6.1347\n",
      "  Batch [690/1299] D_loss: -1.5672, G_loss: 6.7523\n",
      "  Batch [700/1299] D_loss: -2.0400, G_loss: 4.0899\n",
      "  Batch [710/1299] D_loss: -3.4539, G_loss: 2.6799\n",
      "  Batch [720/1299] D_loss: -2.2454, G_loss: 2.0960\n",
      "  Batch [730/1299] D_loss: -1.1275, G_loss: 1.0668\n",
      "  Batch [740/1299] D_loss: -2.3367, G_loss: 2.1049\n",
      "  Batch [750/1299] D_loss: -2.0320, G_loss: -0.6382\n",
      "  Batch [760/1299] D_loss: -2.5085, G_loss: 0.2667\n",
      "  Batch [770/1299] D_loss: -0.5478, G_loss: 1.5596\n",
      "  Batch [780/1299] D_loss: -2.3429, G_loss: 2.1006\n",
      "  Batch [790/1299] D_loss: -2.1885, G_loss: 2.0256\n",
      "  Batch [800/1299] D_loss: -1.0573, G_loss: 2.3252\n",
      "  Batch [810/1299] D_loss: -2.7558, G_loss: 2.3288\n",
      "  Batch [820/1299] D_loss: -3.3564, G_loss: 3.0034\n",
      "  Batch [830/1299] D_loss: -4.6358, G_loss: 4.2341\n",
      "  Batch [840/1299] D_loss: -1.7116, G_loss: 5.1529\n",
      "  Batch [850/1299] D_loss: -1.6294, G_loss: 1.6910\n",
      "  Batch [860/1299] D_loss: -2.9249, G_loss: 2.0660\n",
      "  Batch [870/1299] D_loss: -1.9387, G_loss: 2.8074\n",
      "  Batch [880/1299] D_loss: -1.2143, G_loss: -0.0786\n",
      "  Batch [890/1299] D_loss: -2.2204, G_loss: -0.3642\n",
      "  Batch [900/1299] D_loss: -2.3006, G_loss: -1.6607\n",
      "  Batch [910/1299] D_loss: -2.6093, G_loss: -1.3562\n",
      "  Batch [920/1299] D_loss: -2.2055, G_loss: -2.2059\n",
      "  Batch [930/1299] D_loss: -2.0231, G_loss: -3.4960\n",
      "  Batch [940/1299] D_loss: -1.4830, G_loss: -4.7783\n",
      "  Batch [950/1299] D_loss: -2.3580, G_loss: -2.7015\n",
      "  Batch [960/1299] D_loss: -0.3928, G_loss: -4.8045\n",
      "  Batch [970/1299] D_loss: -3.0787, G_loss: -4.3088\n",
      "  Batch [980/1299] D_loss: -2.9650, G_loss: -0.8343\n",
      "  Batch [990/1299] D_loss: -2.5214, G_loss: 0.5898\n",
      "  Batch [1000/1299] D_loss: -2.9550, G_loss: 3.6356\n",
      "  Batch [1010/1299] D_loss: -2.5666, G_loss: 5.1485\n",
      "  Batch [1020/1299] D_loss: -2.3655, G_loss: 6.1456\n",
      "  Batch [1030/1299] D_loss: -2.6262, G_loss: 5.3316\n",
      "  Batch [1040/1299] D_loss: -1.5468, G_loss: 5.6978\n",
      "  Batch [1050/1299] D_loss: -3.2631, G_loss: 7.3009\n",
      "  Batch [1060/1299] D_loss: -2.1601, G_loss: 6.4421\n",
      "  Batch [1070/1299] D_loss: -1.3552, G_loss: 0.5654\n",
      "  Batch [1080/1299] D_loss: -1.1535, G_loss: -0.2398\n",
      "  Batch [1090/1299] D_loss: -2.6464, G_loss: -0.5175\n",
      "  Batch [1100/1299] D_loss: -1.4867, G_loss: -1.0269\n",
      "  Batch [1110/1299] D_loss: -0.8461, G_loss: -1.4067\n",
      "  Batch [1120/1299] D_loss: -1.0052, G_loss: -0.9125\n",
      "  Batch [1130/1299] D_loss: -1.6596, G_loss: 1.7127\n",
      "  Batch [1140/1299] D_loss: -1.9548, G_loss: 1.5428\n",
      "  Batch [1150/1299] D_loss: -2.1346, G_loss: 2.9230\n",
      "  Batch [1160/1299] D_loss: -0.7991, G_loss: 3.7227\n",
      "  Batch [1170/1299] D_loss: -2.9745, G_loss: 1.4255\n",
      "  Batch [1180/1299] D_loss: -2.5262, G_loss: -1.7305\n",
      "  Batch [1190/1299] D_loss: -2.3735, G_loss: -2.3112\n",
      "  Batch [1200/1299] D_loss: -0.3227, G_loss: -2.7097\n",
      "  Batch [1210/1299] D_loss: -3.6821, G_loss: -3.5961\n",
      "  Batch [1220/1299] D_loss: -2.9782, G_loss: -5.5298\n",
      "  Batch [1230/1299] D_loss: -1.6350, G_loss: -6.6301\n",
      "  Batch [1240/1299] D_loss: -1.1512, G_loss: -7.1310\n",
      "  Batch [1250/1299] D_loss: -2.1559, G_loss: -5.8369\n",
      "  Batch [1260/1299] D_loss: -1.8996, G_loss: -5.9222\n",
      "  Batch [1270/1299] D_loss: -2.7271, G_loss: -5.2075\n",
      "  Batch [1280/1299] D_loss: -3.2668, G_loss: -5.0740\n",
      "  Batch [1290/1299] D_loss: -0.8199, G_loss: -3.6405\n",
      "\n",
      "Epoch 80 Summary:\n",
      "  Average D_loss: -1.9815\n",
      "  Average G_loss: 0.3749\n",
      "\n",
      "Epoch [81/100]\n",
      "  Batch [0/1299] D_loss: -1.7904, G_loss: -0.5418\n",
      "  Batch [10/1299] D_loss: -1.8393, G_loss: 0.8497\n",
      "  Batch [20/1299] D_loss: -3.2095, G_loss: 2.3254\n",
      "  Batch [30/1299] D_loss: -2.5510, G_loss: 2.4514\n",
      "  Batch [40/1299] D_loss: -2.1847, G_loss: 2.2036\n",
      "  Batch [50/1299] D_loss: -1.9075, G_loss: 3.0750\n",
      "  Batch [60/1299] D_loss: -2.4061, G_loss: 4.2353\n",
      "  Batch [70/1299] D_loss: -1.4071, G_loss: 4.2740\n",
      "  Batch [80/1299] D_loss: -1.4476, G_loss: 2.1244\n",
      "  Batch [90/1299] D_loss: -2.5920, G_loss: 4.2995\n",
      "  Batch [100/1299] D_loss: -2.2528, G_loss: 4.2419\n",
      "  Batch [110/1299] D_loss: -0.5039, G_loss: 3.3210\n",
      "  Batch [120/1299] D_loss: -1.1723, G_loss: 2.9574\n",
      "  Batch [130/1299] D_loss: -0.0684, G_loss: 0.2586\n",
      "  Batch [140/1299] D_loss: -2.1353, G_loss: -1.2548\n",
      "  Batch [150/1299] D_loss: -2.0152, G_loss: -1.8519\n",
      "  Batch [160/1299] D_loss: -1.0817, G_loss: -1.7291\n",
      "  Batch [170/1299] D_loss: -2.2040, G_loss: -6.4730\n",
      "  Batch [180/1299] D_loss: -1.7411, G_loss: -6.1824\n",
      "  Batch [190/1299] D_loss: -3.1616, G_loss: -8.6741\n",
      "  Batch [200/1299] D_loss: -2.0468, G_loss: -7.2211\n",
      "  Batch [210/1299] D_loss: -2.8791, G_loss: -6.1384\n",
      "  Batch [220/1299] D_loss: -2.9393, G_loss: -5.8537\n",
      "  Batch [230/1299] D_loss: -1.5747, G_loss: -4.2579\n",
      "  Batch [240/1299] D_loss: -3.2049, G_loss: -1.9704\n",
      "  Batch [250/1299] D_loss: -1.9433, G_loss: 0.9264\n",
      "  Batch [260/1299] D_loss: -2.8455, G_loss: 5.2701\n",
      "  Batch [270/1299] D_loss: -1.4478, G_loss: 3.0875\n",
      "  Batch [280/1299] D_loss: -0.9105, G_loss: 4.8912\n",
      "  Batch [290/1299] D_loss: -2.5943, G_loss: 4.4530\n",
      "  Batch [300/1299] D_loss: -2.7425, G_loss: 2.5729\n",
      "  Batch [310/1299] D_loss: -1.7938, G_loss: -0.0919\n",
      "  Batch [320/1299] D_loss: -1.5995, G_loss: -1.0388\n",
      "  Batch [330/1299] D_loss: -1.0857, G_loss: -0.3104\n",
      "  Batch [340/1299] D_loss: -2.7727, G_loss: -0.9575\n",
      "  Batch [350/1299] D_loss: -2.9776, G_loss: -0.8955\n",
      "  Batch [360/1299] D_loss: -2.2374, G_loss: -1.8351\n",
      "  Batch [370/1299] D_loss: -2.4887, G_loss: -1.5546\n",
      "  Batch [380/1299] D_loss: -2.8808, G_loss: -0.7299\n",
      "  Batch [390/1299] D_loss: -3.2285, G_loss: 2.8314\n",
      "  Batch [400/1299] D_loss: -2.0722, G_loss: 2.5930\n",
      "  Batch [410/1299] D_loss: -1.8057, G_loss: 2.4687\n",
      "  Batch [420/1299] D_loss: -2.2032, G_loss: 4.5110\n",
      "  Batch [430/1299] D_loss: -2.2261, G_loss: 3.6073\n",
      "  Batch [440/1299] D_loss: -3.6233, G_loss: 2.6421\n",
      "  Batch [450/1299] D_loss: -3.0671, G_loss: 4.1330\n",
      "  Batch [460/1299] D_loss: -2.2656, G_loss: 3.3277\n",
      "  Batch [470/1299] D_loss: -0.9341, G_loss: 4.8808\n",
      "  Batch [480/1299] D_loss: -2.2630, G_loss: 4.4456\n",
      "  Batch [490/1299] D_loss: -2.4227, G_loss: 3.0269\n",
      "  Batch [500/1299] D_loss: -1.0393, G_loss: 5.2213\n",
      "  Batch [510/1299] D_loss: -1.6801, G_loss: 3.9203\n",
      "  Batch [520/1299] D_loss: -1.9968, G_loss: 6.4814\n",
      "  Batch [530/1299] D_loss: -1.1882, G_loss: 3.8780\n",
      "  Batch [540/1299] D_loss: -1.9918, G_loss: 4.3376\n",
      "  Batch [550/1299] D_loss: -1.7420, G_loss: 6.3159\n",
      "  Batch [560/1299] D_loss: -1.4671, G_loss: 3.2020\n",
      "  Batch [570/1299] D_loss: -3.0665, G_loss: 3.2082\n",
      "  Batch [580/1299] D_loss: -2.7634, G_loss: -0.7349\n",
      "  Batch [590/1299] D_loss: -3.3934, G_loss: 0.9788\n",
      "  Batch [600/1299] D_loss: -1.5346, G_loss: 0.8600\n",
      "  Batch [610/1299] D_loss: -3.0383, G_loss: -0.8784\n",
      "  Batch [620/1299] D_loss: -2.5696, G_loss: 0.4263\n",
      "  Batch [630/1299] D_loss: -2.6663, G_loss: 0.1511\n",
      "  Batch [640/1299] D_loss: -2.9442, G_loss: -0.8965\n",
      "  Batch [650/1299] D_loss: -0.2813, G_loss: 2.1348\n",
      "  Batch [660/1299] D_loss: -0.5245, G_loss: 3.5502\n",
      "  Batch [670/1299] D_loss: -2.1646, G_loss: 3.7591\n",
      "  Batch [680/1299] D_loss: -2.1678, G_loss: 4.0412\n",
      "  Batch [690/1299] D_loss: -2.4185, G_loss: 3.9909\n",
      "  Batch [700/1299] D_loss: -0.9323, G_loss: 4.4668\n",
      "  Batch [710/1299] D_loss: -2.1020, G_loss: 4.5888\n",
      "  Batch [720/1299] D_loss: -3.1014, G_loss: 2.2479\n",
      "  Batch [730/1299] D_loss: -1.9530, G_loss: 1.4148\n",
      "  Batch [740/1299] D_loss: -3.5910, G_loss: 1.3427\n",
      "  Batch [750/1299] D_loss: -3.4256, G_loss: -1.8086\n",
      "  Batch [760/1299] D_loss: -3.1585, G_loss: -1.1966\n",
      "  Batch [770/1299] D_loss: -2.1440, G_loss: -2.8795\n",
      "  Batch [780/1299] D_loss: -1.2947, G_loss: -5.1588\n",
      "  Batch [790/1299] D_loss: -1.7476, G_loss: -4.4165\n",
      "  Batch [800/1299] D_loss: -2.0853, G_loss: -4.3029\n",
      "  Batch [810/1299] D_loss: -0.7231, G_loss: -4.2443\n",
      "  Batch [820/1299] D_loss: -2.1711, G_loss: -3.5331\n",
      "  Batch [830/1299] D_loss: -1.2849, G_loss: -1.0173\n",
      "  Batch [840/1299] D_loss: -2.4137, G_loss: 0.5249\n",
      "  Batch [850/1299] D_loss: -1.7416, G_loss: 3.6143\n",
      "  Batch [860/1299] D_loss: -1.2028, G_loss: 1.3126\n",
      "  Batch [870/1299] D_loss: -2.2524, G_loss: -0.4668\n",
      "  Batch [880/1299] D_loss: -0.2235, G_loss: 1.7864\n",
      "  Batch [890/1299] D_loss: -0.8420, G_loss: 3.2780\n",
      "  Batch [900/1299] D_loss: -1.3704, G_loss: 0.9253\n",
      "  Batch [910/1299] D_loss: -2.9656, G_loss: 1.4761\n",
      "  Batch [920/1299] D_loss: -2.2365, G_loss: 2.1804\n",
      "  Batch [930/1299] D_loss: -0.7775, G_loss: 2.7319\n",
      "  Batch [940/1299] D_loss: -1.5324, G_loss: 3.1641\n",
      "  Batch [950/1299] D_loss: -0.9413, G_loss: 1.8307\n",
      "  Batch [960/1299] D_loss: -3.1211, G_loss: 3.0772\n",
      "  Batch [970/1299] D_loss: -1.0544, G_loss: 2.8103\n",
      "  Batch [980/1299] D_loss: -1.9302, G_loss: 2.0665\n",
      "  Batch [990/1299] D_loss: -2.6847, G_loss: 2.5538\n",
      "  Batch [1000/1299] D_loss: -2.1618, G_loss: 1.9223\n",
      "  Batch [1010/1299] D_loss: -1.3506, G_loss: 2.4744\n",
      "  Batch [1020/1299] D_loss: -1.4648, G_loss: 2.4094\n",
      "  Batch [1030/1299] D_loss: -2.5559, G_loss: 1.8593\n",
      "  Batch [1040/1299] D_loss: -2.3059, G_loss: 2.4495\n",
      "  Batch [1050/1299] D_loss: -2.7579, G_loss: 0.5286\n",
      "  Batch [1060/1299] D_loss: -1.8293, G_loss: -1.6090\n",
      "  Batch [1070/1299] D_loss: -2.6237, G_loss: -2.6610\n",
      "  Batch [1080/1299] D_loss: -2.3816, G_loss: -4.7045\n",
      "  Batch [1090/1299] D_loss: -2.7327, G_loss: -6.3754\n",
      "  Batch [1100/1299] D_loss: -1.5929, G_loss: -8.8854\n",
      "  Batch [1110/1299] D_loss: -3.1423, G_loss: -4.9463\n",
      "  Batch [1120/1299] D_loss: -2.0120, G_loss: -5.5992\n",
      "  Batch [1130/1299] D_loss: -1.1231, G_loss: -4.3459\n",
      "  Batch [1140/1299] D_loss: -2.2525, G_loss: -5.9091\n",
      "  Batch [1150/1299] D_loss: -2.2974, G_loss: -1.4037\n",
      "  Batch [1160/1299] D_loss: -2.5633, G_loss: 0.0471\n",
      "  Batch [1170/1299] D_loss: -3.0983, G_loss: -0.1747\n",
      "  Batch [1180/1299] D_loss: -2.8906, G_loss: 3.5170\n",
      "  Batch [1190/1299] D_loss: -2.4228, G_loss: 4.6541\n",
      "  Batch [1200/1299] D_loss: -1.9019, G_loss: 5.1646\n",
      "  Batch [1210/1299] D_loss: -2.7333, G_loss: 4.3814\n",
      "  Batch [1220/1299] D_loss: -1.5552, G_loss: 4.2658\n",
      "  Batch [1230/1299] D_loss: -1.4803, G_loss: 3.2279\n",
      "  Batch [1240/1299] D_loss: -2.8531, G_loss: 3.1328\n",
      "  Batch [1250/1299] D_loss: -3.7177, G_loss: 4.6309\n",
      "  Batch [1260/1299] D_loss: -3.5661, G_loss: 3.0227\n",
      "  Batch [1270/1299] D_loss: -1.9454, G_loss: 3.1216\n",
      "  Batch [1280/1299] D_loss: -2.0950, G_loss: 2.3619\n",
      "  Batch [1290/1299] D_loss: -1.1316, G_loss: 3.8158\n",
      "\n",
      "Epoch 81 Summary:\n",
      "  Average D_loss: -1.9900\n",
      "  Average G_loss: 0.9674\n",
      "\n",
      "Models saved at epoch 81:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_153042_dataset+cell_type/generator_20250121_153042_dataset+cell_type_epoch_81.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_153042_dataset+cell_type/discriminator_20250121_153042_dataset+cell_type_epoch_81.pt\n",
      "\n",
      "Epoch [82/100]\n",
      "  Batch [0/1299] D_loss: -2.4432, G_loss: 2.6594\n",
      "  Batch [10/1299] D_loss: -4.0241, G_loss: 2.1863\n",
      "  Batch [20/1299] D_loss: -2.0231, G_loss: 0.7891\n",
      "  Batch [30/1299] D_loss: -2.5597, G_loss: 0.9306\n",
      "  Batch [40/1299] D_loss: -1.7976, G_loss: 3.5083\n",
      "  Batch [50/1299] D_loss: -1.7153, G_loss: 4.5362\n",
      "  Batch [60/1299] D_loss: -1.2637, G_loss: 0.4597\n",
      "  Batch [70/1299] D_loss: -1.8986, G_loss: -1.7669\n",
      "  Batch [80/1299] D_loss: -2.5110, G_loss: -1.0394\n",
      "  Batch [90/1299] D_loss: -2.9500, G_loss: -3.2649\n",
      "  Batch [100/1299] D_loss: -2.4479, G_loss: -5.3651\n",
      "  Batch [110/1299] D_loss: -1.2787, G_loss: -4.7043\n",
      "  Batch [120/1299] D_loss: -0.9610, G_loss: -4.1018\n",
      "  Batch [130/1299] D_loss: -3.4208, G_loss: -3.9888\n",
      "  Batch [140/1299] D_loss: -1.0250, G_loss: -5.3251\n",
      "  Batch [150/1299] D_loss: -1.2867, G_loss: -3.7306\n",
      "  Batch [160/1299] D_loss: -2.4754, G_loss: -3.1251\n",
      "  Batch [170/1299] D_loss: -2.0937, G_loss: -1.6024\n",
      "  Batch [180/1299] D_loss: -2.3236, G_loss: -1.1881\n",
      "  Batch [190/1299] D_loss: -2.1581, G_loss: 2.3801\n",
      "  Batch [200/1299] D_loss: -1.9236, G_loss: 2.8641\n",
      "  Batch [210/1299] D_loss: -2.7335, G_loss: 4.2728\n",
      "  Batch [220/1299] D_loss: -2.3305, G_loss: 3.3325\n",
      "  Batch [230/1299] D_loss: -3.4846, G_loss: 5.0393\n",
      "  Batch [240/1299] D_loss: -1.8982, G_loss: 2.1834\n",
      "  Batch [250/1299] D_loss: -3.4657, G_loss: 2.3294\n",
      "  Batch [260/1299] D_loss: -3.5461, G_loss: 1.1337\n",
      "  Batch [270/1299] D_loss: -1.9710, G_loss: 1.4054\n",
      "  Batch [280/1299] D_loss: -1.9211, G_loss: 1.6538\n",
      "  Batch [290/1299] D_loss: -0.8105, G_loss: 1.4900\n",
      "  Batch [300/1299] D_loss: -1.8873, G_loss: 1.5175\n",
      "  Batch [310/1299] D_loss: -2.0067, G_loss: 0.7783\n",
      "  Batch [320/1299] D_loss: -2.8513, G_loss: 1.0398\n",
      "  Batch [330/1299] D_loss: -1.6804, G_loss: 0.4770\n",
      "  Batch [340/1299] D_loss: -2.2020, G_loss: 0.6308\n",
      "  Batch [350/1299] D_loss: -3.0705, G_loss: 1.4828\n",
      "  Batch [360/1299] D_loss: -2.3599, G_loss: 3.0656\n",
      "  Batch [370/1299] D_loss: -1.1322, G_loss: 0.6653\n",
      "  Batch [380/1299] D_loss: -2.0354, G_loss: 2.8865\n",
      "  Batch [390/1299] D_loss: -0.9876, G_loss: 4.6557\n",
      "  Batch [400/1299] D_loss: -2.1912, G_loss: 4.4120\n",
      "  Batch [410/1299] D_loss: -3.7115, G_loss: 2.5829\n",
      "  Batch [420/1299] D_loss: -2.4626, G_loss: 1.8257\n",
      "  Batch [430/1299] D_loss: -1.7836, G_loss: 2.8525\n",
      "  Batch [440/1299] D_loss: -3.3933, G_loss: 3.3530\n",
      "  Batch [450/1299] D_loss: -1.7199, G_loss: 0.7091\n",
      "  Batch [460/1299] D_loss: -3.0460, G_loss: 0.8011\n",
      "  Batch [470/1299] D_loss: -1.4275, G_loss: 3.1138\n",
      "  Batch [480/1299] D_loss: -2.5356, G_loss: 1.3750\n",
      "  Batch [490/1299] D_loss: -2.4558, G_loss: -0.5594\n",
      "  Batch [500/1299] D_loss: -2.0674, G_loss: -1.2179\n",
      "  Batch [510/1299] D_loss: -1.5407, G_loss: -1.7633\n",
      "  Batch [520/1299] D_loss: -2.6337, G_loss: -4.7408\n",
      "  Batch [530/1299] D_loss: -1.6408, G_loss: -3.1545\n",
      "  Batch [540/1299] D_loss: -2.3763, G_loss: -8.3521\n",
      "  Batch [550/1299] D_loss: -2.4231, G_loss: -7.5265\n",
      "  Batch [560/1299] D_loss: -0.5872, G_loss: -5.8693\n",
      "  Batch [570/1299] D_loss: -0.8673, G_loss: -4.6364\n",
      "  Batch [580/1299] D_loss: -2.5150, G_loss: -2.6233\n",
      "  Batch [590/1299] D_loss: -4.1417, G_loss: -4.4537\n",
      "  Batch [600/1299] D_loss: -1.3776, G_loss: -1.4082\n",
      "  Batch [610/1299] D_loss: -4.1222, G_loss: -1.1015\n",
      "  Batch [620/1299] D_loss: -2.3401, G_loss: 1.6795\n",
      "  Batch [630/1299] D_loss: -2.1772, G_loss: 1.7048\n",
      "  Batch [640/1299] D_loss: -2.8286, G_loss: 5.7834\n",
      "  Batch [650/1299] D_loss: -1.9932, G_loss: 3.4258\n",
      "  Batch [660/1299] D_loss: -1.9117, G_loss: 3.6172\n",
      "  Batch [670/1299] D_loss: -1.2300, G_loss: 1.7914\n",
      "  Batch [680/1299] D_loss: -1.2232, G_loss: 0.7087\n",
      "  Batch [690/1299] D_loss: -3.1810, G_loss: 0.3242\n",
      "  Batch [700/1299] D_loss: -2.6024, G_loss: 0.8026\n",
      "  Batch [710/1299] D_loss: -2.3942, G_loss: 0.3797\n",
      "  Batch [720/1299] D_loss: -3.6066, G_loss: -0.0536\n",
      "  Batch [730/1299] D_loss: -2.2121, G_loss: -1.2054\n",
      "  Batch [740/1299] D_loss: -1.1513, G_loss: -0.4912\n",
      "  Batch [750/1299] D_loss: -2.1130, G_loss: 0.5878\n",
      "  Batch [760/1299] D_loss: -1.1286, G_loss: -0.7587\n",
      "  Batch [770/1299] D_loss: -2.9207, G_loss: -2.1485\n",
      "  Batch [780/1299] D_loss: -2.9235, G_loss: -1.4285\n",
      "  Batch [790/1299] D_loss: -1.5588, G_loss: 0.6239\n",
      "  Batch [800/1299] D_loss: -2.5815, G_loss: 2.7383\n",
      "  Batch [810/1299] D_loss: -1.8040, G_loss: 4.1601\n",
      "  Batch [820/1299] D_loss: -1.7549, G_loss: 3.5694\n",
      "  Batch [830/1299] D_loss: -1.4657, G_loss: 6.1189\n",
      "  Batch [840/1299] D_loss: -2.3294, G_loss: 6.0548\n",
      "  Batch [850/1299] D_loss: -2.0411, G_loss: 3.9250\n",
      "  Batch [860/1299] D_loss: -1.9164, G_loss: 4.1322\n",
      "  Batch [870/1299] D_loss: -2.7572, G_loss: 5.1737\n",
      "  Batch [880/1299] D_loss: -1.8329, G_loss: 3.3571\n",
      "  Batch [890/1299] D_loss: -0.6351, G_loss: 2.5775\n",
      "  Batch [900/1299] D_loss: -2.2501, G_loss: 1.9203\n",
      "  Batch [910/1299] D_loss: -1.8537, G_loss: 1.3664\n",
      "  Batch [920/1299] D_loss: -3.5585, G_loss: 1.4838\n",
      "  Batch [930/1299] D_loss: -1.5697, G_loss: 2.9511\n",
      "  Batch [940/1299] D_loss: -2.8386, G_loss: 3.6350\n",
      "  Batch [950/1299] D_loss: -2.0802, G_loss: 3.7066\n",
      "  Batch [960/1299] D_loss: -1.7250, G_loss: 2.4010\n",
      "  Batch [970/1299] D_loss: -2.6294, G_loss: 2.6047\n",
      "  Batch [980/1299] D_loss: -2.8084, G_loss: 1.4868\n",
      "  Batch [990/1299] D_loss: -2.2960, G_loss: 1.8215\n",
      "  Batch [1000/1299] D_loss: -1.5746, G_loss: -0.6641\n",
      "  Batch [1010/1299] D_loss: -2.4027, G_loss: 0.3485\n",
      "  Batch [1020/1299] D_loss: -1.7703, G_loss: 0.0298\n",
      "  Batch [1030/1299] D_loss: -1.1679, G_loss: 1.4374\n",
      "  Batch [1040/1299] D_loss: -2.6946, G_loss: 0.7029\n",
      "  Batch [1050/1299] D_loss: -2.3060, G_loss: 1.1097\n",
      "  Batch [1060/1299] D_loss: -2.1658, G_loss: 2.2077\n",
      "  Batch [1070/1299] D_loss: -3.0046, G_loss: 3.0336\n",
      "  Batch [1080/1299] D_loss: -1.7983, G_loss: 4.6182\n",
      "  Batch [1090/1299] D_loss: -1.8907, G_loss: 3.5849\n",
      "  Batch [1100/1299] D_loss: -1.5268, G_loss: 4.1368\n",
      "  Batch [1110/1299] D_loss: -1.3924, G_loss: 1.7628\n",
      "  Batch [1120/1299] D_loss: -2.6347, G_loss: 1.1282\n",
      "  Batch [1130/1299] D_loss: -2.0304, G_loss: 0.9662\n",
      "  Batch [1140/1299] D_loss: -2.0919, G_loss: -2.2323\n",
      "  Batch [1150/1299] D_loss: -0.8049, G_loss: -3.3750\n",
      "  Batch [1160/1299] D_loss: -2.2279, G_loss: -4.7548\n",
      "  Batch [1170/1299] D_loss: -1.5693, G_loss: -4.3805\n",
      "  Batch [1180/1299] D_loss: -2.4392, G_loss: -3.6592\n",
      "  Batch [1190/1299] D_loss: -1.7063, G_loss: -3.8971\n",
      "  Batch [1200/1299] D_loss: -3.4194, G_loss: -4.9514\n",
      "  Batch [1210/1299] D_loss: -2.2861, G_loss: -4.2238\n",
      "  Batch [1220/1299] D_loss: -2.0132, G_loss: -2.4454\n",
      "  Batch [1230/1299] D_loss: -2.4293, G_loss: 1.0259\n",
      "  Batch [1240/1299] D_loss: -2.2756, G_loss: 4.1121\n",
      "  Batch [1250/1299] D_loss: -2.8751, G_loss: 4.4944\n",
      "  Batch [1260/1299] D_loss: -2.5246, G_loss: 6.6611\n",
      "  Batch [1270/1299] D_loss: -3.5904, G_loss: 7.8012\n",
      "  Batch [1280/1299] D_loss: -1.8299, G_loss: 5.0064\n",
      "  Batch [1290/1299] D_loss: -3.1986, G_loss: 3.8154\n",
      "\n",
      "Epoch 82 Summary:\n",
      "  Average D_loss: -1.9632\n",
      "  Average G_loss: 0.7481\n",
      "\n",
      "Epoch [83/100]\n",
      "  Batch [0/1299] D_loss: -1.4543, G_loss: 7.0854\n",
      "  Batch [10/1299] D_loss: -2.3121, G_loss: 4.2343\n",
      "  Batch [20/1299] D_loss: -2.3228, G_loss: 5.0030\n",
      "  Batch [30/1299] D_loss: -2.0481, G_loss: 3.2521\n",
      "  Batch [40/1299] D_loss: -1.6682, G_loss: 3.3826\n",
      "  Batch [50/1299] D_loss: -2.1158, G_loss: 3.0715\n",
      "  Batch [60/1299] D_loss: -2.4272, G_loss: 2.3193\n",
      "  Batch [70/1299] D_loss: -0.9640, G_loss: 2.7672\n",
      "  Batch [80/1299] D_loss: -3.2552, G_loss: 1.8258\n",
      "  Batch [90/1299] D_loss: -1.5978, G_loss: -0.5542\n",
      "  Batch [100/1299] D_loss: -2.2301, G_loss: -0.8126\n",
      "  Batch [110/1299] D_loss: -2.0154, G_loss: -2.1233\n",
      "  Batch [120/1299] D_loss: -2.4773, G_loss: -1.3031\n",
      "  Batch [130/1299] D_loss: -2.3606, G_loss: -0.6344\n",
      "  Batch [140/1299] D_loss: -1.0687, G_loss: 0.3770\n",
      "  Batch [150/1299] D_loss: -2.7920, G_loss: 0.5152\n",
      "  Batch [160/1299] D_loss: -1.8058, G_loss: 2.9010\n",
      "  Batch [170/1299] D_loss: -0.6785, G_loss: 5.4099\n",
      "  Batch [180/1299] D_loss: -2.9935, G_loss: 4.4708\n",
      "  Batch [190/1299] D_loss: -3.1318, G_loss: 2.5218\n",
      "  Batch [200/1299] D_loss: -1.4208, G_loss: 1.7121\n",
      "  Batch [210/1299] D_loss: -1.4976, G_loss: 1.1627\n",
      "  Batch [220/1299] D_loss: -2.5007, G_loss: 0.4812\n",
      "  Batch [230/1299] D_loss: -2.4612, G_loss: -1.6278\n",
      "  Batch [240/1299] D_loss: -2.7817, G_loss: -2.0492\n",
      "  Batch [250/1299] D_loss: -2.8994, G_loss: -1.7041\n",
      "  Batch [260/1299] D_loss: -2.9271, G_loss: -2.8343\n",
      "  Batch [270/1299] D_loss: -1.6633, G_loss: -3.7221\n",
      "  Batch [280/1299] D_loss: -1.2116, G_loss: -4.6688\n",
      "  Batch [290/1299] D_loss: -2.2078, G_loss: -6.5099\n",
      "  Batch [300/1299] D_loss: -2.4982, G_loss: -5.4709\n",
      "  Batch [310/1299] D_loss: -2.8045, G_loss: -5.5811\n",
      "  Batch [320/1299] D_loss: -2.9556, G_loss: -3.7242\n",
      "  Batch [330/1299] D_loss: -1.9472, G_loss: -3.3263\n",
      "  Batch [340/1299] D_loss: -2.2653, G_loss: -1.6013\n",
      "  Batch [350/1299] D_loss: -3.2852, G_loss: 0.5921\n",
      "  Batch [360/1299] D_loss: -2.7868, G_loss: 3.0689\n",
      "  Batch [370/1299] D_loss: -1.4418, G_loss: 3.9530\n",
      "  Batch [380/1299] D_loss: -2.3085, G_loss: 3.5747\n",
      "  Batch [390/1299] D_loss: -3.1610, G_loss: 4.7756\n",
      "  Batch [400/1299] D_loss: -2.1014, G_loss: 2.5457\n",
      "  Batch [410/1299] D_loss: -2.3069, G_loss: 3.1492\n",
      "  Batch [420/1299] D_loss: -1.8597, G_loss: 2.6740\n",
      "  Batch [430/1299] D_loss: -1.7265, G_loss: 0.7020\n",
      "  Batch [440/1299] D_loss: -2.0405, G_loss: 0.5175\n",
      "  Batch [450/1299] D_loss: -3.1546, G_loss: 1.0223\n",
      "  Batch [460/1299] D_loss: -3.0026, G_loss: 0.2297\n",
      "  Batch [470/1299] D_loss: -0.6682, G_loss: -0.5009\n",
      "  Batch [480/1299] D_loss: -2.4825, G_loss: -1.0727\n",
      "  Batch [490/1299] D_loss: -1.9015, G_loss: -1.5227\n",
      "  Batch [500/1299] D_loss: -2.1253, G_loss: -0.8625\n",
      "  Batch [510/1299] D_loss: -2.7787, G_loss: 0.1130\n",
      "  Batch [520/1299] D_loss: -1.8215, G_loss: 1.7608\n",
      "  Batch [530/1299] D_loss: -2.1556, G_loss: 3.7282\n",
      "  Batch [540/1299] D_loss: -1.9932, G_loss: 3.9451\n",
      "  Batch [550/1299] D_loss: -1.9490, G_loss: 3.7086\n",
      "  Batch [560/1299] D_loss: -1.4986, G_loss: 1.9899\n",
      "  Batch [570/1299] D_loss: -1.8890, G_loss: 3.2169\n",
      "  Batch [580/1299] D_loss: -1.8241, G_loss: 2.6665\n",
      "  Batch [590/1299] D_loss: -2.7493, G_loss: 0.5755\n",
      "  Batch [600/1299] D_loss: -0.9470, G_loss: 1.8233\n",
      "  Batch [610/1299] D_loss: -1.8905, G_loss: 1.0108\n",
      "  Batch [620/1299] D_loss: -1.7326, G_loss: 1.6979\n",
      "  Batch [630/1299] D_loss: -2.7787, G_loss: 1.3429\n",
      "  Batch [640/1299] D_loss: -2.1215, G_loss: 1.9417\n",
      "  Batch [650/1299] D_loss: -2.4321, G_loss: 3.0059\n",
      "  Batch [660/1299] D_loss: -2.2001, G_loss: 3.3841\n",
      "  Batch [670/1299] D_loss: -2.4009, G_loss: 4.0226\n",
      "  Batch [680/1299] D_loss: -3.1997, G_loss: 3.6532\n",
      "  Batch [690/1299] D_loss: -2.6640, G_loss: 0.0564\n",
      "  Batch [700/1299] D_loss: -2.0312, G_loss: 0.2184\n",
      "  Batch [710/1299] D_loss: -1.3286, G_loss: -0.0947\n",
      "  Batch [720/1299] D_loss: -2.2321, G_loss: 0.4813\n",
      "  Batch [730/1299] D_loss: -2.0710, G_loss: -1.6942\n",
      "  Batch [740/1299] D_loss: -0.4011, G_loss: -2.6805\n",
      "  Batch [750/1299] D_loss: -1.9508, G_loss: -5.7665\n",
      "  Batch [760/1299] D_loss: -2.6168, G_loss: -5.9148\n",
      "  Batch [770/1299] D_loss: -2.4650, G_loss: -5.2346\n",
      "  Batch [780/1299] D_loss: -1.3923, G_loss: -4.4403\n",
      "  Batch [790/1299] D_loss: -2.2780, G_loss: -3.1216\n",
      "  Batch [800/1299] D_loss: -1.8774, G_loss: -3.2777\n",
      "  Batch [810/1299] D_loss: -2.9883, G_loss: -2.5689\n",
      "  Batch [820/1299] D_loss: -3.1436, G_loss: -1.4391\n",
      "  Batch [830/1299] D_loss: -2.6046, G_loss: -1.3991\n",
      "  Batch [840/1299] D_loss: -2.2899, G_loss: 0.7482\n",
      "  Batch [850/1299] D_loss: -3.5762, G_loss: 3.6151\n",
      "  Batch [860/1299] D_loss: -1.2041, G_loss: 2.9496\n",
      "  Batch [870/1299] D_loss: -1.3930, G_loss: 3.5611\n",
      "  Batch [880/1299] D_loss: -2.6627, G_loss: 4.4686\n",
      "  Batch [890/1299] D_loss: -2.0177, G_loss: 2.8887\n",
      "  Batch [900/1299] D_loss: -1.3538, G_loss: 3.6549\n",
      "  Batch [910/1299] D_loss: -1.8311, G_loss: 1.1630\n",
      "  Batch [920/1299] D_loss: -1.9754, G_loss: 0.2214\n",
      "  Batch [930/1299] D_loss: -3.1763, G_loss: -0.7443\n",
      "  Batch [940/1299] D_loss: -3.0377, G_loss: -0.2982\n",
      "  Batch [950/1299] D_loss: -1.7950, G_loss: 0.1622\n",
      "  Batch [960/1299] D_loss: -1.9674, G_loss: 0.7638\n",
      "  Batch [970/1299] D_loss: -1.2288, G_loss: 1.0645\n",
      "  Batch [980/1299] D_loss: -1.6265, G_loss: 2.5525\n",
      "  Batch [990/1299] D_loss: -1.2492, G_loss: 3.2105\n",
      "  Batch [1000/1299] D_loss: -1.3404, G_loss: 2.1992\n",
      "  Batch [1010/1299] D_loss: -2.2784, G_loss: 3.0650\n",
      "  Batch [1020/1299] D_loss: -1.7355, G_loss: 2.3587\n",
      "  Batch [1030/1299] D_loss: -2.1311, G_loss: -0.0588\n",
      "  Batch [1040/1299] D_loss: -2.1661, G_loss: 1.1800\n",
      "  Batch [1050/1299] D_loss: -1.3442, G_loss: 0.6238\n",
      "  Batch [1060/1299] D_loss: -2.6506, G_loss: -0.4452\n",
      "  Batch [1070/1299] D_loss: -2.0457, G_loss: -0.8438\n",
      "  Batch [1080/1299] D_loss: -2.0435, G_loss: -1.6784\n",
      "  Batch [1090/1299] D_loss: -2.1327, G_loss: -3.5586\n",
      "  Batch [1100/1299] D_loss: -0.9892, G_loss: -5.1316\n",
      "  Batch [1110/1299] D_loss: -2.0487, G_loss: -4.8900\n",
      "  Batch [1120/1299] D_loss: -1.5109, G_loss: -4.4961\n",
      "  Batch [1130/1299] D_loss: -2.4004, G_loss: -3.1924\n",
      "  Batch [1140/1299] D_loss: -2.5277, G_loss: -1.5183\n",
      "  Batch [1150/1299] D_loss: -2.9322, G_loss: 1.0617\n",
      "  Batch [1160/1299] D_loss: -3.1872, G_loss: 1.1576\n",
      "  Batch [1170/1299] D_loss: -2.5816, G_loss: 2.6507\n",
      "  Batch [1180/1299] D_loss: -0.5182, G_loss: 3.2576\n",
      "  Batch [1190/1299] D_loss: -1.7474, G_loss: 3.2847\n",
      "  Batch [1200/1299] D_loss: -3.0856, G_loss: 3.0049\n",
      "  Batch [1210/1299] D_loss: -1.4809, G_loss: 2.6509\n",
      "  Batch [1220/1299] D_loss: -1.6032, G_loss: 3.0278\n",
      "  Batch [1230/1299] D_loss: -1.7983, G_loss: 3.5037\n",
      "  Batch [1240/1299] D_loss: -2.1970, G_loss: 2.6314\n",
      "  Batch [1250/1299] D_loss: -2.6177, G_loss: 2.4504\n",
      "  Batch [1260/1299] D_loss: -0.9458, G_loss: -0.4281\n",
      "  Batch [1270/1299] D_loss: -1.6479, G_loss: -0.8001\n",
      "  Batch [1280/1299] D_loss: -2.4291, G_loss: -0.6322\n",
      "  Batch [1290/1299] D_loss: -2.4465, G_loss: -1.0107\n",
      "\n",
      "Epoch 83 Summary:\n",
      "  Average D_loss: -1.9266\n",
      "  Average G_loss: 0.5779\n",
      "\n",
      "Epoch [84/100]\n",
      "  Batch [0/1299] D_loss: -2.4059, G_loss: -2.0813\n",
      "  Batch [10/1299] D_loss: -1.4088, G_loss: -1.3120\n",
      "  Batch [20/1299] D_loss: -3.0204, G_loss: 0.4236\n",
      "  Batch [30/1299] D_loss: -1.7452, G_loss: 1.0701\n",
      "  Batch [40/1299] D_loss: -1.8079, G_loss: 2.6365\n",
      "  Batch [50/1299] D_loss: -2.3318, G_loss: 3.4535\n",
      "  Batch [60/1299] D_loss: -2.0906, G_loss: 2.1399\n",
      "  Batch [70/1299] D_loss: -2.6617, G_loss: 3.1760\n",
      "  Batch [80/1299] D_loss: -2.4236, G_loss: 3.4745\n",
      "  Batch [90/1299] D_loss: -3.2468, G_loss: 2.8668\n",
      "  Batch [100/1299] D_loss: -1.3522, G_loss: 4.2946\n",
      "  Batch [110/1299] D_loss: -1.5359, G_loss: 5.1592\n",
      "  Batch [120/1299] D_loss: -0.9428, G_loss: 5.2799\n",
      "  Batch [130/1299] D_loss: -1.9610, G_loss: 5.0725\n",
      "  Batch [140/1299] D_loss: -1.8364, G_loss: 2.4977\n",
      "  Batch [150/1299] D_loss: -2.0827, G_loss: 2.8977\n",
      "  Batch [160/1299] D_loss: -2.3218, G_loss: 3.2190\n",
      "  Batch [170/1299] D_loss: -2.4508, G_loss: 1.2648\n",
      "  Batch [180/1299] D_loss: -3.5730, G_loss: 3.0767\n",
      "  Batch [190/1299] D_loss: -1.4714, G_loss: 4.6559\n",
      "  Batch [200/1299] D_loss: -2.6473, G_loss: 5.0988\n",
      "  Batch [210/1299] D_loss: -2.4290, G_loss: 5.3360\n",
      "  Batch [220/1299] D_loss: -2.5577, G_loss: 4.4711\n",
      "  Batch [230/1299] D_loss: -1.0623, G_loss: 3.6357\n",
      "  Batch [240/1299] D_loss: -2.9754, G_loss: 1.8823\n",
      "  Batch [250/1299] D_loss: -2.4181, G_loss: 1.9194\n",
      "  Batch [260/1299] D_loss: -2.6516, G_loss: 0.8986\n",
      "  Batch [270/1299] D_loss: -2.3327, G_loss: 0.8534\n",
      "  Batch [280/1299] D_loss: -1.9814, G_loss: 0.9211\n",
      "  Batch [290/1299] D_loss: -1.5572, G_loss: 1.4916\n",
      "  Batch [300/1299] D_loss: -2.9980, G_loss: 0.8722\n",
      "  Batch [310/1299] D_loss: -2.1397, G_loss: 1.1738\n",
      "  Batch [320/1299] D_loss: -1.9903, G_loss: 2.5701\n",
      "  Batch [330/1299] D_loss: -1.8937, G_loss: 4.1179\n",
      "  Batch [340/1299] D_loss: -1.2752, G_loss: 4.3542\n",
      "  Batch [350/1299] D_loss: -1.7597, G_loss: 2.5131\n",
      "  Batch [360/1299] D_loss: -3.1170, G_loss: 3.7233\n",
      "  Batch [370/1299] D_loss: -0.9909, G_loss: 4.7818\n",
      "  Batch [380/1299] D_loss: -3.1508, G_loss: 2.5531\n",
      "  Batch [390/1299] D_loss: -2.3024, G_loss: 2.6398\n",
      "  Batch [400/1299] D_loss: -2.4955, G_loss: 3.1875\n",
      "  Batch [410/1299] D_loss: -3.1455, G_loss: 1.5501\n",
      "  Batch [420/1299] D_loss: -0.6579, G_loss: 0.4074\n",
      "  Batch [430/1299] D_loss: -1.9283, G_loss: 1.0297\n",
      "  Batch [440/1299] D_loss: -2.0246, G_loss: 3.4702\n",
      "  Batch [450/1299] D_loss: -0.8223, G_loss: 1.9769\n",
      "  Batch [460/1299] D_loss: -0.8975, G_loss: 2.7005\n",
      "  Batch [470/1299] D_loss: -1.8378, G_loss: 2.0817\n",
      "  Batch [480/1299] D_loss: -2.0071, G_loss: 3.6152\n",
      "  Batch [490/1299] D_loss: -0.8858, G_loss: 5.6765\n",
      "  Batch [500/1299] D_loss: -3.2592, G_loss: 4.1350\n",
      "  Batch [510/1299] D_loss: -2.8736, G_loss: 6.1624\n",
      "  Batch [520/1299] D_loss: -1.5150, G_loss: 4.2038\n",
      "  Batch [530/1299] D_loss: -1.9510, G_loss: 2.2902\n",
      "  Batch [540/1299] D_loss: -3.3112, G_loss: 0.8471\n",
      "  Batch [550/1299] D_loss: -1.4290, G_loss: 1.4890\n",
      "  Batch [560/1299] D_loss: -1.8731, G_loss: -0.3140\n",
      "  Batch [570/1299] D_loss: -2.5088, G_loss: 1.0993\n",
      "  Batch [580/1299] D_loss: -2.1931, G_loss: 3.9581\n",
      "  Batch [590/1299] D_loss: -1.8545, G_loss: 1.9425\n",
      "  Batch [600/1299] D_loss: -2.5068, G_loss: 3.7107\n",
      "  Batch [610/1299] D_loss: -2.3499, G_loss: 4.2325\n",
      "  Batch [620/1299] D_loss: -1.5951, G_loss: 1.8328\n",
      "  Batch [630/1299] D_loss: -1.2602, G_loss: 1.9192\n",
      "  Batch [640/1299] D_loss: -1.4714, G_loss: 2.5034\n",
      "  Batch [650/1299] D_loss: -2.7023, G_loss: 0.0709\n",
      "  Batch [660/1299] D_loss: -2.1107, G_loss: 1.4666\n",
      "  Batch [670/1299] D_loss: -2.2274, G_loss: -0.7336\n",
      "  Batch [680/1299] D_loss: -2.3472, G_loss: 0.5450\n",
      "  Batch [690/1299] D_loss: -2.6560, G_loss: 0.4685\n",
      "  Batch [700/1299] D_loss: -1.7166, G_loss: -0.8602\n",
      "  Batch [710/1299] D_loss: -0.4562, G_loss: -2.3914\n",
      "  Batch [720/1299] D_loss: -2.1564, G_loss: 0.6120\n",
      "  Batch [730/1299] D_loss: -2.5813, G_loss: 1.3239\n",
      "  Batch [740/1299] D_loss: -3.1308, G_loss: 2.9958\n",
      "  Batch [750/1299] D_loss: -2.8834, G_loss: 6.2536\n",
      "  Batch [760/1299] D_loss: -1.8335, G_loss: 6.2030\n",
      "  Batch [770/1299] D_loss: -1.9536, G_loss: 8.9522\n",
      "  Batch [780/1299] D_loss: -2.5312, G_loss: 6.4537\n",
      "  Batch [790/1299] D_loss: -1.9552, G_loss: 4.4192\n",
      "  Batch [800/1299] D_loss: -2.3513, G_loss: 3.1715\n",
      "  Batch [810/1299] D_loss: -1.5921, G_loss: 2.0463\n",
      "  Batch [820/1299] D_loss: -3.3964, G_loss: 2.3648\n",
      "  Batch [830/1299] D_loss: -1.0194, G_loss: 1.9704\n",
      "  Batch [840/1299] D_loss: -2.2097, G_loss: 1.7561\n",
      "  Batch [850/1299] D_loss: -1.4879, G_loss: 1.7269\n",
      "  Batch [860/1299] D_loss: -1.5134, G_loss: 2.8991\n",
      "  Batch [870/1299] D_loss: -1.7308, G_loss: 2.9617\n",
      "  Batch [880/1299] D_loss: -2.2123, G_loss: 3.1039\n",
      "  Batch [890/1299] D_loss: -1.5901, G_loss: 2.0232\n",
      "  Batch [900/1299] D_loss: -1.5458, G_loss: 3.9787\n",
      "  Batch [910/1299] D_loss: -2.1137, G_loss: 3.5351\n",
      "  Batch [920/1299] D_loss: -2.5594, G_loss: 2.9983\n",
      "  Batch [930/1299] D_loss: -2.0636, G_loss: 2.6022\n",
      "  Batch [940/1299] D_loss: -1.7618, G_loss: 2.7642\n",
      "  Batch [950/1299] D_loss: -2.6398, G_loss: 3.3353\n",
      "  Batch [960/1299] D_loss: -2.8208, G_loss: 0.8951\n",
      "  Batch [970/1299] D_loss: -2.2481, G_loss: 1.1131\n",
      "  Batch [980/1299] D_loss: -1.2285, G_loss: 1.7129\n",
      "  Batch [990/1299] D_loss: -1.6495, G_loss: 2.2200\n",
      "  Batch [1000/1299] D_loss: -0.9626, G_loss: 1.0963\n",
      "  Batch [1010/1299] D_loss: -2.9263, G_loss: 4.8125\n",
      "  Batch [1020/1299] D_loss: -1.7844, G_loss: 3.9946\n",
      "  Batch [1030/1299] D_loss: -1.8471, G_loss: 3.4671\n",
      "  Batch [1040/1299] D_loss: -0.4289, G_loss: 1.9696\n",
      "  Batch [1050/1299] D_loss: -1.5166, G_loss: 2.0740\n",
      "  Batch [1060/1299] D_loss: -3.2113, G_loss: 1.3211\n",
      "  Batch [1070/1299] D_loss: -1.8036, G_loss: 1.7584\n",
      "  Batch [1080/1299] D_loss: -0.8963, G_loss: 1.4172\n",
      "  Batch [1090/1299] D_loss: -2.6057, G_loss: 1.9613\n",
      "  Batch [1100/1299] D_loss: -2.0804, G_loss: 3.8745\n",
      "  Batch [1110/1299] D_loss: -1.8701, G_loss: 4.3201\n",
      "  Batch [1120/1299] D_loss: -0.8809, G_loss: 4.2455\n",
      "  Batch [1130/1299] D_loss: -1.4626, G_loss: 2.7019\n",
      "  Batch [1140/1299] D_loss: -1.5088, G_loss: 2.1714\n",
      "  Batch [1150/1299] D_loss: -1.5136, G_loss: 1.1120\n",
      "  Batch [1160/1299] D_loss: -1.9877, G_loss: 0.5930\n",
      "  Batch [1170/1299] D_loss: -2.4180, G_loss: 0.2899\n",
      "  Batch [1180/1299] D_loss: -2.9771, G_loss: -0.5559\n",
      "  Batch [1190/1299] D_loss: -2.2738, G_loss: 0.0411\n",
      "  Batch [1200/1299] D_loss: -2.0219, G_loss: 1.4310\n",
      "  Batch [1210/1299] D_loss: -2.4823, G_loss: 3.5561\n",
      "  Batch [1220/1299] D_loss: -2.3375, G_loss: 4.7299\n",
      "  Batch [1230/1299] D_loss: -2.1965, G_loss: 2.1867\n",
      "  Batch [1240/1299] D_loss: -1.0477, G_loss: 2.1288\n",
      "  Batch [1250/1299] D_loss: -1.7636, G_loss: 5.6179\n",
      "  Batch [1260/1299] D_loss: -1.6792, G_loss: 6.0822\n",
      "  Batch [1270/1299] D_loss: -2.5275, G_loss: 4.9085\n",
      "  Batch [1280/1299] D_loss: -2.4342, G_loss: 3.8031\n",
      "  Batch [1290/1299] D_loss: -2.5272, G_loss: 3.4748\n",
      "\n",
      "Epoch 84 Summary:\n",
      "  Average D_loss: -1.9188\n",
      "  Average G_loss: 2.7140\n",
      "\n",
      "Epoch [85/100]\n",
      "  Batch [0/1299] D_loss: -3.1316, G_loss: 3.1855\n",
      "  Batch [10/1299] D_loss: -2.6156, G_loss: 2.0346\n",
      "  Batch [20/1299] D_loss: -1.6861, G_loss: -2.0502\n",
      "  Batch [30/1299] D_loss: -1.5619, G_loss: -1.7338\n",
      "  Batch [40/1299] D_loss: -2.0667, G_loss: -3.1745\n",
      "  Batch [50/1299] D_loss: -1.7869, G_loss: -2.1378\n",
      "  Batch [60/1299] D_loss: -3.0868, G_loss: -3.1099\n",
      "  Batch [70/1299] D_loss: -2.3423, G_loss: -3.6497\n",
      "  Batch [80/1299] D_loss: -2.5410, G_loss: -3.8191\n",
      "  Batch [90/1299] D_loss: -1.4030, G_loss: -3.8325\n",
      "  Batch [100/1299] D_loss: -2.7854, G_loss: -2.7084\n",
      "  Batch [110/1299] D_loss: -1.6816, G_loss: 0.9343\n",
      "  Batch [120/1299] D_loss: -2.4970, G_loss: 4.8790\n",
      "  Batch [130/1299] D_loss: -2.1496, G_loss: 6.3245\n",
      "  Batch [140/1299] D_loss: -2.0070, G_loss: 4.1891\n",
      "  Batch [150/1299] D_loss: -2.0183, G_loss: 1.7932\n",
      "  Batch [160/1299] D_loss: -3.1071, G_loss: 2.0243\n",
      "  Batch [170/1299] D_loss: -1.7286, G_loss: 2.0781\n",
      "  Batch [180/1299] D_loss: -0.4292, G_loss: 2.1368\n",
      "  Batch [190/1299] D_loss: -1.6205, G_loss: 2.6074\n",
      "  Batch [200/1299] D_loss: -1.8669, G_loss: 4.2215\n",
      "  Batch [210/1299] D_loss: -2.1978, G_loss: 5.1812\n",
      "  Batch [220/1299] D_loss: -2.0728, G_loss: 4.9774\n",
      "  Batch [230/1299] D_loss: -2.4205, G_loss: 3.3157\n",
      "  Batch [240/1299] D_loss: -2.6046, G_loss: 1.9421\n",
      "  Batch [250/1299] D_loss: -2.0152, G_loss: 1.9251\n",
      "  Batch [260/1299] D_loss: -2.4609, G_loss: 3.2377\n",
      "  Batch [270/1299] D_loss: -2.5039, G_loss: 2.8921\n",
      "  Batch [280/1299] D_loss: -2.8299, G_loss: 1.1772\n",
      "  Batch [290/1299] D_loss: -1.2907, G_loss: 1.9292\n",
      "  Batch [300/1299] D_loss: -2.1247, G_loss: 3.0210\n",
      "  Batch [310/1299] D_loss: -1.1965, G_loss: 1.6957\n",
      "  Batch [320/1299] D_loss: -0.3662, G_loss: 1.1209\n",
      "  Batch [330/1299] D_loss: -1.2062, G_loss: 1.2186\n",
      "  Batch [340/1299] D_loss: -2.2085, G_loss: 1.9333\n",
      "  Batch [350/1299] D_loss: -2.8292, G_loss: 0.3824\n",
      "  Batch [360/1299] D_loss: -1.9555, G_loss: -1.0780\n",
      "  Batch [370/1299] D_loss: -2.3099, G_loss: 0.4356\n",
      "  Batch [380/1299] D_loss: -1.3884, G_loss: -0.7428\n",
      "  Batch [390/1299] D_loss: -1.8562, G_loss: -2.8511\n",
      "  Batch [400/1299] D_loss: -2.5526, G_loss: -2.8438\n",
      "  Batch [410/1299] D_loss: -0.3132, G_loss: -5.4355\n",
      "  Batch [420/1299] D_loss: -1.6530, G_loss: -6.1702\n",
      "  Batch [430/1299] D_loss: -2.3226, G_loss: -5.4432\n",
      "  Batch [440/1299] D_loss: -2.0469, G_loss: -2.8225\n",
      "  Batch [450/1299] D_loss: -1.4917, G_loss: -2.7548\n",
      "  Batch [460/1299] D_loss: -1.5739, G_loss: -1.9258\n",
      "  Batch [470/1299] D_loss: -1.8206, G_loss: -1.5640\n",
      "  Batch [480/1299] D_loss: -2.1406, G_loss: -0.7450\n",
      "  Batch [490/1299] D_loss: -1.4440, G_loss: -0.3644\n",
      "  Batch [500/1299] D_loss: -1.9980, G_loss: 0.5897\n",
      "  Batch [510/1299] D_loss: -2.5665, G_loss: 1.8153\n",
      "  Batch [520/1299] D_loss: -3.6780, G_loss: 4.4968\n",
      "  Batch [530/1299] D_loss: -2.5022, G_loss: 2.2222\n",
      "  Batch [540/1299] D_loss: -2.6302, G_loss: 2.3339\n",
      "  Batch [550/1299] D_loss: -2.3624, G_loss: 1.6300\n",
      "  Batch [560/1299] D_loss: -2.1268, G_loss: 3.7045\n",
      "  Batch [570/1299] D_loss: -1.9604, G_loss: 2.7201\n",
      "  Batch [580/1299] D_loss: -2.2211, G_loss: 0.4180\n",
      "  Batch [590/1299] D_loss: -1.8990, G_loss: 1.6264\n",
      "  Batch [600/1299] D_loss: -2.1783, G_loss: 0.4196\n",
      "  Batch [610/1299] D_loss: -1.9971, G_loss: 1.8340\n",
      "  Batch [620/1299] D_loss: -1.0926, G_loss: 3.1369\n",
      "  Batch [630/1299] D_loss: -2.4499, G_loss: 2.6455\n",
      "  Batch [640/1299] D_loss: -1.2101, G_loss: 2.0725\n",
      "  Batch [650/1299] D_loss: -1.4441, G_loss: 2.3637\n",
      "  Batch [660/1299] D_loss: -1.2814, G_loss: 2.7559\n",
      "  Batch [670/1299] D_loss: -2.0936, G_loss: 3.1903\n",
      "  Batch [680/1299] D_loss: -1.6241, G_loss: 3.3750\n",
      "  Batch [690/1299] D_loss: -2.3323, G_loss: 4.0347\n",
      "  Batch [700/1299] D_loss: -1.7111, G_loss: -1.2501\n",
      "  Batch [710/1299] D_loss: -2.2606, G_loss: -2.0057\n",
      "  Batch [720/1299] D_loss: -1.3676, G_loss: -1.9710\n",
      "  Batch [730/1299] D_loss: -2.3408, G_loss: -0.8295\n",
      "  Batch [740/1299] D_loss: -1.0855, G_loss: -2.7972\n",
      "  Batch [750/1299] D_loss: -2.9042, G_loss: -3.3534\n",
      "  Batch [760/1299] D_loss: -3.2004, G_loss: -3.2747\n",
      "  Batch [770/1299] D_loss: -1.5651, G_loss: -4.0009\n",
      "  Batch [780/1299] D_loss: -1.9017, G_loss: -1.1855\n",
      "  Batch [790/1299] D_loss: -1.6987, G_loss: 2.5571\n",
      "  Batch [800/1299] D_loss: -2.2295, G_loss: 5.6571\n",
      "  Batch [810/1299] D_loss: -2.7009, G_loss: 6.0155\n",
      "  Batch [820/1299] D_loss: -1.3878, G_loss: 3.9086\n",
      "  Batch [830/1299] D_loss: -2.2247, G_loss: 4.8907\n",
      "  Batch [840/1299] D_loss: -2.4070, G_loss: 5.7344\n",
      "  Batch [850/1299] D_loss: -2.0210, G_loss: 4.6258\n",
      "  Batch [860/1299] D_loss: -2.6385, G_loss: 5.1707\n",
      "  Batch [870/1299] D_loss: -2.5528, G_loss: 4.4453\n",
      "  Batch [880/1299] D_loss: -2.7696, G_loss: 3.1546\n",
      "  Batch [890/1299] D_loss: -2.1555, G_loss: 2.6055\n",
      "  Batch [900/1299] D_loss: -1.5616, G_loss: 2.1064\n",
      "  Batch [910/1299] D_loss: -2.8071, G_loss: 1.4481\n",
      "  Batch [920/1299] D_loss: -1.7480, G_loss: 1.0249\n",
      "  Batch [930/1299] D_loss: -1.9328, G_loss: 1.3711\n",
      "  Batch [940/1299] D_loss: -2.1099, G_loss: 0.1252\n",
      "  Batch [950/1299] D_loss: -3.2147, G_loss: 3.9931\n",
      "  Batch [960/1299] D_loss: -1.5252, G_loss: 4.4072\n",
      "  Batch [970/1299] D_loss: -1.4000, G_loss: 4.4452\n",
      "  Batch [980/1299] D_loss: -1.6524, G_loss: 5.4116\n",
      "  Batch [990/1299] D_loss: -2.1561, G_loss: 4.0169\n",
      "  Batch [1000/1299] D_loss: -2.4204, G_loss: 2.3953\n",
      "  Batch [1010/1299] D_loss: -1.4332, G_loss: 3.1194\n",
      "  Batch [1020/1299] D_loss: -1.6904, G_loss: 3.6548\n",
      "  Batch [1030/1299] D_loss: -2.5461, G_loss: 0.9519\n",
      "  Batch [1040/1299] D_loss: -1.7647, G_loss: -0.0769\n",
      "  Batch [1050/1299] D_loss: -1.7654, G_loss: 0.8973\n",
      "  Batch [1060/1299] D_loss: -3.0433, G_loss: 0.7029\n",
      "  Batch [1070/1299] D_loss: -2.5175, G_loss: 0.4768\n",
      "  Batch [1080/1299] D_loss: -3.3887, G_loss: 1.3726\n",
      "  Batch [1090/1299] D_loss: -0.8872, G_loss: 3.5046\n",
      "  Batch [1100/1299] D_loss: -1.4852, G_loss: 3.3293\n",
      "  Batch [1110/1299] D_loss: -1.5681, G_loss: 3.0034\n",
      "  Batch [1120/1299] D_loss: -1.7311, G_loss: 4.6142\n",
      "  Batch [1130/1299] D_loss: -1.8598, G_loss: 3.3368\n",
      "  Batch [1140/1299] D_loss: -2.1780, G_loss: 3.6547\n",
      "  Batch [1150/1299] D_loss: -2.0878, G_loss: 1.2529\n",
      "  Batch [1160/1299] D_loss: -1.6413, G_loss: 3.8689\n",
      "  Batch [1170/1299] D_loss: -2.3956, G_loss: 2.6344\n",
      "  Batch [1180/1299] D_loss: -1.9555, G_loss: 1.9445\n",
      "  Batch [1190/1299] D_loss: -1.7792, G_loss: 2.9836\n",
      "  Batch [1200/1299] D_loss: -2.7273, G_loss: 0.6462\n",
      "  Batch [1210/1299] D_loss: -1.8865, G_loss: 2.2859\n",
      "  Batch [1220/1299] D_loss: -2.0183, G_loss: 2.7172\n",
      "  Batch [1230/1299] D_loss: -2.6330, G_loss: 0.4699\n",
      "  Batch [1240/1299] D_loss: -1.5427, G_loss: -0.1877\n",
      "  Batch [1250/1299] D_loss: -2.2376, G_loss: -0.4584\n",
      "  Batch [1260/1299] D_loss: -2.1072, G_loss: 0.8237\n",
      "  Batch [1270/1299] D_loss: -2.7116, G_loss: 5.2432\n",
      "  Batch [1280/1299] D_loss: -2.0650, G_loss: 4.8703\n",
      "  Batch [1290/1299] D_loss: -2.4286, G_loss: 2.1729\n",
      "\n",
      "Epoch 85 Summary:\n",
      "  Average D_loss: -1.8793\n",
      "  Average G_loss: 1.4520\n",
      "\n",
      "Epoch [86/100]\n",
      "  Batch [0/1299] D_loss: -2.5422, G_loss: 3.4450\n",
      "  Batch [10/1299] D_loss: -2.1266, G_loss: 2.7086\n",
      "  Batch [20/1299] D_loss: -3.1356, G_loss: 1.9406\n",
      "  Batch [30/1299] D_loss: -1.9391, G_loss: 1.8187\n",
      "  Batch [40/1299] D_loss: -1.9055, G_loss: 2.0177\n",
      "  Batch [50/1299] D_loss: -2.1296, G_loss: 2.7641\n",
      "  Batch [60/1299] D_loss: -2.4374, G_loss: 0.3987\n",
      "  Batch [70/1299] D_loss: -2.7639, G_loss: 1.8102\n",
      "  Batch [80/1299] D_loss: -1.6763, G_loss: 2.6289\n",
      "  Batch [90/1299] D_loss: -2.9494, G_loss: 3.4525\n",
      "  Batch [100/1299] D_loss: -2.4401, G_loss: 5.2740\n",
      "  Batch [110/1299] D_loss: -2.2555, G_loss: 2.7450\n",
      "  Batch [120/1299] D_loss: -2.3653, G_loss: 0.0889\n",
      "  Batch [130/1299] D_loss: -1.0788, G_loss: -0.0909\n",
      "  Batch [140/1299] D_loss: -2.2591, G_loss: 0.1404\n",
      "  Batch [150/1299] D_loss: -1.7529, G_loss: 0.1191\n",
      "  Batch [160/1299] D_loss: -2.6874, G_loss: 0.0187\n",
      "  Batch [170/1299] D_loss: -1.4984, G_loss: 0.7421\n",
      "  Batch [180/1299] D_loss: -2.4969, G_loss: 1.4106\n",
      "  Batch [190/1299] D_loss: -2.8724, G_loss: 5.0552\n",
      "  Batch [200/1299] D_loss: -1.8504, G_loss: 3.5574\n",
      "  Batch [210/1299] D_loss: -1.3306, G_loss: 4.7875\n",
      "  Batch [220/1299] D_loss: -2.4637, G_loss: 6.3188\n",
      "  Batch [230/1299] D_loss: -1.9217, G_loss: 7.6696\n",
      "  Batch [240/1299] D_loss: -1.2582, G_loss: 2.8542\n",
      "  Batch [250/1299] D_loss: -2.4670, G_loss: 1.0294\n",
      "  Batch [260/1299] D_loss: -1.5846, G_loss: 0.4535\n",
      "  Batch [270/1299] D_loss: -2.0121, G_loss: 2.4757\n",
      "  Batch [280/1299] D_loss: -1.4665, G_loss: 0.9212\n",
      "  Batch [290/1299] D_loss: -2.3651, G_loss: 1.5000\n",
      "  Batch [300/1299] D_loss: -2.1336, G_loss: 0.3232\n",
      "  Batch [310/1299] D_loss: -2.6473, G_loss: 0.8712\n",
      "  Batch [320/1299] D_loss: -1.6543, G_loss: 3.1529\n",
      "  Batch [330/1299] D_loss: -1.3810, G_loss: 2.5628\n",
      "  Batch [340/1299] D_loss: -2.1184, G_loss: 4.1233\n",
      "  Batch [350/1299] D_loss: -1.9577, G_loss: 2.9305\n",
      "  Batch [360/1299] D_loss: -2.2734, G_loss: 4.0417\n",
      "  Batch [370/1299] D_loss: -1.8273, G_loss: 2.6410\n",
      "  Batch [380/1299] D_loss: -2.1411, G_loss: 4.1279\n",
      "  Batch [390/1299] D_loss: -2.0136, G_loss: 1.8326\n",
      "  Batch [400/1299] D_loss: -2.8856, G_loss: 1.1358\n",
      "  Batch [410/1299] D_loss: -2.0876, G_loss: 0.1552\n",
      "  Batch [420/1299] D_loss: -2.2415, G_loss: 1.4258\n",
      "  Batch [430/1299] D_loss: -2.3741, G_loss: 1.4923\n",
      "  Batch [440/1299] D_loss: -1.8452, G_loss: 3.5549\n",
      "  Batch [450/1299] D_loss: -1.3381, G_loss: 0.7559\n",
      "  Batch [460/1299] D_loss: -2.4341, G_loss: 2.1361\n",
      "  Batch [470/1299] D_loss: -1.9776, G_loss: 1.1237\n",
      "  Batch [480/1299] D_loss: -2.6199, G_loss: 0.9988\n",
      "  Batch [490/1299] D_loss: -2.5654, G_loss: -0.5264\n",
      "  Batch [500/1299] D_loss: -2.1599, G_loss: -1.3459\n",
      "  Batch [510/1299] D_loss: -2.6503, G_loss: 0.2739\n",
      "  Batch [520/1299] D_loss: -2.6620, G_loss: -2.6861\n",
      "  Batch [530/1299] D_loss: -2.4311, G_loss: 0.2349\n",
      "  Batch [540/1299] D_loss: -1.6486, G_loss: 0.0139\n",
      "  Batch [550/1299] D_loss: -2.4433, G_loss: 1.6472\n",
      "  Batch [560/1299] D_loss: -2.4785, G_loss: 3.3396\n",
      "  Batch [570/1299] D_loss: -1.8353, G_loss: 4.8755\n",
      "  Batch [580/1299] D_loss: -0.4654, G_loss: 4.8173\n",
      "  Batch [590/1299] D_loss: -1.6086, G_loss: 4.7883\n",
      "  Batch [600/1299] D_loss: -1.1659, G_loss: 2.6563\n",
      "  Batch [610/1299] D_loss: -1.4168, G_loss: 2.3149\n",
      "  Batch [620/1299] D_loss: -2.3423, G_loss: 0.6067\n",
      "  Batch [630/1299] D_loss: -1.6441, G_loss: 1.0637\n",
      "  Batch [640/1299] D_loss: -2.3478, G_loss: 2.0185\n",
      "  Batch [650/1299] D_loss: -1.4930, G_loss: 2.7448\n",
      "  Batch [660/1299] D_loss: -1.4724, G_loss: 0.7452\n",
      "  Batch [670/1299] D_loss: -2.2993, G_loss: 1.7504\n",
      "  Batch [680/1299] D_loss: -2.4625, G_loss: 1.9631\n",
      "  Batch [690/1299] D_loss: -1.8580, G_loss: 2.8854\n",
      "  Batch [700/1299] D_loss: -2.3791, G_loss: 2.8163\n",
      "  Batch [710/1299] D_loss: -1.6816, G_loss: 1.9419\n",
      "  Batch [720/1299] D_loss: -1.5278, G_loss: 3.8849\n",
      "  Batch [730/1299] D_loss: -1.7608, G_loss: 5.3082\n",
      "  Batch [740/1299] D_loss: -1.5715, G_loss: 3.8034\n",
      "  Batch [750/1299] D_loss: -2.8345, G_loss: 1.8548\n",
      "  Batch [760/1299] D_loss: -1.8225, G_loss: 1.3506\n",
      "  Batch [770/1299] D_loss: -0.9828, G_loss: 1.9744\n",
      "  Batch [780/1299] D_loss: -1.6300, G_loss: 0.2225\n",
      "  Batch [790/1299] D_loss: -2.0435, G_loss: -1.5935\n",
      "  Batch [800/1299] D_loss: -2.8196, G_loss: -1.0361\n",
      "  Batch [810/1299] D_loss: -1.4517, G_loss: -1.1880\n",
      "  Batch [820/1299] D_loss: -1.4887, G_loss: -2.2612\n",
      "  Batch [830/1299] D_loss: -2.3331, G_loss: -2.6208\n",
      "  Batch [840/1299] D_loss: -2.0428, G_loss: -4.0566\n",
      "  Batch [850/1299] D_loss: -2.1749, G_loss: -4.0770\n",
      "  Batch [860/1299] D_loss: -2.3520, G_loss: -4.4381\n",
      "  Batch [870/1299] D_loss: -2.0731, G_loss: -3.2379\n",
      "  Batch [880/1299] D_loss: -1.9170, G_loss: -2.7027\n",
      "  Batch [890/1299] D_loss: -1.7566, G_loss: -2.4223\n",
      "  Batch [900/1299] D_loss: -2.3537, G_loss: -2.6810\n",
      "  Batch [910/1299] D_loss: -2.2920, G_loss: -0.3208\n",
      "  Batch [920/1299] D_loss: -2.5293, G_loss: -0.1170\n",
      "  Batch [930/1299] D_loss: -2.2658, G_loss: 0.3353\n",
      "  Batch [940/1299] D_loss: -2.9789, G_loss: 0.7343\n",
      "  Batch [950/1299] D_loss: -2.1345, G_loss: 1.6077\n",
      "  Batch [960/1299] D_loss: -2.4816, G_loss: 2.3755\n",
      "  Batch [970/1299] D_loss: -2.5227, G_loss: 1.9527\n",
      "  Batch [980/1299] D_loss: -2.7213, G_loss: 2.9462\n",
      "  Batch [990/1299] D_loss: -1.6905, G_loss: 1.0334\n",
      "  Batch [1000/1299] D_loss: -1.9616, G_loss: 2.5049\n",
      "  Batch [1010/1299] D_loss: -2.3496, G_loss: 0.9466\n",
      "  Batch [1020/1299] D_loss: -1.8656, G_loss: -0.3572\n",
      "  Batch [1030/1299] D_loss: -2.4817, G_loss: -2.2713\n",
      "  Batch [1040/1299] D_loss: -2.1112, G_loss: -3.7017\n",
      "  Batch [1050/1299] D_loss: -0.8183, G_loss: -3.4180\n",
      "  Batch [1060/1299] D_loss: -2.1237, G_loss: -2.8772\n",
      "  Batch [1070/1299] D_loss: -2.4372, G_loss: -3.1713\n",
      "  Batch [1080/1299] D_loss: -1.2597, G_loss: -3.5066\n",
      "  Batch [1090/1299] D_loss: -1.9965, G_loss: -2.0653\n",
      "  Batch [1100/1299] D_loss: -2.1307, G_loss: -1.0494\n",
      "  Batch [1110/1299] D_loss: -2.1481, G_loss: -1.5608\n",
      "  Batch [1120/1299] D_loss: -1.8615, G_loss: -0.6074\n",
      "  Batch [1130/1299] D_loss: -3.0491, G_loss: -0.1622\n",
      "  Batch [1140/1299] D_loss: -1.6713, G_loss: 0.9985\n",
      "  Batch [1150/1299] D_loss: -2.1594, G_loss: 0.4263\n",
      "  Batch [1160/1299] D_loss: -2.0290, G_loss: 1.7397\n",
      "  Batch [1170/1299] D_loss: -1.7835, G_loss: 1.3481\n",
      "  Batch [1180/1299] D_loss: -2.4091, G_loss: 0.4363\n",
      "  Batch [1190/1299] D_loss: -2.3353, G_loss: 1.2865\n",
      "  Batch [1200/1299] D_loss: -1.2230, G_loss: 2.3918\n",
      "  Batch [1210/1299] D_loss: -2.2277, G_loss: 1.8761\n",
      "  Batch [1220/1299] D_loss: -1.9938, G_loss: 0.8173\n",
      "  Batch [1230/1299] D_loss: -2.1744, G_loss: 1.0488\n",
      "  Batch [1240/1299] D_loss: -2.9686, G_loss: 1.0672\n",
      "  Batch [1250/1299] D_loss: -2.2089, G_loss: -0.2804\n",
      "  Batch [1260/1299] D_loss: -1.2632, G_loss: -1.4000\n",
      "  Batch [1270/1299] D_loss: -2.5945, G_loss: -1.9039\n",
      "  Batch [1280/1299] D_loss: -1.7918, G_loss: -2.0857\n",
      "  Batch [1290/1299] D_loss: -1.6775, G_loss: -2.0202\n",
      "\n",
      "Epoch 86 Summary:\n",
      "  Average D_loss: -1.8579\n",
      "  Average G_loss: 0.8745\n",
      "\n",
      "Epoch [87/100]\n",
      "  Batch [0/1299] D_loss: -1.6236, G_loss: -2.6888\n",
      "  Batch [10/1299] D_loss: -3.2687, G_loss: -2.6908\n",
      "  Batch [20/1299] D_loss: -1.3645, G_loss: -1.4564\n",
      "  Batch [30/1299] D_loss: -2.2507, G_loss: -0.4821\n",
      "  Batch [40/1299] D_loss: -2.1241, G_loss: 1.4088\n",
      "  Batch [50/1299] D_loss: -1.7518, G_loss: 2.2548\n",
      "  Batch [60/1299] D_loss: -2.2294, G_loss: 2.7285\n",
      "  Batch [70/1299] D_loss: -1.5380, G_loss: 3.5703\n",
      "  Batch [80/1299] D_loss: -2.3855, G_loss: 2.6193\n",
      "  Batch [90/1299] D_loss: -1.8805, G_loss: 0.9744\n",
      "  Batch [100/1299] D_loss: -1.8855, G_loss: 0.0090\n",
      "  Batch [110/1299] D_loss: -2.3357, G_loss: -0.0573\n",
      "  Batch [120/1299] D_loss: -3.0673, G_loss: -1.3476\n",
      "  Batch [130/1299] D_loss: -0.9426, G_loss: -1.7662\n",
      "  Batch [140/1299] D_loss: -1.5166, G_loss: -2.3114\n",
      "  Batch [150/1299] D_loss: -2.7248, G_loss: -1.5436\n",
      "  Batch [160/1299] D_loss: -1.8064, G_loss: -3.2642\n",
      "  Batch [170/1299] D_loss: -1.7605, G_loss: -2.4599\n",
      "  Batch [180/1299] D_loss: -3.2200, G_loss: 0.2694\n",
      "  Batch [190/1299] D_loss: -1.7993, G_loss: -0.3374\n",
      "  Batch [200/1299] D_loss: -1.7941, G_loss: 0.4582\n",
      "  Batch [210/1299] D_loss: -2.3906, G_loss: 0.0638\n",
      "  Batch [220/1299] D_loss: -3.0565, G_loss: 0.9049\n",
      "  Batch [230/1299] D_loss: -2.0698, G_loss: 1.7671\n",
      "  Batch [240/1299] D_loss: -1.3939, G_loss: 0.8679\n",
      "  Batch [250/1299] D_loss: -1.6148, G_loss: 2.1490\n",
      "  Batch [260/1299] D_loss: -2.1019, G_loss: 1.7682\n",
      "  Batch [270/1299] D_loss: -2.5853, G_loss: 4.5600\n",
      "  Batch [280/1299] D_loss: -2.2336, G_loss: 2.3542\n",
      "  Batch [290/1299] D_loss: -1.8067, G_loss: 3.2709\n",
      "  Batch [300/1299] D_loss: -2.4306, G_loss: 2.5048\n",
      "  Batch [310/1299] D_loss: -2.2192, G_loss: 3.0417\n",
      "  Batch [320/1299] D_loss: -3.7907, G_loss: -1.7672\n",
      "  Batch [330/1299] D_loss: -1.5544, G_loss: -1.5172\n",
      "  Batch [340/1299] D_loss: -1.8406, G_loss: 0.5625\n",
      "  Batch [350/1299] D_loss: -2.3250, G_loss: 2.7602\n",
      "  Batch [360/1299] D_loss: -1.9417, G_loss: 3.1668\n",
      "  Batch [370/1299] D_loss: -2.0302, G_loss: 2.9784\n",
      "  Batch [380/1299] D_loss: -1.7352, G_loss: 3.1471\n",
      "  Batch [390/1299] D_loss: -2.4979, G_loss: 1.1007\n",
      "  Batch [400/1299] D_loss: -2.4233, G_loss: 0.3237\n",
      "  Batch [410/1299] D_loss: -1.8336, G_loss: -0.1706\n",
      "  Batch [420/1299] D_loss: -2.1565, G_loss: -0.4471\n",
      "  Batch [430/1299] D_loss: -2.1888, G_loss: 0.5766\n",
      "  Batch [440/1299] D_loss: -0.8278, G_loss: 0.4772\n",
      "  Batch [450/1299] D_loss: -1.7769, G_loss: -1.1452\n",
      "  Batch [460/1299] D_loss: -2.3437, G_loss: 0.0413\n",
      "  Batch [470/1299] D_loss: -2.1401, G_loss: 4.0130\n",
      "  Batch [480/1299] D_loss: -1.3547, G_loss: 3.5445\n",
      "  Batch [490/1299] D_loss: -1.3524, G_loss: 2.5568\n",
      "  Batch [500/1299] D_loss: -2.3308, G_loss: 1.8853\n",
      "  Batch [510/1299] D_loss: -2.4316, G_loss: 4.0404\n",
      "  Batch [520/1299] D_loss: -1.9574, G_loss: 4.3736\n",
      "  Batch [530/1299] D_loss: -1.5985, G_loss: 3.2926\n",
      "  Batch [540/1299] D_loss: -1.0955, G_loss: 1.5674\n",
      "  Batch [550/1299] D_loss: -1.6228, G_loss: 3.4089\n",
      "  Batch [560/1299] D_loss: -2.1851, G_loss: 0.9353\n",
      "  Batch [570/1299] D_loss: -1.3590, G_loss: 0.5451\n",
      "  Batch [580/1299] D_loss: -2.3063, G_loss: 0.8303\n",
      "  Batch [590/1299] D_loss: -1.6551, G_loss: -1.2002\n",
      "  Batch [600/1299] D_loss: -2.3372, G_loss: -1.7992\n",
      "  Batch [610/1299] D_loss: -2.1047, G_loss: -2.3980\n",
      "  Batch [620/1299] D_loss: -1.7274, G_loss: -2.6587\n",
      "  Batch [630/1299] D_loss: -1.3510, G_loss: -4.5051\n",
      "  Batch [640/1299] D_loss: -0.9276, G_loss: -4.3404\n",
      "  Batch [650/1299] D_loss: -1.8597, G_loss: -3.4113\n",
      "  Batch [660/1299] D_loss: -1.6121, G_loss: -3.5665\n",
      "  Batch [670/1299] D_loss: -1.7095, G_loss: -3.6962\n",
      "  Batch [680/1299] D_loss: -2.1049, G_loss: -2.0486\n",
      "  Batch [690/1299] D_loss: -2.8783, G_loss: -0.3414\n",
      "  Batch [700/1299] D_loss: -2.6545, G_loss: 1.7517\n",
      "  Batch [710/1299] D_loss: -2.8179, G_loss: 2.8463\n",
      "  Batch [720/1299] D_loss: -1.9502, G_loss: 4.6783\n",
      "  Batch [730/1299] D_loss: -1.7340, G_loss: 2.1233\n",
      "  Batch [740/1299] D_loss: -2.8987, G_loss: 4.2999\n",
      "  Batch [750/1299] D_loss: -2.2528, G_loss: 4.3186\n",
      "  Batch [760/1299] D_loss: -2.4490, G_loss: 1.1781\n",
      "  Batch [770/1299] D_loss: -1.9112, G_loss: 0.0466\n",
      "  Batch [780/1299] D_loss: -2.5454, G_loss: 1.4813\n",
      "  Batch [790/1299] D_loss: -2.2585, G_loss: 0.0895\n",
      "  Batch [800/1299] D_loss: -2.3594, G_loss: 2.4457\n",
      "  Batch [810/1299] D_loss: -1.8380, G_loss: 0.7383\n",
      "  Batch [820/1299] D_loss: -2.7386, G_loss: -0.9783\n",
      "  Batch [830/1299] D_loss: -2.1648, G_loss: 0.4940\n",
      "  Batch [840/1299] D_loss: -2.7712, G_loss: -0.4537\n",
      "  Batch [850/1299] D_loss: -2.0058, G_loss: 1.1125\n",
      "  Batch [860/1299] D_loss: -2.2485, G_loss: 2.2700\n",
      "  Batch [870/1299] D_loss: -2.8729, G_loss: 3.3585\n",
      "  Batch [880/1299] D_loss: -2.3164, G_loss: 0.4481\n",
      "  Batch [890/1299] D_loss: -2.4466, G_loss: 3.0443\n",
      "  Batch [900/1299] D_loss: -1.7369, G_loss: 3.7019\n",
      "  Batch [910/1299] D_loss: -2.1717, G_loss: 2.1470\n",
      "  Batch [920/1299] D_loss: -2.1378, G_loss: 2.5980\n",
      "  Batch [930/1299] D_loss: -2.2089, G_loss: 0.6052\n",
      "  Batch [940/1299] D_loss: -1.7572, G_loss: 0.7979\n",
      "  Batch [950/1299] D_loss: -2.5486, G_loss: 0.4080\n",
      "  Batch [960/1299] D_loss: -2.6401, G_loss: 0.2930\n",
      "  Batch [970/1299] D_loss: -2.2488, G_loss: 2.0802\n",
      "  Batch [980/1299] D_loss: -1.0057, G_loss: 2.1160\n",
      "  Batch [990/1299] D_loss: -1.4665, G_loss: -1.0456\n",
      "  Batch [1000/1299] D_loss: -1.5939, G_loss: -1.3713\n",
      "  Batch [1010/1299] D_loss: -1.5945, G_loss: -1.3925\n",
      "  Batch [1020/1299] D_loss: -0.7600, G_loss: -3.9597\n",
      "  Batch [1030/1299] D_loss: -3.2266, G_loss: -3.0563\n",
      "  Batch [1040/1299] D_loss: -2.9172, G_loss: -5.9092\n",
      "  Batch [1050/1299] D_loss: -0.8510, G_loss: -5.2053\n",
      "  Batch [1060/1299] D_loss: -2.2778, G_loss: -4.5781\n",
      "  Batch [1070/1299] D_loss: -2.6296, G_loss: -2.8145\n",
      "  Batch [1080/1299] D_loss: -1.8404, G_loss: -1.8272\n",
      "  Batch [1090/1299] D_loss: -2.4653, G_loss: -2.3109\n",
      "  Batch [1100/1299] D_loss: -2.1832, G_loss: -1.1607\n",
      "  Batch [1110/1299] D_loss: -1.6547, G_loss: -1.5658\n",
      "  Batch [1120/1299] D_loss: -2.0105, G_loss: -0.5110\n",
      "  Batch [1130/1299] D_loss: -2.7931, G_loss: -0.6449\n",
      "  Batch [1140/1299] D_loss: -1.8727, G_loss: -0.6291\n",
      "  Batch [1150/1299] D_loss: -2.6891, G_loss: 0.6745\n",
      "  Batch [1160/1299] D_loss: -2.0655, G_loss: 2.1310\n",
      "  Batch [1170/1299] D_loss: -2.2436, G_loss: 1.5839\n",
      "  Batch [1180/1299] D_loss: -1.8531, G_loss: 1.6056\n",
      "  Batch [1190/1299] D_loss: -1.6420, G_loss: 2.9051\n",
      "  Batch [1200/1299] D_loss: -1.8014, G_loss: 2.4369\n",
      "  Batch [1210/1299] D_loss: -1.9980, G_loss: 2.2925\n",
      "  Batch [1220/1299] D_loss: -2.8905, G_loss: 2.4150\n",
      "  Batch [1230/1299] D_loss: -1.2659, G_loss: 2.4787\n",
      "  Batch [1240/1299] D_loss: -2.2928, G_loss: 1.9356\n",
      "  Batch [1250/1299] D_loss: -1.7657, G_loss: 2.6218\n",
      "  Batch [1260/1299] D_loss: -2.5223, G_loss: 2.2216\n",
      "  Batch [1270/1299] D_loss: -2.6176, G_loss: 1.9291\n",
      "  Batch [1280/1299] D_loss: -2.3789, G_loss: 2.2673\n",
      "  Batch [1290/1299] D_loss: -1.7401, G_loss: 1.0299\n",
      "\n",
      "Epoch 87 Summary:\n",
      "  Average D_loss: -1.8693\n",
      "  Average G_loss: 0.5595\n",
      "\n",
      "Epoch [88/100]\n",
      "  Batch [0/1299] D_loss: -1.2424, G_loss: 0.1578\n",
      "  Batch [10/1299] D_loss: -2.7651, G_loss: 3.1215\n",
      "  Batch [20/1299] D_loss: -2.2311, G_loss: 0.7902\n",
      "  Batch [30/1299] D_loss: -1.3739, G_loss: -0.4133\n",
      "  Batch [40/1299] D_loss: -1.5907, G_loss: 0.6266\n",
      "  Batch [50/1299] D_loss: -1.4556, G_loss: 1.7476\n",
      "  Batch [60/1299] D_loss: -1.8352, G_loss: 2.6735\n",
      "  Batch [70/1299] D_loss: -1.9371, G_loss: 1.4246\n",
      "  Batch [80/1299] D_loss: -2.2956, G_loss: 2.6955\n",
      "  Batch [90/1299] D_loss: -2.1218, G_loss: 0.9702\n",
      "  Batch [100/1299] D_loss: -1.5580, G_loss: 2.3307\n",
      "  Batch [110/1299] D_loss: -1.4703, G_loss: 1.0147\n",
      "  Batch [120/1299] D_loss: -2.0311, G_loss: 1.4790\n",
      "  Batch [130/1299] D_loss: -1.8663, G_loss: 0.0561\n",
      "  Batch [140/1299] D_loss: -2.9556, G_loss: -1.0135\n",
      "  Batch [150/1299] D_loss: -0.7677, G_loss: -3.4536\n",
      "  Batch [160/1299] D_loss: -1.5400, G_loss: -3.2604\n",
      "  Batch [170/1299] D_loss: -1.9359, G_loss: -2.4995\n",
      "  Batch [180/1299] D_loss: -1.3773, G_loss: -2.3970\n",
      "  Batch [190/1299] D_loss: -1.1939, G_loss: -4.6370\n",
      "  Batch [200/1299] D_loss: -1.1638, G_loss: -4.5818\n",
      "  Batch [210/1299] D_loss: -2.8900, G_loss: -4.5256\n",
      "  Batch [220/1299] D_loss: -3.0481, G_loss: -2.0842\n",
      "  Batch [230/1299] D_loss: -2.5011, G_loss: -0.5924\n",
      "  Batch [240/1299] D_loss: -3.5756, G_loss: -1.0734\n",
      "  Batch [250/1299] D_loss: -1.7764, G_loss: 1.0979\n",
      "  Batch [260/1299] D_loss: -2.4562, G_loss: 2.7008\n",
      "  Batch [270/1299] D_loss: -0.3746, G_loss: 3.8468\n",
      "  Batch [280/1299] D_loss: -2.3808, G_loss: 2.8584\n",
      "  Batch [290/1299] D_loss: -1.7551, G_loss: 2.9478\n",
      "  Batch [300/1299] D_loss: -2.3565, G_loss: 1.1166\n",
      "  Batch [310/1299] D_loss: -1.5164, G_loss: 1.2475\n",
      "  Batch [320/1299] D_loss: -1.9100, G_loss: 0.8598\n",
      "  Batch [330/1299] D_loss: -1.6081, G_loss: 2.4887\n",
      "  Batch [340/1299] D_loss: -3.2230, G_loss: 5.1006\n",
      "  Batch [350/1299] D_loss: -3.2711, G_loss: 4.5015\n",
      "  Batch [360/1299] D_loss: -1.5558, G_loss: 2.5409\n",
      "  Batch [370/1299] D_loss: -2.6150, G_loss: 2.3377\n",
      "  Batch [380/1299] D_loss: -0.9128, G_loss: -0.4293\n",
      "  Batch [390/1299] D_loss: -1.8369, G_loss: 1.2404\n",
      "  Batch [400/1299] D_loss: -2.3629, G_loss: 0.0465\n",
      "  Batch [410/1299] D_loss: -2.2182, G_loss: 0.6567\n",
      "  Batch [420/1299] D_loss: -2.4183, G_loss: 1.4226\n",
      "  Batch [430/1299] D_loss: -3.0170, G_loss: 2.2633\n",
      "  Batch [440/1299] D_loss: -1.6544, G_loss: 3.0066\n",
      "  Batch [450/1299] D_loss: -1.4200, G_loss: 1.9891\n",
      "  Batch [460/1299] D_loss: -1.5153, G_loss: 1.2047\n",
      "  Batch [470/1299] D_loss: -2.5262, G_loss: 2.6160\n",
      "  Batch [480/1299] D_loss: -2.6209, G_loss: 3.8749\n",
      "  Batch [490/1299] D_loss: -1.2907, G_loss: 3.1221\n",
      "  Batch [500/1299] D_loss: -2.0721, G_loss: 4.2511\n",
      "  Batch [510/1299] D_loss: -2.9779, G_loss: 5.5777\n",
      "  Batch [520/1299] D_loss: -2.0970, G_loss: 1.5876\n",
      "  Batch [530/1299] D_loss: -2.3694, G_loss: 2.0046\n",
      "  Batch [540/1299] D_loss: -2.3996, G_loss: 0.8225\n",
      "  Batch [550/1299] D_loss: -3.0130, G_loss: -0.4844\n",
      "  Batch [560/1299] D_loss: -2.4934, G_loss: -1.3311\n",
      "  Batch [570/1299] D_loss: -2.3265, G_loss: -2.9353\n",
      "  Batch [580/1299] D_loss: -2.6812, G_loss: -2.9375\n",
      "  Batch [590/1299] D_loss: -2.0646, G_loss: -3.9627\n",
      "  Batch [600/1299] D_loss: -1.3406, G_loss: -5.1482\n",
      "  Batch [610/1299] D_loss: -1.8365, G_loss: -3.2374\n",
      "  Batch [620/1299] D_loss: -1.5860, G_loss: -3.1651\n",
      "  Batch [630/1299] D_loss: -1.9855, G_loss: -3.2806\n",
      "  Batch [640/1299] D_loss: -2.5688, G_loss: -1.9575\n",
      "  Batch [650/1299] D_loss: -1.5820, G_loss: -1.5425\n",
      "  Batch [660/1299] D_loss: -1.2691, G_loss: -0.5140\n",
      "  Batch [670/1299] D_loss: -2.6214, G_loss: 0.7976\n",
      "  Batch [680/1299] D_loss: -2.5369, G_loss: 2.2662\n",
      "  Batch [690/1299] D_loss: -1.8908, G_loss: 6.0669\n",
      "  Batch [700/1299] D_loss: -0.9869, G_loss: 5.2176\n",
      "  Batch [710/1299] D_loss: -2.7279, G_loss: 4.4930\n",
      "  Batch [720/1299] D_loss: -2.2812, G_loss: 3.5977\n",
      "  Batch [730/1299] D_loss: -3.2853, G_loss: 3.1413\n",
      "  Batch [740/1299] D_loss: -1.2978, G_loss: 4.9771\n",
      "  Batch [750/1299] D_loss: -2.8148, G_loss: 1.9662\n",
      "  Batch [760/1299] D_loss: -1.8236, G_loss: 2.7711\n",
      "  Batch [770/1299] D_loss: -2.5605, G_loss: 2.8590\n",
      "  Batch [780/1299] D_loss: -3.3126, G_loss: 3.2137\n",
      "  Batch [790/1299] D_loss: -1.5413, G_loss: 3.9525\n",
      "  Batch [800/1299] D_loss: -1.8270, G_loss: 2.4382\n",
      "  Batch [810/1299] D_loss: -1.5268, G_loss: 2.7400\n",
      "  Batch [820/1299] D_loss: -1.5120, G_loss: 3.0769\n",
      "  Batch [830/1299] D_loss: -1.0832, G_loss: 3.7167\n",
      "  Batch [840/1299] D_loss: -2.2616, G_loss: 5.0453\n",
      "  Batch [850/1299] D_loss: -1.7489, G_loss: 4.9237\n",
      "  Batch [860/1299] D_loss: -1.1236, G_loss: 4.2738\n",
      "  Batch [870/1299] D_loss: -1.6895, G_loss: 4.7436\n",
      "  Batch [880/1299] D_loss: -2.2114, G_loss: 3.7108\n",
      "  Batch [890/1299] D_loss: -2.7791, G_loss: 2.1489\n",
      "  Batch [900/1299] D_loss: -2.2106, G_loss: 0.5611\n",
      "  Batch [910/1299] D_loss: -1.1060, G_loss: 0.1694\n",
      "  Batch [920/1299] D_loss: -2.5455, G_loss: 0.2530\n",
      "  Batch [930/1299] D_loss: -1.4452, G_loss: 1.2002\n",
      "  Batch [940/1299] D_loss: -2.5437, G_loss: -0.0246\n",
      "  Batch [950/1299] D_loss: -2.8200, G_loss: 0.6384\n",
      "  Batch [960/1299] D_loss: -2.0376, G_loss: 0.2554\n",
      "  Batch [970/1299] D_loss: -2.3338, G_loss: 0.1543\n",
      "  Batch [980/1299] D_loss: -1.6748, G_loss: 0.8099\n",
      "  Batch [990/1299] D_loss: -0.9469, G_loss: 1.3475\n",
      "  Batch [1000/1299] D_loss: -2.1466, G_loss: 3.4429\n",
      "  Batch [1010/1299] D_loss: -2.3960, G_loss: 3.1850\n",
      "  Batch [1020/1299] D_loss: -1.7073, G_loss: 3.6524\n",
      "  Batch [1030/1299] D_loss: -1.3012, G_loss: 1.7221\n",
      "  Batch [1040/1299] D_loss: -1.5635, G_loss: 1.5679\n",
      "  Batch [1050/1299] D_loss: -1.9304, G_loss: 1.3701\n",
      "  Batch [1060/1299] D_loss: -3.5492, G_loss: 0.4130\n",
      "  Batch [1070/1299] D_loss: -1.7915, G_loss: -1.3326\n",
      "  Batch [1080/1299] D_loss: -1.8383, G_loss: 0.0103\n",
      "  Batch [1090/1299] D_loss: -1.0394, G_loss: -1.8668\n",
      "  Batch [1100/1299] D_loss: -2.8510, G_loss: -2.5583\n",
      "  Batch [1110/1299] D_loss: -1.9575, G_loss: -1.4753\n",
      "  Batch [1120/1299] D_loss: -1.9529, G_loss: -0.8777\n",
      "  Batch [1130/1299] D_loss: -2.0602, G_loss: -1.1591\n",
      "  Batch [1140/1299] D_loss: -2.3359, G_loss: 2.2028\n",
      "  Batch [1150/1299] D_loss: -2.6432, G_loss: 4.5992\n",
      "  Batch [1160/1299] D_loss: -1.3668, G_loss: 6.4129\n",
      "  Batch [1170/1299] D_loss: -1.8404, G_loss: 4.7083\n",
      "  Batch [1180/1299] D_loss: -1.4547, G_loss: 5.6726\n",
      "  Batch [1190/1299] D_loss: -2.2624, G_loss: 6.4646\n",
      "  Batch [1200/1299] D_loss: -2.9045, G_loss: 5.1457\n",
      "  Batch [1210/1299] D_loss: -2.4577, G_loss: 0.1839\n",
      "  Batch [1220/1299] D_loss: -2.7236, G_loss: 2.4819\n",
      "  Batch [1230/1299] D_loss: -1.0223, G_loss: 1.7066\n",
      "  Batch [1240/1299] D_loss: -1.8925, G_loss: 4.8153\n",
      "  Batch [1250/1299] D_loss: -1.5982, G_loss: 5.7772\n",
      "  Batch [1260/1299] D_loss: -1.3009, G_loss: 6.6076\n",
      "  Batch [1270/1299] D_loss: -1.6627, G_loss: 3.8990\n",
      "  Batch [1280/1299] D_loss: -2.2938, G_loss: 7.7190\n",
      "  Batch [1290/1299] D_loss: -2.7803, G_loss: 4.5570\n",
      "\n",
      "Epoch 88 Summary:\n",
      "  Average D_loss: -1.8859\n",
      "  Average G_loss: 1.4980\n",
      "\n",
      "Epoch [89/100]\n",
      "  Batch [0/1299] D_loss: -1.0009, G_loss: 2.8686\n",
      "  Batch [10/1299] D_loss: -3.0350, G_loss: 0.6396\n",
      "  Batch [20/1299] D_loss: -2.6812, G_loss: 1.1609\n",
      "  Batch [30/1299] D_loss: -3.0898, G_loss: 1.5396\n",
      "  Batch [40/1299] D_loss: -2.4240, G_loss: 3.5532\n",
      "  Batch [50/1299] D_loss: -2.5111, G_loss: 5.1214\n",
      "  Batch [60/1299] D_loss: -2.0057, G_loss: 0.8230\n",
      "  Batch [70/1299] D_loss: -2.3993, G_loss: 2.4989\n",
      "  Batch [80/1299] D_loss: -2.6201, G_loss: 0.2534\n",
      "  Batch [90/1299] D_loss: -1.6308, G_loss: -0.1780\n",
      "  Batch [100/1299] D_loss: -1.7850, G_loss: 3.4607\n",
      "  Batch [110/1299] D_loss: -1.8772, G_loss: 4.6471\n",
      "  Batch [120/1299] D_loss: -2.7329, G_loss: 1.1670\n",
      "  Batch [130/1299] D_loss: -2.6719, G_loss: 2.6843\n",
      "  Batch [140/1299] D_loss: -2.7876, G_loss: 5.9770\n",
      "  Batch [150/1299] D_loss: -2.8637, G_loss: 6.7231\n",
      "  Batch [160/1299] D_loss: -2.3797, G_loss: 3.4292\n",
      "  Batch [170/1299] D_loss: -1.0129, G_loss: 0.3768\n",
      "  Batch [180/1299] D_loss: -1.9459, G_loss: 2.3242\n",
      "  Batch [190/1299] D_loss: -3.0567, G_loss: 0.6723\n",
      "  Batch [200/1299] D_loss: -2.4135, G_loss: 0.3873\n",
      "  Batch [210/1299] D_loss: -1.6782, G_loss: 0.4981\n",
      "  Batch [220/1299] D_loss: -1.6638, G_loss: 1.1883\n",
      "  Batch [230/1299] D_loss: -0.7862, G_loss: 1.3101\n",
      "  Batch [240/1299] D_loss: -1.9074, G_loss: 2.6705\n",
      "  Batch [250/1299] D_loss: -2.3560, G_loss: 2.3587\n",
      "  Batch [260/1299] D_loss: -2.9231, G_loss: 5.6985\n",
      "  Batch [270/1299] D_loss: -2.2841, G_loss: 3.4394\n",
      "  Batch [280/1299] D_loss: -2.5917, G_loss: -0.3869\n",
      "  Batch [290/1299] D_loss: -1.8140, G_loss: 0.1275\n",
      "  Batch [300/1299] D_loss: -1.0157, G_loss: 0.8074\n",
      "  Batch [310/1299] D_loss: -3.3858, G_loss: 5.0756\n",
      "  Batch [320/1299] D_loss: -2.4811, G_loss: 4.1057\n",
      "  Batch [330/1299] D_loss: -2.7295, G_loss: 4.3244\n",
      "  Batch [340/1299] D_loss: -1.8030, G_loss: 2.3894\n",
      "  Batch [350/1299] D_loss: -2.6521, G_loss: 6.1493\n",
      "  Batch [360/1299] D_loss: -2.5841, G_loss: 2.4255\n",
      "  Batch [370/1299] D_loss: -1.7127, G_loss: 3.9421\n",
      "  Batch [380/1299] D_loss: -0.7950, G_loss: 4.2719\n",
      "  Batch [390/1299] D_loss: -2.8372, G_loss: -0.9426\n",
      "  Batch [400/1299] D_loss: -2.0785, G_loss: -1.1110\n",
      "  Batch [410/1299] D_loss: -2.2559, G_loss: -1.6558\n",
      "  Batch [420/1299] D_loss: -2.1715, G_loss: 0.1735\n",
      "  Batch [430/1299] D_loss: -2.9073, G_loss: 1.9025\n",
      "  Batch [440/1299] D_loss: -1.9141, G_loss: 7.5224\n",
      "  Batch [450/1299] D_loss: -1.7416, G_loss: 4.8696\n",
      "  Batch [460/1299] D_loss: -1.6703, G_loss: 2.6565\n",
      "  Batch [470/1299] D_loss: -2.2972, G_loss: 3.9517\n",
      "  Batch [480/1299] D_loss: -3.4195, G_loss: 1.5221\n",
      "  Batch [490/1299] D_loss: -1.5308, G_loss: 2.9085\n",
      "  Batch [500/1299] D_loss: -3.2807, G_loss: 0.0857\n",
      "  Batch [510/1299] D_loss: -2.2591, G_loss: 3.6675\n",
      "  Batch [520/1299] D_loss: -1.9555, G_loss: 5.3080\n",
      "  Batch [530/1299] D_loss: -2.2679, G_loss: 3.3404\n",
      "  Batch [540/1299] D_loss: -1.5191, G_loss: 5.1420\n",
      "  Batch [550/1299] D_loss: -1.6337, G_loss: 4.6664\n",
      "  Batch [560/1299] D_loss: -3.4218, G_loss: 5.4031\n",
      "  Batch [570/1299] D_loss: -2.5408, G_loss: 1.6413\n",
      "  Batch [580/1299] D_loss: -1.1297, G_loss: 1.8888\n",
      "  Batch [590/1299] D_loss: -2.4563, G_loss: -0.8782\n",
      "  Batch [600/1299] D_loss: -2.8339, G_loss: -1.3894\n",
      "  Batch [610/1299] D_loss: -2.5765, G_loss: -2.0455\n",
      "  Batch [620/1299] D_loss: -1.8033, G_loss: -0.1747\n",
      "  Batch [630/1299] D_loss: -2.7701, G_loss: 2.7322\n",
      "  Batch [640/1299] D_loss: -2.7589, G_loss: 6.2400\n",
      "  Batch [650/1299] D_loss: -0.8669, G_loss: 4.4824\n",
      "  Batch [660/1299] D_loss: -1.4213, G_loss: 5.9890\n",
      "  Batch [670/1299] D_loss: -1.3294, G_loss: 5.6186\n",
      "  Batch [680/1299] D_loss: -2.5880, G_loss: 3.1441\n",
      "  Batch [690/1299] D_loss: -2.0791, G_loss: 2.7233\n",
      "  Batch [700/1299] D_loss: -2.1940, G_loss: 3.3141\n",
      "  Batch [710/1299] D_loss: -2.0014, G_loss: 3.1161\n",
      "  Batch [720/1299] D_loss: -1.6687, G_loss: 3.6734\n",
      "  Batch [730/1299] D_loss: -1.1686, G_loss: 3.7464\n",
      "  Batch [740/1299] D_loss: -2.4341, G_loss: 4.3095\n",
      "  Batch [750/1299] D_loss: -3.1212, G_loss: 2.1532\n",
      "  Batch [760/1299] D_loss: -3.3771, G_loss: 3.5298\n",
      "  Batch [770/1299] D_loss: -1.4068, G_loss: 1.4372\n",
      "  Batch [780/1299] D_loss: -1.3975, G_loss: 3.9047\n",
      "  Batch [790/1299] D_loss: -2.1404, G_loss: 2.6450\n",
      "  Batch [800/1299] D_loss: -1.1298, G_loss: 3.8675\n",
      "  Batch [810/1299] D_loss: -1.9504, G_loss: 3.8697\n",
      "  Batch [820/1299] D_loss: -2.5241, G_loss: 6.3974\n",
      "  Batch [830/1299] D_loss: -2.1523, G_loss: 4.1821\n",
      "  Batch [840/1299] D_loss: -1.8564, G_loss: 1.5471\n",
      "  Batch [850/1299] D_loss: -2.4578, G_loss: 4.0930\n",
      "  Batch [860/1299] D_loss: -1.6761, G_loss: 2.5848\n",
      "  Batch [870/1299] D_loss: -1.5160, G_loss: 3.0377\n",
      "  Batch [880/1299] D_loss: -2.1315, G_loss: 2.6659\n",
      "  Batch [890/1299] D_loss: -2.5609, G_loss: 2.7064\n",
      "  Batch [900/1299] D_loss: -1.5748, G_loss: 2.0462\n",
      "  Batch [910/1299] D_loss: -1.8579, G_loss: 1.7606\n",
      "  Batch [920/1299] D_loss: -1.9730, G_loss: 4.0645\n",
      "  Batch [930/1299] D_loss: -3.1197, G_loss: 1.9643\n",
      "  Batch [940/1299] D_loss: -1.3685, G_loss: 3.7749\n",
      "  Batch [950/1299] D_loss: -1.8721, G_loss: 6.2512\n",
      "  Batch [960/1299] D_loss: -2.4935, G_loss: 4.1778\n",
      "  Batch [970/1299] D_loss: -2.3362, G_loss: 5.3193\n",
      "  Batch [980/1299] D_loss: -1.8938, G_loss: 2.8192\n",
      "  Batch [990/1299] D_loss: -0.5940, G_loss: 1.6979\n",
      "  Batch [1000/1299] D_loss: -1.8080, G_loss: 5.1107\n",
      "  Batch [1010/1299] D_loss: -2.4118, G_loss: 5.3234\n",
      "  Batch [1020/1299] D_loss: -1.7873, G_loss: 2.5647\n",
      "  Batch [1030/1299] D_loss: 0.1834, G_loss: 3.7404\n",
      "  Batch [1040/1299] D_loss: -1.6819, G_loss: 5.4518\n",
      "  Batch [1050/1299] D_loss: -1.5117, G_loss: 4.5717\n",
      "  Batch [1060/1299] D_loss: -2.5187, G_loss: 0.1271\n",
      "  Batch [1070/1299] D_loss: -1.5178, G_loss: 3.3879\n",
      "  Batch [1080/1299] D_loss: -1.4834, G_loss: 1.8589\n",
      "  Batch [1090/1299] D_loss: -2.2565, G_loss: 4.7353\n",
      "  Batch [1100/1299] D_loss: -1.3736, G_loss: 6.3358\n",
      "  Batch [1110/1299] D_loss: -2.4701, G_loss: 0.8729\n",
      "  Batch [1120/1299] D_loss: -1.9958, G_loss: -0.3677\n",
      "  Batch [1130/1299] D_loss: -2.1727, G_loss: -1.5512\n",
      "  Batch [1140/1299] D_loss: -1.6551, G_loss: -1.3242\n",
      "  Batch [1150/1299] D_loss: -2.2541, G_loss: 1.9332\n",
      "  Batch [1160/1299] D_loss: -1.5653, G_loss: 0.6634\n",
      "  Batch [1170/1299] D_loss: -2.9773, G_loss: 3.9472\n",
      "  Batch [1180/1299] D_loss: -1.4197, G_loss: 5.3394\n",
      "  Batch [1190/1299] D_loss: -1.9585, G_loss: 4.4752\n",
      "  Batch [1200/1299] D_loss: -1.0848, G_loss: 4.9814\n",
      "  Batch [1210/1299] D_loss: -2.4960, G_loss: 5.2289\n",
      "  Batch [1220/1299] D_loss: -2.1987, G_loss: 3.6387\n",
      "  Batch [1230/1299] D_loss: -1.7748, G_loss: 3.8748\n",
      "  Batch [1240/1299] D_loss: -2.2693, G_loss: 5.3319\n",
      "  Batch [1250/1299] D_loss: -2.7604, G_loss: 3.2033\n",
      "  Batch [1260/1299] D_loss: -1.8770, G_loss: 3.5826\n",
      "  Batch [1270/1299] D_loss: -1.9314, G_loss: 6.8660\n",
      "  Batch [1280/1299] D_loss: -2.1101, G_loss: 5.7142\n",
      "  Batch [1290/1299] D_loss: -1.6655, G_loss: 3.3046\n",
      "\n",
      "Epoch 89 Summary:\n",
      "  Average D_loss: -1.8692\n",
      "  Average G_loss: 2.9492\n",
      "\n",
      "Epoch [90/100]\n",
      "  Batch [0/1299] D_loss: -1.7392, G_loss: 3.0702\n",
      "  Batch [10/1299] D_loss: -2.0209, G_loss: 3.5632\n",
      "  Batch [20/1299] D_loss: -2.7807, G_loss: 4.6541\n",
      "  Batch [30/1299] D_loss: -1.7725, G_loss: 3.5649\n",
      "  Batch [40/1299] D_loss: -1.2261, G_loss: 0.7361\n",
      "  Batch [50/1299] D_loss: -1.8324, G_loss: -0.1018\n",
      "  Batch [60/1299] D_loss: -2.1126, G_loss: 0.6487\n",
      "  Batch [70/1299] D_loss: -2.0388, G_loss: -0.4409\n",
      "  Batch [80/1299] D_loss: -2.6013, G_loss: -2.5887\n",
      "  Batch [90/1299] D_loss: -2.3551, G_loss: -3.3464\n",
      "  Batch [100/1299] D_loss: -1.6780, G_loss: -3.8540\n",
      "  Batch [110/1299] D_loss: -2.2054, G_loss: -4.1565\n",
      "  Batch [120/1299] D_loss: -2.0764, G_loss: -5.4836\n",
      "  Batch [130/1299] D_loss: -1.9328, G_loss: -5.4016\n",
      "  Batch [140/1299] D_loss: -0.5571, G_loss: -4.3911\n",
      "  Batch [150/1299] D_loss: -1.1502, G_loss: -3.4760\n",
      "  Batch [160/1299] D_loss: -2.2329, G_loss: -0.0641\n",
      "  Batch [170/1299] D_loss: -2.9481, G_loss: 0.9831\n",
      "  Batch [180/1299] D_loss: -2.8207, G_loss: 2.7437\n",
      "  Batch [190/1299] D_loss: -2.1917, G_loss: 6.5976\n",
      "  Batch [200/1299] D_loss: -2.1530, G_loss: 6.4512\n",
      "  Batch [210/1299] D_loss: -2.6300, G_loss: 3.2694\n",
      "  Batch [220/1299] D_loss: -2.9299, G_loss: 2.2011\n",
      "  Batch [230/1299] D_loss: -1.5506, G_loss: 1.1435\n",
      "  Batch [240/1299] D_loss: -2.8611, G_loss: 0.5127\n",
      "  Batch [250/1299] D_loss: -1.9082, G_loss: 2.4168\n",
      "  Batch [260/1299] D_loss: -3.1853, G_loss: 6.0391\n",
      "  Batch [270/1299] D_loss: -2.5826, G_loss: 7.1310\n",
      "  Batch [280/1299] D_loss: -2.4514, G_loss: 2.1745\n",
      "  Batch [290/1299] D_loss: -1.8625, G_loss: 3.9877\n",
      "  Batch [300/1299] D_loss: -3.8672, G_loss: 3.1725\n",
      "  Batch [310/1299] D_loss: -1.4261, G_loss: 2.7701\n",
      "  Batch [320/1299] D_loss: -1.7088, G_loss: 1.6674\n",
      "  Batch [330/1299] D_loss: -2.4326, G_loss: 0.0448\n",
      "  Batch [340/1299] D_loss: -2.1132, G_loss: -0.5630\n",
      "  Batch [350/1299] D_loss: -0.7483, G_loss: -0.9392\n",
      "  Batch [360/1299] D_loss: -2.0105, G_loss: -0.2193\n",
      "  Batch [370/1299] D_loss: -1.8328, G_loss: 0.2456\n",
      "  Batch [380/1299] D_loss: -1.1445, G_loss: 2.2414\n",
      "  Batch [390/1299] D_loss: -2.3431, G_loss: 2.7904\n",
      "  Batch [400/1299] D_loss: -2.2401, G_loss: 3.2929\n",
      "  Batch [410/1299] D_loss: -2.1999, G_loss: 4.0788\n",
      "  Batch [420/1299] D_loss: -2.8725, G_loss: 4.9459\n",
      "  Batch [430/1299] D_loss: -2.3355, G_loss: 1.7067\n",
      "  Batch [440/1299] D_loss: -2.4214, G_loss: 2.7705\n",
      "  Batch [450/1299] D_loss: -1.6232, G_loss: 4.0560\n",
      "  Batch [460/1299] D_loss: -1.6580, G_loss: 2.7726\n",
      "  Batch [470/1299] D_loss: -2.7046, G_loss: 3.4174\n",
      "  Batch [480/1299] D_loss: -1.8445, G_loss: 3.0492\n",
      "  Batch [490/1299] D_loss: -2.0220, G_loss: 1.4522\n",
      "  Batch [500/1299] D_loss: -2.2542, G_loss: 3.3614\n",
      "  Batch [510/1299] D_loss: -1.0007, G_loss: 2.0242\n",
      "  Batch [520/1299] D_loss: -1.4611, G_loss: 1.9200\n",
      "  Batch [530/1299] D_loss: -2.0205, G_loss: 4.1974\n",
      "  Batch [540/1299] D_loss: -2.9871, G_loss: 4.0700\n",
      "  Batch [550/1299] D_loss: -2.1667, G_loss: 4.6695\n",
      "  Batch [560/1299] D_loss: -2.3626, G_loss: 3.5335\n",
      "  Batch [570/1299] D_loss: -1.4927, G_loss: 6.5172\n",
      "  Batch [580/1299] D_loss: -2.0157, G_loss: 5.0484\n",
      "  Batch [590/1299] D_loss: -2.4159, G_loss: 5.1992\n",
      "  Batch [600/1299] D_loss: -1.8180, G_loss: 2.3538\n",
      "  Batch [610/1299] D_loss: -2.0007, G_loss: 3.7085\n",
      "  Batch [620/1299] D_loss: -1.8259, G_loss: 2.2044\n",
      "  Batch [630/1299] D_loss: -2.2305, G_loss: 1.0799\n",
      "  Batch [640/1299] D_loss: -1.4183, G_loss: 2.4301\n",
      "  Batch [650/1299] D_loss: -2.5685, G_loss: 3.8105\n",
      "  Batch [660/1299] D_loss: -0.2678, G_loss: 4.5258\n",
      "  Batch [670/1299] D_loss: -1.7322, G_loss: 4.0053\n",
      "  Batch [680/1299] D_loss: -2.6033, G_loss: 4.2139\n",
      "  Batch [690/1299] D_loss: -1.1929, G_loss: 3.0125\n",
      "  Batch [700/1299] D_loss: -2.3269, G_loss: 1.7224\n",
      "  Batch [710/1299] D_loss: -1.9569, G_loss: 3.0490\n",
      "  Batch [720/1299] D_loss: -3.2572, G_loss: 2.7511\n",
      "  Batch [730/1299] D_loss: -1.1113, G_loss: 3.3287\n",
      "  Batch [740/1299] D_loss: -1.5969, G_loss: 2.1504\n",
      "  Batch [750/1299] D_loss: -2.2020, G_loss: 3.1872\n",
      "  Batch [760/1299] D_loss: -1.8778, G_loss: 0.9586\n",
      "  Batch [770/1299] D_loss: -2.5912, G_loss: 1.9767\n",
      "  Batch [780/1299] D_loss: -2.6046, G_loss: 2.5182\n",
      "  Batch [790/1299] D_loss: -1.9543, G_loss: 2.5248\n",
      "  Batch [800/1299] D_loss: -2.1807, G_loss: 0.9363\n",
      "  Batch [810/1299] D_loss: -1.8018, G_loss: -0.9400\n",
      "  Batch [820/1299] D_loss: -2.4469, G_loss: -5.3814\n",
      "  Batch [830/1299] D_loss: -2.8952, G_loss: -5.0259\n",
      "  Batch [840/1299] D_loss: -0.8149, G_loss: -7.5757\n",
      "  Batch [850/1299] D_loss: -1.4007, G_loss: -5.7370\n",
      "  Batch [860/1299] D_loss: -1.8504, G_loss: -5.1804\n",
      "  Batch [870/1299] D_loss: -2.5803, G_loss: -4.4825\n",
      "  Batch [880/1299] D_loss: -0.6475, G_loss: -4.1149\n",
      "  Batch [890/1299] D_loss: -3.1544, G_loss: -3.4948\n",
      "  Batch [900/1299] D_loss: -1.9234, G_loss: -2.8555\n",
      "  Batch [910/1299] D_loss: -1.6657, G_loss: -1.6219\n",
      "  Batch [920/1299] D_loss: -2.3452, G_loss: 1.8503\n",
      "  Batch [930/1299] D_loss: -2.6499, G_loss: 4.0373\n",
      "  Batch [940/1299] D_loss: -1.3429, G_loss: 2.8389\n",
      "  Batch [950/1299] D_loss: -1.7432, G_loss: 3.6474\n",
      "  Batch [960/1299] D_loss: -1.3145, G_loss: 4.5001\n",
      "  Batch [970/1299] D_loss: -1.2837, G_loss: 4.4233\n",
      "  Batch [980/1299] D_loss: -1.7434, G_loss: 3.0827\n",
      "  Batch [990/1299] D_loss: -2.8489, G_loss: 4.3728\n",
      "  Batch [1000/1299] D_loss: -1.5022, G_loss: 3.5697\n",
      "  Batch [1010/1299] D_loss: -1.0364, G_loss: 1.8233\n",
      "  Batch [1020/1299] D_loss: -2.2829, G_loss: 2.4216\n",
      "  Batch [1030/1299] D_loss: -2.5556, G_loss: 2.5258\n",
      "  Batch [1040/1299] D_loss: -2.9456, G_loss: 0.0947\n",
      "  Batch [1050/1299] D_loss: -1.9637, G_loss: 1.3720\n",
      "  Batch [1060/1299] D_loss: -1.6074, G_loss: 0.4494\n",
      "  Batch [1070/1299] D_loss: -2.4690, G_loss: -0.1462\n",
      "  Batch [1080/1299] D_loss: -1.9361, G_loss: 0.0376\n",
      "  Batch [1090/1299] D_loss: -1.9664, G_loss: 0.0928\n",
      "  Batch [1100/1299] D_loss: -2.9722, G_loss: 2.4075\n",
      "  Batch [1110/1299] D_loss: -1.5957, G_loss: 4.5480\n",
      "  Batch [1120/1299] D_loss: -2.9363, G_loss: 5.9482\n",
      "  Batch [1130/1299] D_loss: -1.8356, G_loss: 3.6157\n",
      "  Batch [1140/1299] D_loss: -2.5192, G_loss: 5.9149\n",
      "  Batch [1150/1299] D_loss: -1.9801, G_loss: 5.3080\n",
      "  Batch [1160/1299] D_loss: -2.7581, G_loss: 5.1354\n",
      "  Batch [1170/1299] D_loss: -2.3119, G_loss: 4.2228\n",
      "  Batch [1180/1299] D_loss: -1.9287, G_loss: 5.1315\n",
      "  Batch [1190/1299] D_loss: -1.0239, G_loss: 3.9634\n",
      "  Batch [1200/1299] D_loss: -1.4996, G_loss: 3.0345\n",
      "  Batch [1210/1299] D_loss: -1.3069, G_loss: 4.1783\n",
      "  Batch [1220/1299] D_loss: -2.6821, G_loss: 2.2014\n",
      "  Batch [1230/1299] D_loss: -2.7485, G_loss: 3.0765\n",
      "  Batch [1240/1299] D_loss: -2.5937, G_loss: 4.7538\n",
      "  Batch [1250/1299] D_loss: -2.2115, G_loss: 2.9208\n",
      "  Batch [1260/1299] D_loss: -1.9598, G_loss: 0.9942\n",
      "  Batch [1270/1299] D_loss: -1.5068, G_loss: -1.0409\n",
      "  Batch [1280/1299] D_loss: -2.8347, G_loss: 1.0966\n",
      "  Batch [1290/1299] D_loss: -1.5347, G_loss: 3.6547\n",
      "\n",
      "Epoch 90 Summary:\n",
      "  Average D_loss: -1.8549\n",
      "  Average G_loss: 1.8153\n",
      "\n",
      "Epoch [91/100]\n",
      "  Batch [0/1299] D_loss: -1.8621, G_loss: 1.5532\n",
      "  Batch [10/1299] D_loss: -1.8821, G_loss: 2.4004\n",
      "  Batch [20/1299] D_loss: -2.5537, G_loss: 1.3040\n",
      "  Batch [30/1299] D_loss: -2.3045, G_loss: 3.0188\n",
      "  Batch [40/1299] D_loss: -1.5870, G_loss: 2.7279\n",
      "  Batch [50/1299] D_loss: -1.8560, G_loss: 3.1363\n",
      "  Batch [60/1299] D_loss: -0.8578, G_loss: 4.1715\n",
      "  Batch [70/1299] D_loss: -2.5650, G_loss: 4.1736\n",
      "  Batch [80/1299] D_loss: -1.9923, G_loss: 1.3966\n",
      "  Batch [90/1299] D_loss: -2.1170, G_loss: 0.4256\n",
      "  Batch [100/1299] D_loss: -2.3964, G_loss: 0.5406\n",
      "  Batch [110/1299] D_loss: -0.6693, G_loss: 1.1900\n",
      "  Batch [120/1299] D_loss: -1.3677, G_loss: 0.6376\n",
      "  Batch [130/1299] D_loss: -0.4111, G_loss: -1.0729\n",
      "  Batch [140/1299] D_loss: -2.0816, G_loss: -0.4412\n",
      "  Batch [150/1299] D_loss: -2.8321, G_loss: -3.4513\n",
      "  Batch [160/1299] D_loss: -2.3266, G_loss: -1.6660\n",
      "  Batch [170/1299] D_loss: -1.8227, G_loss: 1.1268\n",
      "  Batch [180/1299] D_loss: -1.7623, G_loss: 1.1741\n",
      "  Batch [190/1299] D_loss: -1.2785, G_loss: 3.3812\n",
      "  Batch [200/1299] D_loss: -1.1324, G_loss: 3.4326\n",
      "  Batch [210/1299] D_loss: -2.3508, G_loss: 5.1063\n",
      "  Batch [220/1299] D_loss: -0.7873, G_loss: 6.2336\n",
      "  Batch [230/1299] D_loss: -2.8161, G_loss: 3.6397\n",
      "  Batch [240/1299] D_loss: -2.6731, G_loss: 2.9085\n",
      "  Batch [250/1299] D_loss: -2.2355, G_loss: 0.8331\n",
      "  Batch [260/1299] D_loss: -0.5819, G_loss: 3.3038\n",
      "  Batch [270/1299] D_loss: -3.2440, G_loss: 3.8820\n",
      "  Batch [280/1299] D_loss: -2.1888, G_loss: 3.2055\n",
      "  Batch [290/1299] D_loss: -1.7966, G_loss: 3.8604\n",
      "  Batch [300/1299] D_loss: -2.2073, G_loss: 1.6189\n",
      "  Batch [310/1299] D_loss: -1.7792, G_loss: 2.7621\n",
      "  Batch [320/1299] D_loss: -1.2616, G_loss: 1.9386\n",
      "  Batch [330/1299] D_loss: -2.0397, G_loss: 1.1895\n",
      "  Batch [340/1299] D_loss: -1.7102, G_loss: 1.0772\n",
      "  Batch [350/1299] D_loss: -2.1229, G_loss: 0.9042\n",
      "  Batch [360/1299] D_loss: -1.4304, G_loss: -0.1392\n",
      "  Batch [370/1299] D_loss: -1.8060, G_loss: 0.4983\n",
      "  Batch [380/1299] D_loss: -2.6523, G_loss: -0.5663\n",
      "  Batch [390/1299] D_loss: -2.3760, G_loss: 1.3477\n",
      "  Batch [400/1299] D_loss: -1.6627, G_loss: 3.4204\n",
      "  Batch [410/1299] D_loss: -3.1580, G_loss: 4.9927\n",
      "  Batch [420/1299] D_loss: -2.0195, G_loss: 4.5570\n",
      "  Batch [430/1299] D_loss: -3.1262, G_loss: 4.3232\n",
      "  Batch [440/1299] D_loss: -1.2454, G_loss: 5.9266\n",
      "  Batch [450/1299] D_loss: -1.0940, G_loss: 7.5234\n",
      "  Batch [460/1299] D_loss: -2.0692, G_loss: 4.4321\n",
      "  Batch [470/1299] D_loss: -3.1902, G_loss: 2.6013\n",
      "  Batch [480/1299] D_loss: -1.2862, G_loss: 3.8605\n",
      "  Batch [490/1299] D_loss: -2.7177, G_loss: 4.3762\n",
      "  Batch [500/1299] D_loss: -1.3534, G_loss: 1.8699\n",
      "  Batch [510/1299] D_loss: -2.6972, G_loss: 3.0385\n",
      "  Batch [520/1299] D_loss: -1.8189, G_loss: 4.8578\n",
      "  Batch [530/1299] D_loss: -2.0799, G_loss: 4.3027\n",
      "  Batch [540/1299] D_loss: -2.7580, G_loss: 4.2345\n",
      "  Batch [550/1299] D_loss: -1.4761, G_loss: 1.9264\n",
      "  Batch [560/1299] D_loss: -1.2492, G_loss: 2.1104\n",
      "  Batch [570/1299] D_loss: -1.0641, G_loss: 0.5690\n",
      "  Batch [580/1299] D_loss: -1.9985, G_loss: 1.8679\n",
      "  Batch [590/1299] D_loss: -2.2292, G_loss: 2.1783\n",
      "  Batch [600/1299] D_loss: -2.5532, G_loss: 5.5260\n",
      "  Batch [610/1299] D_loss: -0.2660, G_loss: 2.6354\n",
      "  Batch [620/1299] D_loss: -1.5132, G_loss: 2.2359\n",
      "  Batch [630/1299] D_loss: -1.4301, G_loss: 2.1983\n",
      "  Batch [640/1299] D_loss: -2.0177, G_loss: 3.1435\n",
      "  Batch [650/1299] D_loss: -1.0707, G_loss: 2.4589\n",
      "  Batch [660/1299] D_loss: -1.2616, G_loss: 4.3745\n",
      "  Batch [670/1299] D_loss: -2.6745, G_loss: 2.8399\n",
      "  Batch [680/1299] D_loss: -2.9337, G_loss: 1.5163\n",
      "  Batch [690/1299] D_loss: -2.0017, G_loss: 0.0066\n",
      "  Batch [700/1299] D_loss: -1.4298, G_loss: 1.4351\n",
      "  Batch [710/1299] D_loss: -2.2716, G_loss: -0.8361\n",
      "  Batch [720/1299] D_loss: -1.5252, G_loss: -2.0992\n",
      "  Batch [730/1299] D_loss: -1.7502, G_loss: -5.9571\n",
      "  Batch [740/1299] D_loss: -1.8044, G_loss: -5.6340\n",
      "  Batch [750/1299] D_loss: -2.0397, G_loss: -7.1640\n",
      "  Batch [760/1299] D_loss: -1.1176, G_loss: -7.6072\n",
      "  Batch [770/1299] D_loss: -2.5450, G_loss: -6.5541\n",
      "  Batch [780/1299] D_loss: -3.5180, G_loss: -5.9011\n",
      "  Batch [790/1299] D_loss: -2.0777, G_loss: -3.4059\n",
      "  Batch [800/1299] D_loss: -2.8005, G_loss: -0.3395\n",
      "  Batch [810/1299] D_loss: -2.6540, G_loss: 1.5476\n",
      "  Batch [820/1299] D_loss: -1.4888, G_loss: 1.0800\n",
      "  Batch [830/1299] D_loss: -3.1843, G_loss: 3.3902\n",
      "  Batch [840/1299] D_loss: -3.3144, G_loss: 5.3012\n",
      "  Batch [850/1299] D_loss: -0.8241, G_loss: 4.4591\n",
      "  Batch [860/1299] D_loss: -1.1073, G_loss: 2.0963\n",
      "  Batch [870/1299] D_loss: -3.0485, G_loss: 4.5780\n",
      "  Batch [880/1299] D_loss: -1.6797, G_loss: 3.7696\n",
      "  Batch [890/1299] D_loss: -3.1007, G_loss: 3.6934\n",
      "  Batch [900/1299] D_loss: -1.6382, G_loss: 2.6869\n",
      "  Batch [910/1299] D_loss: -1.5142, G_loss: 0.8065\n",
      "  Batch [920/1299] D_loss: -1.0657, G_loss: 2.2930\n",
      "  Batch [930/1299] D_loss: -1.2614, G_loss: 4.0730\n",
      "  Batch [940/1299] D_loss: -2.4050, G_loss: 3.7557\n",
      "  Batch [950/1299] D_loss: -1.3077, G_loss: 4.3518\n",
      "  Batch [960/1299] D_loss: -2.1189, G_loss: 3.5437\n",
      "  Batch [970/1299] D_loss: -1.2710, G_loss: 5.8109\n",
      "  Batch [980/1299] D_loss: -1.5380, G_loss: 5.6860\n",
      "  Batch [990/1299] D_loss: -2.2051, G_loss: 4.0825\n",
      "  Batch [1000/1299] D_loss: -1.5666, G_loss: 3.3978\n",
      "  Batch [1010/1299] D_loss: -1.3728, G_loss: 3.3716\n",
      "  Batch [1020/1299] D_loss: -1.5982, G_loss: 4.4645\n",
      "  Batch [1030/1299] D_loss: -1.9506, G_loss: 3.7965\n",
      "  Batch [1040/1299] D_loss: -1.8643, G_loss: 5.4217\n",
      "  Batch [1050/1299] D_loss: -2.0380, G_loss: 4.0769\n",
      "  Batch [1060/1299] D_loss: -1.4200, G_loss: 2.5079\n",
      "  Batch [1070/1299] D_loss: -1.4841, G_loss: 1.2471\n",
      "  Batch [1080/1299] D_loss: -2.5794, G_loss: 2.9441\n",
      "  Batch [1090/1299] D_loss: -2.5782, G_loss: 2.1205\n",
      "  Batch [1100/1299] D_loss: -1.6293, G_loss: 6.4417\n",
      "  Batch [1110/1299] D_loss: -1.1871, G_loss: 5.3864\n",
      "  Batch [1120/1299] D_loss: -2.5405, G_loss: 2.1572\n",
      "  Batch [1130/1299] D_loss: -0.9622, G_loss: 0.6572\n",
      "  Batch [1140/1299] D_loss: -2.8527, G_loss: 1.2677\n",
      "  Batch [1150/1299] D_loss: -1.7240, G_loss: 4.0046\n",
      "  Batch [1160/1299] D_loss: -2.2824, G_loss: 4.6648\n",
      "  Batch [1170/1299] D_loss: -2.1349, G_loss: 5.6543\n",
      "  Batch [1180/1299] D_loss: -2.9818, G_loss: 5.3606\n",
      "  Batch [1190/1299] D_loss: -1.9869, G_loss: 6.3139\n",
      "  Batch [1200/1299] D_loss: -1.2883, G_loss: 7.4985\n",
      "  Batch [1210/1299] D_loss: -1.6585, G_loss: 5.0560\n",
      "  Batch [1220/1299] D_loss: -3.1739, G_loss: 4.7336\n",
      "  Batch [1230/1299] D_loss: -3.1258, G_loss: 3.5292\n",
      "  Batch [1240/1299] D_loss: -1.6408, G_loss: 3.7308\n",
      "  Batch [1250/1299] D_loss: -2.0695, G_loss: 3.0152\n",
      "  Batch [1260/1299] D_loss: -1.6813, G_loss: 2.2604\n",
      "  Batch [1270/1299] D_loss: -2.5707, G_loss: 1.8691\n",
      "  Batch [1280/1299] D_loss: -2.1884, G_loss: 0.5016\n",
      "  Batch [1290/1299] D_loss: -2.7117, G_loss: -2.2864\n",
      "\n",
      "Epoch 91 Summary:\n",
      "  Average D_loss: -1.8325\n",
      "  Average G_loss: 2.3004\n",
      "\n",
      "Epoch [92/100]\n",
      "  Batch [0/1299] D_loss: -1.9083, G_loss: -3.4405\n",
      "  Batch [10/1299] D_loss: -2.6022, G_loss: -2.5606\n",
      "  Batch [20/1299] D_loss: -2.1534, G_loss: -2.6801\n",
      "  Batch [30/1299] D_loss: -3.0990, G_loss: -3.4576\n",
      "  Batch [40/1299] D_loss: -1.9906, G_loss: -3.0874\n",
      "  Batch [50/1299] D_loss: -1.8761, G_loss: -2.1345\n",
      "  Batch [60/1299] D_loss: -2.6104, G_loss: 1.4493\n",
      "  Batch [70/1299] D_loss: -1.8277, G_loss: 4.3152\n",
      "  Batch [80/1299] D_loss: -1.2876, G_loss: 6.1214\n",
      "  Batch [90/1299] D_loss: -1.5974, G_loss: 4.5569\n",
      "  Batch [100/1299] D_loss: -1.8947, G_loss: 3.4482\n",
      "  Batch [110/1299] D_loss: -1.8826, G_loss: 1.4606\n",
      "  Batch [120/1299] D_loss: -2.2030, G_loss: 2.4610\n",
      "  Batch [130/1299] D_loss: -1.9566, G_loss: 6.2212\n",
      "  Batch [140/1299] D_loss: -1.2644, G_loss: 3.6391\n",
      "  Batch [150/1299] D_loss: -2.3305, G_loss: 3.5727\n",
      "  Batch [160/1299] D_loss: -1.1319, G_loss: 1.7077\n",
      "  Batch [170/1299] D_loss: -2.7019, G_loss: 3.5479\n",
      "  Batch [180/1299] D_loss: -1.4752, G_loss: 3.6744\n",
      "  Batch [190/1299] D_loss: -2.5802, G_loss: 3.8999\n",
      "  Batch [200/1299] D_loss: -2.9007, G_loss: 4.6073\n",
      "  Batch [210/1299] D_loss: -1.8830, G_loss: 5.5209\n",
      "  Batch [220/1299] D_loss: -3.0785, G_loss: 1.8842\n",
      "  Batch [230/1299] D_loss: -1.2115, G_loss: 1.1453\n",
      "  Batch [240/1299] D_loss: -1.0964, G_loss: 1.7248\n",
      "  Batch [250/1299] D_loss: -2.0141, G_loss: 2.0263\n",
      "  Batch [260/1299] D_loss: -1.6905, G_loss: 2.3001\n",
      "  Batch [270/1299] D_loss: -2.1589, G_loss: 0.8773\n",
      "  Batch [280/1299] D_loss: -2.6315, G_loss: 0.7357\n",
      "  Batch [290/1299] D_loss: -3.4424, G_loss: 1.1458\n",
      "  Batch [300/1299] D_loss: -2.3516, G_loss: 2.9657\n",
      "  Batch [310/1299] D_loss: -2.5568, G_loss: 3.8046\n",
      "  Batch [320/1299] D_loss: -2.3761, G_loss: 5.6847\n",
      "  Batch [330/1299] D_loss: -0.9940, G_loss: 2.8784\n",
      "  Batch [340/1299] D_loss: -1.6578, G_loss: 4.0861\n",
      "  Batch [350/1299] D_loss: -0.5383, G_loss: 2.7908\n",
      "  Batch [360/1299] D_loss: -2.0350, G_loss: 4.5604\n",
      "  Batch [370/1299] D_loss: -2.7069, G_loss: 5.2090\n",
      "  Batch [380/1299] D_loss: -1.9702, G_loss: 4.3737\n",
      "  Batch [390/1299] D_loss: -1.4932, G_loss: 6.0733\n",
      "  Batch [400/1299] D_loss: -1.9931, G_loss: 3.9750\n",
      "  Batch [410/1299] D_loss: -3.2990, G_loss: 5.0738\n",
      "  Batch [420/1299] D_loss: -2.1314, G_loss: 3.5819\n",
      "  Batch [430/1299] D_loss: -1.4457, G_loss: 2.3545\n",
      "  Batch [440/1299] D_loss: -2.7114, G_loss: 4.1124\n",
      "  Batch [450/1299] D_loss: -1.8571, G_loss: 4.3212\n",
      "  Batch [460/1299] D_loss: -1.1788, G_loss: 2.0589\n",
      "  Batch [470/1299] D_loss: -1.0603, G_loss: 1.2910\n",
      "  Batch [480/1299] D_loss: -2.0503, G_loss: 2.8724\n",
      "  Batch [490/1299] D_loss: -1.6443, G_loss: 4.9219\n",
      "  Batch [500/1299] D_loss: -1.4737, G_loss: 2.6786\n",
      "  Batch [510/1299] D_loss: -1.6903, G_loss: 2.9214\n",
      "  Batch [520/1299] D_loss: -2.0243, G_loss: 2.6607\n",
      "  Batch [530/1299] D_loss: -0.5775, G_loss: 1.3445\n",
      "  Batch [540/1299] D_loss: -1.8436, G_loss: 2.3937\n",
      "  Batch [550/1299] D_loss: -1.4207, G_loss: 2.0346\n",
      "  Batch [560/1299] D_loss: -0.1657, G_loss: 1.8665\n",
      "  Batch [570/1299] D_loss: -3.0292, G_loss: 1.9801\n",
      "  Batch [580/1299] D_loss: -1.5447, G_loss: 2.5181\n",
      "  Batch [590/1299] D_loss: -2.2737, G_loss: 0.7893\n",
      "  Batch [600/1299] D_loss: -2.0642, G_loss: 1.2813\n",
      "  Batch [610/1299] D_loss: -1.8717, G_loss: 0.6819\n",
      "  Batch [620/1299] D_loss: -2.7614, G_loss: 0.5741\n",
      "  Batch [630/1299] D_loss: -1.9594, G_loss: 0.1144\n",
      "  Batch [640/1299] D_loss: -2.1939, G_loss: -0.4736\n",
      "  Batch [650/1299] D_loss: -1.5074, G_loss: -1.8360\n",
      "  Batch [660/1299] D_loss: -2.0249, G_loss: -2.7075\n",
      "  Batch [670/1299] D_loss: -1.8704, G_loss: -4.8945\n",
      "  Batch [680/1299] D_loss: -1.9061, G_loss: -4.3229\n",
      "  Batch [690/1299] D_loss: -1.5487, G_loss: -4.7488\n",
      "  Batch [700/1299] D_loss: -1.8345, G_loss: -2.8325\n",
      "  Batch [710/1299] D_loss: -2.4529, G_loss: -4.5063\n",
      "  Batch [720/1299] D_loss: -3.2169, G_loss: -3.6327\n",
      "  Batch [730/1299] D_loss: -0.3132, G_loss: -4.9874\n",
      "  Batch [740/1299] D_loss: -1.8685, G_loss: -4.1231\n",
      "  Batch [750/1299] D_loss: -2.6555, G_loss: -1.8804\n",
      "  Batch [760/1299] D_loss: -1.8732, G_loss: -2.0048\n",
      "  Batch [770/1299] D_loss: -1.9232, G_loss: 1.2529\n",
      "  Batch [780/1299] D_loss: -1.5912, G_loss: 4.1161\n",
      "  Batch [790/1299] D_loss: -1.7839, G_loss: 4.9660\n",
      "  Batch [800/1299] D_loss: -2.1501, G_loss: 3.9976\n",
      "  Batch [810/1299] D_loss: -1.8388, G_loss: 4.5893\n",
      "  Batch [820/1299] D_loss: -2.9180, G_loss: 4.0203\n",
      "  Batch [830/1299] D_loss: -1.6147, G_loss: 2.8731\n",
      "  Batch [840/1299] D_loss: -2.0015, G_loss: 3.0803\n",
      "  Batch [850/1299] D_loss: -1.7713, G_loss: 4.4222\n",
      "  Batch [860/1299] D_loss: -1.1982, G_loss: 1.7726\n",
      "  Batch [870/1299] D_loss: -2.0028, G_loss: 1.5933\n",
      "  Batch [880/1299] D_loss: -1.5174, G_loss: 1.1595\n",
      "  Batch [890/1299] D_loss: -1.4683, G_loss: 2.1645\n",
      "  Batch [900/1299] D_loss: -1.7286, G_loss: 1.3678\n",
      "  Batch [910/1299] D_loss: -2.2090, G_loss: 3.5426\n",
      "  Batch [920/1299] D_loss: -3.5102, G_loss: 2.4177\n",
      "  Batch [930/1299] D_loss: -2.5521, G_loss: 2.3933\n",
      "  Batch [940/1299] D_loss: -2.1067, G_loss: 3.7390\n",
      "  Batch [950/1299] D_loss: -1.6529, G_loss: 3.9909\n",
      "  Batch [960/1299] D_loss: -2.3256, G_loss: 3.1364\n",
      "  Batch [970/1299] D_loss: -2.2821, G_loss: 2.5337\n",
      "  Batch [980/1299] D_loss: -1.6110, G_loss: 1.2910\n",
      "  Batch [990/1299] D_loss: -1.6218, G_loss: 2.8440\n",
      "  Batch [1000/1299] D_loss: -1.6906, G_loss: 2.0740\n",
      "  Batch [1010/1299] D_loss: -3.8488, G_loss: 3.1815\n",
      "  Batch [1020/1299] D_loss: -1.0693, G_loss: 2.2709\n",
      "  Batch [1030/1299] D_loss: -2.1944, G_loss: 2.2446\n",
      "  Batch [1040/1299] D_loss: -1.3788, G_loss: 0.3753\n",
      "  Batch [1050/1299] D_loss: -1.6826, G_loss: 2.6956\n",
      "  Batch [1060/1299] D_loss: -1.5919, G_loss: 1.0741\n",
      "  Batch [1070/1299] D_loss: -2.3269, G_loss: 1.9878\n",
      "  Batch [1080/1299] D_loss: -2.0476, G_loss: 1.7919\n",
      "  Batch [1090/1299] D_loss: -1.7066, G_loss: 2.8137\n",
      "  Batch [1100/1299] D_loss: -2.6230, G_loss: 4.6826\n",
      "  Batch [1110/1299] D_loss: -2.6707, G_loss: 1.6289\n",
      "  Batch [1120/1299] D_loss: -2.2003, G_loss: 2.2058\n",
      "  Batch [1130/1299] D_loss: -2.2697, G_loss: -0.4961\n",
      "  Batch [1140/1299] D_loss: -2.2445, G_loss: 1.0121\n",
      "  Batch [1150/1299] D_loss: -1.7443, G_loss: -0.2570\n",
      "  Batch [1160/1299] D_loss: -2.4716, G_loss: -1.1843\n",
      "  Batch [1170/1299] D_loss: -0.9057, G_loss: -1.1364\n",
      "  Batch [1180/1299] D_loss: -1.7667, G_loss: -0.4528\n",
      "  Batch [1190/1299] D_loss: -2.1657, G_loss: 3.4346\n",
      "  Batch [1200/1299] D_loss: -3.1626, G_loss: 6.9942\n",
      "  Batch [1210/1299] D_loss: -2.3574, G_loss: 5.2323\n",
      "  Batch [1220/1299] D_loss: -1.6867, G_loss: 5.2413\n",
      "  Batch [1230/1299] D_loss: -0.8294, G_loss: 4.7980\n",
      "  Batch [1240/1299] D_loss: -1.8564, G_loss: 3.7636\n",
      "  Batch [1250/1299] D_loss: -1.4188, G_loss: 2.8878\n",
      "  Batch [1260/1299] D_loss: -1.9285, G_loss: 2.1297\n",
      "  Batch [1270/1299] D_loss: -2.7829, G_loss: 0.9925\n",
      "  Batch [1280/1299] D_loss: -1.7853, G_loss: -1.8632\n",
      "  Batch [1290/1299] D_loss: -1.3469, G_loss: 0.6703\n",
      "\n",
      "Epoch 92 Summary:\n",
      "  Average D_loss: -1.8118\n",
      "  Average G_loss: 1.8791\n",
      "\n",
      "Epoch [93/100]\n",
      "  Batch [0/1299] D_loss: -1.2718, G_loss: -1.0453\n",
      "  Batch [10/1299] D_loss: -1.6157, G_loss: -1.4402\n",
      "  Batch [20/1299] D_loss: -0.6553, G_loss: 1.5591\n",
      "  Batch [30/1299] D_loss: -2.0642, G_loss: 1.7264\n",
      "  Batch [40/1299] D_loss: -2.1185, G_loss: 4.0437\n",
      "  Batch [50/1299] D_loss: -2.3684, G_loss: 5.4055\n",
      "  Batch [60/1299] D_loss: -1.6386, G_loss: 4.4114\n",
      "  Batch [70/1299] D_loss: -2.3673, G_loss: 3.9808\n",
      "  Batch [80/1299] D_loss: -2.1149, G_loss: 4.1332\n",
      "  Batch [90/1299] D_loss: -2.1088, G_loss: 4.3407\n",
      "  Batch [100/1299] D_loss: -1.7807, G_loss: 2.0676\n",
      "  Batch [110/1299] D_loss: -1.5424, G_loss: 1.4468\n",
      "  Batch [120/1299] D_loss: -0.7442, G_loss: 2.4324\n",
      "  Batch [130/1299] D_loss: -1.9917, G_loss: 3.8867\n",
      "  Batch [140/1299] D_loss: -2.7080, G_loss: 2.0311\n",
      "  Batch [150/1299] D_loss: -0.7393, G_loss: 3.4936\n",
      "  Batch [160/1299] D_loss: -2.5889, G_loss: -0.9595\n",
      "  Batch [170/1299] D_loss: -2.5338, G_loss: 1.9854\n",
      "  Batch [180/1299] D_loss: -1.5846, G_loss: 3.8583\n",
      "  Batch [190/1299] D_loss: -2.5446, G_loss: 3.2791\n",
      "  Batch [200/1299] D_loss: -1.7463, G_loss: 4.2385\n",
      "  Batch [210/1299] D_loss: -0.9488, G_loss: 1.9982\n",
      "  Batch [220/1299] D_loss: -2.1273, G_loss: 1.7220\n",
      "  Batch [230/1299] D_loss: -2.0733, G_loss: 3.7015\n",
      "  Batch [240/1299] D_loss: -1.6138, G_loss: 3.3326\n",
      "  Batch [250/1299] D_loss: -2.4884, G_loss: 2.8112\n",
      "  Batch [260/1299] D_loss: -0.8678, G_loss: 1.9892\n",
      "  Batch [270/1299] D_loss: -3.0421, G_loss: 1.2854\n",
      "  Batch [280/1299] D_loss: -1.7192, G_loss: 0.3155\n",
      "  Batch [290/1299] D_loss: -0.7222, G_loss: 1.5730\n",
      "  Batch [300/1299] D_loss: -1.9188, G_loss: 2.8278\n",
      "  Batch [310/1299] D_loss: -2.1284, G_loss: 1.0975\n",
      "  Batch [320/1299] D_loss: -2.8695, G_loss: 2.1337\n",
      "  Batch [330/1299] D_loss: -1.8595, G_loss: 4.3850\n",
      "  Batch [340/1299] D_loss: -0.7053, G_loss: 2.3692\n",
      "  Batch [350/1299] D_loss: -1.6766, G_loss: 1.8583\n",
      "  Batch [360/1299] D_loss: -1.4774, G_loss: 2.5967\n",
      "  Batch [370/1299] D_loss: -0.8225, G_loss: 1.5662\n",
      "  Batch [380/1299] D_loss: -1.9487, G_loss: 1.8063\n",
      "  Batch [390/1299] D_loss: -3.0431, G_loss: 8.5635\n",
      "  Batch [400/1299] D_loss: -2.1817, G_loss: 5.5745\n",
      "  Batch [410/1299] D_loss: -2.3999, G_loss: 7.2716\n",
      "  Batch [420/1299] D_loss: -2.7883, G_loss: 4.0671\n",
      "  Batch [430/1299] D_loss: -2.5332, G_loss: 2.3683\n",
      "  Batch [440/1299] D_loss: -1.6907, G_loss: 1.8387\n",
      "  Batch [450/1299] D_loss: -1.7511, G_loss: 1.5640\n",
      "  Batch [460/1299] D_loss: -2.6776, G_loss: 3.8742\n",
      "  Batch [470/1299] D_loss: -1.8166, G_loss: 0.3357\n",
      "  Batch [480/1299] D_loss: -1.5167, G_loss: 4.8417\n",
      "  Batch [490/1299] D_loss: -1.4815, G_loss: 1.8564\n",
      "  Batch [500/1299] D_loss: -1.9992, G_loss: -0.8372\n",
      "  Batch [510/1299] D_loss: -2.4444, G_loss: 4.0691\n",
      "  Batch [520/1299] D_loss: -1.9098, G_loss: 3.8030\n",
      "  Batch [530/1299] D_loss: -1.8021, G_loss: 6.0813\n",
      "  Batch [540/1299] D_loss: -1.7709, G_loss: 0.7656\n",
      "  Batch [550/1299] D_loss: -2.3460, G_loss: 3.3208\n",
      "  Batch [560/1299] D_loss: -2.5988, G_loss: 5.2861\n",
      "  Batch [570/1299] D_loss: -2.4453, G_loss: 7.0282\n",
      "  Batch [580/1299] D_loss: -1.0308, G_loss: 5.0444\n",
      "  Batch [590/1299] D_loss: -1.5894, G_loss: 2.8174\n",
      "  Batch [600/1299] D_loss: -2.4501, G_loss: 1.6036\n",
      "  Batch [610/1299] D_loss: -1.4203, G_loss: 0.2259\n",
      "  Batch [620/1299] D_loss: -2.4018, G_loss: 1.8761\n",
      "  Batch [630/1299] D_loss: -2.5536, G_loss: 0.1237\n",
      "  Batch [640/1299] D_loss: -2.0034, G_loss: -0.3353\n",
      "  Batch [650/1299] D_loss: -2.5844, G_loss: -0.0930\n",
      "  Batch [660/1299] D_loss: -0.5379, G_loss: 3.2755\n",
      "  Batch [670/1299] D_loss: -1.5068, G_loss: 4.9540\n",
      "  Batch [680/1299] D_loss: -1.4226, G_loss: 2.3217\n",
      "  Batch [690/1299] D_loss: -1.6658, G_loss: 2.8439\n",
      "  Batch [700/1299] D_loss: -2.1568, G_loss: 2.8920\n",
      "  Batch [710/1299] D_loss: -1.8315, G_loss: 2.9410\n",
      "  Batch [720/1299] D_loss: -1.6733, G_loss: 4.6267\n",
      "  Batch [730/1299] D_loss: -1.9138, G_loss: 2.5019\n",
      "  Batch [740/1299] D_loss: -1.8186, G_loss: 4.2491\n",
      "  Batch [750/1299] D_loss: -1.5239, G_loss: 3.3863\n",
      "  Batch [760/1299] D_loss: -1.9091, G_loss: 4.1710\n",
      "  Batch [770/1299] D_loss: -2.8905, G_loss: 2.2160\n",
      "  Batch [780/1299] D_loss: -1.1596, G_loss: 0.9562\n",
      "  Batch [790/1299] D_loss: -1.0859, G_loss: 3.4154\n",
      "  Batch [800/1299] D_loss: -1.1060, G_loss: 0.7243\n",
      "  Batch [810/1299] D_loss: -2.7344, G_loss: 1.2806\n",
      "  Batch [820/1299] D_loss: -1.4496, G_loss: 2.9217\n",
      "  Batch [830/1299] D_loss: -1.3099, G_loss: 3.5695\n",
      "  Batch [840/1299] D_loss: -1.4671, G_loss: 0.3746\n",
      "  Batch [850/1299] D_loss: -2.4655, G_loss: 2.9193\n",
      "  Batch [860/1299] D_loss: -1.5919, G_loss: 3.9484\n",
      "  Batch [870/1299] D_loss: -1.4302, G_loss: 6.4772\n",
      "  Batch [880/1299] D_loss: -0.8703, G_loss: 4.8224\n",
      "  Batch [890/1299] D_loss: -2.6260, G_loss: 4.4153\n",
      "  Batch [900/1299] D_loss: -1.1080, G_loss: 3.5277\n",
      "  Batch [910/1299] D_loss: -2.1062, G_loss: 0.7518\n",
      "  Batch [920/1299] D_loss: -2.5815, G_loss: 3.6001\n",
      "  Batch [930/1299] D_loss: -1.5315, G_loss: 2.7043\n",
      "  Batch [940/1299] D_loss: -1.8920, G_loss: 2.1488\n",
      "  Batch [950/1299] D_loss: -1.4749, G_loss: 0.8135\n",
      "  Batch [960/1299] D_loss: -2.2159, G_loss: 1.9279\n",
      "  Batch [970/1299] D_loss: -1.6934, G_loss: 4.0302\n",
      "  Batch [980/1299] D_loss: -2.9753, G_loss: 3.3706\n",
      "  Batch [990/1299] D_loss: -2.9063, G_loss: 1.6136\n",
      "  Batch [1000/1299] D_loss: -1.3660, G_loss: 2.2659\n",
      "  Batch [1010/1299] D_loss: -2.5333, G_loss: 3.2140\n",
      "  Batch [1020/1299] D_loss: -3.3290, G_loss: 5.6785\n",
      "  Batch [1030/1299] D_loss: -2.0837, G_loss: 5.0556\n",
      "  Batch [1040/1299] D_loss: -2.2046, G_loss: 6.3464\n",
      "  Batch [1050/1299] D_loss: -2.0736, G_loss: 2.5458\n",
      "  Batch [1060/1299] D_loss: -1.9821, G_loss: -0.4536\n",
      "  Batch [1070/1299] D_loss: -2.0264, G_loss: -0.2777\n",
      "  Batch [1080/1299] D_loss: -2.1983, G_loss: -0.1648\n",
      "  Batch [1090/1299] D_loss: -0.9342, G_loss: 0.1786\n",
      "  Batch [1100/1299] D_loss: -0.9382, G_loss: 1.6880\n",
      "  Batch [1110/1299] D_loss: -1.6860, G_loss: -0.4585\n",
      "  Batch [1120/1299] D_loss: -2.9183, G_loss: 0.1701\n",
      "  Batch [1130/1299] D_loss: -1.9065, G_loss: -0.9270\n",
      "  Batch [1140/1299] D_loss: -1.5536, G_loss: -0.4806\n",
      "  Batch [1150/1299] D_loss: -2.1975, G_loss: 0.6534\n",
      "  Batch [1160/1299] D_loss: -1.7755, G_loss: 2.3731\n",
      "  Batch [1170/1299] D_loss: -3.0093, G_loss: 2.3208\n",
      "  Batch [1180/1299] D_loss: -1.3103, G_loss: 2.8726\n",
      "  Batch [1190/1299] D_loss: -2.5134, G_loss: 2.2190\n",
      "  Batch [1200/1299] D_loss: -1.7635, G_loss: 3.8368\n",
      "  Batch [1210/1299] D_loss: -1.2632, G_loss: 4.1230\n",
      "  Batch [1220/1299] D_loss: -2.4770, G_loss: 5.7746\n",
      "  Batch [1230/1299] D_loss: -1.7475, G_loss: 4.5622\n",
      "  Batch [1240/1299] D_loss: -1.9384, G_loss: 5.9785\n",
      "  Batch [1250/1299] D_loss: -1.1571, G_loss: 6.2834\n",
      "  Batch [1260/1299] D_loss: -1.0948, G_loss: 3.5973\n",
      "  Batch [1270/1299] D_loss: -2.7023, G_loss: 1.6686\n",
      "  Batch [1280/1299] D_loss: -1.2954, G_loss: 1.7654\n",
      "  Batch [1290/1299] D_loss: -1.9827, G_loss: 2.1734\n",
      "\n",
      "Epoch 93 Summary:\n",
      "  Average D_loss: -1.7945\n",
      "  Average G_loss: 2.7967\n",
      "\n",
      "Epoch [94/100]\n",
      "  Batch [0/1299] D_loss: -1.9727, G_loss: 1.6268\n",
      "  Batch [10/1299] D_loss: -2.4906, G_loss: 1.0754\n",
      "  Batch [20/1299] D_loss: -0.5930, G_loss: 1.5202\n",
      "  Batch [30/1299] D_loss: -2.7490, G_loss: 0.7798\n",
      "  Batch [40/1299] D_loss: -1.7878, G_loss: 2.0161\n",
      "  Batch [50/1299] D_loss: -1.8553, G_loss: 3.1780\n",
      "  Batch [60/1299] D_loss: -1.9571, G_loss: 2.2370\n",
      "  Batch [70/1299] D_loss: -2.1835, G_loss: 1.6653\n",
      "  Batch [80/1299] D_loss: -2.0731, G_loss: 6.3935\n",
      "  Batch [90/1299] D_loss: -0.7657, G_loss: 5.4702\n",
      "  Batch [100/1299] D_loss: -2.3568, G_loss: 4.9674\n",
      "  Batch [110/1299] D_loss: -1.5960, G_loss: 3.8348\n",
      "  Batch [120/1299] D_loss: -2.3318, G_loss: 6.6295\n",
      "  Batch [130/1299] D_loss: -2.0678, G_loss: 5.5042\n",
      "  Batch [140/1299] D_loss: -1.8044, G_loss: 5.0529\n",
      "  Batch [150/1299] D_loss: -2.1702, G_loss: 3.7410\n",
      "  Batch [160/1299] D_loss: -2.8180, G_loss: 2.4825\n",
      "  Batch [170/1299] D_loss: -1.4895, G_loss: 2.7725\n",
      "  Batch [180/1299] D_loss: 0.0010, G_loss: 4.8179\n",
      "  Batch [190/1299] D_loss: -1.9148, G_loss: 3.3831\n",
      "  Batch [200/1299] D_loss: -2.1382, G_loss: 0.5112\n",
      "  Batch [210/1299] D_loss: -2.8303, G_loss: 0.2731\n",
      "  Batch [220/1299] D_loss: -2.0619, G_loss: 2.1234\n",
      "  Batch [230/1299] D_loss: -2.2489, G_loss: 3.2022\n",
      "  Batch [240/1299] D_loss: -2.0899, G_loss: 3.2769\n",
      "  Batch [250/1299] D_loss: -2.3659, G_loss: 1.6373\n",
      "  Batch [260/1299] D_loss: -1.9617, G_loss: 0.5611\n",
      "  Batch [270/1299] D_loss: -1.8831, G_loss: 1.4141\n",
      "  Batch [280/1299] D_loss: -2.1112, G_loss: 2.3539\n",
      "  Batch [290/1299] D_loss: -3.1339, G_loss: 4.5853\n",
      "  Batch [300/1299] D_loss: -2.3576, G_loss: 6.2010\n",
      "  Batch [310/1299] D_loss: -2.6354, G_loss: 3.2500\n",
      "  Batch [320/1299] D_loss: -1.8988, G_loss: 2.9900\n",
      "  Batch [330/1299] D_loss: -1.2636, G_loss: 3.2593\n",
      "  Batch [340/1299] D_loss: -2.4230, G_loss: 3.6751\n",
      "  Batch [350/1299] D_loss: -1.6194, G_loss: 2.6755\n",
      "  Batch [360/1299] D_loss: -2.8139, G_loss: 2.2513\n",
      "  Batch [370/1299] D_loss: -1.8845, G_loss: -0.8023\n",
      "  Batch [380/1299] D_loss: -2.2460, G_loss: -0.9221\n",
      "  Batch [390/1299] D_loss: -1.7281, G_loss: -0.1005\n",
      "  Batch [400/1299] D_loss: -2.6103, G_loss: -2.7756\n",
      "  Batch [410/1299] D_loss: -2.1164, G_loss: -3.4064\n",
      "  Batch [420/1299] D_loss: -2.0736, G_loss: -2.4751\n",
      "  Batch [430/1299] D_loss: -2.7444, G_loss: -4.4509\n",
      "  Batch [440/1299] D_loss: -2.2111, G_loss: -5.0494\n",
      "  Batch [450/1299] D_loss: -1.8210, G_loss: -5.3038\n",
      "  Batch [460/1299] D_loss: -2.1454, G_loss: -3.4352\n",
      "  Batch [470/1299] D_loss: -1.2417, G_loss: -1.5639\n",
      "  Batch [480/1299] D_loss: -2.9620, G_loss: -0.2170\n",
      "  Batch [490/1299] D_loss: -1.7742, G_loss: 0.7840\n",
      "  Batch [500/1299] D_loss: -2.5441, G_loss: 3.0583\n",
      "  Batch [510/1299] D_loss: -2.7296, G_loss: 2.0166\n",
      "  Batch [520/1299] D_loss: -2.6332, G_loss: 3.6728\n",
      "  Batch [530/1299] D_loss: -1.8976, G_loss: 5.6170\n",
      "  Batch [540/1299] D_loss: -2.6644, G_loss: 5.1078\n",
      "  Batch [550/1299] D_loss: -1.3690, G_loss: 1.8660\n",
      "  Batch [560/1299] D_loss: -1.5961, G_loss: 1.9339\n",
      "  Batch [570/1299] D_loss: -1.6690, G_loss: 3.9833\n",
      "  Batch [580/1299] D_loss: -1.8889, G_loss: 3.1224\n",
      "  Batch [590/1299] D_loss: -2.4939, G_loss: 2.8705\n",
      "  Batch [600/1299] D_loss: -2.5112, G_loss: 2.3267\n",
      "  Batch [610/1299] D_loss: -0.4650, G_loss: 4.1836\n",
      "  Batch [620/1299] D_loss: -1.5980, G_loss: 0.4827\n",
      "  Batch [630/1299] D_loss: -1.7968, G_loss: 1.6483\n",
      "  Batch [640/1299] D_loss: -0.8476, G_loss: 1.5424\n",
      "  Batch [650/1299] D_loss: -2.5595, G_loss: 5.2746\n",
      "  Batch [660/1299] D_loss: -1.4581, G_loss: 3.2778\n",
      "  Batch [670/1299] D_loss: -2.1692, G_loss: 4.1349\n",
      "  Batch [680/1299] D_loss: -2.0326, G_loss: 5.2643\n",
      "  Batch [690/1299] D_loss: -1.2112, G_loss: 5.5769\n",
      "  Batch [700/1299] D_loss: -3.2692, G_loss: 4.5316\n",
      "  Batch [710/1299] D_loss: -1.2557, G_loss: 6.9604\n",
      "  Batch [720/1299] D_loss: -1.7956, G_loss: 7.1434\n",
      "  Batch [730/1299] D_loss: -1.9308, G_loss: 3.5468\n",
      "  Batch [740/1299] D_loss: -0.9679, G_loss: 3.4819\n",
      "  Batch [750/1299] D_loss: -2.0084, G_loss: 5.0511\n",
      "  Batch [760/1299] D_loss: -2.1551, G_loss: 2.8437\n",
      "  Batch [770/1299] D_loss: -1.2764, G_loss: 3.2957\n",
      "  Batch [780/1299] D_loss: -1.6547, G_loss: 2.9348\n",
      "  Batch [790/1299] D_loss: -2.3421, G_loss: 4.5511\n",
      "  Batch [800/1299] D_loss: -1.2287, G_loss: 2.3988\n",
      "  Batch [810/1299] D_loss: -1.6561, G_loss: 1.2771\n",
      "  Batch [820/1299] D_loss: -1.7619, G_loss: 3.2214\n",
      "  Batch [830/1299] D_loss: -2.1765, G_loss: 1.5147\n",
      "  Batch [840/1299] D_loss: -1.9003, G_loss: 3.5455\n",
      "  Batch [850/1299] D_loss: -1.6441, G_loss: 3.9822\n",
      "  Batch [860/1299] D_loss: -1.6173, G_loss: 3.1591\n",
      "  Batch [870/1299] D_loss: -1.9074, G_loss: 0.5584\n",
      "  Batch [880/1299] D_loss: -2.4432, G_loss: -1.4094\n",
      "  Batch [890/1299] D_loss: -2.3015, G_loss: 0.1127\n",
      "  Batch [900/1299] D_loss: -3.0974, G_loss: 3.0482\n",
      "  Batch [910/1299] D_loss: -1.6156, G_loss: 2.3714\n",
      "  Batch [920/1299] D_loss: -1.5643, G_loss: 1.4220\n",
      "  Batch [930/1299] D_loss: -2.0126, G_loss: 3.6571\n",
      "  Batch [940/1299] D_loss: -2.3948, G_loss: 6.2836\n",
      "  Batch [950/1299] D_loss: -2.3676, G_loss: 5.4333\n",
      "  Batch [960/1299] D_loss: -0.1585, G_loss: 5.5772\n",
      "  Batch [970/1299] D_loss: -4.0157, G_loss: 5.3234\n",
      "  Batch [980/1299] D_loss: -1.9595, G_loss: 7.7736\n",
      "  Batch [990/1299] D_loss: -1.0462, G_loss: 3.9962\n",
      "  Batch [1000/1299] D_loss: -2.2461, G_loss: 3.8469\n",
      "  Batch [1010/1299] D_loss: -1.4733, G_loss: 1.6319\n",
      "  Batch [1020/1299] D_loss: -1.8816, G_loss: 2.5064\n",
      "  Batch [1030/1299] D_loss: -2.5038, G_loss: 3.5693\n",
      "  Batch [1040/1299] D_loss: -1.8485, G_loss: 5.5847\n",
      "  Batch [1050/1299] D_loss: -1.8507, G_loss: 0.2528\n",
      "  Batch [1060/1299] D_loss: -2.2367, G_loss: 2.2610\n",
      "  Batch [1070/1299] D_loss: -2.2533, G_loss: -0.1017\n",
      "  Batch [1080/1299] D_loss: -1.1832, G_loss: 1.6374\n",
      "  Batch [1090/1299] D_loss: -2.0630, G_loss: 2.1703\n",
      "  Batch [1100/1299] D_loss: -1.8032, G_loss: 1.8893\n",
      "  Batch [1110/1299] D_loss: -1.5766, G_loss: 3.3418\n",
      "  Batch [1120/1299] D_loss: -2.0634, G_loss: 3.3871\n",
      "  Batch [1130/1299] D_loss: -2.0996, G_loss: 5.9947\n",
      "  Batch [1140/1299] D_loss: -3.2008, G_loss: 3.5368\n",
      "  Batch [1150/1299] D_loss: -2.0330, G_loss: 3.9479\n",
      "  Batch [1160/1299] D_loss: -1.4063, G_loss: 3.1658\n",
      "  Batch [1170/1299] D_loss: -2.5344, G_loss: 1.2438\n",
      "  Batch [1180/1299] D_loss: -2.2381, G_loss: 1.0339\n",
      "  Batch [1190/1299] D_loss: -2.4674, G_loss: -1.6479\n",
      "  Batch [1200/1299] D_loss: -2.4238, G_loss: -1.7663\n",
      "  Batch [1210/1299] D_loss: -2.5729, G_loss: 1.4865\n",
      "  Batch [1220/1299] D_loss: -1.3631, G_loss: 1.6468\n",
      "  Batch [1230/1299] D_loss: -1.7067, G_loss: 0.7528\n",
      "  Batch [1240/1299] D_loss: -1.5614, G_loss: 1.0278\n",
      "  Batch [1250/1299] D_loss: -2.1627, G_loss: 4.6743\n",
      "  Batch [1260/1299] D_loss: -0.4470, G_loss: 3.4964\n",
      "  Batch [1270/1299] D_loss: -1.8657, G_loss: 3.2421\n",
      "  Batch [1280/1299] D_loss: -3.2218, G_loss: 5.1889\n",
      "  Batch [1290/1299] D_loss: -3.5008, G_loss: 7.1740\n",
      "\n",
      "Epoch 94 Summary:\n",
      "  Average D_loss: -1.7910\n",
      "  Average G_loss: 2.5993\n",
      "\n",
      "Epoch [95/100]\n",
      "  Batch [0/1299] D_loss: -1.6745, G_loss: 5.5026\n",
      "  Batch [10/1299] D_loss: -2.0499, G_loss: 5.3959\n",
      "  Batch [20/1299] D_loss: -2.4067, G_loss: 4.6909\n",
      "  Batch [30/1299] D_loss: -2.5941, G_loss: 5.3731\n",
      "  Batch [40/1299] D_loss: -1.8844, G_loss: 5.8194\n",
      "  Batch [50/1299] D_loss: -1.8135, G_loss: 3.1788\n",
      "  Batch [60/1299] D_loss: -2.9667, G_loss: 4.0279\n",
      "  Batch [70/1299] D_loss: -0.4721, G_loss: 3.8999\n",
      "  Batch [80/1299] D_loss: -2.2091, G_loss: 3.4798\n",
      "  Batch [90/1299] D_loss: -1.0082, G_loss: 3.1437\n",
      "  Batch [100/1299] D_loss: -3.1037, G_loss: 4.5420\n",
      "  Batch [110/1299] D_loss: -1.5621, G_loss: 1.3030\n",
      "  Batch [120/1299] D_loss: -2.5985, G_loss: 3.0847\n",
      "  Batch [130/1299] D_loss: -2.5244, G_loss: 2.7192\n",
      "  Batch [140/1299] D_loss: -1.2612, G_loss: 4.5112\n",
      "  Batch [150/1299] D_loss: -0.9718, G_loss: 4.5161\n",
      "  Batch [160/1299] D_loss: -1.2224, G_loss: 3.1633\n",
      "  Batch [170/1299] D_loss: -1.8894, G_loss: 2.3751\n",
      "  Batch [180/1299] D_loss: -1.2254, G_loss: 4.5919\n",
      "  Batch [190/1299] D_loss: -2.8587, G_loss: 4.2628\n",
      "  Batch [200/1299] D_loss: -1.4543, G_loss: 2.8945\n",
      "  Batch [210/1299] D_loss: -1.1274, G_loss: 0.7121\n",
      "  Batch [220/1299] D_loss: -1.1770, G_loss: -0.1032\n",
      "  Batch [230/1299] D_loss: -2.8163, G_loss: 2.0181\n",
      "  Batch [240/1299] D_loss: -1.5927, G_loss: 2.2511\n",
      "  Batch [250/1299] D_loss: -1.7554, G_loss: 3.7345\n",
      "  Batch [260/1299] D_loss: -1.6458, G_loss: 7.7807\n",
      "  Batch [270/1299] D_loss: -1.6166, G_loss: 4.6035\n",
      "  Batch [280/1299] D_loss: -1.7809, G_loss: 4.5318\n",
      "  Batch [290/1299] D_loss: -2.4482, G_loss: 4.0716\n",
      "  Batch [300/1299] D_loss: -2.1449, G_loss: 3.7337\n",
      "  Batch [310/1299] D_loss: -1.6527, G_loss: 3.5356\n",
      "  Batch [320/1299] D_loss: -1.5614, G_loss: 3.4705\n",
      "  Batch [330/1299] D_loss: -2.1425, G_loss: 2.4880\n",
      "  Batch [340/1299] D_loss: -2.4181, G_loss: 4.2594\n",
      "  Batch [350/1299] D_loss: -0.6980, G_loss: 3.2894\n",
      "  Batch [360/1299] D_loss: -1.0044, G_loss: 4.3326\n",
      "  Batch [370/1299] D_loss: -2.4748, G_loss: 4.9959\n",
      "  Batch [380/1299] D_loss: -1.8678, G_loss: 2.4556\n",
      "  Batch [390/1299] D_loss: -2.2644, G_loss: 0.7025\n",
      "  Batch [400/1299] D_loss: -1.5826, G_loss: 1.7003\n",
      "  Batch [410/1299] D_loss: -2.0804, G_loss: 1.7596\n",
      "  Batch [420/1299] D_loss: -0.4970, G_loss: 0.6170\n",
      "  Batch [430/1299] D_loss: -2.5108, G_loss: 3.7329\n",
      "  Batch [440/1299] D_loss: -1.6882, G_loss: 5.5174\n",
      "  Batch [450/1299] D_loss: -1.7381, G_loss: 6.5745\n",
      "  Batch [460/1299] D_loss: -1.1306, G_loss: 5.1685\n",
      "  Batch [470/1299] D_loss: -0.5168, G_loss: 4.8243\n",
      "  Batch [480/1299] D_loss: -2.0034, G_loss: 1.9865\n",
      "  Batch [490/1299] D_loss: -2.0007, G_loss: 2.3196\n",
      "  Batch [500/1299] D_loss: -2.8299, G_loss: 1.7440\n",
      "  Batch [510/1299] D_loss: -1.3682, G_loss: 1.4912\n",
      "  Batch [520/1299] D_loss: -0.7409, G_loss: 1.0429\n",
      "  Batch [530/1299] D_loss: -2.8349, G_loss: 2.1491\n",
      "  Batch [540/1299] D_loss: -2.4258, G_loss: 2.4111\n",
      "  Batch [550/1299] D_loss: -2.7393, G_loss: 1.8858\n",
      "  Batch [560/1299] D_loss: -1.7880, G_loss: 2.5449\n",
      "  Batch [570/1299] D_loss: -2.2085, G_loss: 4.0723\n",
      "  Batch [580/1299] D_loss: -3.0697, G_loss: 5.3953\n",
      "  Batch [590/1299] D_loss: -2.0560, G_loss: 1.7675\n",
      "  Batch [600/1299] D_loss: -1.1528, G_loss: 2.5010\n",
      "  Batch [610/1299] D_loss: -3.0399, G_loss: 1.9633\n",
      "  Batch [620/1299] D_loss: -1.7175, G_loss: 3.4845\n",
      "  Batch [630/1299] D_loss: -2.5279, G_loss: 4.2048\n",
      "  Batch [640/1299] D_loss: -2.6804, G_loss: 2.4788\n",
      "  Batch [650/1299] D_loss: -1.6045, G_loss: 1.1398\n",
      "  Batch [660/1299] D_loss: -2.3131, G_loss: 1.5355\n",
      "  Batch [670/1299] D_loss: -1.9657, G_loss: -0.4375\n",
      "  Batch [680/1299] D_loss: -2.2915, G_loss: 1.3604\n",
      "  Batch [690/1299] D_loss: -2.0099, G_loss: 2.0688\n",
      "  Batch [700/1299] D_loss: -1.0917, G_loss: 2.5043\n",
      "  Batch [710/1299] D_loss: -2.9510, G_loss: 3.1911\n",
      "  Batch [720/1299] D_loss: -1.5974, G_loss: 2.8474\n",
      "  Batch [730/1299] D_loss: -1.9929, G_loss: 3.0121\n",
      "  Batch [740/1299] D_loss: -0.8748, G_loss: 4.1713\n",
      "  Batch [750/1299] D_loss: -2.7919, G_loss: 2.4158\n",
      "  Batch [760/1299] D_loss: -1.8605, G_loss: 3.8257\n",
      "  Batch [770/1299] D_loss: -2.1740, G_loss: 3.5468\n",
      "  Batch [780/1299] D_loss: -1.3127, G_loss: 3.6561\n",
      "  Batch [790/1299] D_loss: -1.9166, G_loss: 4.7146\n",
      "  Batch [800/1299] D_loss: -1.5106, G_loss: 3.8795\n",
      "  Batch [810/1299] D_loss: -0.6048, G_loss: 4.9879\n",
      "  Batch [820/1299] D_loss: -1.9406, G_loss: 4.8750\n",
      "  Batch [830/1299] D_loss: -2.4993, G_loss: 3.7910\n",
      "  Batch [840/1299] D_loss: -2.5107, G_loss: 1.8053\n",
      "  Batch [850/1299] D_loss: -1.5666, G_loss: 0.1991\n",
      "  Batch [860/1299] D_loss: -3.3762, G_loss: -0.7659\n",
      "  Batch [870/1299] D_loss: -1.9413, G_loss: -0.8112\n",
      "  Batch [880/1299] D_loss: -0.9716, G_loss: -1.0044\n",
      "  Batch [890/1299] D_loss: -1.8052, G_loss: -0.5285\n",
      "  Batch [900/1299] D_loss: -2.4646, G_loss: 0.9271\n",
      "  Batch [910/1299] D_loss: -2.1621, G_loss: 1.4281\n",
      "  Batch [920/1299] D_loss: -1.9345, G_loss: 3.5402\n",
      "  Batch [930/1299] D_loss: -2.3685, G_loss: 6.2976\n",
      "  Batch [940/1299] D_loss: -1.9605, G_loss: 6.4095\n",
      "  Batch [950/1299] D_loss: -1.5641, G_loss: 6.7490\n",
      "  Batch [960/1299] D_loss: -2.5706, G_loss: 6.7455\n",
      "  Batch [970/1299] D_loss: -1.8649, G_loss: 4.7506\n",
      "  Batch [980/1299] D_loss: -2.0898, G_loss: 3.4422\n",
      "  Batch [990/1299] D_loss: -2.1686, G_loss: 3.5353\n",
      "  Batch [1000/1299] D_loss: -2.3861, G_loss: 4.1729\n",
      "  Batch [1010/1299] D_loss: -1.9803, G_loss: 2.3656\n",
      "  Batch [1020/1299] D_loss: -2.3716, G_loss: 1.2594\n",
      "  Batch [1030/1299] D_loss: -0.7663, G_loss: 1.0120\n",
      "  Batch [1040/1299] D_loss: -2.4129, G_loss: -0.0306\n",
      "  Batch [1050/1299] D_loss: -1.4160, G_loss: -1.1719\n",
      "  Batch [1060/1299] D_loss: -2.3517, G_loss: -4.0094\n",
      "  Batch [1070/1299] D_loss: -2.0696, G_loss: -2.8469\n",
      "  Batch [1080/1299] D_loss: -2.8835, G_loss: -1.8152\n",
      "  Batch [1090/1299] D_loss: -2.8955, G_loss: 1.0540\n",
      "  Batch [1100/1299] D_loss: -0.9393, G_loss: 1.5988\n",
      "  Batch [1110/1299] D_loss: -2.7425, G_loss: 4.8319\n",
      "  Batch [1120/1299] D_loss: -1.9243, G_loss: 5.7391\n",
      "  Batch [1130/1299] D_loss: -2.1523, G_loss: 7.0228\n",
      "  Batch [1140/1299] D_loss: -1.8534, G_loss: 5.7598\n",
      "  Batch [1150/1299] D_loss: -1.8437, G_loss: 4.4465\n",
      "  Batch [1160/1299] D_loss: -1.5212, G_loss: 5.3612\n",
      "  Batch [1170/1299] D_loss: -2.1515, G_loss: 3.9381\n",
      "  Batch [1180/1299] D_loss: -2.3741, G_loss: 1.6870\n",
      "  Batch [1190/1299] D_loss: -1.9539, G_loss: 2.3571\n",
      "  Batch [1200/1299] D_loss: -2.1687, G_loss: 2.9149\n",
      "  Batch [1210/1299] D_loss: -2.4987, G_loss: 1.3430\n",
      "  Batch [1220/1299] D_loss: -2.0433, G_loss: 1.4042\n",
      "  Batch [1230/1299] D_loss: -1.6247, G_loss: 2.0608\n",
      "  Batch [1240/1299] D_loss: -2.7265, G_loss: 5.4362\n",
      "  Batch [1250/1299] D_loss: -1.1905, G_loss: 5.8223\n",
      "  Batch [1260/1299] D_loss: -1.1017, G_loss: 2.9585\n",
      "  Batch [1270/1299] D_loss: -2.1845, G_loss: 3.4031\n",
      "  Batch [1280/1299] D_loss: -2.7616, G_loss: 4.0635\n",
      "  Batch [1290/1299] D_loss: -1.5497, G_loss: 4.4825\n",
      "\n",
      "Epoch 95 Summary:\n",
      "  Average D_loss: -1.7885\n",
      "  Average G_loss: 2.9929\n",
      "\n",
      "Epoch [96/100]\n",
      "  Batch [0/1299] D_loss: -0.9355, G_loss: 3.1037\n",
      "  Batch [10/1299] D_loss: -1.9127, G_loss: 2.0078\n",
      "  Batch [20/1299] D_loss: -2.8437, G_loss: 2.4314\n",
      "  Batch [30/1299] D_loss: -2.8702, G_loss: 3.1141\n",
      "  Batch [40/1299] D_loss: -1.9412, G_loss: 3.4637\n",
      "  Batch [50/1299] D_loss: -1.9859, G_loss: 3.6062\n",
      "  Batch [60/1299] D_loss: -1.9672, G_loss: 3.6617\n",
      "  Batch [70/1299] D_loss: -2.6119, G_loss: 3.5160\n",
      "  Batch [80/1299] D_loss: -2.8136, G_loss: 3.7930\n",
      "  Batch [90/1299] D_loss: -1.8399, G_loss: 4.6526\n",
      "  Batch [100/1299] D_loss: -1.3903, G_loss: 3.7465\n",
      "  Batch [110/1299] D_loss: -2.5086, G_loss: 3.4864\n",
      "  Batch [120/1299] D_loss: -2.3781, G_loss: 4.8800\n",
      "  Batch [130/1299] D_loss: -1.5860, G_loss: 3.2215\n",
      "  Batch [140/1299] D_loss: -2.3816, G_loss: 1.7547\n",
      "  Batch [150/1299] D_loss: -2.6906, G_loss: 2.7391\n",
      "  Batch [160/1299] D_loss: -1.3460, G_loss: 1.6507\n",
      "  Batch [170/1299] D_loss: -0.8558, G_loss: 1.6468\n",
      "  Batch [180/1299] D_loss: -1.3094, G_loss: 1.4683\n",
      "  Batch [190/1299] D_loss: -1.3560, G_loss: 1.4185\n",
      "  Batch [200/1299] D_loss: -1.3074, G_loss: 2.8034\n",
      "  Batch [210/1299] D_loss: -1.9942, G_loss: 1.7573\n",
      "  Batch [220/1299] D_loss: -1.5509, G_loss: 1.4961\n",
      "  Batch [230/1299] D_loss: -1.4434, G_loss: 1.8269\n",
      "  Batch [240/1299] D_loss: -2.3415, G_loss: 2.1107\n",
      "  Batch [250/1299] D_loss: -3.1910, G_loss: 3.9336\n",
      "  Batch [260/1299] D_loss: -1.8643, G_loss: 4.3453\n",
      "  Batch [270/1299] D_loss: -3.4848, G_loss: 4.9418\n",
      "  Batch [280/1299] D_loss: -2.5222, G_loss: 2.9610\n",
      "  Batch [290/1299] D_loss: -2.5153, G_loss: 4.3595\n",
      "  Batch [300/1299] D_loss: -1.2170, G_loss: 4.2200\n",
      "  Batch [310/1299] D_loss: -2.4470, G_loss: 4.2382\n",
      "  Batch [320/1299] D_loss: -2.8106, G_loss: 3.2413\n",
      "  Batch [330/1299] D_loss: -2.3961, G_loss: 5.3331\n",
      "  Batch [340/1299] D_loss: -3.0611, G_loss: 5.2891\n",
      "  Batch [350/1299] D_loss: -2.3529, G_loss: 6.5800\n",
      "  Batch [360/1299] D_loss: -2.5465, G_loss: 4.2425\n",
      "  Batch [370/1299] D_loss: -1.2742, G_loss: 3.1097\n",
      "  Batch [380/1299] D_loss: -2.1659, G_loss: 3.5558\n",
      "  Batch [390/1299] D_loss: -1.7334, G_loss: 4.5342\n",
      "  Batch [400/1299] D_loss: -1.4120, G_loss: 2.3080\n",
      "  Batch [410/1299] D_loss: -2.3685, G_loss: 2.8462\n",
      "  Batch [420/1299] D_loss: -1.2625, G_loss: 1.8941\n",
      "  Batch [430/1299] D_loss: -2.2982, G_loss: 3.4402\n",
      "  Batch [440/1299] D_loss: -2.3029, G_loss: 1.9172\n",
      "  Batch [450/1299] D_loss: -1.9277, G_loss: 0.4386\n",
      "  Batch [460/1299] D_loss: -1.9195, G_loss: 0.6141\n",
      "  Batch [470/1299] D_loss: -1.1373, G_loss: -0.9538\n",
      "  Batch [480/1299] D_loss: -0.9875, G_loss: -3.1497\n",
      "  Batch [490/1299] D_loss: -2.1539, G_loss: -2.6525\n",
      "  Batch [500/1299] D_loss: -1.2317, G_loss: -3.1992\n",
      "  Batch [510/1299] D_loss: -2.1121, G_loss: -3.5666\n",
      "  Batch [520/1299] D_loss: -1.7851, G_loss: -3.1882\n",
      "  Batch [530/1299] D_loss: -2.2211, G_loss: -3.6635\n",
      "  Batch [540/1299] D_loss: -1.9803, G_loss: -1.6087\n",
      "  Batch [550/1299] D_loss: -2.0219, G_loss: 1.2914\n",
      "  Batch [560/1299] D_loss: -2.6132, G_loss: 2.0823\n",
      "  Batch [570/1299] D_loss: -2.2414, G_loss: 6.5870\n",
      "  Batch [580/1299] D_loss: -0.8911, G_loss: 5.4561\n",
      "  Batch [590/1299] D_loss: -2.3167, G_loss: 5.6316\n",
      "  Batch [600/1299] D_loss: -1.9270, G_loss: 5.3992\n",
      "  Batch [610/1299] D_loss: -2.1981, G_loss: 5.5823\n",
      "  Batch [620/1299] D_loss: -1.2932, G_loss: 4.8352\n",
      "  Batch [630/1299] D_loss: -1.1419, G_loss: 2.9385\n",
      "  Batch [640/1299] D_loss: -2.0318, G_loss: 2.0364\n",
      "  Batch [650/1299] D_loss: -1.9557, G_loss: 1.1623\n",
      "  Batch [660/1299] D_loss: -1.6575, G_loss: 2.9573\n",
      "  Batch [670/1299] D_loss: -1.0243, G_loss: -0.3990\n",
      "  Batch [680/1299] D_loss: -1.9186, G_loss: -0.5011\n",
      "  Batch [690/1299] D_loss: -1.2756, G_loss: -1.8374\n",
      "  Batch [700/1299] D_loss: -1.7236, G_loss: 0.5816\n",
      "  Batch [710/1299] D_loss: -1.5104, G_loss: -0.1372\n",
      "  Batch [720/1299] D_loss: -2.6643, G_loss: 1.1373\n",
      "  Batch [730/1299] D_loss: -1.7040, G_loss: 2.3927\n",
      "  Batch [740/1299] D_loss: -2.0838, G_loss: 2.7448\n",
      "  Batch [750/1299] D_loss: -1.7036, G_loss: 4.7778\n",
      "  Batch [760/1299] D_loss: -2.3377, G_loss: 3.9331\n",
      "  Batch [770/1299] D_loss: -1.4611, G_loss: 4.7205\n",
      "  Batch [780/1299] D_loss: -1.7966, G_loss: 3.9469\n",
      "  Batch [790/1299] D_loss: -2.1747, G_loss: 2.5470\n",
      "  Batch [800/1299] D_loss: -1.6769, G_loss: 3.5537\n",
      "  Batch [810/1299] D_loss: -1.9842, G_loss: 5.3565\n",
      "  Batch [820/1299] D_loss: -1.3115, G_loss: 3.5133\n",
      "  Batch [830/1299] D_loss: -0.6785, G_loss: 2.9440\n",
      "  Batch [840/1299] D_loss: -2.2511, G_loss: 3.2234\n",
      "  Batch [850/1299] D_loss: -3.1770, G_loss: 4.6706\n",
      "  Batch [860/1299] D_loss: -1.4314, G_loss: 3.2560\n",
      "  Batch [870/1299] D_loss: -1.9704, G_loss: 2.1733\n",
      "  Batch [880/1299] D_loss: -2.6026, G_loss: 0.8601\n",
      "  Batch [890/1299] D_loss: -0.5749, G_loss: 0.5095\n",
      "  Batch [900/1299] D_loss: -2.6241, G_loss: -1.3662\n",
      "  Batch [910/1299] D_loss: -0.8398, G_loss: -2.7376\n",
      "  Batch [920/1299] D_loss: -2.0214, G_loss: -1.1029\n",
      "  Batch [930/1299] D_loss: -1.5277, G_loss: 0.3975\n",
      "  Batch [940/1299] D_loss: -2.7500, G_loss: 2.8107\n",
      "  Batch [950/1299] D_loss: -2.6290, G_loss: 7.1074\n",
      "  Batch [960/1299] D_loss: -1.4160, G_loss: 4.6332\n",
      "  Batch [970/1299] D_loss: -3.0801, G_loss: 5.6482\n",
      "  Batch [980/1299] D_loss: -1.2800, G_loss: 6.0257\n",
      "  Batch [990/1299] D_loss: -1.7580, G_loss: 3.4389\n",
      "  Batch [1000/1299] D_loss: -1.4186, G_loss: 3.0804\n",
      "  Batch [1010/1299] D_loss: -1.5211, G_loss: 2.1296\n",
      "  Batch [1020/1299] D_loss: -1.7378, G_loss: 0.1581\n",
      "  Batch [1030/1299] D_loss: -2.1128, G_loss: -0.7766\n",
      "  Batch [1040/1299] D_loss: -0.6248, G_loss: -0.0145\n",
      "  Batch [1050/1299] D_loss: -1.6781, G_loss: 0.1845\n",
      "  Batch [1060/1299] D_loss: -2.1444, G_loss: 0.4748\n",
      "  Batch [1070/1299] D_loss: -2.9880, G_loss: 6.0011\n",
      "  Batch [1080/1299] D_loss: -2.1658, G_loss: 4.5722\n",
      "  Batch [1090/1299] D_loss: -1.5692, G_loss: 6.0210\n",
      "  Batch [1100/1299] D_loss: -1.4904, G_loss: 7.4387\n",
      "  Batch [1110/1299] D_loss: -2.9574, G_loss: 5.2484\n",
      "  Batch [1120/1299] D_loss: -3.1520, G_loss: 2.6926\n",
      "  Batch [1130/1299] D_loss: -3.2262, G_loss: 2.6072\n",
      "  Batch [1140/1299] D_loss: -2.7991, G_loss: 2.3555\n",
      "  Batch [1150/1299] D_loss: -1.8125, G_loss: 2.6215\n",
      "  Batch [1160/1299] D_loss: -1.5493, G_loss: 3.4018\n",
      "  Batch [1170/1299] D_loss: -1.6598, G_loss: 4.1514\n",
      "  Batch [1180/1299] D_loss: -2.0593, G_loss: 3.4304\n",
      "  Batch [1190/1299] D_loss: -2.0859, G_loss: 2.3535\n",
      "  Batch [1200/1299] D_loss: -2.0872, G_loss: 3.3873\n",
      "  Batch [1210/1299] D_loss: -1.5587, G_loss: 4.2113\n",
      "  Batch [1220/1299] D_loss: -2.7198, G_loss: 3.5143\n",
      "  Batch [1230/1299] D_loss: -1.7645, G_loss: 5.2011\n",
      "  Batch [1240/1299] D_loss: -3.0047, G_loss: 3.1320\n",
      "  Batch [1250/1299] D_loss: -2.4252, G_loss: 5.3619\n",
      "  Batch [1260/1299] D_loss: -1.3524, G_loss: 1.0049\n",
      "  Batch [1270/1299] D_loss: -2.7045, G_loss: 3.0731\n",
      "  Batch [1280/1299] D_loss: -2.4229, G_loss: 3.7567\n",
      "  Batch [1290/1299] D_loss: -1.9247, G_loss: 1.4546\n",
      "\n",
      "Epoch 96 Summary:\n",
      "  Average D_loss: -1.7862\n",
      "  Average G_loss: 2.6644\n",
      "\n",
      "Epoch [97/100]\n",
      "  Batch [0/1299] D_loss: -1.6324, G_loss: -0.0508\n",
      "  Batch [10/1299] D_loss: -1.8302, G_loss: -3.6086\n",
      "  Batch [20/1299] D_loss: -0.4019, G_loss: -0.6398\n",
      "  Batch [30/1299] D_loss: -2.7188, G_loss: 0.8530\n",
      "  Batch [40/1299] D_loss: -1.5196, G_loss: 2.9595\n",
      "  Batch [50/1299] D_loss: -2.2149, G_loss: 1.9986\n",
      "  Batch [60/1299] D_loss: -1.7866, G_loss: 2.2469\n",
      "  Batch [70/1299] D_loss: -1.8648, G_loss: 3.3627\n",
      "  Batch [80/1299] D_loss: -3.1032, G_loss: 2.6919\n",
      "  Batch [90/1299] D_loss: -1.6185, G_loss: 2.4502\n",
      "  Batch [100/1299] D_loss: -2.2928, G_loss: 2.6639\n",
      "  Batch [110/1299] D_loss: -2.3866, G_loss: 2.4458\n",
      "  Batch [120/1299] D_loss: -2.9327, G_loss: 3.8124\n",
      "  Batch [130/1299] D_loss: -2.3665, G_loss: 3.8928\n",
      "  Batch [140/1299] D_loss: -2.1499, G_loss: 5.2825\n",
      "  Batch [150/1299] D_loss: -1.5051, G_loss: 3.0978\n",
      "  Batch [160/1299] D_loss: -0.7386, G_loss: 3.9961\n",
      "  Batch [170/1299] D_loss: -2.2742, G_loss: 3.3803\n",
      "  Batch [180/1299] D_loss: -2.4287, G_loss: 1.8461\n",
      "  Batch [190/1299] D_loss: -2.0054, G_loss: -0.2375\n",
      "  Batch [200/1299] D_loss: -1.8601, G_loss: -3.3739\n",
      "  Batch [210/1299] D_loss: -1.6107, G_loss: -1.9922\n",
      "  Batch [220/1299] D_loss: -2.3434, G_loss: 1.0065\n",
      "  Batch [230/1299] D_loss: -1.5326, G_loss: 2.3515\n",
      "  Batch [240/1299] D_loss: -1.3094, G_loss: 5.7910\n",
      "  Batch [250/1299] D_loss: -2.5484, G_loss: 6.1088\n",
      "  Batch [260/1299] D_loss: -2.4286, G_loss: 8.0485\n",
      "  Batch [270/1299] D_loss: -2.4746, G_loss: 7.6464\n",
      "  Batch [280/1299] D_loss: -1.1360, G_loss: 5.1455\n",
      "  Batch [290/1299] D_loss: -2.5382, G_loss: 6.3127\n",
      "  Batch [300/1299] D_loss: -2.2788, G_loss: 3.3599\n",
      "  Batch [310/1299] D_loss: -1.7746, G_loss: 1.3425\n",
      "  Batch [320/1299] D_loss: -2.3002, G_loss: 2.9534\n",
      "  Batch [330/1299] D_loss: -2.6412, G_loss: 2.8856\n",
      "  Batch [340/1299] D_loss: -1.8619, G_loss: 2.5887\n",
      "  Batch [350/1299] D_loss: -2.6513, G_loss: 2.3407\n",
      "  Batch [360/1299] D_loss: -1.2289, G_loss: 3.9511\n",
      "  Batch [370/1299] D_loss: -0.5342, G_loss: 4.0797\n",
      "  Batch [380/1299] D_loss: -2.3974, G_loss: 4.2901\n",
      "  Batch [390/1299] D_loss: -2.3676, G_loss: 5.8834\n",
      "  Batch [400/1299] D_loss: -2.2397, G_loss: 6.8643\n",
      "  Batch [410/1299] D_loss: -1.8032, G_loss: 6.2590\n",
      "  Batch [420/1299] D_loss: -1.0293, G_loss: 3.8433\n",
      "  Batch [430/1299] D_loss: -1.6479, G_loss: 3.8419\n",
      "  Batch [440/1299] D_loss: -1.4113, G_loss: 5.1752\n",
      "  Batch [450/1299] D_loss: -1.0639, G_loss: 2.8230\n",
      "  Batch [460/1299] D_loss: -2.0884, G_loss: 1.2365\n",
      "  Batch [470/1299] D_loss: -1.8080, G_loss: 2.7986\n",
      "  Batch [480/1299] D_loss: -2.3719, G_loss: 3.3979\n",
      "  Batch [490/1299] D_loss: -3.0211, G_loss: 5.2916\n",
      "  Batch [500/1299] D_loss: -2.0717, G_loss: 1.8274\n",
      "  Batch [510/1299] D_loss: -1.8644, G_loss: 0.6156\n",
      "  Batch [520/1299] D_loss: -2.6287, G_loss: -0.7106\n",
      "  Batch [530/1299] D_loss: -1.1660, G_loss: -0.1140\n",
      "  Batch [540/1299] D_loss: -2.0109, G_loss: 2.1790\n",
      "  Batch [550/1299] D_loss: -2.4977, G_loss: 4.5642\n",
      "  Batch [560/1299] D_loss: -2.0070, G_loss: 4.6975\n",
      "  Batch [570/1299] D_loss: -2.0865, G_loss: 6.2734\n",
      "  Batch [580/1299] D_loss: -1.6939, G_loss: 7.0355\n",
      "  Batch [590/1299] D_loss: -2.8013, G_loss: 6.4685\n",
      "  Batch [600/1299] D_loss: -1.9873, G_loss: 1.1236\n",
      "  Batch [610/1299] D_loss: -1.3614, G_loss: 1.0013\n",
      "  Batch [620/1299] D_loss: -2.8523, G_loss: -1.7258\n",
      "  Batch [630/1299] D_loss: -1.9822, G_loss: 0.9693\n",
      "  Batch [640/1299] D_loss: -1.0033, G_loss: 0.6762\n",
      "  Batch [650/1299] D_loss: -2.4452, G_loss: 1.8359\n",
      "  Batch [660/1299] D_loss: -1.9975, G_loss: 4.1081\n",
      "  Batch [670/1299] D_loss: -2.9067, G_loss: 0.4942\n",
      "  Batch [680/1299] D_loss: -4.3104, G_loss: 5.7441\n",
      "  Batch [690/1299] D_loss: 0.6046, G_loss: 5.3588\n",
      "  Batch [700/1299] D_loss: -2.7297, G_loss: 4.3367\n",
      "  Batch [710/1299] D_loss: -2.8784, G_loss: 2.7583\n",
      "  Batch [720/1299] D_loss: -1.8011, G_loss: 1.0653\n",
      "  Batch [730/1299] D_loss: -1.0885, G_loss: 2.1157\n",
      "  Batch [740/1299] D_loss: -2.4433, G_loss: 1.2124\n",
      "  Batch [750/1299] D_loss: -1.6123, G_loss: 4.0565\n",
      "  Batch [760/1299] D_loss: -1.6658, G_loss: 6.9876\n",
      "  Batch [770/1299] D_loss: -1.6644, G_loss: 6.4262\n",
      "  Batch [780/1299] D_loss: -2.1935, G_loss: 2.6926\n",
      "  Batch [790/1299] D_loss: -2.2971, G_loss: 0.1211\n",
      "  Batch [800/1299] D_loss: -2.0287, G_loss: -1.5783\n",
      "  Batch [810/1299] D_loss: -3.1691, G_loss: 2.6043\n",
      "  Batch [820/1299] D_loss: -2.1253, G_loss: 2.9955\n",
      "  Batch [830/1299] D_loss: -1.2392, G_loss: 2.1165\n",
      "  Batch [840/1299] D_loss: -2.3482, G_loss: 1.3427\n",
      "  Batch [850/1299] D_loss: -2.0582, G_loss: 2.3236\n",
      "  Batch [860/1299] D_loss: -1.8680, G_loss: 6.3219\n",
      "  Batch [870/1299] D_loss: -2.0682, G_loss: 5.6230\n",
      "  Batch [880/1299] D_loss: -2.1366, G_loss: 6.2935\n",
      "  Batch [890/1299] D_loss: -2.5065, G_loss: 7.8911\n",
      "  Batch [900/1299] D_loss: -2.4151, G_loss: 5.7048\n",
      "  Batch [910/1299] D_loss: -2.6679, G_loss: 5.1740\n",
      "  Batch [920/1299] D_loss: -2.4432, G_loss: 3.5962\n",
      "  Batch [930/1299] D_loss: -1.8578, G_loss: 3.3669\n",
      "  Batch [940/1299] D_loss: -1.9128, G_loss: 1.0240\n",
      "  Batch [950/1299] D_loss: -2.1051, G_loss: 0.1126\n",
      "  Batch [960/1299] D_loss: -1.8295, G_loss: 0.3370\n",
      "  Batch [970/1299] D_loss: -2.3262, G_loss: 0.1122\n",
      "  Batch [980/1299] D_loss: -1.8661, G_loss: -0.4340\n",
      "  Batch [990/1299] D_loss: -2.4794, G_loss: 3.5486\n",
      "  Batch [1000/1299] D_loss: -2.4324, G_loss: 3.2606\n",
      "  Batch [1010/1299] D_loss: -2.2620, G_loss: 4.5158\n",
      "  Batch [1020/1299] D_loss: -2.2742, G_loss: 10.4801\n",
      "  Batch [1030/1299] D_loss: -0.7311, G_loss: 5.7988\n",
      "  Batch [1040/1299] D_loss: -2.6820, G_loss: 2.9247\n",
      "  Batch [1050/1299] D_loss: -2.2465, G_loss: 2.8973\n",
      "  Batch [1060/1299] D_loss: -1.7567, G_loss: 4.8210\n",
      "  Batch [1070/1299] D_loss: -1.0531, G_loss: 1.4657\n",
      "  Batch [1080/1299] D_loss: -1.9282, G_loss: -1.4665\n",
      "  Batch [1090/1299] D_loss: -2.4919, G_loss: -0.4595\n",
      "  Batch [1100/1299] D_loss: -2.2240, G_loss: 0.8195\n",
      "  Batch [1110/1299] D_loss: -2.1258, G_loss: 2.7157\n",
      "  Batch [1120/1299] D_loss: -1.7526, G_loss: 3.0653\n",
      "  Batch [1130/1299] D_loss: -3.4241, G_loss: 3.9810\n",
      "  Batch [1140/1299] D_loss: -0.6836, G_loss: 4.2087\n",
      "  Batch [1150/1299] D_loss: -1.3450, G_loss: 4.7023\n",
      "  Batch [1160/1299] D_loss: -1.7718, G_loss: 1.7894\n",
      "  Batch [1170/1299] D_loss: -1.5737, G_loss: 1.2515\n",
      "  Batch [1180/1299] D_loss: -2.5553, G_loss: 0.2593\n",
      "  Batch [1190/1299] D_loss: -1.9687, G_loss: 2.1994\n",
      "  Batch [1200/1299] D_loss: -1.5286, G_loss: 2.6181\n",
      "  Batch [1210/1299] D_loss: -1.7946, G_loss: 1.4549\n",
      "  Batch [1220/1299] D_loss: -1.3406, G_loss: 0.8931\n",
      "  Batch [1230/1299] D_loss: -1.6010, G_loss: 5.5055\n",
      "  Batch [1240/1299] D_loss: -2.1203, G_loss: 5.7910\n",
      "  Batch [1250/1299] D_loss: -2.7179, G_loss: 4.8062\n",
      "  Batch [1260/1299] D_loss: -1.1759, G_loss: 1.6394\n",
      "  Batch [1270/1299] D_loss: -1.7922, G_loss: 6.0876\n",
      "  Batch [1280/1299] D_loss: -1.2563, G_loss: 5.9785\n",
      "  Batch [1290/1299] D_loss: -1.6252, G_loss: 4.8395\n",
      "\n",
      "Epoch 97 Summary:\n",
      "  Average D_loss: -1.7966\n",
      "  Average G_loss: 3.1120\n",
      "\n",
      "Epoch [98/100]\n",
      "  Batch [0/1299] D_loss: -2.0059, G_loss: 4.3061\n",
      "  Batch [10/1299] D_loss: -1.0607, G_loss: 1.8738\n",
      "  Batch [20/1299] D_loss: -1.6806, G_loss: 2.2504\n",
      "  Batch [30/1299] D_loss: -1.9898, G_loss: 4.2045\n",
      "  Batch [40/1299] D_loss: -1.6716, G_loss: 2.5432\n",
      "  Batch [50/1299] D_loss: -1.5277, G_loss: 1.1594\n",
      "  Batch [60/1299] D_loss: -1.9809, G_loss: 0.9143\n",
      "  Batch [70/1299] D_loss: -1.5713, G_loss: -0.7437\n",
      "  Batch [80/1299] D_loss: -0.5415, G_loss: -1.6199\n",
      "  Batch [90/1299] D_loss: -1.0154, G_loss: -2.4953\n",
      "  Batch [100/1299] D_loss: -0.5282, G_loss: -2.4129\n",
      "  Batch [110/1299] D_loss: -1.9928, G_loss: -3.4281\n",
      "  Batch [120/1299] D_loss: -2.0639, G_loss: 0.2380\n",
      "  Batch [130/1299] D_loss: -3.2046, G_loss: 3.2367\n",
      "  Batch [140/1299] D_loss: -0.2178, G_loss: 4.6838\n",
      "  Batch [150/1299] D_loss: -1.5895, G_loss: 5.0876\n",
      "  Batch [160/1299] D_loss: -2.6161, G_loss: 4.8104\n",
      "  Batch [170/1299] D_loss: -2.7585, G_loss: 6.6202\n",
      "  Batch [180/1299] D_loss: -2.1249, G_loss: 4.4726\n",
      "  Batch [190/1299] D_loss: -1.8844, G_loss: 6.6639\n",
      "  Batch [200/1299] D_loss: -2.4355, G_loss: 6.6779\n",
      "  Batch [210/1299] D_loss: -1.9732, G_loss: 3.8694\n",
      "  Batch [220/1299] D_loss: -2.8076, G_loss: 2.3263\n",
      "  Batch [230/1299] D_loss: -2.3876, G_loss: 6.2549\n",
      "  Batch [240/1299] D_loss: -1.0666, G_loss: 5.2962\n",
      "  Batch [250/1299] D_loss: -2.4195, G_loss: 5.1664\n",
      "  Batch [260/1299] D_loss: -1.2787, G_loss: 3.8428\n",
      "  Batch [270/1299] D_loss: -2.5082, G_loss: 4.3484\n",
      "  Batch [280/1299] D_loss: -2.4420, G_loss: 4.8864\n",
      "  Batch [290/1299] D_loss: -2.0165, G_loss: 3.5937\n",
      "  Batch [300/1299] D_loss: -1.9928, G_loss: 3.1753\n",
      "  Batch [310/1299] D_loss: -1.7130, G_loss: 2.0634\n",
      "  Batch [320/1299] D_loss: -2.6172, G_loss: 3.7135\n",
      "  Batch [330/1299] D_loss: -1.3718, G_loss: 3.0040\n",
      "  Batch [340/1299] D_loss: -1.5659, G_loss: -0.4519\n",
      "  Batch [350/1299] D_loss: -1.8896, G_loss: 0.2737\n",
      "  Batch [360/1299] D_loss: -2.7254, G_loss: 0.1216\n",
      "  Batch [370/1299] D_loss: -1.4183, G_loss: -2.7399\n",
      "  Batch [380/1299] D_loss: -1.2742, G_loss: -2.2241\n",
      "  Batch [390/1299] D_loss: -1.9680, G_loss: -3.8518\n",
      "  Batch [400/1299] D_loss: -1.9765, G_loss: -3.3781\n",
      "  Batch [410/1299] D_loss: -1.8786, G_loss: -2.2160\n",
      "  Batch [420/1299] D_loss: -0.9188, G_loss: -2.2842\n",
      "  Batch [430/1299] D_loss: -2.1616, G_loss: -3.5756\n",
      "  Batch [440/1299] D_loss: -0.7822, G_loss: -2.1278\n",
      "  Batch [450/1299] D_loss: -1.1647, G_loss: -0.5899\n",
      "  Batch [460/1299] D_loss: -1.5577, G_loss: 3.9901\n",
      "  Batch [470/1299] D_loss: -2.4187, G_loss: 3.8310\n",
      "  Batch [480/1299] D_loss: -2.3067, G_loss: 6.3952\n",
      "  Batch [490/1299] D_loss: -2.0949, G_loss: 6.0235\n",
      "  Batch [500/1299] D_loss: -1.3253, G_loss: 4.7457\n",
      "  Batch [510/1299] D_loss: -2.2121, G_loss: 3.9445\n",
      "  Batch [520/1299] D_loss: -0.8747, G_loss: 2.0509\n",
      "  Batch [530/1299] D_loss: -1.5444, G_loss: 2.1739\n",
      "  Batch [540/1299] D_loss: -2.0882, G_loss: 1.2530\n",
      "  Batch [550/1299] D_loss: -2.1791, G_loss: 2.6357\n",
      "  Batch [560/1299] D_loss: -1.6221, G_loss: 3.8210\n",
      "  Batch [570/1299] D_loss: -2.6027, G_loss: 1.8775\n",
      "  Batch [580/1299] D_loss: -2.2210, G_loss: 2.3510\n",
      "  Batch [590/1299] D_loss: -1.3847, G_loss: -0.6687\n",
      "  Batch [600/1299] D_loss: -1.9273, G_loss: 2.5055\n",
      "  Batch [610/1299] D_loss: -1.5466, G_loss: 3.0560\n",
      "  Batch [620/1299] D_loss: -1.5738, G_loss: 1.0606\n",
      "  Batch [630/1299] D_loss: -2.1396, G_loss: -0.7914\n",
      "  Batch [640/1299] D_loss: -1.2662, G_loss: 1.8115\n",
      "  Batch [650/1299] D_loss: -1.8475, G_loss: 1.2860\n",
      "  Batch [660/1299] D_loss: -1.9196, G_loss: 1.9654\n",
      "  Batch [670/1299] D_loss: -3.6717, G_loss: 6.7297\n",
      "  Batch [680/1299] D_loss: -2.3497, G_loss: 6.0236\n",
      "  Batch [690/1299] D_loss: -0.8732, G_loss: 2.4522\n",
      "  Batch [700/1299] D_loss: -1.5977, G_loss: 3.0118\n",
      "  Batch [710/1299] D_loss: -1.7205, G_loss: 6.4331\n",
      "  Batch [720/1299] D_loss: -1.2883, G_loss: 3.4336\n",
      "  Batch [730/1299] D_loss: -2.6679, G_loss: 2.5199\n",
      "  Batch [740/1299] D_loss: -3.3904, G_loss: 1.9920\n",
      "  Batch [750/1299] D_loss: -1.7404, G_loss: 4.3874\n",
      "  Batch [760/1299] D_loss: -2.2584, G_loss: 1.2090\n",
      "  Batch [770/1299] D_loss: -1.1727, G_loss: 2.6281\n",
      "  Batch [780/1299] D_loss: -0.5414, G_loss: 3.3513\n",
      "  Batch [790/1299] D_loss: -2.3715, G_loss: 2.5109\n",
      "  Batch [800/1299] D_loss: -1.5034, G_loss: 0.4889\n",
      "  Batch [810/1299] D_loss: -1.5400, G_loss: 0.8775\n",
      "  Batch [820/1299] D_loss: -2.3109, G_loss: 0.6226\n",
      "  Batch [830/1299] D_loss: -1.8607, G_loss: 3.0337\n",
      "  Batch [840/1299] D_loss: -2.0606, G_loss: 2.6901\n",
      "  Batch [850/1299] D_loss: -1.8663, G_loss: 2.2578\n",
      "  Batch [860/1299] D_loss: -2.5627, G_loss: 3.2080\n",
      "  Batch [870/1299] D_loss: -2.5236, G_loss: 5.3782\n",
      "  Batch [880/1299] D_loss: -2.1120, G_loss: 5.0759\n",
      "  Batch [890/1299] D_loss: -1.5990, G_loss: 2.6106\n",
      "  Batch [900/1299] D_loss: -2.4927, G_loss: 4.4020\n",
      "  Batch [910/1299] D_loss: -2.7058, G_loss: 0.5748\n",
      "  Batch [920/1299] D_loss: -2.0327, G_loss: 2.5676\n",
      "  Batch [930/1299] D_loss: -1.6300, G_loss: 1.5101\n",
      "  Batch [940/1299] D_loss: -1.1193, G_loss: 1.8848\n",
      "  Batch [950/1299] D_loss: -2.6748, G_loss: 2.4015\n",
      "  Batch [960/1299] D_loss: -2.2767, G_loss: 2.4263\n",
      "  Batch [970/1299] D_loss: -2.1570, G_loss: 2.2518\n",
      "  Batch [980/1299] D_loss: -2.3191, G_loss: 2.6646\n",
      "  Batch [990/1299] D_loss: -1.4824, G_loss: 3.2248\n",
      "  Batch [1000/1299] D_loss: -1.5598, G_loss: 4.1946\n",
      "  Batch [1010/1299] D_loss: -0.7131, G_loss: 0.8168\n",
      "  Batch [1020/1299] D_loss: -2.2993, G_loss: 1.6809\n",
      "  Batch [1030/1299] D_loss: -1.3998, G_loss: 0.2767\n",
      "  Batch [1040/1299] D_loss: -2.7113, G_loss: -0.3745\n",
      "  Batch [1050/1299] D_loss: -1.9643, G_loss: -0.0737\n",
      "  Batch [1060/1299] D_loss: -2.6399, G_loss: -0.5794\n",
      "  Batch [1070/1299] D_loss: -1.4058, G_loss: -1.3003\n",
      "  Batch [1080/1299] D_loss: -0.8745, G_loss: 1.9565\n",
      "  Batch [1090/1299] D_loss: -2.3456, G_loss: 5.3862\n",
      "  Batch [1100/1299] D_loss: -1.4503, G_loss: 4.0382\n",
      "  Batch [1110/1299] D_loss: -1.6701, G_loss: 4.4414\n",
      "  Batch [1120/1299] D_loss: -1.1788, G_loss: 4.9827\n",
      "  Batch [1130/1299] D_loss: -1.9111, G_loss: 5.9285\n",
      "  Batch [1140/1299] D_loss: -2.6226, G_loss: 5.5909\n",
      "  Batch [1150/1299] D_loss: -1.0036, G_loss: 2.9879\n",
      "  Batch [1160/1299] D_loss: -2.9762, G_loss: 1.7668\n",
      "  Batch [1170/1299] D_loss: -1.7822, G_loss: 0.8605\n",
      "  Batch [1180/1299] D_loss: -2.5405, G_loss: 3.7182\n",
      "  Batch [1190/1299] D_loss: -2.0253, G_loss: 2.1292\n",
      "  Batch [1200/1299] D_loss: -2.1681, G_loss: 4.5287\n",
      "  Batch [1210/1299] D_loss: -1.8621, G_loss: 4.3019\n",
      "  Batch [1220/1299] D_loss: -1.6659, G_loss: 2.8363\n",
      "  Batch [1230/1299] D_loss: -2.6155, G_loss: 3.2800\n",
      "  Batch [1240/1299] D_loss: -2.2769, G_loss: 3.7055\n",
      "  Batch [1250/1299] D_loss: -0.8738, G_loss: 2.1847\n",
      "  Batch [1260/1299] D_loss: -0.9992, G_loss: 0.5705\n",
      "  Batch [1270/1299] D_loss: -1.2715, G_loss: -2.2372\n",
      "  Batch [1280/1299] D_loss: -1.8654, G_loss: -1.4280\n",
      "  Batch [1290/1299] D_loss: -1.8728, G_loss: 0.6975\n",
      "\n",
      "Epoch 98 Summary:\n",
      "  Average D_loss: -1.7691\n",
      "  Average G_loss: 2.2281\n",
      "\n",
      "Epoch [99/100]\n",
      "  Batch [0/1299] D_loss: -2.1145, G_loss: 1.7641\n",
      "  Batch [10/1299] D_loss: -2.2421, G_loss: 1.3793\n",
      "  Batch [20/1299] D_loss: -2.4278, G_loss: 5.7411\n",
      "  Batch [30/1299] D_loss: -1.6153, G_loss: 6.7596\n",
      "  Batch [40/1299] D_loss: -1.5286, G_loss: 6.8429\n",
      "  Batch [50/1299] D_loss: -2.1194, G_loss: 6.8285\n",
      "  Batch [60/1299] D_loss: -2.3508, G_loss: 5.6839\n",
      "  Batch [70/1299] D_loss: -2.6706, G_loss: 5.3110\n",
      "  Batch [80/1299] D_loss: -2.1455, G_loss: 4.1845\n",
      "  Batch [90/1299] D_loss: -2.6184, G_loss: 5.8530\n",
      "  Batch [100/1299] D_loss: -0.9030, G_loss: 3.8197\n",
      "  Batch [110/1299] D_loss: -1.5115, G_loss: 2.6553\n",
      "  Batch [120/1299] D_loss: -1.4997, G_loss: 2.5947\n",
      "  Batch [130/1299] D_loss: -2.1872, G_loss: 3.1492\n",
      "  Batch [140/1299] D_loss: -1.3557, G_loss: 3.7097\n",
      "  Batch [150/1299] D_loss: -2.2069, G_loss: 4.5363\n",
      "  Batch [160/1299] D_loss: -2.0640, G_loss: 3.5018\n",
      "  Batch [170/1299] D_loss: -1.9080, G_loss: 0.5370\n",
      "  Batch [180/1299] D_loss: -1.6042, G_loss: 0.7027\n",
      "  Batch [190/1299] D_loss: -1.3287, G_loss: 1.0713\n",
      "  Batch [200/1299] D_loss: -2.4990, G_loss: 1.3703\n",
      "  Batch [210/1299] D_loss: -2.4933, G_loss: 6.1060\n",
      "  Batch [220/1299] D_loss: -1.9962, G_loss: 7.4881\n",
      "  Batch [230/1299] D_loss: -1.8647, G_loss: 3.0193\n",
      "  Batch [240/1299] D_loss: -2.2948, G_loss: 6.2332\n",
      "  Batch [250/1299] D_loss: -1.0895, G_loss: 5.2481\n",
      "  Batch [260/1299] D_loss: -2.3271, G_loss: 5.8857\n",
      "  Batch [270/1299] D_loss: -1.5571, G_loss: 4.4637\n",
      "  Batch [280/1299] D_loss: -0.6892, G_loss: 4.7645\n",
      "  Batch [290/1299] D_loss: -2.1814, G_loss: 3.2970\n",
      "  Batch [300/1299] D_loss: -2.5077, G_loss: 4.2878\n",
      "  Batch [310/1299] D_loss: -2.0084, G_loss: 3.3809\n",
      "  Batch [320/1299] D_loss: -1.7369, G_loss: 3.9880\n",
      "  Batch [330/1299] D_loss: -1.5961, G_loss: 2.1721\n",
      "  Batch [340/1299] D_loss: -2.0094, G_loss: 3.8694\n",
      "  Batch [350/1299] D_loss: -1.9861, G_loss: 3.7429\n",
      "  Batch [360/1299] D_loss: -1.4897, G_loss: 5.1247\n",
      "  Batch [370/1299] D_loss: -1.8758, G_loss: 4.0929\n",
      "  Batch [380/1299] D_loss: -1.5647, G_loss: 6.7602\n",
      "  Batch [390/1299] D_loss: -2.0543, G_loss: 6.1999\n",
      "  Batch [400/1299] D_loss: -2.2293, G_loss: 6.3690\n",
      "  Batch [410/1299] D_loss: -2.1140, G_loss: 4.4142\n",
      "  Batch [420/1299] D_loss: -1.7309, G_loss: 1.8665\n",
      "  Batch [430/1299] D_loss: -2.9547, G_loss: 3.5143\n",
      "  Batch [440/1299] D_loss: -0.5575, G_loss: 1.9711\n",
      "  Batch [450/1299] D_loss: -1.8599, G_loss: -1.3348\n",
      "  Batch [460/1299] D_loss: -0.4272, G_loss: -1.0284\n",
      "  Batch [470/1299] D_loss: -1.6588, G_loss: -1.8345\n",
      "  Batch [480/1299] D_loss: -1.8934, G_loss: 2.4819\n",
      "  Batch [490/1299] D_loss: -1.4325, G_loss: 1.9934\n",
      "  Batch [500/1299] D_loss: -2.0436, G_loss: 6.6090\n",
      "  Batch [510/1299] D_loss: -2.2931, G_loss: 6.8839\n",
      "  Batch [520/1299] D_loss: -2.8539, G_loss: 5.1390\n",
      "  Batch [530/1299] D_loss: -1.5076, G_loss: 2.5990\n",
      "  Batch [540/1299] D_loss: -1.3873, G_loss: 2.1347\n",
      "  Batch [550/1299] D_loss: -1.8703, G_loss: 3.7247\n",
      "  Batch [560/1299] D_loss: -1.8522, G_loss: 5.8818\n",
      "  Batch [570/1299] D_loss: -3.4246, G_loss: 6.1707\n",
      "  Batch [580/1299] D_loss: -1.2526, G_loss: 1.6452\n",
      "  Batch [590/1299] D_loss: -1.5888, G_loss: -0.8574\n",
      "  Batch [600/1299] D_loss: -2.7698, G_loss: -1.7960\n",
      "  Batch [610/1299] D_loss: -2.6116, G_loss: -4.1732\n",
      "  Batch [620/1299] D_loss: -2.4153, G_loss: -0.4047\n",
      "  Batch [630/1299] D_loss: -2.1050, G_loss: 2.1882\n",
      "  Batch [640/1299] D_loss: -1.9286, G_loss: 2.1825\n",
      "  Batch [650/1299] D_loss: -2.2161, G_loss: 0.9481\n",
      "  Batch [660/1299] D_loss: -1.0995, G_loss: 6.3528\n",
      "  Batch [670/1299] D_loss: -1.8340, G_loss: 4.5703\n",
      "  Batch [680/1299] D_loss: -1.9505, G_loss: 5.4087\n",
      "  Batch [690/1299] D_loss: -3.7702, G_loss: 3.8724\n",
      "  Batch [700/1299] D_loss: -3.1481, G_loss: 1.3801\n",
      "  Batch [710/1299] D_loss: -2.6920, G_loss: 3.3426\n",
      "  Batch [720/1299] D_loss: -1.5957, G_loss: 3.2165\n",
      "  Batch [730/1299] D_loss: -1.0088, G_loss: 4.1932\n",
      "  Batch [740/1299] D_loss: -2.0543, G_loss: 4.7951\n",
      "  Batch [750/1299] D_loss: -3.0234, G_loss: 2.3578\n",
      "  Batch [760/1299] D_loss: -2.1911, G_loss: 4.5389\n",
      "  Batch [770/1299] D_loss: -2.5597, G_loss: 4.6872\n",
      "  Batch [780/1299] D_loss: -1.2489, G_loss: 2.0052\n",
      "  Batch [790/1299] D_loss: -2.5717, G_loss: 2.5202\n",
      "  Batch [800/1299] D_loss: -2.0156, G_loss: 0.0744\n",
      "  Batch [810/1299] D_loss: -1.1727, G_loss: 2.7080\n",
      "  Batch [820/1299] D_loss: -0.5128, G_loss: 2.0313\n",
      "  Batch [830/1299] D_loss: -1.3732, G_loss: 3.1348\n",
      "  Batch [840/1299] D_loss: -1.4715, G_loss: 3.7108\n",
      "  Batch [850/1299] D_loss: -3.3824, G_loss: 5.4026\n",
      "  Batch [860/1299] D_loss: -1.3782, G_loss: 6.0252\n",
      "  Batch [870/1299] D_loss: -1.5609, G_loss: 6.4193\n",
      "  Batch [880/1299] D_loss: -2.5055, G_loss: 4.7502\n",
      "  Batch [890/1299] D_loss: -0.2355, G_loss: 1.5996\n",
      "  Batch [900/1299] D_loss: -1.9516, G_loss: 3.6264\n",
      "  Batch [910/1299] D_loss: -1.9087, G_loss: 0.2467\n",
      "  Batch [920/1299] D_loss: -2.4762, G_loss: 5.7346\n",
      "  Batch [930/1299] D_loss: -2.1342, G_loss: 2.2353\n",
      "  Batch [940/1299] D_loss: -1.2680, G_loss: 1.2785\n",
      "  Batch [950/1299] D_loss: -1.5097, G_loss: 0.3542\n",
      "  Batch [960/1299] D_loss: -1.9368, G_loss: 1.0672\n",
      "  Batch [970/1299] D_loss: -1.4442, G_loss: 2.8676\n",
      "  Batch [980/1299] D_loss: -2.1142, G_loss: 4.7234\n",
      "  Batch [990/1299] D_loss: -1.0309, G_loss: 4.9016\n",
      "  Batch [1000/1299] D_loss: -2.0861, G_loss: 3.0403\n",
      "  Batch [1010/1299] D_loss: -2.5052, G_loss: 6.8440\n",
      "  Batch [1020/1299] D_loss: -1.9634, G_loss: 4.5117\n",
      "  Batch [1030/1299] D_loss: -1.7266, G_loss: 5.7849\n",
      "  Batch [1040/1299] D_loss: -1.6766, G_loss: 4.1788\n",
      "  Batch [1050/1299] D_loss: -0.9871, G_loss: 2.4867\n",
      "  Batch [1060/1299] D_loss: -0.9823, G_loss: 0.4354\n",
      "  Batch [1070/1299] D_loss: -1.0374, G_loss: 1.4395\n",
      "  Batch [1080/1299] D_loss: -1.3561, G_loss: 2.0901\n",
      "  Batch [1090/1299] D_loss: -1.8505, G_loss: 0.9860\n",
      "  Batch [1100/1299] D_loss: -2.0951, G_loss: -0.6787\n",
      "  Batch [1110/1299] D_loss: -2.4835, G_loss: 1.6984\n",
      "  Batch [1120/1299] D_loss: -2.6825, G_loss: -0.1611\n",
      "  Batch [1130/1299] D_loss: -2.6680, G_loss: -1.7773\n",
      "  Batch [1140/1299] D_loss: -3.1364, G_loss: -1.8206\n",
      "  Batch [1150/1299] D_loss: -1.9563, G_loss: 2.2175\n",
      "  Batch [1160/1299] D_loss: -2.0819, G_loss: 5.8793\n",
      "  Batch [1170/1299] D_loss: -3.3913, G_loss: 7.4966\n",
      "  Batch [1180/1299] D_loss: -2.1886, G_loss: 4.4695\n",
      "  Batch [1190/1299] D_loss: -3.0406, G_loss: 6.5750\n",
      "  Batch [1200/1299] D_loss: -1.7753, G_loss: 2.7964\n",
      "  Batch [1210/1299] D_loss: -1.2197, G_loss: 3.2236\n",
      "  Batch [1220/1299] D_loss: -2.4478, G_loss: 4.4272\n",
      "  Batch [1230/1299] D_loss: -1.5500, G_loss: 1.8707\n",
      "  Batch [1240/1299] D_loss: -2.5119, G_loss: -2.7876\n",
      "  Batch [1250/1299] D_loss: -2.3071, G_loss: -0.4738\n",
      "  Batch [1260/1299] D_loss: -2.1751, G_loss: 0.6186\n",
      "  Batch [1270/1299] D_loss: -1.8265, G_loss: 0.6206\n",
      "  Batch [1280/1299] D_loss: -2.2355, G_loss: -0.3071\n",
      "  Batch [1290/1299] D_loss: -0.8331, G_loss: 1.9827\n",
      "\n",
      "Epoch 99 Summary:\n",
      "  Average D_loss: -1.7709\n",
      "  Average G_loss: 3.1776\n",
      "\n",
      "Epoch [100/100]\n",
      "  Batch [0/1299] D_loss: -3.1039, G_loss: 5.3964\n",
      "  Batch [10/1299] D_loss: -2.7606, G_loss: 5.4889\n",
      "  Batch [20/1299] D_loss: -2.8476, G_loss: 6.4685\n",
      "  Batch [30/1299] D_loss: -0.9569, G_loss: 2.3635\n",
      "  Batch [40/1299] D_loss: -2.0873, G_loss: 3.7793\n",
      "  Batch [50/1299] D_loss: -1.3024, G_loss: 3.6663\n",
      "  Batch [60/1299] D_loss: -2.4163, G_loss: 2.1011\n",
      "  Batch [70/1299] D_loss: -1.3655, G_loss: 0.2301\n",
      "  Batch [80/1299] D_loss: -1.5546, G_loss: -0.5191\n",
      "  Batch [90/1299] D_loss: -1.8847, G_loss: 2.3763\n",
      "  Batch [100/1299] D_loss: -2.6576, G_loss: 1.2921\n",
      "  Batch [110/1299] D_loss: -2.4538, G_loss: -0.9737\n",
      "  Batch [120/1299] D_loss: -1.6419, G_loss: 3.5317\n",
      "  Batch [130/1299] D_loss: -2.9227, G_loss: 4.3361\n",
      "  Batch [140/1299] D_loss: -1.7964, G_loss: 6.9525\n",
      "  Batch [150/1299] D_loss: -4.1210, G_loss: 3.0451\n",
      "  Batch [160/1299] D_loss: -3.1713, G_loss: 2.9384\n",
      "  Batch [170/1299] D_loss: -1.5797, G_loss: 4.4543\n",
      "  Batch [180/1299] D_loss: -1.7780, G_loss: 3.4474\n",
      "  Batch [190/1299] D_loss: -2.3065, G_loss: 4.0809\n",
      "  Batch [200/1299] D_loss: -2.6708, G_loss: 2.8356\n",
      "  Batch [210/1299] D_loss: -1.9094, G_loss: 0.5460\n",
      "  Batch [220/1299] D_loss: -1.6264, G_loss: 1.2110\n",
      "  Batch [230/1299] D_loss: -2.0650, G_loss: 5.1304\n",
      "  Batch [240/1299] D_loss: -3.2064, G_loss: 5.6833\n",
      "  Batch [250/1299] D_loss: -1.9939, G_loss: 2.6178\n",
      "  Batch [260/1299] D_loss: -2.8960, G_loss: 1.8638\n",
      "  Batch [270/1299] D_loss: -1.6185, G_loss: 0.2992\n",
      "  Batch [280/1299] D_loss: -1.1480, G_loss: 2.9894\n",
      "  Batch [290/1299] D_loss: -1.5110, G_loss: 3.7479\n",
      "  Batch [300/1299] D_loss: -2.9380, G_loss: 4.2124\n",
      "  Batch [310/1299] D_loss: -2.2480, G_loss: 6.7350\n",
      "  Batch [320/1299] D_loss: -3.0807, G_loss: 6.2118\n",
      "  Batch [330/1299] D_loss: -1.9740, G_loss: 4.8813\n",
      "  Batch [340/1299] D_loss: -2.9960, G_loss: 5.3714\n",
      "  Batch [350/1299] D_loss: -1.8272, G_loss: 4.2328\n",
      "  Batch [360/1299] D_loss: -1.8002, G_loss: 3.4697\n",
      "  Batch [370/1299] D_loss: -1.4511, G_loss: 4.4204\n",
      "  Batch [380/1299] D_loss: -1.7444, G_loss: 3.6877\n",
      "  Batch [390/1299] D_loss: -1.7127, G_loss: 3.0151\n",
      "  Batch [400/1299] D_loss: -2.0856, G_loss: 2.0399\n",
      "  Batch [410/1299] D_loss: -1.7728, G_loss: 1.4289\n",
      "  Batch [420/1299] D_loss: -2.3384, G_loss: 1.3079\n",
      "  Batch [430/1299] D_loss: -1.9243, G_loss: 3.6346\n",
      "  Batch [440/1299] D_loss: -2.0596, G_loss: 2.4882\n",
      "  Batch [450/1299] D_loss: -1.7973, G_loss: 4.4960\n",
      "  Batch [460/1299] D_loss: -0.8365, G_loss: 3.5396\n",
      "  Batch [470/1299] D_loss: -2.6834, G_loss: 4.9281\n",
      "  Batch [480/1299] D_loss: -2.6287, G_loss: 6.7190\n",
      "  Batch [490/1299] D_loss: -1.6556, G_loss: 4.9505\n",
      "  Batch [500/1299] D_loss: -1.1948, G_loss: 3.5036\n",
      "  Batch [510/1299] D_loss: -2.3249, G_loss: 4.1426\n",
      "  Batch [520/1299] D_loss: -0.8168, G_loss: 4.7393\n",
      "  Batch [530/1299] D_loss: -1.8011, G_loss: 2.6611\n",
      "  Batch [540/1299] D_loss: -1.9867, G_loss: 2.7277\n",
      "  Batch [550/1299] D_loss: -1.1590, G_loss: 0.5974\n",
      "  Batch [560/1299] D_loss: -1.4190, G_loss: 0.8666\n",
      "  Batch [570/1299] D_loss: -1.8793, G_loss: 1.5945\n",
      "  Batch [580/1299] D_loss: -1.4586, G_loss: 0.9475\n",
      "  Batch [590/1299] D_loss: -2.3505, G_loss: -0.5907\n",
      "  Batch [600/1299] D_loss: -2.0197, G_loss: 0.3732\n",
      "  Batch [610/1299] D_loss: -2.2936, G_loss: 3.3358\n",
      "  Batch [620/1299] D_loss: -1.2661, G_loss: 3.2537\n",
      "  Batch [630/1299] D_loss: -2.5487, G_loss: 4.6487\n",
      "  Batch [640/1299] D_loss: -1.9556, G_loss: 5.2550\n",
      "  Batch [650/1299] D_loss: -2.7300, G_loss: 8.1841\n",
      "  Batch [660/1299] D_loss: -2.1846, G_loss: 5.9442\n",
      "  Batch [670/1299] D_loss: -1.7594, G_loss: 4.8414\n",
      "  Batch [680/1299] D_loss: -2.9532, G_loss: 6.2752\n",
      "  Batch [690/1299] D_loss: -1.1585, G_loss: 5.0427\n",
      "  Batch [700/1299] D_loss: -1.7950, G_loss: 4.9588\n",
      "  Batch [710/1299] D_loss: -1.8544, G_loss: 1.7639\n",
      "  Batch [720/1299] D_loss: -1.6982, G_loss: 4.6439\n",
      "  Batch [730/1299] D_loss: -1.6554, G_loss: 3.3862\n",
      "  Batch [740/1299] D_loss: -1.2035, G_loss: 3.7127\n",
      "  Batch [750/1299] D_loss: -1.9694, G_loss: 5.1017\n",
      "  Batch [760/1299] D_loss: -1.6824, G_loss: 4.1227\n",
      "  Batch [770/1299] D_loss: -1.9920, G_loss: 3.5322\n",
      "  Batch [780/1299] D_loss: -2.2611, G_loss: 3.3342\n",
      "  Batch [790/1299] D_loss: -1.2894, G_loss: 3.3535\n",
      "  Batch [800/1299] D_loss: -1.8886, G_loss: 2.4885\n",
      "  Batch [810/1299] D_loss: -1.5514, G_loss: 3.9708\n",
      "  Batch [820/1299] D_loss: -1.9986, G_loss: 2.7709\n",
      "  Batch [830/1299] D_loss: -1.4020, G_loss: 4.6351\n",
      "  Batch [840/1299] D_loss: -1.4830, G_loss: 3.7800\n",
      "  Batch [850/1299] D_loss: -1.4770, G_loss: 3.5672\n",
      "  Batch [860/1299] D_loss: -2.3473, G_loss: 6.4340\n",
      "  Batch [870/1299] D_loss: -1.4283, G_loss: 4.9680\n",
      "  Batch [880/1299] D_loss: -1.9567, G_loss: 4.8979\n",
      "  Batch [890/1299] D_loss: -1.2354, G_loss: 5.1562\n",
      "  Batch [900/1299] D_loss: -2.3977, G_loss: 4.7455\n",
      "  Batch [910/1299] D_loss: -2.0214, G_loss: 3.2976\n",
      "  Batch [920/1299] D_loss: -0.8879, G_loss: 2.9150\n",
      "  Batch [930/1299] D_loss: -0.7824, G_loss: 1.8886\n",
      "  Batch [940/1299] D_loss: -2.8432, G_loss: 2.4562\n",
      "  Batch [950/1299] D_loss: -1.0804, G_loss: 0.6602\n",
      "  Batch [960/1299] D_loss: -1.8963, G_loss: 0.2107\n",
      "  Batch [970/1299] D_loss: -2.4162, G_loss: 2.0855\n",
      "  Batch [980/1299] D_loss: -1.5235, G_loss: 1.4197\n",
      "  Batch [990/1299] D_loss: -2.1003, G_loss: 4.7773\n",
      "  Batch [1000/1299] D_loss: -2.1001, G_loss: 4.0573\n",
      "  Batch [1010/1299] D_loss: -1.4986, G_loss: 6.1461\n",
      "  Batch [1020/1299] D_loss: -2.0649, G_loss: 5.6869\n",
      "  Batch [1030/1299] D_loss: -1.4600, G_loss: 4.0420\n",
      "  Batch [1040/1299] D_loss: -0.9041, G_loss: 4.6182\n",
      "  Batch [1050/1299] D_loss: -2.2260, G_loss: 4.1428\n",
      "  Batch [1060/1299] D_loss: -1.0895, G_loss: 4.6230\n",
      "  Batch [1070/1299] D_loss: -0.6428, G_loss: 5.4843\n",
      "  Batch [1080/1299] D_loss: -0.7507, G_loss: 3.5623\n",
      "  Batch [1090/1299] D_loss: -0.2931, G_loss: 3.1780\n",
      "  Batch [1100/1299] D_loss: -1.9795, G_loss: 1.1307\n",
      "  Batch [1110/1299] D_loss: -2.4288, G_loss: 1.7199\n",
      "  Batch [1120/1299] D_loss: -2.0111, G_loss: 4.8570\n",
      "  Batch [1130/1299] D_loss: -2.4511, G_loss: 3.4714\n",
      "  Batch [1140/1299] D_loss: -2.0210, G_loss: 3.6780\n",
      "  Batch [1150/1299] D_loss: -2.2446, G_loss: 4.2155\n",
      "  Batch [1160/1299] D_loss: -2.1250, G_loss: 2.6640\n",
      "  Batch [1170/1299] D_loss: -0.9792, G_loss: 0.6218\n",
      "  Batch [1180/1299] D_loss: -2.0284, G_loss: 4.1362\n",
      "  Batch [1190/1299] D_loss: -2.6289, G_loss: 2.6304\n",
      "  Batch [1200/1299] D_loss: -1.1376, G_loss: 1.9849\n",
      "  Batch [1210/1299] D_loss: -2.1590, G_loss: -0.9991\n",
      "  Batch [1220/1299] D_loss: -2.2497, G_loss: -0.0427\n",
      "  Batch [1230/1299] D_loss: -2.1465, G_loss: -1.7212\n",
      "  Batch [1240/1299] D_loss: -0.7362, G_loss: -0.3373\n",
      "  Batch [1250/1299] D_loss: -2.5274, G_loss: -1.4085\n",
      "  Batch [1260/1299] D_loss: -2.3028, G_loss: 0.7557\n",
      "  Batch [1270/1299] D_loss: -2.3649, G_loss: 0.7415\n",
      "  Batch [1280/1299] D_loss: -2.4939, G_loss: 2.3233\n",
      "  Batch [1290/1299] D_loss: -2.2645, G_loss: 5.2125\n",
      "\n",
      "Epoch 100 Summary:\n",
      "  Average D_loss: -1.6997\n",
      "  Average G_loss: 3.3171\n"
     ]
    }
   ],
   "source": [
    "def main(selected_categories=None):\n",
    "    \"\"\"\n",
    "    Train the GAN with selected categorical variables\n",
    "    Args:\n",
    "        selected_categories: List of column names to use as categorical variables.\n",
    "                           If None, uses all columns except 'cell_id'\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'epochs': 100,\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 32,\n",
    "        'nb_layers': 3,\n",
    "        'hdim': 256,\n",
    "        'lr': 1e-4,\n",
    "        'nb_critic': 5,\n",
    "        'lambda_gp': 10  # Gradient penalty coefficient\n",
    "    }\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/\"\n",
    "    \n",
    "    # Load expression matrix\n",
    "    # matrix with cells as columns and genes as rows\n",
    "    with h5py.File(data_path+'combined_normalized_data.h5', 'r') as f:\n",
    "        x_train = f['matrix'][:]\n",
    "    \n",
    "    # Load all categorical variables from single file\n",
    "    cat_data = pd.read_csv(data_path+'combined_metadata.csv', sep=';')\n",
    "    print(\"Categorical data shape:\", cat_data.shape)\n",
    "    print(\"Available categorical variables:\", [col for col in cat_data.columns if col != 'cell_id'])\n",
    "    \n",
    "    # Determine which categories to use\n",
    "    if selected_categories is None:\n",
    "        # Use all columns except cell_id\n",
    "        categories_to_use = [col for col in cat_data.columns if col != 'cell_id']\n",
    "    else:\n",
    "        # Validate selected categories\n",
    "        invalid_categories = [cat for cat in selected_categories if cat not in cat_data.columns]\n",
    "        if invalid_categories:\n",
    "            raise ValueError(f\"Invalid categories: {invalid_categories}\")\n",
    "        categories_to_use = selected_categories\n",
    "    \n",
    "    print(f\"\\nUsing categorical variables: {categories_to_use}\")\n",
    "    \n",
    "    # Create dictionaries and inverse mappings for categorical variables\n",
    "    cat_dicts = []\n",
    "    encoded_covs = []\n",
    "    \n",
    "    # Process each selected column as a categorical variable\n",
    "    for column in categories_to_use:\n",
    "        # Get the column data\n",
    "        cat_vec = cat_data[column]\n",
    "        print(f\"\\nProcessing categorical variable: {column}\")\n",
    "        \n",
    "        # Create list of unique category names, sorted\n",
    "        dict_inv = np.array(list(sorted(set(cat_vec.values))))\n",
    "        dict_map = {t: i for i, t in enumerate(dict_inv)}\n",
    "        cat_dicts.append(dict_inv)\n",
    "        \n",
    "        # Convert categorical variables to integers\n",
    "        encoded = np.vectorize(lambda t: dict_map[t])(cat_vec)\n",
    "        encoded = encoded.reshape(-1, 1)  # Reshape to column vector\n",
    "        encoded_covs.append(encoded)\n",
    "        \n",
    "        print(f\"Categories in {column}:\", dict_inv)\n",
    "        print(f\"Number of categories:\", len(dict_inv))\n",
    "    \n",
    "    # Combine all categorical covariates\n",
    "    cat_covs = np.hstack(encoded_covs)\n",
    "    print(\"\\nCombined categorical covariates shape:\", cat_covs.shape)\n",
    "    \n",
    "    # Load numerical covariates (currently empty)\n",
    "    num_covs = np.zeros((x_train.shape[0], 0))\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to device\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)  # Keep on CPU for DataLoader\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(x_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    vocab_sizes = [len(c) for c in cat_dicts]\n",
    "    print(\"\\nVocabulary sizes for categorical variables:\", vocab_sizes)\n",
    "    nb_numeric = num_covs.shape[-1]\n",
    "    x_dim = x_train.shape[-1]\n",
    "    \n",
    "    generator = Generator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "        z_dim=CONFIG['latent_dim']).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers']).to(device)\n",
    "    \n",
    "    # Define save function\n",
    "    def save_models(generator, discriminator, epoch):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # create save directory\n",
    "        categories_str = \"+\".join(categories_to_use)\n",
    "        save_dir = os.path.join(data_path, \"saved_models\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Create run folder\n",
    "        run_dir = os.path.join(save_dir, f\"run_{timestamp}_{categories_str}\")\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "        # Save model initialization parameters\n",
    "        model_config = {\n",
    "            'x_dim': x_dim,\n",
    "            'vocab_sizes': vocab_sizes,\n",
    "            'nb_numeric': nb_numeric,\n",
    "            'h_dims': [CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "            'z_dim': CONFIG['latent_dim'],\n",
    "            'categories': categories_to_use,\n",
    "            'training_config': CONFIG}\n",
    "        config_path = os.path.join(run_dir, 'model_config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(model_config, f, indent=4)\n",
    "        \n",
    "        # Save generator\n",
    "        generator_path = os.path.join(run_dir, f\"generator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(generator.state_dict(), generator_path)\n",
    "        \n",
    "        \n",
    "        # Save discriminator\n",
    "        discriminator_path = os.path.join(run_dir, f\"discriminator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(discriminator.state_dict(), discriminator_path)\n",
    "        \n",
    "        print(f\"\\nModels saved at epoch {epoch + 1}:\")\n",
    "        print(f\"Generator: {generator_path}\")\n",
    "        print(f\"Discriminator: {discriminator_path}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        if wandb.run is not None:\n",
    "            wandb.save(generator_path)\n",
    "            wandb.save(discriminator_path)\n",
    "\n",
    "    # Initialize wandb with unique run name\n",
    "    run_name = f\"run_{int(time.time())}\"  # Uses timestamp for unique name\n",
    "    wandb.init(\n",
    "        project='adversarial_gene_expr',\n",
    "        config=CONFIG,\n",
    "        name=run_name,\n",
    "        reinit=True  # Ensures new run each time\n",
    "    )\n",
    "    \n",
    "    # Add selected categories to wandb config\n",
    "    wandb.config.update({'selected_categories': categories_to_use})\n",
    "    \n",
    "    # Train model\n",
    "    train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=train_loader,\n",
    "        cat_covs=cat_covs,\n",
    "        num_covs=num_covs,\n",
    "        config=CONFIG,\n",
    "        device=device,\n",
    "        save_fn=save_models\n",
    "        #save_fn=None\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    # Use specific categories:\n",
    "    main(selected_categories=['dataset','cell_type'])\n",
    "    \n",
    "    # Or use all available categories:\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for data gneration\n",
    "def inspect_generator_dims(generator):\n",
    "    \"\"\"\n",
    "    Inspect the generator's dimensions and architecture\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Generator model\n",
    "    \n",
    "    Returns:\n",
    "        dict containing dimension information\n",
    "    \"\"\"\n",
    "    # Get embedding dimensions\n",
    "    embedding_dims = [emb.embedding_dim for emb in generator.embeddings]\n",
    "    total_embedding_dim = sum(embedding_dims)\n",
    "    \n",
    "    # Get first layer dimension\n",
    "    first_layer_in_dim = generator.network[0].in_features\n",
    "    \n",
    "    return {\n",
    "        'embedding_dims': embedding_dims,\n",
    "        'total_embedding_dim': total_embedding_dim,\n",
    "        'first_layer_in_dim': first_layer_in_dim,\n",
    "        'recommended_latent_dim': first_layer_in_dim - total_embedding_dim\n",
    "    }\n",
    "\n",
    "def generate_expression_profiles(generator, n_samples, dataset_category, cell_type_category, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate gene expression profiles using the trained cWGAN generator\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained Generator model\n",
    "        n_samples: Number of profiles to generate\n",
    "        dataset_category: Integer indicating which dataset category to generate\n",
    "        cell_type_category: Integer indicating which cell type to generate\n",
    "        device: Device to run generation on ('cuda', 'mps', or 'cpu')\n",
    "        debug: If True, print debugging information\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of generated expression profiles with shape (n_samples, n_genes)\n",
    "    \"\"\"\n",
    "    # Set generator to eval mode\n",
    "    generator.eval()\n",
    "    \n",
    "    # Inspect dimensions\n",
    "    dims = inspect_generator_dims(generator)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Generator dimensions:\")\n",
    "        for k, v in dims.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    \n",
    "    # Create latent vectors\n",
    "    latent_dim = dims['recommended_latent_dim']\n",
    "    z = torch.randn(n_samples, latent_dim, device=device)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nLatent vector shape: {z.shape}\")\n",
    "    \n",
    "    # Create categorical condition tensor with dataset and cell type\n",
    "    num_embeddings = len(generator.embeddings)\n",
    "    cat_covs = torch.zeros((n_samples, num_embeddings), dtype=torch.long, device=device)\n",
    "    cat_covs[:, 0] = dataset_category  # Set dataset category\n",
    "    cat_covs[:, 1] = cell_type_category  # Set cell type category\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Categorical covariates shape: {cat_covs.shape}\")\n",
    "        print(f\"Number of embedding layers: {num_embeddings}\")\n",
    "    \n",
    "    # Create empty numeric covariates tensor\n",
    "    num_covs = torch.zeros((n_samples, 0), device=device)\n",
    "    \n",
    "    # Generate samples\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings\n",
    "            embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(generator.embeddings)]\n",
    "            embedded = torch.cat(embeddings, dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Embedded shape: {embedded.shape}\")\n",
    "            \n",
    "            # Concatenate inputs\n",
    "            gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Generator input shape: {gen_input.shape}\")\n",
    "                print(f\"First layer input dim: {generator.network[0].in_features}\")\n",
    "                print(f\"First layer weight shape: {generator.network[0].weight.shape}\")\n",
    "            \n",
    "            # Generate samples\n",
    "            fake_samples = generator.network(gen_input)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(\"\\nError during generation:\")\n",
    "        print(e)\n",
    "        print(\"\\nGenerator architecture:\")\n",
    "        print(generator)\n",
    "        raise\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    return fake_samples.cpu().numpy()\n",
    "\n",
    "def generate_and_save_profiles(generator, samples_per_combination, save_path, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate expression profiles with custom sample counts for each dataset and cell type combination\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained Generator model\n",
    "        samples_per_combination: Dictionary with tuples as keys (dataset_num, cell_type_num) \n",
    "                               and number of samples as values\n",
    "        save_path: Path to save the generated profiles\n",
    "        device: Device to run generation on ('cuda', 'mps', or 'cpu')\n",
    "        debug: If True, print debugging information\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "    all_categories = []\n",
    "    \n",
    "    # Cell type mapping\n",
    "    cell_type_names = {\n",
    "        0: 'B cells',\n",
    "        1: 'Dendritic cells',\n",
    "        2: 'Endothelial cells',\n",
    "        3: 'Erythrocytes',\n",
    "        4: 'Fibroblasts',\n",
    "        5: 'Granulocytes',\n",
    "        6: 'Macrophages',\n",
    "        7: 'Monocytes',\n",
    "        8: 'NK cells',\n",
    "        9: 'T cells'\n",
    "    }\n",
    "    \n",
    "    # Generate samples for specified combinations\n",
    "    for (dataset_num, cell_type_num), n_samples in samples_per_combination.items():\n",
    "        dataset_category = dataset_num - 1  # Convert dataset number (1-7) to category index (0-6)\n",
    "        cell_type_category = cell_type_num\n",
    "        cell_type_name = cell_type_names[cell_type_num]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nGenerating {n_samples} samples for dataset{dataset_num}, {cell_type_name}\")\n",
    "        \n",
    "        samples = generate_expression_profiles(\n",
    "            generator, \n",
    "            n_samples, \n",
    "            dataset_category,\n",
    "            cell_type_category,\n",
    "            device,\n",
    "            debug=debug\n",
    "        )\n",
    "        all_samples.append(samples)\n",
    "        all_categories.extend([f'dataset{dataset_num}_{cell_type_name}'] * n_samples)\n",
    "\n",
    "    print(\"Save location: \"+str(save_path))\n",
    "\n",
    "    # Combine all samples\n",
    "    all_samples = np.vstack(all_samples)\n",
    "    \n",
    "    # Save generated profiles\n",
    "    np.save(f'{save_path}_profiles.npy', all_samples)\n",
    "    \n",
    "    # Save category labels\n",
    "    with open(f'{save_path}_categories.txt', 'w') as f:\n",
    "        for category in all_categories:\n",
    "            f.write(f'{category}\\n')\n",
    "\n",
    "    \n",
    "            \n",
    "    return all_samples, all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Save location: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_150654_dataset_cell_type_generated_data\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "\n",
    "# Set directories\n",
    "# 3 hidden layers\n",
    "data_path = \"~/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/\"\n",
    "run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/saved_models/run_20250121_150654_dataset_cell_type\"\n",
    "generator_model = \"generator_20250121_150654_dataset_cell_type_epoch_61.pt\"\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(run_dir, 'model_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "    \n",
    "# Initialize models with saved configuration\n",
    "generator = Generator(\n",
    "    x_dim=model_config['x_dim'],\n",
    "    vocab_sizes=model_config['vocab_sizes'],\n",
    "    nb_numeric=model_config['nb_numeric'],\n",
    "    h_dims=model_config['h_dims'],\n",
    "    z_dim=model_config['z_dim']).to(device)\n",
    "    \n",
    "discriminator = Discriminator(\n",
    "    x_dim=model_config['x_dim'],\n",
    "    vocab_sizes=model_config['vocab_sizes'],\n",
    "    nb_numeric=model_config['nb_numeric'],\n",
    "    h_dims=model_config['h_dims']).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#discriminator_path = os.path.join(run_dir, \"discriminator.pt\")\n",
    "#discriminator.load_state_dict(torch.load(discriminator_path, map_location=device, weights_only=True))\n",
    "generator_path = os.path.join(run_dir, generator_model)\n",
    "generator.load_state_dict(torch.load(generator_path, map_location=device, weights_only=True))\n",
    "\n",
    "meta = pd.read_csv(data_path+'combined_metadata.csv', sep=';')\n",
    "# create a dict listing datasets and celltypes\n",
    "counts = meta.groupby(['dataset', 'cell_type']).size().to_dict()\n",
    "\n",
    "# Convert the dict keys from tuple of strings to tuple of numbers\n",
    "samples_dict = {}\n",
    "for (dataset, cell_type), count in counts.items():\n",
    "    # Extract dataset number (assuming format 'dataset1', 'dataset2', etc.)\n",
    "    dataset_num = int(dataset.replace('dataset', ''))\n",
    "    \n",
    "    # Get cell type number (assuming you have a mapping for cell types to numbers)\n",
    "    cell_type_map = {\n",
    "        'B cells': 0,\n",
    "        'Dendritic cells': 1,\n",
    "        'Endothelial cells': 2,\n",
    "        'Erythrocytes': 3,\n",
    "        'Fibroblasts': 4,\n",
    "        'Granulocytes': 5,\n",
    "        'Macrophages': 6,\n",
    "        'Monocytes': 7,\n",
    "        'NK cells': 8,\n",
    "        'T cells': 9\n",
    "    }\n",
    "    cell_type_num = cell_type_map[cell_type]\n",
    "    \n",
    "    # Add to new dictionary with numerical tuple as key\n",
    "    samples_dict[(dataset_num, cell_type_num)] = count\n",
    "\n",
    "\n",
    "#samples_dict = {\n",
    "#    (1, 0): 1000,  # 1000 samples from dataset1, cell type 0\n",
    "#    (1, 1): 500,   # 500 samples from dataset1, cell type 1\n",
    "#    (3, 2): 750    # 750 samples from dataset3, cell type 2\n",
    "#}\n",
    "\n",
    "## this function will generate data based on the relative quantities in samples_dict\n",
    "## and will generate a profiles.npy (containing expression profile for each sample)\n",
    "## and a categories.txt (containing labels for each samples) files\n",
    "all_samples, categories = generate_and_save_profiles(\n",
    "    generator,\n",
    "    samples_per_combination=samples_dict,\n",
    "    save_path=run_dir+'_generated_data',\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.231811</td>\n",
       "      <td>5.012554</td>\n",
       "      <td>4.040084</td>\n",
       "      <td>3.683613</td>\n",
       "      <td>0.436647</td>\n",
       "      <td>3.761825</td>\n",
       "      <td>0.943701</td>\n",
       "      <td>3.246516</td>\n",
       "      <td>3.937541</td>\n",
       "      <td>1.443107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149865</td>\n",
       "      <td>0.011442</td>\n",
       "      <td>0.036922</td>\n",
       "      <td>-0.018661</td>\n",
       "      <td>-0.022489</td>\n",
       "      <td>0.022302</td>\n",
       "      <td>0.062507</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>-0.021453</td>\n",
       "      <td>dataset1_B cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001106</td>\n",
       "      <td>6.723053</td>\n",
       "      <td>4.880553</td>\n",
       "      <td>4.649127</td>\n",
       "      <td>0.605724</td>\n",
       "      <td>-1.628438</td>\n",
       "      <td>0.288405</td>\n",
       "      <td>4.578445</td>\n",
       "      <td>3.061715</td>\n",
       "      <td>1.110912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019799</td>\n",
       "      <td>0.044679</td>\n",
       "      <td>-0.010733</td>\n",
       "      <td>0.033751</td>\n",
       "      <td>0.013674</td>\n",
       "      <td>0.077913</td>\n",
       "      <td>-0.068833</td>\n",
       "      <td>-0.030298</td>\n",
       "      <td>-0.013414</td>\n",
       "      <td>dataset1_B cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.027212</td>\n",
       "      <td>5.233882</td>\n",
       "      <td>3.615242</td>\n",
       "      <td>3.108646</td>\n",
       "      <td>1.175094</td>\n",
       "      <td>3.089045</td>\n",
       "      <td>1.575116</td>\n",
       "      <td>2.513422</td>\n",
       "      <td>3.686309</td>\n",
       "      <td>2.185993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097830</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.010542</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>-0.001769</td>\n",
       "      <td>0.073058</td>\n",
       "      <td>0.056470</td>\n",
       "      <td>0.017281</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>dataset1_B cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.265750</td>\n",
       "      <td>5.726051</td>\n",
       "      <td>4.193067</td>\n",
       "      <td>4.037908</td>\n",
       "      <td>0.014425</td>\n",
       "      <td>2.711107</td>\n",
       "      <td>-0.021076</td>\n",
       "      <td>3.636868</td>\n",
       "      <td>2.291311</td>\n",
       "      <td>2.954214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052932</td>\n",
       "      <td>0.059226</td>\n",
       "      <td>0.050422</td>\n",
       "      <td>-0.082419</td>\n",
       "      <td>0.037294</td>\n",
       "      <td>-0.017001</td>\n",
       "      <td>-0.044325</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>0.038056</td>\n",
       "      <td>dataset1_B cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.428651</td>\n",
       "      <td>1.383675</td>\n",
       "      <td>0.202624</td>\n",
       "      <td>0.219082</td>\n",
       "      <td>0.698924</td>\n",
       "      <td>4.395520</td>\n",
       "      <td>3.265499</td>\n",
       "      <td>0.442863</td>\n",
       "      <td>3.801274</td>\n",
       "      <td>-0.605415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140012</td>\n",
       "      <td>0.399742</td>\n",
       "      <td>0.205119</td>\n",
       "      <td>0.209274</td>\n",
       "      <td>-0.028910</td>\n",
       "      <td>0.215225</td>\n",
       "      <td>0.685272</td>\n",
       "      <td>0.397794</td>\n",
       "      <td>0.269825</td>\n",
       "      <td>dataset1_B cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41583</th>\n",
       "      <td>-0.043463</td>\n",
       "      <td>4.471334</td>\n",
       "      <td>3.290458</td>\n",
       "      <td>2.960582</td>\n",
       "      <td>0.126797</td>\n",
       "      <td>4.563755</td>\n",
       "      <td>1.354325</td>\n",
       "      <td>2.588662</td>\n",
       "      <td>4.697986</td>\n",
       "      <td>1.769787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151689</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.021533</td>\n",
       "      <td>-0.071441</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>-0.011879</td>\n",
       "      <td>-0.017619</td>\n",
       "      <td>0.025726</td>\n",
       "      <td>0.027298</td>\n",
       "      <td>dataset7_T cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41584</th>\n",
       "      <td>6.171608</td>\n",
       "      <td>0.434414</td>\n",
       "      <td>-0.085250</td>\n",
       "      <td>-0.186500</td>\n",
       "      <td>3.630826</td>\n",
       "      <td>2.031105</td>\n",
       "      <td>3.854700</td>\n",
       "      <td>-0.312018</td>\n",
       "      <td>3.564202</td>\n",
       "      <td>-0.271754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071843</td>\n",
       "      <td>0.059604</td>\n",
       "      <td>0.076521</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>-0.059938</td>\n",
       "      <td>0.051268</td>\n",
       "      <td>-0.016612</td>\n",
       "      <td>0.197256</td>\n",
       "      <td>0.061592</td>\n",
       "      <td>dataset7_T cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41585</th>\n",
       "      <td>0.575687</td>\n",
       "      <td>0.140930</td>\n",
       "      <td>-0.099514</td>\n",
       "      <td>-0.132665</td>\n",
       "      <td>-0.021235</td>\n",
       "      <td>0.022371</td>\n",
       "      <td>-0.057032</td>\n",
       "      <td>-0.097039</td>\n",
       "      <td>1.081624</td>\n",
       "      <td>-0.245499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066604</td>\n",
       "      <td>0.021906</td>\n",
       "      <td>0.117857</td>\n",
       "      <td>0.044744</td>\n",
       "      <td>-0.019693</td>\n",
       "      <td>0.005893</td>\n",
       "      <td>0.024329</td>\n",
       "      <td>-0.012449</td>\n",
       "      <td>-0.008558</td>\n",
       "      <td>dataset7_T cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41586</th>\n",
       "      <td>0.902666</td>\n",
       "      <td>5.474178</td>\n",
       "      <td>3.976243</td>\n",
       "      <td>3.669129</td>\n",
       "      <td>0.097642</td>\n",
       "      <td>-0.206113</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>3.302111</td>\n",
       "      <td>1.744207</td>\n",
       "      <td>2.544704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046760</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.035773</td>\n",
       "      <td>0.013885</td>\n",
       "      <td>0.066925</td>\n",
       "      <td>-0.038613</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>-0.001759</td>\n",
       "      <td>dataset7_T cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41587</th>\n",
       "      <td>0.233810</td>\n",
       "      <td>0.261204</td>\n",
       "      <td>0.221613</td>\n",
       "      <td>0.149247</td>\n",
       "      <td>0.567148</td>\n",
       "      <td>0.176695</td>\n",
       "      <td>0.093468</td>\n",
       "      <td>0.177456</td>\n",
       "      <td>2.004768</td>\n",
       "      <td>-0.713268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069717</td>\n",
       "      <td>0.026720</td>\n",
       "      <td>0.077667</td>\n",
       "      <td>0.075238</td>\n",
       "      <td>-0.014985</td>\n",
       "      <td>0.067577</td>\n",
       "      <td>-0.002877</td>\n",
       "      <td>-0.050849</td>\n",
       "      <td>0.007343</td>\n",
       "      <td>dataset7_T cells</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41588 rows  1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.231811  5.012554  4.040084  3.683613  0.436647  3.761825  0.943701   \n",
       "1      0.001106  6.723053  4.880553  4.649127  0.605724 -1.628438  0.288405   \n",
       "2      3.027212  5.233882  3.615242  3.108646  1.175094  3.089045  1.575116   \n",
       "3      0.265750  5.726051  4.193067  4.037908  0.014425  2.711107 -0.021076   \n",
       "4      4.428651  1.383675  0.202624  0.219082  0.698924  4.395520  3.265499   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "41583 -0.043463  4.471334  3.290458  2.960582  0.126797  4.563755  1.354325   \n",
       "41584  6.171608  0.434414 -0.085250 -0.186500  3.630826  2.031105  3.854700   \n",
       "41585  0.575687  0.140930 -0.099514 -0.132665 -0.021235  0.022371 -0.057032   \n",
       "41586  0.902666  5.474178  3.976243  3.669129  0.097642 -0.206113  0.381000   \n",
       "41587  0.233810  0.261204  0.221613  0.149247  0.567148  0.176695  0.093468   \n",
       "\n",
       "              7         8         9  ...       991       992       993  \\\n",
       "0      3.246516  3.937541  1.443107  ...  0.149865  0.011442  0.036922   \n",
       "1      4.578445  3.061715  1.110912  ...  0.019799  0.044679 -0.010733   \n",
       "2      2.513422  3.686309  2.185993  ...  0.097830  0.009697  0.010542   \n",
       "3      3.636868  2.291311  2.954214  ...  0.052932  0.059226  0.050422   \n",
       "4      0.442863  3.801274 -0.605415  ...  0.140012  0.399742  0.205119   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "41583  2.588662  4.697986  1.769787  ...  0.151689  0.023442  0.021533   \n",
       "41584 -0.312018  3.564202 -0.271754  ...  0.071843  0.059604  0.076521   \n",
       "41585 -0.097039  1.081624 -0.245499  ...  0.066604  0.021906  0.117857   \n",
       "41586  3.302111  1.744207  2.544704  ...  0.046760  0.017676  0.001327   \n",
       "41587  0.177456  2.004768 -0.713268  ...  0.069717  0.026720  0.077667   \n",
       "\n",
       "            994       995       996       997       998       999  \\\n",
       "0     -0.018661 -0.022489  0.022302  0.062507  0.022485 -0.021453   \n",
       "1      0.033751  0.013674  0.077913 -0.068833 -0.030298 -0.013414   \n",
       "2      0.000982 -0.001769  0.073058  0.056470  0.017281  0.005731   \n",
       "3     -0.082419  0.037294 -0.017001 -0.044325 -0.001857  0.038056   \n",
       "4      0.209274 -0.028910  0.215225  0.685272  0.397794  0.269825   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "41583 -0.071441  0.003788 -0.011879 -0.017619  0.025726  0.027298   \n",
       "41584  0.019365 -0.059938  0.051268 -0.016612  0.197256  0.061592   \n",
       "41585  0.044744 -0.019693  0.005893  0.024329 -0.012449 -0.008558   \n",
       "41586  0.035773  0.013885  0.066925 -0.038613  0.012564 -0.001759   \n",
       "41587  0.075238 -0.014985  0.067577 -0.002877 -0.050849  0.007343   \n",
       "\n",
       "                 labels  \n",
       "0      dataset1_B cells  \n",
       "1      dataset1_B cells  \n",
       "2      dataset1_B cells  \n",
       "3      dataset1_B cells  \n",
       "4      dataset1_B cells  \n",
       "...                 ...  \n",
       "41583  dataset7_T cells  \n",
       "41584  dataset7_T cells  \n",
       "41585  dataset7_T cells  \n",
       "41586  dataset7_T cells  \n",
       "41587  dataset7_T cells  \n",
       "\n",
       "[41588 rows x 1001 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load and analyze generated data\n",
    "# Load the generated profiles\n",
    "profiles = np.load(run_dir + '_generated_data_profiles.npy')\n",
    "\n",
    "# Load categories\n",
    "with open(run_dir + '_generated_data_categories.txt', 'r') as f:\n",
    "    categories = [line.strip() for line in f]\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(profiles)\n",
    "\n",
    "# Add categories as a column\n",
    "df['labels'] = categories\n",
    "\n",
    "## \n",
    "# Add separate dataset and cell_type columns\n",
    "df['dataset'] = df['labels'].apply(lambda x: x.split('_')[0])\n",
    "df['cell_type'] = df['labels'].apply(lambda x: x.split('_')[1])\n",
    "# drop the combined labels colum\n",
    "df=df.drop(['labels'], axis=1)\n",
    "\n",
    "# Save a csv file\n",
    "df.to_csv(f'{run_dir}_generated_data.csv', index=False)\n",
    "#\n",
    "with h5py.File(f'{run_dir}_generated_data.h5', 'w') as f:\n",
    "    # Save as matrix - select only numerical values\n",
    "    f.create_dataset('matrix', data=df.select_dtypes(include=[np.number]).values)  # or df.to_numpy()\n",
    "\n",
    "# If you need to save the categorical information separately:\n",
    "df_labels = df.select_dtypes(exclude=[np.number])\n",
    "df_labels.to_csv(f'{run_dir}_generated_labels.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
