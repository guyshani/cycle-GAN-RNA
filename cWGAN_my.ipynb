{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims, z_dim):\n",
    "        \"\"\"\n",
    "        Generator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of output data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "            z_dim: Dimension of latent noise vector\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size)) \n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is latent dim + embedding dim + numeric covariates\n",
    "        input_dim = z_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build generator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, x_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims):\n",
    "        \"\"\"\n",
    "        Discriminator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of input data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size))\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is data dim + embedding dim + numeric covariates\n",
    "        input_dim = x_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build discriminator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        disc_input = torch.cat([x, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(disc_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, dataloader, cat_covs, num_covs, \n",
    "              config, device, score_fn=None, save_fn=None):\n",
    "    \"\"\"\n",
    "    Train the conditional GAN with progress tracking and proper device handling\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    g_optimizer = optim.RMSprop(generator.parameters(), lr=config['lr'])\n",
    "    d_optimizer = optim.RMSprop(discriminator.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Convert covariates to tensors and move to device\n",
    "    cat_covs = torch.tensor(cat_covs, dtype=torch.long).to(device)\n",
    "    num_covs = torch.tensor(num_covs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "    print(f\"Total batches per epoch: {total_batches}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        print(f\"\\nEpoch [{epoch+1}/{config['epochs']}]\")\n",
    "        \n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Move real data to device\n",
    "            real_data = real_data.to(device)\n",
    "            \n",
    "            # Get random batch of categorical and numerical covariates\n",
    "            batch_indices = torch.randint(0, cat_covs.size(0), (batch_size,))\n",
    "            batch_cat_covs = cat_covs[batch_indices]\n",
    "            batch_num_covs = num_covs[batch_indices]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(config['nb_critic']):\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "                fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate discriminator loss\n",
    "                real_validity = discriminator(real_data, batch_cat_covs, batch_num_covs)\n",
    "                fake_validity = discriminator(fake_data.detach(), batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                d_loss = -(torch.mean(real_validity) - torch.mean(fake_validity))\n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                # Clip discriminator weights (Wasserstein GAN)\n",
    "                for p in discriminator.parameters():\n",
    "                    p.data.clamp_(-0.01, 0.01)\n",
    "                    \n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "            fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "            \n",
    "            # Calculate generator loss\n",
    "            fake_validity = discriminator(fake_data, batch_cat_covs, batch_num_covs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            g_losses.append(g_loss.item())\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Batch [{batch_idx}/{total_batches}] \" \\\n",
    "                      f\"D_loss: {d_loss.item():.4f}, \" \\\n",
    "                      f\"G_loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average D_loss: {avg_d_loss:.4f}\")\n",
    "        print(f\"  Average G_loss: {avg_g_loss:.4f}\")\n",
    "        \n",
    "        # Log metrics\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'd_loss': np.mean(d_losses),\n",
    "                'g_loss': np.mean(g_losses)\n",
    "            })\n",
    "        \n",
    "        # Evaluate and save model if needed\n",
    "        if score_fn is not None and epoch % 10 == 0:\n",
    "            score = score_fn(generator)\n",
    "            print(f'Epoch {epoch}: Score = {score:.4f}')\n",
    "        \n",
    "        if save_fn is not None and epoch % 50 == 0:\n",
    "            save_fn(generator, discriminator, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Categorical data shape: (41588, 3)\n",
      "Available categorical variables: ['dataset', 'cluster']\n",
      "\n",
      "Using categorical variables: ['dataset']\n",
      "\n",
      "Processing categorical variable: dataset\n",
      "Categories in dataset: ['dataset1' 'dataset2' 'dataset3' 'dataset4' 'dataset5' 'dataset6'\n",
      " 'dataset7']\n",
      "Number of categories: 7\n",
      "\n",
      "Combined categorical covariates shape: (41588, 1)\n",
      "\n",
      "Vocabulary sizes for categorical variables: [7]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>d_loss</td><td>▃█▇▆▆▄▅▃▃▃▃▂▃▃▃▂▂▂▃▂▂▃▃▂▃▂▂▁▂▂▂▁▁▁▂▁▂▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>g_loss</td><td>█▆▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▁▁▂▁▂▂▂▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>d_loss</td><td>-0.1862</td></tr><tr><td>epoch</td><td>54</td></tr><tr><td>g_loss</td><td>0.00995</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_1736759030</strong> at: <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/wqbanorz' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/wqbanorz</a><br> View project at: <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 4 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250113_110350-wqbanorz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/guyshani/Documents/PHD/Aim_2/cycle_GAN/wandb/run-20250113_121829-wxud2yrm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/wxud2yrm' target=\"_blank\">run_1736763509</a></strong> to <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/wxud2yrm' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/wxud2yrm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 100 epochs...\n",
      "Total batches per epoch: 1299\n",
      "Using device: mps\n",
      "\n",
      "Epoch [1/100]\n",
      "  Batch [0/1299] D_loss: -0.7218, G_loss: 0.5004\n",
      "  Batch [10/1299] D_loss: -2.0769, G_loss: 1.5124\n",
      "  Batch [20/1299] D_loss: -2.0532, G_loss: 1.8511\n",
      "  Batch [30/1299] D_loss: -0.6816, G_loss: -0.1891\n",
      "  Batch [40/1299] D_loss: -1.5830, G_loss: 0.5061\n",
      "  Batch [50/1299] D_loss: -2.0133, G_loss: 0.2262\n",
      "  Batch [60/1299] D_loss: -1.5099, G_loss: 0.5708\n",
      "  Batch [70/1299] D_loss: -2.2564, G_loss: -0.7216\n",
      "  Batch [80/1299] D_loss: -1.5234, G_loss: 0.9249\n",
      "  Batch [90/1299] D_loss: -0.7501, G_loss: 1.2147\n",
      "  Batch [100/1299] D_loss: -1.1590, G_loss: 0.7080\n",
      "  Batch [110/1299] D_loss: -0.6572, G_loss: -2.0895\n",
      "  Batch [120/1299] D_loss: -1.7128, G_loss: -3.1317\n",
      "  Batch [130/1299] D_loss: -1.4169, G_loss: 0.7607\n",
      "  Batch [140/1299] D_loss: -1.5687, G_loss: 0.9701\n",
      "  Batch [150/1299] D_loss: -0.9465, G_loss: 0.0502\n",
      "  Batch [160/1299] D_loss: -0.8098, G_loss: -0.7656\n",
      "  Batch [170/1299] D_loss: -1.4835, G_loss: 1.3105\n",
      "  Batch [180/1299] D_loss: -0.9246, G_loss: 1.3488\n",
      "  Batch [190/1299] D_loss: -0.2601, G_loss: -2.1799\n",
      "  Batch [200/1299] D_loss: -0.4980, G_loss: 2.0091\n",
      "  Batch [210/1299] D_loss: -1.3520, G_loss: 1.4022\n",
      "  Batch [220/1299] D_loss: -1.0502, G_loss: 0.7044\n",
      "  Batch [230/1299] D_loss: -0.7797, G_loss: -1.4437\n",
      "  Batch [240/1299] D_loss: -0.8639, G_loss: -0.0568\n",
      "  Batch [250/1299] D_loss: -1.4028, G_loss: 0.9506\n",
      "  Batch [260/1299] D_loss: -1.1151, G_loss: 2.5901\n",
      "  Batch [270/1299] D_loss: -0.7962, G_loss: -1.3587\n",
      "  Batch [280/1299] D_loss: -1.1075, G_loss: 0.6107\n",
      "  Batch [290/1299] D_loss: -0.7711, G_loss: 0.9075\n",
      "  Batch [300/1299] D_loss: -1.1787, G_loss: -0.7351\n",
      "  Batch [310/1299] D_loss: -0.8917, G_loss: 0.9234\n",
      "  Batch [320/1299] D_loss: -0.6640, G_loss: 0.1279\n",
      "  Batch [330/1299] D_loss: -1.2031, G_loss: -0.6094\n",
      "  Batch [340/1299] D_loss: -1.2466, G_loss: 1.1756\n",
      "  Batch [350/1299] D_loss: -1.2001, G_loss: 0.4908\n",
      "  Batch [360/1299] D_loss: -0.8686, G_loss: 0.3028\n",
      "  Batch [370/1299] D_loss: -1.0568, G_loss: 0.1403\n",
      "  Batch [380/1299] D_loss: -0.4277, G_loss: 0.9596\n",
      "  Batch [390/1299] D_loss: -0.9278, G_loss: -0.2936\n",
      "  Batch [400/1299] D_loss: -1.0634, G_loss: -0.7805\n",
      "  Batch [410/1299] D_loss: -0.6951, G_loss: 1.6449\n",
      "  Batch [420/1299] D_loss: -0.9091, G_loss: -1.0437\n",
      "  Batch [430/1299] D_loss: -0.4913, G_loss: 0.1072\n",
      "  Batch [440/1299] D_loss: -0.8718, G_loss: 0.2759\n",
      "  Batch [450/1299] D_loss: -0.7462, G_loss: -0.1984\n",
      "  Batch [460/1299] D_loss: -0.5084, G_loss: -0.8665\n",
      "  Batch [470/1299] D_loss: -1.0083, G_loss: 0.2878\n",
      "  Batch [480/1299] D_loss: -0.2539, G_loss: -0.6123\n",
      "  Batch [490/1299] D_loss: -0.7181, G_loss: 0.6397\n",
      "  Batch [500/1299] D_loss: -0.7305, G_loss: 0.3715\n",
      "  Batch [510/1299] D_loss: -0.6049, G_loss: -0.5796\n",
      "  Batch [520/1299] D_loss: -0.6069, G_loss: -1.0442\n",
      "  Batch [530/1299] D_loss: -0.3946, G_loss: 0.8248\n",
      "  Batch [540/1299] D_loss: -0.6503, G_loss: 0.5447\n",
      "  Batch [550/1299] D_loss: -0.7047, G_loss: 0.6506\n",
      "  Batch [560/1299] D_loss: -1.2893, G_loss: 0.3898\n",
      "  Batch [570/1299] D_loss: -0.9327, G_loss: 0.4727\n",
      "  Batch [580/1299] D_loss: -0.3439, G_loss: -1.6570\n",
      "  Batch [590/1299] D_loss: -0.4796, G_loss: 1.0879\n",
      "  Batch [600/1299] D_loss: -0.4414, G_loss: -0.1143\n",
      "  Batch [610/1299] D_loss: -0.9979, G_loss: 0.0945\n",
      "  Batch [620/1299] D_loss: -1.6388, G_loss: 0.8777\n",
      "  Batch [630/1299] D_loss: -0.5942, G_loss: -0.2772\n",
      "  Batch [640/1299] D_loss: -0.5182, G_loss: -0.0747\n",
      "  Batch [650/1299] D_loss: -0.8542, G_loss: 1.0146\n",
      "  Batch [660/1299] D_loss: -0.4786, G_loss: 2.1772\n",
      "  Batch [670/1299] D_loss: -0.5961, G_loss: 0.7017\n",
      "  Batch [680/1299] D_loss: -0.4532, G_loss: 0.1720\n",
      "  Batch [690/1299] D_loss: -0.6019, G_loss: 1.4907\n",
      "  Batch [700/1299] D_loss: -0.2049, G_loss: -0.9798\n",
      "  Batch [710/1299] D_loss: -1.0477, G_loss: 1.5152\n",
      "  Batch [720/1299] D_loss: -0.3935, G_loss: 0.0435\n",
      "  Batch [730/1299] D_loss: -0.2492, G_loss: -0.4418\n",
      "  Batch [740/1299] D_loss: -0.9620, G_loss: -0.4450\n",
      "  Batch [750/1299] D_loss: -0.4385, G_loss: -0.4298\n",
      "  Batch [760/1299] D_loss: -0.7850, G_loss: 0.8202\n",
      "  Batch [770/1299] D_loss: -0.8248, G_loss: 0.1970\n",
      "  Batch [780/1299] D_loss: -0.9691, G_loss: 2.4928\n",
      "  Batch [790/1299] D_loss: -0.9328, G_loss: -3.0612\n",
      "  Batch [800/1299] D_loss: -0.6710, G_loss: 1.7199\n",
      "  Batch [810/1299] D_loss: -0.8914, G_loss: -0.3002\n",
      "  Batch [820/1299] D_loss: -0.6239, G_loss: 0.4922\n",
      "  Batch [830/1299] D_loss: -0.2227, G_loss: -0.7406\n",
      "  Batch [840/1299] D_loss: -0.3650, G_loss: -1.2195\n",
      "  Batch [850/1299] D_loss: -0.2623, G_loss: 0.8683\n",
      "  Batch [860/1299] D_loss: -1.0657, G_loss: 1.1992\n",
      "  Batch [870/1299] D_loss: -0.2745, G_loss: 0.1797\n",
      "  Batch [880/1299] D_loss: -0.2866, G_loss: -0.0188\n",
      "  Batch [890/1299] D_loss: -1.0562, G_loss: 0.6928\n",
      "  Batch [900/1299] D_loss: -0.4880, G_loss: 0.5546\n",
      "  Batch [910/1299] D_loss: -0.5149, G_loss: -0.3099\n",
      "  Batch [920/1299] D_loss: -0.7493, G_loss: 1.6386\n",
      "  Batch [930/1299] D_loss: -0.8082, G_loss: -1.1038\n",
      "  Batch [940/1299] D_loss: -0.5159, G_loss: 0.2161\n",
      "  Batch [950/1299] D_loss: -0.6381, G_loss: 1.8791\n",
      "  Batch [960/1299] D_loss: -0.2719, G_loss: -1.1338\n",
      "  Batch [970/1299] D_loss: -0.8183, G_loss: 1.3041\n",
      "  Batch [980/1299] D_loss: -0.7921, G_loss: -0.8842\n",
      "  Batch [990/1299] D_loss: -0.3730, G_loss: 0.7112\n",
      "  Batch [1000/1299] D_loss: -0.4219, G_loss: -0.9729\n",
      "  Batch [1010/1299] D_loss: -0.7308, G_loss: 1.2379\n",
      "  Batch [1020/1299] D_loss: -0.3421, G_loss: -0.4655\n",
      "  Batch [1030/1299] D_loss: -0.1168, G_loss: 0.9673\n",
      "  Batch [1040/1299] D_loss: -0.2323, G_loss: 0.6691\n",
      "  Batch [1050/1299] D_loss: -1.4076, G_loss: -2.9119\n",
      "  Batch [1060/1299] D_loss: -0.4957, G_loss: 0.2475\n",
      "  Batch [1070/1299] D_loss: -0.5176, G_loss: 0.3182\n",
      "  Batch [1080/1299] D_loss: -0.4660, G_loss: -0.4209\n",
      "  Batch [1090/1299] D_loss: -0.1301, G_loss: 0.5216\n",
      "  Batch [1100/1299] D_loss: -0.7033, G_loss: 0.7400\n",
      "  Batch [1110/1299] D_loss: -0.3057, G_loss: -0.8328\n",
      "  Batch [1120/1299] D_loss: -0.6993, G_loss: 0.8708\n",
      "  Batch [1130/1299] D_loss: -0.3654, G_loss: -1.0996\n",
      "  Batch [1140/1299] D_loss: -0.2767, G_loss: 0.6970\n",
      "  Batch [1150/1299] D_loss: -0.5319, G_loss: -2.9815\n",
      "  Batch [1160/1299] D_loss: -0.0122, G_loss: 1.0310\n",
      "  Batch [1170/1299] D_loss: -0.1851, G_loss: -1.6319\n",
      "  Batch [1180/1299] D_loss: -1.0343, G_loss: -0.0916\n",
      "  Batch [1190/1299] D_loss: -0.9972, G_loss: 0.4216\n",
      "  Batch [1200/1299] D_loss: -0.5771, G_loss: -0.1292\n",
      "  Batch [1210/1299] D_loss: -0.0618, G_loss: 0.4525\n",
      "  Batch [1220/1299] D_loss: -0.1837, G_loss: 0.0113\n",
      "  Batch [1230/1299] D_loss: -0.2595, G_loss: -0.1984\n",
      "  Batch [1240/1299] D_loss: -0.4606, G_loss: 1.5951\n",
      "  Batch [1250/1299] D_loss: -0.2834, G_loss: -1.0668\n",
      "  Batch [1260/1299] D_loss: -0.3773, G_loss: 2.1309\n",
      "  Batch [1270/1299] D_loss: -0.4461, G_loss: -0.6431\n",
      "  Batch [1280/1299] D_loss: -0.3890, G_loss: 0.0969\n",
      "  Batch [1290/1299] D_loss: 0.0662, G_loss: -1.0567\n",
      "\n",
      "Epoch 1 Summary:\n",
      "  Average D_loss: -0.2930\n",
      "  Average G_loss: 0.2342\n",
      "\n",
      "Models saved at epoch 1:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/saved_models/run_20250113_121927_dataset/generator_20250113_121927_dataset_epoch_1.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/saved_models/run_20250113_121927_dataset/discriminator_20250113_121927_dataset_epoch_1.pt\n",
      "\n",
      "Epoch [2/100]\n",
      "  Batch [0/1299] D_loss: -0.3889, G_loss: 1.8697\n",
      "  Batch [10/1299] D_loss: -0.2823, G_loss: -0.7758\n",
      "  Batch [20/1299] D_loss: -0.4590, G_loss: 1.6972\n",
      "  Batch [30/1299] D_loss: -1.0043, G_loss: -0.3430\n",
      "  Batch [40/1299] D_loss: -0.7278, G_loss: 0.5301\n",
      "  Batch [50/1299] D_loss: -0.4837, G_loss: -0.3784\n",
      "  Batch [60/1299] D_loss: -0.6865, G_loss: -0.2221\n",
      "  Batch [70/1299] D_loss: -0.4190, G_loss: 0.4962\n",
      "  Batch [80/1299] D_loss: -0.4863, G_loss: 1.6682\n",
      "  Batch [90/1299] D_loss: -0.6967, G_loss: -0.1333\n",
      "  Batch [100/1299] D_loss: -1.2711, G_loss: -1.1587\n",
      "  Batch [110/1299] D_loss: -0.0670, G_loss: 0.8168\n",
      "  Batch [120/1299] D_loss: -0.7564, G_loss: -2.6422\n",
      "  Batch [130/1299] D_loss: -0.4784, G_loss: 1.5758\n",
      "  Batch [140/1299] D_loss: -0.2985, G_loss: -1.8868\n",
      "  Batch [150/1299] D_loss: -0.2795, G_loss: 0.4002\n",
      "  Batch [160/1299] D_loss: -0.7720, G_loss: 0.9290\n",
      "  Batch [170/1299] D_loss: -0.2702, G_loss: -1.7662\n",
      "  Batch [180/1299] D_loss: -0.2838, G_loss: 1.2476\n",
      "  Batch [190/1299] D_loss: -0.3042, G_loss: -2.2720\n",
      "  Batch [200/1299] D_loss: -1.1953, G_loss: 0.3958\n",
      "  Batch [210/1299] D_loss: -0.4404, G_loss: 0.7867\n",
      "  Batch [220/1299] D_loss: -1.1053, G_loss: 0.7463\n",
      "  Batch [230/1299] D_loss: -0.9451, G_loss: -0.7270\n",
      "  Batch [240/1299] D_loss: -0.0404, G_loss: -0.1866\n",
      "  Batch [250/1299] D_loss: -0.0640, G_loss: 0.4041\n",
      "  Batch [260/1299] D_loss: -0.1639, G_loss: -0.7199\n",
      "  Batch [270/1299] D_loss: -0.8094, G_loss: 1.0797\n",
      "  Batch [280/1299] D_loss: -0.3300, G_loss: -0.1864\n",
      "  Batch [290/1299] D_loss: -0.1479, G_loss: 0.6494\n",
      "  Batch [300/1299] D_loss: -0.0973, G_loss: -0.3147\n",
      "  Batch [310/1299] D_loss: -0.5165, G_loss: 1.0854\n",
      "  Batch [320/1299] D_loss: -0.7327, G_loss: -0.7619\n",
      "  Batch [330/1299] D_loss: -0.3791, G_loss: 0.4366\n",
      "  Batch [340/1299] D_loss: -0.5946, G_loss: -1.3485\n",
      "  Batch [350/1299] D_loss: -0.9595, G_loss: 0.0291\n",
      "  Batch [360/1299] D_loss: -0.6215, G_loss: 0.3240\n",
      "  Batch [370/1299] D_loss: -0.8526, G_loss: -0.3128\n",
      "  Batch [380/1299] D_loss: -1.2669, G_loss: -0.4722\n",
      "  Batch [390/1299] D_loss: -0.0590, G_loss: 0.5814\n",
      "  Batch [400/1299] D_loss: -0.5381, G_loss: -0.8782\n",
      "  Batch [410/1299] D_loss: -0.7004, G_loss: 0.3305\n",
      "  Batch [420/1299] D_loss: -0.7451, G_loss: 0.4217\n",
      "  Batch [430/1299] D_loss: -0.4992, G_loss: -0.2601\n",
      "  Batch [440/1299] D_loss: -0.1262, G_loss: 1.3995\n",
      "  Batch [450/1299] D_loss: -0.4936, G_loss: -0.4341\n",
      "  Batch [460/1299] D_loss: -0.2520, G_loss: -0.6920\n",
      "  Batch [470/1299] D_loss: -0.2545, G_loss: 0.9795\n",
      "  Batch [480/1299] D_loss: -0.7073, G_loss: 2.0543\n",
      "  Batch [490/1299] D_loss: -0.6767, G_loss: 0.7748\n",
      "  Batch [500/1299] D_loss: -0.8623, G_loss: 0.7758\n",
      "  Batch [510/1299] D_loss: -0.3717, G_loss: -1.3951\n",
      "  Batch [520/1299] D_loss: -0.5975, G_loss: 0.9868\n",
      "  Batch [530/1299] D_loss: -0.5906, G_loss: -0.5259\n",
      "  Batch [540/1299] D_loss: -0.9169, G_loss: -0.8160\n",
      "  Batch [550/1299] D_loss: -0.8720, G_loss: 1.4698\n",
      "  Batch [560/1299] D_loss: -0.5248, G_loss: 0.1110\n",
      "  Batch [570/1299] D_loss: -1.4512, G_loss: 0.1031\n",
      "  Batch [580/1299] D_loss: -0.6361, G_loss: 0.1414\n",
      "  Batch [590/1299] D_loss: -0.1835, G_loss: -1.4579\n",
      "  Batch [600/1299] D_loss: -0.0932, G_loss: 0.3778\n",
      "  Batch [610/1299] D_loss: 0.0198, G_loss: 0.3211\n",
      "  Batch [620/1299] D_loss: -0.5746, G_loss: -0.8493\n",
      "  Batch [630/1299] D_loss: -0.0110, G_loss: -1.2116\n",
      "  Batch [640/1299] D_loss: -0.1705, G_loss: 2.0636\n",
      "  Batch [650/1299] D_loss: -0.7922, G_loss: 0.4725\n",
      "  Batch [660/1299] D_loss: -0.4360, G_loss: 0.0468\n",
      "  Batch [670/1299] D_loss: 0.0043, G_loss: -1.4876\n",
      "  Batch [680/1299] D_loss: -0.6217, G_loss: 2.1951\n",
      "  Batch [690/1299] D_loss: -0.3497, G_loss: -0.3682\n",
      "  Batch [700/1299] D_loss: -0.7247, G_loss: -0.3245\n",
      "  Batch [710/1299] D_loss: -0.4389, G_loss: -1.0815\n",
      "  Batch [720/1299] D_loss: -0.5533, G_loss: 1.6573\n",
      "  Batch [730/1299] D_loss: -0.6776, G_loss: -0.2139\n",
      "  Batch [740/1299] D_loss: -0.9704, G_loss: 1.3270\n",
      "  Batch [750/1299] D_loss: -0.7415, G_loss: -0.4772\n",
      "  Batch [760/1299] D_loss: -0.4759, G_loss: 1.0391\n",
      "  Batch [770/1299] D_loss: -0.9455, G_loss: -1.5066\n",
      "  Batch [780/1299] D_loss: 0.1549, G_loss: -1.0186\n",
      "  Batch [790/1299] D_loss: -0.1461, G_loss: 0.1875\n",
      "  Batch [800/1299] D_loss: -0.1817, G_loss: 2.3642\n",
      "  Batch [810/1299] D_loss: -0.0973, G_loss: -0.7547\n",
      "  Batch [820/1299] D_loss: -0.6552, G_loss: 0.6582\n",
      "  Batch [830/1299] D_loss: -0.3559, G_loss: -0.5619\n",
      "  Batch [840/1299] D_loss: -0.3299, G_loss: 0.6150\n",
      "  Batch [850/1299] D_loss: -0.6687, G_loss: 0.0676\n",
      "  Batch [860/1299] D_loss: -0.1570, G_loss: 1.4601\n",
      "  Batch [870/1299] D_loss: -0.3372, G_loss: -2.0737\n",
      "  Batch [880/1299] D_loss: 0.0239, G_loss: -1.2832\n",
      "  Batch [890/1299] D_loss: -0.1755, G_loss: 1.4427\n",
      "  Batch [900/1299] D_loss: -0.0253, G_loss: -0.6124\n",
      "  Batch [910/1299] D_loss: -0.0134, G_loss: 1.1519\n",
      "  Batch [920/1299] D_loss: -0.8808, G_loss: -0.6844\n",
      "  Batch [930/1299] D_loss: -0.0625, G_loss: -1.2213\n",
      "  Batch [940/1299] D_loss: -0.2883, G_loss: 0.4926\n",
      "  Batch [950/1299] D_loss: -0.0771, G_loss: -1.2857\n",
      "  Batch [960/1299] D_loss: -0.4849, G_loss: 1.7631\n",
      "  Batch [970/1299] D_loss: -0.5161, G_loss: -0.6487\n",
      "  Batch [980/1299] D_loss: -0.4173, G_loss: 1.4184\n",
      "  Batch [990/1299] D_loss: -0.0535, G_loss: 0.1081\n",
      "  Batch [1000/1299] D_loss: -0.0568, G_loss: 0.2047\n",
      "  Batch [1010/1299] D_loss: -0.6816, G_loss: 0.5532\n",
      "  Batch [1020/1299] D_loss: -0.9740, G_loss: 0.5778\n",
      "  Batch [1030/1299] D_loss: -0.3703, G_loss: 0.9622\n",
      "  Batch [1040/1299] D_loss: -0.2780, G_loss: -1.8923\n",
      "  Batch [1050/1299] D_loss: -0.1491, G_loss: 1.1861\n",
      "  Batch [1060/1299] D_loss: -0.7641, G_loss: -0.2300\n",
      "  Batch [1070/1299] D_loss: -0.4234, G_loss: 0.3578\n",
      "  Batch [1080/1299] D_loss: -0.7519, G_loss: 1.0565\n",
      "  Batch [1090/1299] D_loss: -0.1999, G_loss: 0.7645\n",
      "  Batch [1100/1299] D_loss: -1.0561, G_loss: -3.1392\n",
      "  Batch [1110/1299] D_loss: 0.0138, G_loss: 0.9507\n",
      "  Batch [1120/1299] D_loss: -1.3275, G_loss: 4.3620\n",
      "  Batch [1130/1299] D_loss: -0.0180, G_loss: 0.0123\n",
      "  Batch [1140/1299] D_loss: -0.1003, G_loss: 0.5798\n",
      "  Batch [1150/1299] D_loss: 0.0300, G_loss: 0.6078\n",
      "  Batch [1160/1299] D_loss: -0.2131, G_loss: -0.2091\n",
      "  Batch [1170/1299] D_loss: -0.0717, G_loss: 0.1698\n",
      "  Batch [1180/1299] D_loss: -0.4443, G_loss: 0.8467\n",
      "  Batch [1190/1299] D_loss: -0.1436, G_loss: -0.2318\n",
      "  Batch [1200/1299] D_loss: -0.7348, G_loss: 0.0129\n",
      "  Batch [1210/1299] D_loss: -0.6624, G_loss: -0.1571\n",
      "  Batch [1220/1299] D_loss: -0.0318, G_loss: -0.0185\n",
      "  Batch [1230/1299] D_loss: -0.8514, G_loss: 0.0591\n",
      "  Batch [1240/1299] D_loss: -0.6797, G_loss: 1.5475\n",
      "  Batch [1250/1299] D_loss: -0.4963, G_loss: 1.2109\n",
      "  Batch [1260/1299] D_loss: -0.2905, G_loss: 1.9052\n",
      "  Batch [1270/1299] D_loss: 0.0828, G_loss: -2.1089\n",
      "  Batch [1280/1299] D_loss: -1.2108, G_loss: 2.1468\n",
      "  Batch [1290/1299] D_loss: -0.3100, G_loss: 0.6835\n",
      "\n",
      "Epoch 2 Summary:\n",
      "  Average D_loss: -0.1845\n",
      "  Average G_loss: 0.1288\n",
      "\n",
      "Epoch [3/100]\n",
      "  Batch [0/1299] D_loss: 0.1279, G_loss: -1.2386\n",
      "  Batch [10/1299] D_loss: 0.2157, G_loss: 0.9218\n",
      "  Batch [20/1299] D_loss: -0.2120, G_loss: -1.1655\n",
      "  Batch [30/1299] D_loss: -0.1260, G_loss: 1.3903\n",
      "  Batch [40/1299] D_loss: -0.8941, G_loss: -1.8908\n",
      "  Batch [50/1299] D_loss: -0.5752, G_loss: 2.2010\n",
      "  Batch [60/1299] D_loss: -0.3755, G_loss: -1.1546\n",
      "  Batch [70/1299] D_loss: 0.0821, G_loss: 1.3818\n",
      "  Batch [80/1299] D_loss: -0.7249, G_loss: 2.6955\n",
      "  Batch [90/1299] D_loss: -0.7767, G_loss: 0.5440\n",
      "  Batch [100/1299] D_loss: -0.0503, G_loss: -0.0777\n",
      "  Batch [110/1299] D_loss: -0.3235, G_loss: 0.9761\n",
      "  Batch [120/1299] D_loss: 0.0699, G_loss: -1.4684\n",
      "  Batch [130/1299] D_loss: -0.6682, G_loss: 0.9817\n",
      "  Batch [140/1299] D_loss: 0.0966, G_loss: -0.8571\n",
      "  Batch [150/1299] D_loss: -0.7039, G_loss: 3.1065\n",
      "  Batch [160/1299] D_loss: -0.8108, G_loss: -0.6886\n",
      "  Batch [170/1299] D_loss: -0.1141, G_loss: -0.2663\n",
      "  Batch [180/1299] D_loss: -0.3538, G_loss: -0.9717\n",
      "  Batch [190/1299] D_loss: -0.5540, G_loss: 0.1549\n",
      "  Batch [200/1299] D_loss: -0.9965, G_loss: -0.3032\n",
      "  Batch [210/1299] D_loss: -0.7234, G_loss: 0.8461\n",
      "  Batch [220/1299] D_loss: -0.6276, G_loss: -0.9056\n",
      "  Batch [230/1299] D_loss: -0.4546, G_loss: 0.1613\n",
      "  Batch [240/1299] D_loss: -0.7209, G_loss: 0.8552\n",
      "  Batch [250/1299] D_loss: -0.1190, G_loss: -1.1039\n",
      "  Batch [260/1299] D_loss: -0.2671, G_loss: 0.7268\n",
      "  Batch [270/1299] D_loss: -1.5983, G_loss: -1.9589\n",
      "  Batch [280/1299] D_loss: -0.3447, G_loss: 1.2860\n",
      "  Batch [290/1299] D_loss: -0.6402, G_loss: -0.8455\n",
      "  Batch [300/1299] D_loss: -0.4931, G_loss: -1.4577\n",
      "  Batch [310/1299] D_loss: -0.7952, G_loss: -1.0489\n",
      "  Batch [320/1299] D_loss: -1.6975, G_loss: 1.9498\n",
      "  Batch [330/1299] D_loss: -0.1230, G_loss: -1.5797\n",
      "  Batch [340/1299] D_loss: -0.7092, G_loss: 1.9081\n",
      "  Batch [350/1299] D_loss: -0.0551, G_loss: 0.7225\n",
      "  Batch [360/1299] D_loss: -0.2359, G_loss: -1.9244\n",
      "  Batch [370/1299] D_loss: -0.8898, G_loss: 3.5648\n",
      "  Batch [380/1299] D_loss: -0.1246, G_loss: -0.1809\n",
      "  Batch [390/1299] D_loss: -0.0549, G_loss: -0.4683\n",
      "  Batch [400/1299] D_loss: -0.0309, G_loss: 0.8062\n",
      "  Batch [410/1299] D_loss: -0.3326, G_loss: -0.7239\n",
      "  Batch [420/1299] D_loss: -0.0496, G_loss: 1.4000\n",
      "  Batch [430/1299] D_loss: -0.6856, G_loss: -1.1047\n",
      "  Batch [440/1299] D_loss: -0.5213, G_loss: 0.2298\n",
      "  Batch [450/1299] D_loss: -1.7951, G_loss: 1.1012\n",
      "  Batch [460/1299] D_loss: -0.6434, G_loss: 0.8633\n",
      "  Batch [470/1299] D_loss: -0.1001, G_loss: -1.7610\n",
      "  Batch [480/1299] D_loss: -0.1319, G_loss: 0.2375\n",
      "  Batch [490/1299] D_loss: -0.5466, G_loss: 0.0973\n",
      "  Batch [500/1299] D_loss: -0.5779, G_loss: -0.7567\n",
      "  Batch [510/1299] D_loss: -0.8172, G_loss: -3.1292\n",
      "  Batch [520/1299] D_loss: -0.3328, G_loss: 0.7791\n",
      "  Batch [530/1299] D_loss: -0.5490, G_loss: 0.3569\n",
      "  Batch [540/1299] D_loss: -0.5388, G_loss: 0.1224\n",
      "  Batch [550/1299] D_loss: -0.4886, G_loss: 2.0754\n",
      "  Batch [560/1299] D_loss: -0.5989, G_loss: -1.6250\n",
      "  Batch [570/1299] D_loss: -1.0757, G_loss: 0.5853\n",
      "  Batch [580/1299] D_loss: -1.3316, G_loss: -0.9818\n",
      "  Batch [590/1299] D_loss: -0.6741, G_loss: 0.7963\n",
      "  Batch [600/1299] D_loss: -0.9969, G_loss: 1.0225\n",
      "  Batch [610/1299] D_loss: 0.1004, G_loss: 0.5957\n",
      "  Batch [620/1299] D_loss: -0.2005, G_loss: 0.1530\n",
      "  Batch [630/1299] D_loss: -0.1510, G_loss: -1.4669\n",
      "  Batch [640/1299] D_loss: 0.1909, G_loss: 1.1886\n",
      "  Batch [650/1299] D_loss: -0.3222, G_loss: -0.2363\n",
      "  Batch [660/1299] D_loss: 0.2523, G_loss: -1.0263\n",
      "  Batch [670/1299] D_loss: -0.1356, G_loss: 1.6259\n",
      "  Batch [680/1299] D_loss: -0.1143, G_loss: 0.1357\n",
      "  Batch [690/1299] D_loss: 0.0887, G_loss: -0.5369\n",
      "  Batch [700/1299] D_loss: -0.9222, G_loss: 2.4127\n",
      "  Batch [710/1299] D_loss: -0.2322, G_loss: -1.0759\n",
      "  Batch [720/1299] D_loss: -1.0395, G_loss: 3.2329\n",
      "  Batch [730/1299] D_loss: -0.3516, G_loss: -2.6815\n",
      "  Batch [740/1299] D_loss: -0.0069, G_loss: 0.4587\n",
      "  Batch [750/1299] D_loss: -0.4417, G_loss: -0.0753\n",
      "  Batch [760/1299] D_loss: -1.0776, G_loss: 0.0769\n",
      "  Batch [770/1299] D_loss: -0.4321, G_loss: -0.0334\n",
      "  Batch [780/1299] D_loss: -0.5518, G_loss: 0.4834\n",
      "  Batch [790/1299] D_loss: -0.2142, G_loss: 0.8036\n",
      "  Batch [800/1299] D_loss: -0.6472, G_loss: -0.8419\n",
      "  Batch [810/1299] D_loss: -0.4158, G_loss: 1.0741\n",
      "  Batch [820/1299] D_loss: -0.6742, G_loss: 0.3546\n",
      "  Batch [830/1299] D_loss: -0.3565, G_loss: 0.2724\n",
      "  Batch [840/1299] D_loss: -0.1236, G_loss: -1.0900\n",
      "  Batch [850/1299] D_loss: -0.8429, G_loss: 0.5477\n",
      "  Batch [860/1299] D_loss: -0.3685, G_loss: -0.2194\n",
      "  Batch [870/1299] D_loss: -0.8356, G_loss: 0.1482\n",
      "  Batch [880/1299] D_loss: -0.4806, G_loss: 0.1465\n",
      "  Batch [890/1299] D_loss: -0.6016, G_loss: -2.0634\n",
      "  Batch [900/1299] D_loss: -0.2002, G_loss: 1.6783\n",
      "  Batch [910/1299] D_loss: -0.4118, G_loss: -0.8009\n",
      "  Batch [920/1299] D_loss: -0.8547, G_loss: -0.9168\n",
      "  Batch [930/1299] D_loss: -0.3425, G_loss: 0.8458\n",
      "  Batch [940/1299] D_loss: -0.6315, G_loss: -1.4094\n",
      "  Batch [950/1299] D_loss: -1.3133, G_loss: 1.1956\n",
      "  Batch [960/1299] D_loss: -0.7365, G_loss: -0.8979\n",
      "  Batch [970/1299] D_loss: -0.2453, G_loss: 3.5962\n",
      "  Batch [980/1299] D_loss: -0.1915, G_loss: -0.4874\n",
      "  Batch [990/1299] D_loss: 0.0870, G_loss: -1.3203\n",
      "  Batch [1000/1299] D_loss: 0.0071, G_loss: 1.1525\n",
      "  Batch [1010/1299] D_loss: 0.1262, G_loss: -0.6230\n",
      "  Batch [1020/1299] D_loss: 0.1673, G_loss: 0.6389\n",
      "  Batch [1030/1299] D_loss: -0.6227, G_loss: 2.5241\n",
      "  Batch [1040/1299] D_loss: -0.2324, G_loss: -1.5787\n",
      "  Batch [1050/1299] D_loss: -0.5020, G_loss: 2.1160\n",
      "  Batch [1060/1299] D_loss: -0.0775, G_loss: -2.0326\n",
      "  Batch [1070/1299] D_loss: -0.6204, G_loss: -1.4566\n",
      "  Batch [1080/1299] D_loss: 0.0300, G_loss: 0.5772\n",
      "  Batch [1090/1299] D_loss: -0.4652, G_loss: -0.0759\n",
      "  Batch [1100/1299] D_loss: -0.9003, G_loss: 1.8252\n",
      "  Batch [1110/1299] D_loss: -0.4512, G_loss: 0.1697\n",
      "  Batch [1120/1299] D_loss: -0.0032, G_loss: 0.0092\n",
      "  Batch [1130/1299] D_loss: -0.2047, G_loss: 2.3353\n",
      "  Batch [1140/1299] D_loss: -0.4393, G_loss: -0.4679\n",
      "  Batch [1150/1299] D_loss: -0.2464, G_loss: 0.7385\n",
      "  Batch [1160/1299] D_loss: -0.7444, G_loss: -0.8180\n",
      "  Batch [1170/1299] D_loss: -0.2944, G_loss: -0.3019\n",
      "  Batch [1180/1299] D_loss: -0.0224, G_loss: 1.0283\n",
      "  Batch [1190/1299] D_loss: -0.4438, G_loss: 0.4242\n",
      "  Batch [1200/1299] D_loss: -0.4536, G_loss: 2.2012\n",
      "  Batch [1210/1299] D_loss: -0.4893, G_loss: 0.0255\n",
      "  Batch [1220/1299] D_loss: -0.5915, G_loss: 0.1531\n",
      "  Batch [1230/1299] D_loss: -0.1065, G_loss: 0.7052\n",
      "  Batch [1240/1299] D_loss: -1.2494, G_loss: -2.9460\n",
      "  Batch [1250/1299] D_loss: -0.0682, G_loss: -0.0270\n",
      "  Batch [1260/1299] D_loss: -1.1431, G_loss: 0.8710\n",
      "  Batch [1270/1299] D_loss: -0.3082, G_loss: -2.0809\n",
      "  Batch [1280/1299] D_loss: -0.3616, G_loss: 1.2836\n",
      "  Batch [1290/1299] D_loss: -0.7251, G_loss: -2.1165\n",
      "\n",
      "Epoch 3 Summary:\n",
      "  Average D_loss: -0.2000\n",
      "  Average G_loss: 0.1042\n",
      "\n",
      "Epoch [4/100]\n",
      "  Batch [0/1299] D_loss: -0.5386, G_loss: 1.5561\n",
      "  Batch [10/1299] D_loss: -0.7168, G_loss: 0.1368\n",
      "  Batch [20/1299] D_loss: -0.1985, G_loss: -0.7411\n",
      "  Batch [30/1299] D_loss: -0.5275, G_loss: -0.5951\n",
      "  Batch [40/1299] D_loss: -0.4417, G_loss: 0.2428\n",
      "  Batch [50/1299] D_loss: -0.7697, G_loss: 0.7574\n",
      "  Batch [60/1299] D_loss: -0.0113, G_loss: -0.5086\n",
      "  Batch [70/1299] D_loss: -0.6177, G_loss: 0.9181\n",
      "  Batch [80/1299] D_loss: -0.3386, G_loss: -2.2034\n",
      "  Batch [90/1299] D_loss: -0.1562, G_loss: 0.3594\n",
      "  Batch [100/1299] D_loss: -0.7614, G_loss: 0.5606\n",
      "  Batch [110/1299] D_loss: -0.6067, G_loss: 0.2778\n",
      "  Batch [120/1299] D_loss: -0.7232, G_loss: -0.3744\n",
      "  Batch [130/1299] D_loss: -0.1712, G_loss: 1.0744\n",
      "  Batch [140/1299] D_loss: -0.3855, G_loss: -2.5777\n",
      "  Batch [150/1299] D_loss: -0.7427, G_loss: 2.3631\n",
      "  Batch [160/1299] D_loss: -0.9261, G_loss: -2.6128\n",
      "  Batch [170/1299] D_loss: -0.6244, G_loss: 1.4230\n",
      "  Batch [180/1299] D_loss: -0.2631, G_loss: -1.8922\n",
      "  Batch [190/1299] D_loss: -0.1776, G_loss: 0.1071\n",
      "  Batch [200/1299] D_loss: -0.3805, G_loss: 2.2542\n",
      "  Batch [210/1299] D_loss: 0.0276, G_loss: -1.2065\n",
      "  Batch [220/1299] D_loss: -0.1618, G_loss: -0.1688\n",
      "  Batch [230/1299] D_loss: -0.3227, G_loss: 0.1586\n",
      "  Batch [240/1299] D_loss: -0.4626, G_loss: -0.3363\n",
      "  Batch [250/1299] D_loss: -1.0009, G_loss: -1.2238\n",
      "  Batch [260/1299] D_loss: -0.0371, G_loss: -0.8420\n",
      "  Batch [270/1299] D_loss: -0.1426, G_loss: 1.7513\n",
      "  Batch [280/1299] D_loss: -0.6441, G_loss: -0.1006\n",
      "  Batch [290/1299] D_loss: -0.1099, G_loss: -0.5466\n",
      "  Batch [300/1299] D_loss: -0.5076, G_loss: 1.1637\n",
      "  Batch [310/1299] D_loss: -0.5617, G_loss: -0.8092\n",
      "  Batch [320/1299] D_loss: -0.0149, G_loss: -0.6377\n",
      "  Batch [330/1299] D_loss: -0.3865, G_loss: 0.6358\n",
      "  Batch [340/1299] D_loss: -0.5910, G_loss: -1.5567\n",
      "  Batch [350/1299] D_loss: -0.3936, G_loss: 1.5211\n",
      "  Batch [360/1299] D_loss: -0.1866, G_loss: 0.1209\n",
      "  Batch [370/1299] D_loss: -3.1049, G_loss: -1.1852\n",
      "  Batch [380/1299] D_loss: -0.5129, G_loss: -0.1719\n",
      "  Batch [390/1299] D_loss: -0.8580, G_loss: -0.9027\n",
      "  Batch [400/1299] D_loss: -0.5535, G_loss: 0.7478\n",
      "  Batch [410/1299] D_loss: -0.5960, G_loss: 0.1716\n",
      "  Batch [420/1299] D_loss: -0.8905, G_loss: 0.0425\n",
      "  Batch [430/1299] D_loss: -0.7377, G_loss: 1.2645\n",
      "  Batch [440/1299] D_loss: -0.4497, G_loss: -1.0930\n",
      "  Batch [450/1299] D_loss: 0.1319, G_loss: -1.1183\n",
      "  Batch [460/1299] D_loss: -0.0529, G_loss: 1.4224\n",
      "  Batch [470/1299] D_loss: -0.6783, G_loss: 0.0305\n",
      "  Batch [480/1299] D_loss: -0.8521, G_loss: 1.0376\n",
      "  Batch [490/1299] D_loss: -0.1821, G_loss: 0.1271\n",
      "  Batch [500/1299] D_loss: -0.5516, G_loss: -0.5845\n",
      "  Batch [510/1299] D_loss: -0.2894, G_loss: 0.4358\n",
      "  Batch [520/1299] D_loss: -0.0249, G_loss: 0.9402\n",
      "  Batch [530/1299] D_loss: -0.3707, G_loss: -0.3658\n",
      "  Batch [540/1299] D_loss: -0.2360, G_loss: -0.5199\n",
      "  Batch [550/1299] D_loss: -0.5032, G_loss: 1.9684\n",
      "  Batch [560/1299] D_loss: -0.1119, G_loss: -0.3948\n",
      "  Batch [570/1299] D_loss: -0.6969, G_loss: 3.2561\n",
      "  Batch [580/1299] D_loss: 0.1001, G_loss: -1.9248\n",
      "  Batch [590/1299] D_loss: -0.4976, G_loss: -2.7303\n",
      "  Batch [600/1299] D_loss: -1.0174, G_loss: 3.1202\n",
      "  Batch [610/1299] D_loss: -0.4033, G_loss: 0.2033\n",
      "  Batch [620/1299] D_loss: -0.4172, G_loss: 1.3293\n",
      "  Batch [630/1299] D_loss: -0.0544, G_loss: 0.4958\n",
      "  Batch [640/1299] D_loss: -0.1321, G_loss: 0.2622\n",
      "  Batch [650/1299] D_loss: -0.7121, G_loss: -2.6278\n",
      "  Batch [660/1299] D_loss: -0.2911, G_loss: 2.4568\n",
      "  Batch [670/1299] D_loss: -0.4767, G_loss: -0.6737\n",
      "  Batch [680/1299] D_loss: -0.3130, G_loss: -0.5199\n",
      "  Batch [690/1299] D_loss: -0.9766, G_loss: -0.8858\n",
      "  Batch [700/1299] D_loss: 0.0218, G_loss: 0.5073\n",
      "  Batch [710/1299] D_loss: -0.3614, G_loss: -1.9129\n",
      "  Batch [720/1299] D_loss: -0.1327, G_loss: 1.4656\n",
      "  Batch [730/1299] D_loss: -1.3466, G_loss: -2.7360\n",
      "  Batch [740/1299] D_loss: -0.1230, G_loss: -0.1270\n",
      "  Batch [750/1299] D_loss: -0.0893, G_loss: 1.4208\n",
      "  Batch [760/1299] D_loss: -1.3752, G_loss: -3.0231\n",
      "  Batch [770/1299] D_loss: -0.5242, G_loss: 1.1421\n",
      "  Batch [780/1299] D_loss: -0.1794, G_loss: 0.1615\n",
      "  Batch [790/1299] D_loss: -0.2419, G_loss: -1.4777\n",
      "  Batch [800/1299] D_loss: -0.3376, G_loss: -0.0432\n",
      "  Batch [810/1299] D_loss: -0.6030, G_loss: -2.5003\n",
      "  Batch [820/1299] D_loss: -0.0819, G_loss: -0.1794\n",
      "  Batch [830/1299] D_loss: -0.5560, G_loss: 2.0219\n",
      "  Batch [840/1299] D_loss: -0.5089, G_loss: -3.1773\n",
      "  Batch [850/1299] D_loss: 0.0093, G_loss: 1.1850\n",
      "  Batch [860/1299] D_loss: -0.3511, G_loss: 0.0929\n",
      "  Batch [870/1299] D_loss: -1.1638, G_loss: -0.2604\n",
      "  Batch [880/1299] D_loss: -0.7880, G_loss: 0.0437\n",
      "  Batch [890/1299] D_loss: -0.3729, G_loss: 1.2689\n",
      "  Batch [900/1299] D_loss: -0.3393, G_loss: -0.9618\n",
      "  Batch [910/1299] D_loss: -0.2276, G_loss: 0.3420\n",
      "  Batch [920/1299] D_loss: -0.5794, G_loss: -0.2202\n",
      "  Batch [930/1299] D_loss: -0.1183, G_loss: -0.4624\n",
      "  Batch [940/1299] D_loss: -0.3788, G_loss: 0.8203\n",
      "  Batch [950/1299] D_loss: -0.3785, G_loss: 0.7866\n",
      "  Batch [960/1299] D_loss: 0.1099, G_loss: -2.3163\n",
      "  Batch [970/1299] D_loss: -0.3097, G_loss: -0.0925\n",
      "  Batch [980/1299] D_loss: -0.1986, G_loss: -0.5368\n",
      "  Batch [990/1299] D_loss: -0.6271, G_loss: -1.1700\n",
      "  Batch [1000/1299] D_loss: -0.2130, G_loss: 0.9896\n",
      "  Batch [1010/1299] D_loss: -0.3061, G_loss: -1.8575\n",
      "  Batch [1020/1299] D_loss: -0.4811, G_loss: 0.9423\n",
      "  Batch [1030/1299] D_loss: -0.2666, G_loss: 0.3572\n",
      "  Batch [1040/1299] D_loss: -0.0166, G_loss: -0.6341\n",
      "  Batch [1050/1299] D_loss: -0.4704, G_loss: 1.7075\n",
      "  Batch [1060/1299] D_loss: -0.5717, G_loss: 0.7639\n",
      "  Batch [1070/1299] D_loss: -0.0711, G_loss: -0.3827\n",
      "  Batch [1080/1299] D_loss: -0.1499, G_loss: 1.2881\n",
      "  Batch [1090/1299] D_loss: -0.7238, G_loss: 1.8936\n",
      "  Batch [1100/1299] D_loss: -0.0510, G_loss: -1.1755\n",
      "  Batch [1110/1299] D_loss: -1.0005, G_loss: 2.4532\n",
      "  Batch [1120/1299] D_loss: -0.4978, G_loss: -2.3245\n",
      "  Batch [1130/1299] D_loss: -0.6913, G_loss: 1.2510\n",
      "  Batch [1140/1299] D_loss: -0.4533, G_loss: 0.0603\n",
      "  Batch [1150/1299] D_loss: -0.0711, G_loss: -0.8087\n",
      "  Batch [1160/1299] D_loss: -0.2098, G_loss: 0.4432\n",
      "  Batch [1170/1299] D_loss: -0.2678, G_loss: 1.5451\n",
      "  Batch [1180/1299] D_loss: -0.0928, G_loss: -2.6430\n",
      "  Batch [1190/1299] D_loss: -0.3041, G_loss: 1.0362\n",
      "  Batch [1200/1299] D_loss: -0.6447, G_loss: 0.8831\n",
      "  Batch [1210/1299] D_loss: -0.0927, G_loss: -0.2844\n",
      "  Batch [1220/1299] D_loss: 0.0884, G_loss: 0.8724\n",
      "  Batch [1230/1299] D_loss: -0.3347, G_loss: -2.2820\n",
      "  Batch [1240/1299] D_loss: -0.5798, G_loss: 0.3282\n",
      "  Batch [1250/1299] D_loss: -0.1782, G_loss: -0.4408\n",
      "  Batch [1260/1299] D_loss: -0.2967, G_loss: -0.6107\n",
      "  Batch [1270/1299] D_loss: -1.2985, G_loss: -0.9890\n",
      "  Batch [1280/1299] D_loss: -0.2873, G_loss: 0.6285\n",
      "  Batch [1290/1299] D_loss: -0.6200, G_loss: 0.3299\n",
      "\n",
      "Epoch 4 Summary:\n",
      "  Average D_loss: -0.2056\n",
      "  Average G_loss: 0.0747\n",
      "\n",
      "Epoch [5/100]\n",
      "  Batch [0/1299] D_loss: -0.6559, G_loss: 0.9742\n",
      "  Batch [10/1299] D_loss: -0.4725, G_loss: -0.7709\n",
      "  Batch [20/1299] D_loss: -0.7401, G_loss: 0.7413\n",
      "  Batch [30/1299] D_loss: -1.0538, G_loss: -0.1130\n",
      "  Batch [40/1299] D_loss: -0.1272, G_loss: -1.0729\n",
      "  Batch [50/1299] D_loss: -0.2172, G_loss: 2.4059\n",
      "  Batch [60/1299] D_loss: -0.2871, G_loss: 0.7745\n",
      "  Batch [70/1299] D_loss: -0.1332, G_loss: -0.5781\n",
      "  Batch [80/1299] D_loss: -0.6066, G_loss: 2.0639\n",
      "  Batch [90/1299] D_loss: -0.8520, G_loss: 1.4084\n",
      "  Batch [100/1299] D_loss: -0.2474, G_loss: 1.6759\n",
      "  Batch [110/1299] D_loss: -0.7505, G_loss: -2.1385\n",
      "  Batch [120/1299] D_loss: -0.8459, G_loss: 2.2381\n",
      "  Batch [130/1299] D_loss: -0.6948, G_loss: 0.4048\n",
      "  Batch [140/1299] D_loss: -0.2138, G_loss: 0.0188\n",
      "  Batch [150/1299] D_loss: -0.9475, G_loss: -3.2783\n",
      "  Batch [160/1299] D_loss: -0.6837, G_loss: 2.1597\n",
      "  Batch [170/1299] D_loss: 0.1218, G_loss: 0.9070\n",
      "  Batch [180/1299] D_loss: -0.6044, G_loss: 1.0463\n",
      "  Batch [190/1299] D_loss: -1.3423, G_loss: 1.8589\n",
      "  Batch [200/1299] D_loss: -0.1913, G_loss: -0.8388\n",
      "  Batch [210/1299] D_loss: -1.1993, G_loss: 2.4786\n",
      "  Batch [220/1299] D_loss: -0.3596, G_loss: 1.0672\n",
      "  Batch [230/1299] D_loss: -0.5940, G_loss: -0.0781\n",
      "  Batch [240/1299] D_loss: -0.4618, G_loss: -0.5533\n",
      "  Batch [250/1299] D_loss: -0.7276, G_loss: -1.3336\n",
      "  Batch [260/1299] D_loss: -0.5997, G_loss: 0.0690\n",
      "  Batch [270/1299] D_loss: 0.1657, G_loss: -1.2271\n",
      "  Batch [280/1299] D_loss: -1.0646, G_loss: -1.3756\n",
      "  Batch [290/1299] D_loss: -0.6592, G_loss: 0.3423\n",
      "  Batch [300/1299] D_loss: -0.2007, G_loss: 1.5502\n",
      "  Batch [310/1299] D_loss: -0.9391, G_loss: -2.3579\n",
      "  Batch [320/1299] D_loss: -0.0375, G_loss: -0.1859\n",
      "  Batch [330/1299] D_loss: -0.6340, G_loss: 0.0001\n",
      "  Batch [340/1299] D_loss: -0.1276, G_loss: 1.1693\n",
      "  Batch [350/1299] D_loss: -0.7316, G_loss: 0.3543\n",
      "  Batch [360/1299] D_loss: -0.4799, G_loss: -0.5672\n",
      "  Batch [370/1299] D_loss: -0.3599, G_loss: -0.0818\n",
      "  Batch [380/1299] D_loss: -0.4055, G_loss: -0.0098\n",
      "  Batch [390/1299] D_loss: -0.3152, G_loss: 1.7142\n",
      "  Batch [400/1299] D_loss: -0.4359, G_loss: -1.8215\n",
      "  Batch [410/1299] D_loss: -0.5176, G_loss: 1.5977\n",
      "  Batch [420/1299] D_loss: -0.7004, G_loss: 1.1940\n",
      "  Batch [430/1299] D_loss: -0.1266, G_loss: 0.5345\n",
      "  Batch [440/1299] D_loss: -0.6830, G_loss: -0.8188\n",
      "  Batch [450/1299] D_loss: 0.1070, G_loss: 0.8190\n",
      "  Batch [460/1299] D_loss: -0.3277, G_loss: -1.1934\n",
      "  Batch [470/1299] D_loss: -0.2313, G_loss: -0.6804\n",
      "  Batch [480/1299] D_loss: -0.8736, G_loss: -0.7155\n",
      "  Batch [490/1299] D_loss: -0.5945, G_loss: -0.3172\n",
      "  Batch [500/1299] D_loss: -0.1472, G_loss: -1.2275\n",
      "  Batch [510/1299] D_loss: -0.0813, G_loss: -0.2595\n",
      "  Batch [520/1299] D_loss: -0.8871, G_loss: 2.7990\n",
      "  Batch [530/1299] D_loss: -0.2947, G_loss: 0.4133\n",
      "  Batch [540/1299] D_loss: -0.6968, G_loss: 0.3636\n",
      "  Batch [550/1299] D_loss: -0.2534, G_loss: 0.1220\n",
      "  Batch [560/1299] D_loss: -0.4426, G_loss: 0.7187\n",
      "  Batch [570/1299] D_loss: -0.8319, G_loss: -2.1813\n",
      "  Batch [580/1299] D_loss: -0.6865, G_loss: 1.8692\n",
      "  Batch [590/1299] D_loss: -0.5739, G_loss: 0.1798\n",
      "  Batch [600/1299] D_loss: -1.1749, G_loss: -2.2380\n",
      "  Batch [610/1299] D_loss: -0.0959, G_loss: -0.1286\n",
      "  Batch [620/1299] D_loss: -0.7050, G_loss: 3.8875\n",
      "  Batch [630/1299] D_loss: -0.6943, G_loss: -3.4641\n",
      "  Batch [640/1299] D_loss: -0.7830, G_loss: 1.2051\n",
      "  Batch [650/1299] D_loss: -0.8657, G_loss: 0.5391\n",
      "  Batch [660/1299] D_loss: -0.0441, G_loss: -0.5759\n",
      "  Batch [670/1299] D_loss: 0.4807, G_loss: 2.0573\n",
      "  Batch [680/1299] D_loss: -0.3620, G_loss: -2.6350\n",
      "  Batch [690/1299] D_loss: -0.2415, G_loss: 0.4498\n",
      "  Batch [700/1299] D_loss: -0.4727, G_loss: -0.6520\n",
      "  Batch [710/1299] D_loss: -0.6618, G_loss: 2.1085\n",
      "  Batch [720/1299] D_loss: -0.1222, G_loss: -0.2623\n",
      "  Batch [730/1299] D_loss: -0.0101, G_loss: -0.4836\n",
      "  Batch [740/1299] D_loss: -0.0583, G_loss: -1.5062\n",
      "  Batch [750/1299] D_loss: -0.5177, G_loss: 1.3286\n",
      "  Batch [760/1299] D_loss: -0.7340, G_loss: 1.4020\n",
      "  Batch [770/1299] D_loss: -0.8373, G_loss: -1.6188\n",
      "  Batch [780/1299] D_loss: -0.2918, G_loss: -0.2999\n",
      "  Batch [790/1299] D_loss: -0.2370, G_loss: 0.4007\n",
      "  Batch [800/1299] D_loss: -0.6124, G_loss: 1.2904\n",
      "  Batch [810/1299] D_loss: -0.5447, G_loss: -1.4280\n",
      "  Batch [820/1299] D_loss: -0.7295, G_loss: -0.8323\n",
      "  Batch [830/1299] D_loss: -1.1597, G_loss: 1.6919\n",
      "  Batch [840/1299] D_loss: -0.7223, G_loss: 0.4432\n",
      "  Batch [850/1299] D_loss: -1.1002, G_loss: 0.6544\n",
      "  Batch [860/1299] D_loss: -0.4240, G_loss: -1.4813\n",
      "  Batch [870/1299] D_loss: -0.0867, G_loss: 0.4612\n",
      "  Batch [880/1299] D_loss: -1.0152, G_loss: 1.0349\n",
      "  Batch [890/1299] D_loss: -0.9784, G_loss: -0.7707\n",
      "  Batch [900/1299] D_loss: -0.5329, G_loss: -1.8376\n",
      "  Batch [910/1299] D_loss: -0.5619, G_loss: 1.0265\n",
      "  Batch [920/1299] D_loss: -0.1251, G_loss: -1.0795\n",
      "  Batch [930/1299] D_loss: -0.4683, G_loss: -1.5170\n",
      "  Batch [940/1299] D_loss: -0.4978, G_loss: -0.1290\n",
      "  Batch [950/1299] D_loss: -1.3785, G_loss: 2.3331\n",
      "  Batch [960/1299] D_loss: -0.2089, G_loss: 0.2524\n",
      "  Batch [970/1299] D_loss: -0.7377, G_loss: -0.8209\n",
      "  Batch [980/1299] D_loss: -0.9422, G_loss: 1.3286\n",
      "  Batch [990/1299] D_loss: -0.1682, G_loss: 0.8361\n",
      "  Batch [1000/1299] D_loss: -0.6360, G_loss: 0.0585\n",
      "  Batch [1010/1299] D_loss: -0.5552, G_loss: 0.9757\n",
      "  Batch [1020/1299] D_loss: -1.0376, G_loss: 1.0987\n",
      "  Batch [1030/1299] D_loss: -0.7985, G_loss: -0.9429\n",
      "  Batch [1040/1299] D_loss: -0.3847, G_loss: -0.0714\n",
      "  Batch [1050/1299] D_loss: -0.5969, G_loss: 0.7592\n",
      "  Batch [1060/1299] D_loss: -0.1298, G_loss: -0.9489\n",
      "  Batch [1070/1299] D_loss: -0.1382, G_loss: 0.0496\n",
      "  Batch [1080/1299] D_loss: -0.4727, G_loss: 1.7861\n",
      "  Batch [1090/1299] D_loss: -0.4046, G_loss: 1.0353\n",
      "  Batch [1100/1299] D_loss: -0.9356, G_loss: -3.7040\n",
      "  Batch [1110/1299] D_loss: -0.1438, G_loss: 1.6617\n",
      "  Batch [1120/1299] D_loss: 0.1902, G_loss: -1.6986\n",
      "  Batch [1130/1299] D_loss: -0.6929, G_loss: 0.9675\n",
      "  Batch [1140/1299] D_loss: -0.4838, G_loss: 2.1455\n",
      "  Batch [1150/1299] D_loss: -0.2214, G_loss: -1.4669\n",
      "  Batch [1160/1299] D_loss: -0.1710, G_loss: 0.6687\n",
      "  Batch [1170/1299] D_loss: -0.2109, G_loss: 1.0091\n",
      "  Batch [1180/1299] D_loss: -0.7486, G_loss: -0.0040\n",
      "  Batch [1190/1299] D_loss: -0.9443, G_loss: -2.5327\n",
      "  Batch [1200/1299] D_loss: -1.2525, G_loss: 3.8943\n",
      "  Batch [1210/1299] D_loss: -0.1410, G_loss: -0.5577\n",
      "  Batch [1220/1299] D_loss: -0.3957, G_loss: -2.4394\n",
      "  Batch [1230/1299] D_loss: -0.1616, G_loss: 2.4763\n",
      "  Batch [1240/1299] D_loss: -0.1801, G_loss: -2.0282\n",
      "  Batch [1250/1299] D_loss: -0.1630, G_loss: 1.0248\n",
      "  Batch [1260/1299] D_loss: -0.6086, G_loss: -0.1131\n",
      "  Batch [1270/1299] D_loss: -1.2759, G_loss: -2.5546\n",
      "  Batch [1280/1299] D_loss: -0.7635, G_loss: -0.0620\n",
      "  Batch [1290/1299] D_loss: -0.7839, G_loss: 0.9854\n",
      "\n",
      "Epoch 5 Summary:\n",
      "  Average D_loss: -0.2240\n",
      "  Average G_loss: 0.0776\n",
      "\n",
      "Epoch [6/100]\n",
      "  Batch [0/1299] D_loss: -0.4618, G_loss: -0.0816\n",
      "  Batch [10/1299] D_loss: 0.0382, G_loss: -0.7062\n",
      "  Batch [20/1299] D_loss: -0.4390, G_loss: 2.6395\n",
      "  Batch [30/1299] D_loss: -0.4508, G_loss: -0.4091\n",
      "  Batch [40/1299] D_loss: -0.5797, G_loss: 0.2007\n",
      "  Batch [50/1299] D_loss: -0.9996, G_loss: -0.1899\n",
      "  Batch [60/1299] D_loss: 0.0832, G_loss: -0.6234\n",
      "  Batch [70/1299] D_loss: -0.1713, G_loss: -0.1068\n",
      "  Batch [80/1299] D_loss: 0.1281, G_loss: 1.8173\n",
      "  Batch [90/1299] D_loss: -0.7732, G_loss: -0.6212\n",
      "  Batch [100/1299] D_loss: -0.3280, G_loss: -2.1244\n",
      "  Batch [110/1299] D_loss: -0.1101, G_loss: 0.2791\n",
      "  Batch [120/1299] D_loss: -0.7876, G_loss: -0.4874\n",
      "  Batch [130/1299] D_loss: -0.3954, G_loss: -2.1708\n",
      "  Batch [140/1299] D_loss: -0.0809, G_loss: 2.2358\n",
      "  Batch [150/1299] D_loss: -1.4202, G_loss: -1.6108\n",
      "  Batch [160/1299] D_loss: -0.4429, G_loss: -0.9358\n",
      "  Batch [170/1299] D_loss: -0.0010, G_loss: -0.2756\n",
      "  Batch [180/1299] D_loss: -0.3464, G_loss: -0.0143\n",
      "  Batch [190/1299] D_loss: -0.5719, G_loss: 0.3961\n",
      "  Batch [200/1299] D_loss: -0.4762, G_loss: 1.2128\n",
      "  Batch [210/1299] D_loss: -0.3737, G_loss: -0.1406\n",
      "  Batch [220/1299] D_loss: -0.9929, G_loss: 0.6180\n",
      "  Batch [230/1299] D_loss: -0.3698, G_loss: -1.9574\n",
      "  Batch [240/1299] D_loss: -0.6672, G_loss: 0.8467\n",
      "  Batch [250/1299] D_loss: -0.6001, G_loss: -1.1166\n",
      "  Batch [260/1299] D_loss: -2.0988, G_loss: 3.2884\n",
      "  Batch [270/1299] D_loss: -0.9144, G_loss: -1.7580\n",
      "  Batch [280/1299] D_loss: -0.1021, G_loss: -0.3482\n",
      "  Batch [290/1299] D_loss: -0.1307, G_loss: 3.0438\n",
      "  Batch [300/1299] D_loss: -0.3224, G_loss: -0.0511\n",
      "  Batch [310/1299] D_loss: -0.7000, G_loss: 0.3418\n",
      "  Batch [320/1299] D_loss: 0.0590, G_loss: -0.3741\n",
      "  Batch [330/1299] D_loss: -1.2205, G_loss: 1.1912\n",
      "  Batch [340/1299] D_loss: -0.8658, G_loss: -0.9838\n",
      "  Batch [350/1299] D_loss: -0.3373, G_loss: -1.0004\n",
      "  Batch [360/1299] D_loss: -0.1668, G_loss: 0.7961\n",
      "  Batch [370/1299] D_loss: -0.0014, G_loss: 0.6768\n",
      "  Batch [380/1299] D_loss: -0.4863, G_loss: -2.0881\n",
      "  Batch [390/1299] D_loss: -0.0247, G_loss: 1.2905\n",
      "  Batch [400/1299] D_loss: -0.7754, G_loss: -1.6961\n",
      "  Batch [410/1299] D_loss: -0.1148, G_loss: 0.1944\n",
      "  Batch [420/1299] D_loss: -0.4032, G_loss: 1.4885\n",
      "  Batch [430/1299] D_loss: -2.1125, G_loss: -3.2024\n",
      "  Batch [440/1299] D_loss: -0.0642, G_loss: -0.1905\n",
      "  Batch [450/1299] D_loss: -0.3361, G_loss: 1.4011\n",
      "  Batch [460/1299] D_loss: -0.8013, G_loss: -1.4074\n",
      "  Batch [470/1299] D_loss: -0.3429, G_loss: 1.0806\n",
      "  Batch [480/1299] D_loss: -0.4275, G_loss: 0.0991\n",
      "  Batch [490/1299] D_loss: -0.7795, G_loss: 1.3123\n",
      "  Batch [500/1299] D_loss: 0.0235, G_loss: 0.7719\n",
      "  Batch [510/1299] D_loss: -0.2320, G_loss: -1.6068\n",
      "  Batch [520/1299] D_loss: -0.8260, G_loss: 2.2014\n",
      "  Batch [530/1299] D_loss: -0.0741, G_loss: -1.6629\n",
      "  Batch [540/1299] D_loss: -0.6800, G_loss: 0.1380\n",
      "  Batch [550/1299] D_loss: -0.0575, G_loss: -1.8632\n",
      "  Batch [560/1299] D_loss: 0.1712, G_loss: 1.0173\n",
      "  Batch [570/1299] D_loss: -0.3451, G_loss: -0.0512\n",
      "  Batch [580/1299] D_loss: -1.1605, G_loss: -2.8102\n",
      "  Batch [590/1299] D_loss: -1.0396, G_loss: 1.9057\n",
      "  Batch [600/1299] D_loss: -1.0731, G_loss: 0.8820\n",
      "  Batch [610/1299] D_loss: -0.4207, G_loss: -0.9619\n",
      "  Batch [620/1299] D_loss: -0.4879, G_loss: 0.4175\n",
      "  Batch [630/1299] D_loss: -0.2246, G_loss: 0.0259\n",
      "  Batch [640/1299] D_loss: -0.4004, G_loss: 0.0171\n",
      "  Batch [650/1299] D_loss: -0.6437, G_loss: 0.3596\n",
      "  Batch [660/1299] D_loss: -0.6175, G_loss: 0.7227\n",
      "  Batch [670/1299] D_loss: -0.8797, G_loss: -3.0216\n",
      "  Batch [680/1299] D_loss: -0.1341, G_loss: 0.8313\n",
      "  Batch [690/1299] D_loss: -0.2522, G_loss: 0.1431\n",
      "  Batch [700/1299] D_loss: -0.8505, G_loss: 0.2975\n",
      "  Batch [710/1299] D_loss: -0.9762, G_loss: -0.0278\n",
      "  Batch [720/1299] D_loss: 0.8092, G_loss: -1.4027\n",
      "  Batch [730/1299] D_loss: -0.5332, G_loss: 1.2885\n",
      "  Batch [740/1299] D_loss: -0.7276, G_loss: -1.4668\n",
      "  Batch [750/1299] D_loss: -0.9240, G_loss: -3.0718\n",
      "  Batch [760/1299] D_loss: -0.1326, G_loss: 0.8573\n",
      "  Batch [770/1299] D_loss: -0.1922, G_loss: 1.4250\n",
      "  Batch [780/1299] D_loss: 0.2735, G_loss: -2.2115\n",
      "  Batch [790/1299] D_loss: 0.0727, G_loss: -0.1285\n",
      "  Batch [800/1299] D_loss: -0.8214, G_loss: 3.7100\n",
      "  Batch [810/1299] D_loss: -0.2048, G_loss: 0.2664\n",
      "  Batch [820/1299] D_loss: -0.1618, G_loss: -2.0964\n",
      "  Batch [830/1299] D_loss: -1.4470, G_loss: 4.2223\n",
      "  Batch [840/1299] D_loss: -0.2562, G_loss: -0.1541\n",
      "  Batch [850/1299] D_loss: -0.0021, G_loss: -2.3538\n",
      "  Batch [860/1299] D_loss: -0.8314, G_loss: 2.3262\n",
      "  Batch [870/1299] D_loss: -1.1513, G_loss: 1.6050\n",
      "  Batch [880/1299] D_loss: -0.1434, G_loss: -0.7092\n",
      "  Batch [890/1299] D_loss: -0.3210, G_loss: 2.0441\n",
      "  Batch [900/1299] D_loss: -0.2856, G_loss: -2.7164\n",
      "  Batch [910/1299] D_loss: -0.4065, G_loss: -1.0265\n",
      "  Batch [920/1299] D_loss: -0.9982, G_loss: -3.2810\n",
      "  Batch [930/1299] D_loss: -1.1668, G_loss: 2.8945\n",
      "  Batch [940/1299] D_loss: 0.0691, G_loss: -1.2547\n",
      "  Batch [950/1299] D_loss: -0.1981, G_loss: 0.6715\n",
      "  Batch [960/1299] D_loss: -0.6047, G_loss: -0.8130\n",
      "  Batch [970/1299] D_loss: -0.0997, G_loss: 0.9852\n",
      "  Batch [980/1299] D_loss: -0.3798, G_loss: -2.3003\n",
      "  Batch [990/1299] D_loss: -1.1109, G_loss: 0.4077\n",
      "  Batch [1000/1299] D_loss: -0.0788, G_loss: 0.4538\n",
      "  Batch [1010/1299] D_loss: -0.8390, G_loss: -2.6219\n",
      "  Batch [1020/1299] D_loss: -0.1276, G_loss: -0.3245\n",
      "  Batch [1030/1299] D_loss: -0.3925, G_loss: 2.8608\n",
      "  Batch [1040/1299] D_loss: -0.3363, G_loss: 1.1448\n",
      "  Batch [1050/1299] D_loss: -0.0685, G_loss: -3.9718\n",
      "  Batch [1060/1299] D_loss: -0.4882, G_loss: 2.4505\n",
      "  Batch [1070/1299] D_loss: -0.4394, G_loss: 1.2949\n",
      "  Batch [1080/1299] D_loss: 0.0064, G_loss: -3.0717\n",
      "  Batch [1090/1299] D_loss: -0.7352, G_loss: 1.3416\n",
      "  Batch [1100/1299] D_loss: -0.0475, G_loss: 0.3834\n",
      "  Batch [1110/1299] D_loss: -0.4753, G_loss: -0.9536\n",
      "  Batch [1120/1299] D_loss: -0.9236, G_loss: 1.8696\n",
      "  Batch [1130/1299] D_loss: -1.3661, G_loss: 2.9091\n",
      "  Batch [1140/1299] D_loss: -0.6308, G_loss: -2.4607\n",
      "  Batch [1150/1299] D_loss: -1.0486, G_loss: 2.1372\n",
      "  Batch [1160/1299] D_loss: -1.6625, G_loss: 1.9406\n",
      "  Batch [1170/1299] D_loss: -0.0583, G_loss: -0.4593\n",
      "  Batch [1180/1299] D_loss: -0.2560, G_loss: 0.5317\n",
      "  Batch [1190/1299] D_loss: -0.1189, G_loss: 0.8477\n",
      "  Batch [1200/1299] D_loss: -0.8423, G_loss: 0.3885\n",
      "  Batch [1210/1299] D_loss: -0.1176, G_loss: 1.2739\n",
      "  Batch [1220/1299] D_loss: -0.7593, G_loss: -2.0778\n",
      "  Batch [1230/1299] D_loss: -0.2854, G_loss: 1.1116\n",
      "  Batch [1240/1299] D_loss: -1.0695, G_loss: -0.7557\n",
      "  Batch [1250/1299] D_loss: 0.1706, G_loss: -2.1045\n",
      "  Batch [1260/1299] D_loss: -0.1315, G_loss: 1.2800\n",
      "  Batch [1270/1299] D_loss: -0.1185, G_loss: 1.5297\n",
      "  Batch [1280/1299] D_loss: -0.5508, G_loss: -3.0192\n",
      "  Batch [1290/1299] D_loss: -0.1970, G_loss: 0.4087\n",
      "\n",
      "Epoch 6 Summary:\n",
      "  Average D_loss: -0.2118\n",
      "  Average G_loss: 0.0329\n",
      "\n",
      "Epoch [7/100]\n",
      "  Batch [0/1299] D_loss: 0.0096, G_loss: 0.7556\n",
      "  Batch [10/1299] D_loss: 0.1176, G_loss: -1.9168\n",
      "  Batch [20/1299] D_loss: -0.4185, G_loss: 2.4692\n",
      "  Batch [30/1299] D_loss: -0.3463, G_loss: -0.4698\n",
      "  Batch [40/1299] D_loss: -0.6295, G_loss: -1.6471\n",
      "  Batch [50/1299] D_loss: -0.7253, G_loss: -0.6191\n",
      "  Batch [60/1299] D_loss: 0.0110, G_loss: -0.7996\n",
      "  Batch [70/1299] D_loss: -0.4146, G_loss: 1.0694\n",
      "  Batch [80/1299] D_loss: 0.1479, G_loss: 0.4482\n",
      "  Batch [90/1299] D_loss: -0.3436, G_loss: 0.2512\n",
      "  Batch [100/1299] D_loss: -0.9067, G_loss: -1.4727\n",
      "  Batch [110/1299] D_loss: -0.1293, G_loss: -2.0083\n",
      "  Batch [120/1299] D_loss: -0.7785, G_loss: -1.0021\n",
      "  Batch [130/1299] D_loss: -0.2545, G_loss: 0.6648\n",
      "  Batch [140/1299] D_loss: 0.1709, G_loss: -0.9863\n",
      "  Batch [150/1299] D_loss: -0.3949, G_loss: 3.5016\n",
      "  Batch [160/1299] D_loss: -0.4075, G_loss: 0.6140\n",
      "  Batch [170/1299] D_loss: -0.5485, G_loss: -1.8265\n",
      "  Batch [180/1299] D_loss: -0.3546, G_loss: 0.6268\n",
      "  Batch [190/1299] D_loss: -0.7452, G_loss: 1.3638\n",
      "  Batch [200/1299] D_loss: -0.3269, G_loss: -1.6374\n",
      "  Batch [210/1299] D_loss: 0.0673, G_loss: -0.2609\n",
      "  Batch [220/1299] D_loss: -0.3488, G_loss: 0.1979\n",
      "  Batch [230/1299] D_loss: -0.4599, G_loss: -0.1002\n",
      "  Batch [240/1299] D_loss: 0.1357, G_loss: -1.1620\n",
      "  Batch [250/1299] D_loss: -0.1403, G_loss: 1.2452\n",
      "  Batch [260/1299] D_loss: -0.7402, G_loss: 0.5106\n",
      "  Batch [270/1299] D_loss: -0.9331, G_loss: 1.2550\n",
      "  Batch [280/1299] D_loss: -0.8768, G_loss: 2.1324\n",
      "  Batch [290/1299] D_loss: -0.5588, G_loss: -3.0131\n",
      "  Batch [300/1299] D_loss: -0.1493, G_loss: -0.3281\n",
      "  Batch [310/1299] D_loss: -0.3305, G_loss: 1.7375\n",
      "  Batch [320/1299] D_loss: -1.6139, G_loss: -3.0056\n",
      "  Batch [330/1299] D_loss: -0.4127, G_loss: 1.2568\n",
      "  Batch [340/1299] D_loss: -0.6509, G_loss: 1.1489\n",
      "  Batch [350/1299] D_loss: -0.4338, G_loss: -1.1969\n",
      "  Batch [360/1299] D_loss: -1.1322, G_loss: 1.4310\n",
      "  Batch [370/1299] D_loss: -0.5851, G_loss: -1.0969\n",
      "  Batch [380/1299] D_loss: -1.9739, G_loss: -0.2241\n",
      "  Batch [390/1299] D_loss: -0.1237, G_loss: -2.8771\n",
      "  Batch [400/1299] D_loss: -0.0287, G_loss: -0.8284\n",
      "  Batch [410/1299] D_loss: -0.7939, G_loss: 0.9875\n",
      "  Batch [420/1299] D_loss: -0.2794, G_loss: 2.1470\n",
      "  Batch [430/1299] D_loss: 0.0740, G_loss: -2.2834\n",
      "  Batch [440/1299] D_loss: -1.2358, G_loss: 4.3929\n",
      "  Batch [450/1299] D_loss: -0.5492, G_loss: 0.4215\n",
      "  Batch [460/1299] D_loss: -1.0140, G_loss: -1.1513\n",
      "  Batch [470/1299] D_loss: -1.5941, G_loss: -3.1952\n",
      "  Batch [480/1299] D_loss: -1.1461, G_loss: 3.8945\n",
      "  Batch [490/1299] D_loss: -0.9692, G_loss: -2.5485\n",
      "  Batch [500/1299] D_loss: 0.1336, G_loss: -0.4807\n",
      "  Batch [510/1299] D_loss: -0.4651, G_loss: -0.5395\n",
      "  Batch [520/1299] D_loss: -0.3083, G_loss: 1.1422\n",
      "  Batch [530/1299] D_loss: 0.1458, G_loss: -0.5297\n",
      "  Batch [540/1299] D_loss: -0.6940, G_loss: 1.5012\n",
      "  Batch [550/1299] D_loss: -0.8776, G_loss: 1.8412\n",
      "  Batch [560/1299] D_loss: -2.1643, G_loss: 0.0049\n",
      "  Batch [570/1299] D_loss: 0.1807, G_loss: -1.7389\n",
      "  Batch [580/1299] D_loss: -0.6935, G_loss: 1.4634\n",
      "  Batch [590/1299] D_loss: -0.1797, G_loss: 1.2532\n",
      "  Batch [600/1299] D_loss: -1.0872, G_loss: -1.0901\n",
      "  Batch [610/1299] D_loss: -0.9840, G_loss: -0.7412\n",
      "  Batch [620/1299] D_loss: -1.1720, G_loss: 1.2119\n",
      "  Batch [630/1299] D_loss: -0.5526, G_loss: 1.2419\n",
      "  Batch [640/1299] D_loss: -0.0529, G_loss: 1.4159\n",
      "  Batch [650/1299] D_loss: -0.7843, G_loss: -2.3910\n",
      "  Batch [660/1299] D_loss: -0.2365, G_loss: -0.5403\n",
      "  Batch [670/1299] D_loss: -0.0542, G_loss: -0.3705\n",
      "  Batch [680/1299] D_loss: -0.5075, G_loss: 1.6033\n",
      "  Batch [690/1299] D_loss: -0.6054, G_loss: 2.2321\n",
      "  Batch [700/1299] D_loss: -0.8230, G_loss: -2.4262\n",
      "  Batch [710/1299] D_loss: -0.0981, G_loss: -0.1490\n",
      "  Batch [720/1299] D_loss: -0.8919, G_loss: 3.1567\n",
      "  Batch [730/1299] D_loss: -0.2989, G_loss: 0.5102\n",
      "  Batch [740/1299] D_loss: -0.7663, G_loss: 0.6035\n",
      "  Batch [750/1299] D_loss: -0.6001, G_loss: 1.4611\n",
      "  Batch [760/1299] D_loss: -0.9859, G_loss: 1.0459\n",
      "  Batch [770/1299] D_loss: -0.1412, G_loss: 0.3943\n",
      "  Batch [780/1299] D_loss: -0.7705, G_loss: -2.7634\n",
      "  Batch [790/1299] D_loss: -0.4451, G_loss: 1.0648\n",
      "  Batch [800/1299] D_loss: -0.7069, G_loss: 0.6359\n",
      "  Batch [810/1299] D_loss: -0.7376, G_loss: -0.0308\n",
      "  Batch [820/1299] D_loss: -0.3818, G_loss: 0.6326\n",
      "  Batch [830/1299] D_loss: -0.6313, G_loss: 0.8692\n",
      "  Batch [840/1299] D_loss: -0.2684, G_loss: -2.1982\n",
      "  Batch [850/1299] D_loss: -0.4752, G_loss: 1.2829\n",
      "  Batch [860/1299] D_loss: -0.5956, G_loss: -1.4602\n",
      "  Batch [870/1299] D_loss: -1.1657, G_loss: 1.4853\n",
      "  Batch [880/1299] D_loss: -1.0459, G_loss: -0.2969\n",
      "  Batch [890/1299] D_loss: -1.0536, G_loss: -2.9094\n",
      "  Batch [900/1299] D_loss: -0.3240, G_loss: -0.6939\n",
      "  Batch [910/1299] D_loss: -0.0801, G_loss: 2.4633\n",
      "  Batch [920/1299] D_loss: -0.1787, G_loss: -0.4054\n",
      "  Batch [930/1299] D_loss: -0.8711, G_loss: -2.3330\n",
      "  Batch [940/1299] D_loss: -0.4658, G_loss: 1.4134\n",
      "  Batch [950/1299] D_loss: -0.9739, G_loss: 1.0980\n",
      "  Batch [960/1299] D_loss: -0.6560, G_loss: -1.6667\n",
      "  Batch [970/1299] D_loss: -0.4923, G_loss: 1.4899\n",
      "  Batch [980/1299] D_loss: -0.2328, G_loss: 1.7397\n",
      "  Batch [990/1299] D_loss: -0.0361, G_loss: -2.5971\n",
      "  Batch [1000/1299] D_loss: -0.8176, G_loss: -0.0133\n",
      "  Batch [1010/1299] D_loss: -0.9338, G_loss: 0.5636\n",
      "  Batch [1020/1299] D_loss: 0.0140, G_loss: -1.2651\n",
      "  Batch [1030/1299] D_loss: -0.9344, G_loss: 3.2140\n",
      "  Batch [1040/1299] D_loss: -0.5217, G_loss: 0.3215\n",
      "  Batch [1050/1299] D_loss: 0.1917, G_loss: -2.5327\n",
      "  Batch [1060/1299] D_loss: -0.6687, G_loss: 2.4047\n",
      "  Batch [1070/1299] D_loss: -0.7800, G_loss: -1.0149\n",
      "  Batch [1080/1299] D_loss: -0.1353, G_loss: 0.9088\n",
      "  Batch [1090/1299] D_loss: -0.9691, G_loss: 1.6950\n",
      "  Batch [1100/1299] D_loss: -0.6186, G_loss: 0.3419\n",
      "  Batch [1110/1299] D_loss: -0.7947, G_loss: -0.1302\n",
      "  Batch [1120/1299] D_loss: -0.9767, G_loss: 3.1984\n",
      "  Batch [1130/1299] D_loss: -0.0147, G_loss: 0.0322\n",
      "  Batch [1140/1299] D_loss: -0.2409, G_loss: -1.7618\n",
      "  Batch [1150/1299] D_loss: -0.4412, G_loss: -1.4036\n",
      "  Batch [1160/1299] D_loss: -0.3435, G_loss: -0.4101\n",
      "  Batch [1170/1299] D_loss: -0.6094, G_loss: -3.3446\n",
      "  Batch [1180/1299] D_loss: -1.0993, G_loss: 2.4376\n",
      "  Batch [1190/1299] D_loss: -0.4622, G_loss: -2.1416\n",
      "  Batch [1200/1299] D_loss: -0.8561, G_loss: 0.1310\n",
      "  Batch [1210/1299] D_loss: -0.7963, G_loss: 1.1059\n",
      "  Batch [1220/1299] D_loss: -0.8653, G_loss: 0.7614\n",
      "  Batch [1230/1299] D_loss: -0.6275, G_loss: -0.8905\n",
      "  Batch [1240/1299] D_loss: -0.5767, G_loss: -1.0128\n",
      "  Batch [1250/1299] D_loss: -0.9711, G_loss: 1.7211\n",
      "  Batch [1260/1299] D_loss: -0.4651, G_loss: 0.4857\n",
      "  Batch [1270/1299] D_loss: -0.9895, G_loss: -1.5629\n",
      "  Batch [1280/1299] D_loss: -0.6902, G_loss: 1.3906\n",
      "  Batch [1290/1299] D_loss: 0.0581, G_loss: -1.0689\n",
      "\n",
      "Epoch 7 Summary:\n",
      "  Average D_loss: -0.2183\n",
      "  Average G_loss: 0.0429\n",
      "\n",
      "Epoch [8/100]\n",
      "  Batch [0/1299] D_loss: -0.1639, G_loss: 0.5914\n",
      "  Batch [10/1299] D_loss: -0.0557, G_loss: 1.2086\n",
      "  Batch [20/1299] D_loss: -0.1193, G_loss: -1.6258\n",
      "  Batch [30/1299] D_loss: -0.7446, G_loss: 0.7210\n",
      "  Batch [40/1299] D_loss: -0.1780, G_loss: 0.3333\n",
      "  Batch [50/1299] D_loss: -0.1974, G_loss: -0.5939\n",
      "  Batch [60/1299] D_loss: -0.5044, G_loss: 1.4908\n",
      "  Batch [70/1299] D_loss: -1.1190, G_loss: -0.8808\n",
      "  Batch [80/1299] D_loss: 0.2094, G_loss: -0.5019\n",
      "  Batch [90/1299] D_loss: -0.4298, G_loss: 0.5777\n",
      "  Batch [100/1299] D_loss: -0.1569, G_loss: -0.5310\n",
      "  Batch [110/1299] D_loss: -0.6854, G_loss: 2.7938\n",
      "  Batch [120/1299] D_loss: -0.8688, G_loss: -1.9488\n",
      "  Batch [130/1299] D_loss: -0.2957, G_loss: 0.1440\n",
      "  Batch [140/1299] D_loss: 0.0959, G_loss: 0.5939\n",
      "  Batch [150/1299] D_loss: -0.7324, G_loss: -3.7045\n",
      "  Batch [160/1299] D_loss: -0.4777, G_loss: 1.9372\n",
      "  Batch [170/1299] D_loss: -0.0538, G_loss: 0.4317\n",
      "  Batch [180/1299] D_loss: -1.4105, G_loss: -3.5345\n",
      "  Batch [190/1299] D_loss: -0.7436, G_loss: 1.7278\n",
      "  Batch [200/1299] D_loss: 0.1153, G_loss: 2.3657\n",
      "  Batch [210/1299] D_loss: -0.6637, G_loss: -1.7670\n",
      "  Batch [220/1299] D_loss: -0.5501, G_loss: -1.1389\n",
      "  Batch [230/1299] D_loss: -0.7512, G_loss: 1.0914\n",
      "  Batch [240/1299] D_loss: -1.0303, G_loss: -2.6652\n",
      "  Batch [250/1299] D_loss: -0.1211, G_loss: -0.3763\n",
      "  Batch [260/1299] D_loss: -0.7898, G_loss: 2.1661\n",
      "  Batch [270/1299] D_loss: -0.3163, G_loss: 0.2461\n",
      "  Batch [280/1299] D_loss: -0.4508, G_loss: -0.0018\n",
      "  Batch [290/1299] D_loss: 0.3351, G_loss: -0.8449\n",
      "  Batch [300/1299] D_loss: -0.5871, G_loss: 0.5215\n",
      "  Batch [310/1299] D_loss: -0.1682, G_loss: 0.3191\n",
      "  Batch [320/1299] D_loss: -0.3148, G_loss: -2.0070\n",
      "  Batch [330/1299] D_loss: -0.9027, G_loss: 1.4108\n",
      "  Batch [340/1299] D_loss: -0.6157, G_loss: 0.6702\n",
      "  Batch [350/1299] D_loss: -0.3932, G_loss: -2.3703\n",
      "  Batch [360/1299] D_loss: -0.6129, G_loss: 1.1333\n",
      "  Batch [370/1299] D_loss: -1.0091, G_loss: 1.9143\n",
      "  Batch [380/1299] D_loss: -1.4110, G_loss: -2.5665\n",
      "  Batch [390/1299] D_loss: -0.4272, G_loss: -1.5358\n",
      "  Batch [400/1299] D_loss: -0.0261, G_loss: 0.1771\n",
      "  Batch [410/1299] D_loss: -0.4749, G_loss: 2.2357\n",
      "  Batch [420/1299] D_loss: 0.0963, G_loss: -0.1269\n",
      "  Batch [430/1299] D_loss: -0.6079, G_loss: -3.1593\n",
      "  Batch [440/1299] D_loss: -0.6482, G_loss: -1.4508\n",
      "  Batch [450/1299] D_loss: -0.7285, G_loss: 2.5354\n",
      "  Batch [460/1299] D_loss: 0.0961, G_loss: 0.5494\n",
      "  Batch [470/1299] D_loss: -0.1587, G_loss: -0.7788\n",
      "  Batch [480/1299] D_loss: -0.1932, G_loss: 0.5532\n",
      "  Batch [490/1299] D_loss: -0.3959, G_loss: 0.2162\n",
      "  Batch [500/1299] D_loss: -0.1868, G_loss: 1.3435\n",
      "  Batch [510/1299] D_loss: -0.6484, G_loss: -0.9341\n",
      "  Batch [520/1299] D_loss: -0.1407, G_loss: -1.2231\n",
      "  Batch [530/1299] D_loss: -0.1632, G_loss: -0.6527\n",
      "  Batch [540/1299] D_loss: -0.4395, G_loss: 0.5523\n",
      "  Batch [550/1299] D_loss: 0.2094, G_loss: -1.5311\n",
      "  Batch [560/1299] D_loss: 0.0027, G_loss: -0.2052\n",
      "  Batch [570/1299] D_loss: -1.2796, G_loss: 3.2901\n",
      "  Batch [580/1299] D_loss: -0.1447, G_loss: -0.0452\n",
      "  Batch [590/1299] D_loss: -0.2648, G_loss: -1.3079\n",
      "  Batch [600/1299] D_loss: -0.0832, G_loss: -0.4325\n",
      "  Batch [610/1299] D_loss: -0.6076, G_loss: 0.7412\n",
      "  Batch [620/1299] D_loss: -0.6478, G_loss: -1.4417\n",
      "  Batch [630/1299] D_loss: 0.1356, G_loss: -0.3093\n",
      "  Batch [640/1299] D_loss: -0.2043, G_loss: 0.6507\n",
      "  Batch [650/1299] D_loss: -0.8378, G_loss: 3.0076\n",
      "  Batch [660/1299] D_loss: -1.5530, G_loss: -3.6965\n",
      "  Batch [670/1299] D_loss: -0.0216, G_loss: -0.0345\n",
      "  Batch [680/1299] D_loss: 0.0478, G_loss: 1.0351\n",
      "  Batch [690/1299] D_loss: -0.1257, G_loss: 0.0631\n",
      "  Batch [700/1299] D_loss: -0.7498, G_loss: 1.2760\n",
      "  Batch [710/1299] D_loss: -0.4260, G_loss: 1.7699\n",
      "  Batch [720/1299] D_loss: -0.1874, G_loss: -0.5945\n",
      "  Batch [730/1299] D_loss: 0.1497, G_loss: -0.9622\n",
      "  Batch [740/1299] D_loss: -0.3125, G_loss: 0.5445\n",
      "  Batch [750/1299] D_loss: 0.2378, G_loss: 1.4989\n",
      "  Batch [760/1299] D_loss: -0.0683, G_loss: -0.9878\n",
      "  Batch [770/1299] D_loss: -0.2038, G_loss: -0.1872\n",
      "  Batch [780/1299] D_loss: 0.0829, G_loss: -0.9841\n",
      "  Batch [790/1299] D_loss: -0.7799, G_loss: 1.2281\n",
      "  Batch [800/1299] D_loss: -0.7507, G_loss: 0.4509\n",
      "  Batch [810/1299] D_loss: -0.8432, G_loss: 0.5442\n",
      "  Batch [820/1299] D_loss: 0.0600, G_loss: 1.3950\n",
      "  Batch [830/1299] D_loss: -0.1384, G_loss: -1.8081\n",
      "  Batch [840/1299] D_loss: 0.0962, G_loss: -0.8531\n",
      "  Batch [850/1299] D_loss: -0.4507, G_loss: 2.0728\n",
      "  Batch [860/1299] D_loss: -0.7180, G_loss: -0.7872\n",
      "  Batch [870/1299] D_loss: 0.0272, G_loss: -1.5350\n",
      "  Batch [880/1299] D_loss: -0.6432, G_loss: 0.1569\n",
      "  Batch [890/1299] D_loss: -0.8341, G_loss: 0.6950\n",
      "  Batch [900/1299] D_loss: -0.0551, G_loss: -0.0824\n",
      "  Batch [910/1299] D_loss: -0.7726, G_loss: 1.0306\n",
      "  Batch [920/1299] D_loss: -0.8023, G_loss: 2.7905\n",
      "  Batch [930/1299] D_loss: -0.1740, G_loss: 0.9225\n",
      "  Batch [940/1299] D_loss: -0.6516, G_loss: -2.6845\n",
      "  Batch [950/1299] D_loss: -2.0847, G_loss: 2.7713\n",
      "  Batch [960/1299] D_loss: -0.1639, G_loss: 0.1763\n",
      "  Batch [970/1299] D_loss: -0.5063, G_loss: -3.0438\n",
      "  Batch [980/1299] D_loss: -1.7746, G_loss: 5.1276\n",
      "  Batch [990/1299] D_loss: -0.4781, G_loss: -6.3123\n",
      "  Batch [1000/1299] D_loss: -0.6538, G_loss: 3.9530\n",
      "  Batch [1010/1299] D_loss: 0.3879, G_loss: 0.2263\n",
      "  Batch [1020/1299] D_loss: -3.2425, G_loss: -10.7959\n",
      "  Batch [1030/1299] D_loss: 0.0124, G_loss: -0.0586\n",
      "  Batch [1040/1299] D_loss: -0.3072, G_loss: -0.2213\n",
      "  Batch [1050/1299] D_loss: -0.1228, G_loss: -0.0387\n",
      "  Batch [1060/1299] D_loss: -0.2289, G_loss: -0.0018\n",
      "  Batch [1070/1299] D_loss: -0.5055, G_loss: 0.1138\n",
      "  Batch [1080/1299] D_loss: -0.3448, G_loss: 0.1217\n",
      "  Batch [1090/1299] D_loss: -1.4236, G_loss: 0.0397\n",
      "  Batch [1100/1299] D_loss: -0.2186, G_loss: -0.5056\n",
      "  Batch [1110/1299] D_loss: -0.0236, G_loss: 0.1840\n",
      "  Batch [1120/1299] D_loss: -0.0606, G_loss: 0.1974\n",
      "  Batch [1130/1299] D_loss: -0.1055, G_loss: 0.2225\n",
      "  Batch [1140/1299] D_loss: -0.0625, G_loss: 0.2330\n",
      "  Batch [1150/1299] D_loss: -0.1035, G_loss: 0.3008\n",
      "  Batch [1160/1299] D_loss: -0.0542, G_loss: 0.2573\n",
      "  Batch [1170/1299] D_loss: -0.0832, G_loss: 0.2479\n",
      "  Batch [1180/1299] D_loss: -0.0954, G_loss: -0.2247\n",
      "  Batch [1190/1299] D_loss: -0.5637, G_loss: -0.0470\n",
      "  Batch [1200/1299] D_loss: -0.6120, G_loss: -1.6126\n",
      "  Batch [1210/1299] D_loss: -0.9007, G_loss: -0.1232\n",
      "  Batch [1220/1299] D_loss: -0.0508, G_loss: 0.2650\n",
      "  Batch [1230/1299] D_loss: -0.0320, G_loss: 0.3253\n",
      "  Batch [1240/1299] D_loss: -0.0818, G_loss: 0.4827\n",
      "  Batch [1250/1299] D_loss: -0.1719, G_loss: 0.6735\n",
      "  Batch [1260/1299] D_loss: -0.0758, G_loss: 0.4875\n",
      "  Batch [1270/1299] D_loss: -0.0708, G_loss: 0.3539\n",
      "  Batch [1280/1299] D_loss: -0.1049, G_loss: 0.2485\n",
      "  Batch [1290/1299] D_loss: -0.0263, G_loss: 0.0486\n",
      "\n",
      "Epoch 8 Summary:\n",
      "  Average D_loss: -0.1890\n",
      "  Average G_loss: 0.0072\n",
      "\n",
      "Epoch [9/100]\n",
      "  Batch [0/1299] D_loss: -1.5484, G_loss: -1.6465\n",
      "  Batch [10/1299] D_loss: -0.5171, G_loss: -0.1949\n",
      "  Batch [20/1299] D_loss: -0.3299, G_loss: -1.1600\n",
      "  Batch [30/1299] D_loss: -1.3605, G_loss: -0.6570\n",
      "  Batch [40/1299] D_loss: -0.0209, G_loss: 0.1352\n",
      "  Batch [50/1299] D_loss: -1.9676, G_loss: 0.1950\n",
      "  Batch [60/1299] D_loss: -0.6699, G_loss: 0.1973\n",
      "  Batch [70/1299] D_loss: -0.0584, G_loss: 0.2195\n",
      "  Batch [80/1299] D_loss: -0.2229, G_loss: 0.1519\n",
      "  Batch [90/1299] D_loss: -0.0440, G_loss: 0.2273\n",
      "  Batch [100/1299] D_loss: -0.0880, G_loss: 0.2454\n",
      "  Batch [110/1299] D_loss: -0.5310, G_loss: -0.3686\n",
      "  Batch [120/1299] D_loss: -0.0062, G_loss: 0.1897\n",
      "  Batch [130/1299] D_loss: -0.2028, G_loss: 0.3677\n",
      "  Batch [140/1299] D_loss: -0.1350, G_loss: 0.4587\n",
      "  Batch [150/1299] D_loss: -0.1153, G_loss: 0.6063\n",
      "  Batch [160/1299] D_loss: 0.0226, G_loss: 0.2343\n",
      "  Batch [170/1299] D_loss: -0.3470, G_loss: -0.3414\n",
      "  Batch [180/1299] D_loss: -0.6111, G_loss: -0.9443\n",
      "  Batch [190/1299] D_loss: -0.0292, G_loss: 0.1288\n",
      "  Batch [200/1299] D_loss: -0.1914, G_loss: 0.2991\n",
      "  Batch [210/1299] D_loss: -0.0663, G_loss: 0.4459\n",
      "  Batch [220/1299] D_loss: -0.0748, G_loss: 0.3385\n",
      "  Batch [230/1299] D_loss: 0.0328, G_loss: 0.1901\n",
      "  Batch [240/1299] D_loss: -1.1054, G_loss: -0.4951\n",
      "  Batch [250/1299] D_loss: -1.4036, G_loss: -2.3893\n",
      "  Batch [260/1299] D_loss: -0.0378, G_loss: 0.1821\n",
      "  Batch [270/1299] D_loss: 0.0260, G_loss: 0.2850\n",
      "  Batch [280/1299] D_loss: -0.2832, G_loss: 0.6037\n",
      "  Batch [290/1299] D_loss: -0.0961, G_loss: 0.3867\n",
      "  Batch [300/1299] D_loss: -0.0595, G_loss: 0.3473\n",
      "  Batch [310/1299] D_loss: -0.0403, G_loss: 0.3312\n",
      "  Batch [320/1299] D_loss: -0.8650, G_loss: -1.4040\n",
      "  Batch [330/1299] D_loss: -1.0126, G_loss: -2.5407\n",
      "  Batch [340/1299] D_loss: -0.1494, G_loss: 0.0779\n",
      "  Batch [350/1299] D_loss: -0.0868, G_loss: 0.3148\n",
      "  Batch [360/1299] D_loss: -0.2016, G_loss: 0.5456\n",
      "  Batch [370/1299] D_loss: -0.1350, G_loss: 0.5371\n",
      "  Batch [380/1299] D_loss: 0.0183, G_loss: 0.5636\n",
      "  Batch [390/1299] D_loss: 0.1399, G_loss: 0.4388\n",
      "  Batch [400/1299] D_loss: 0.0238, G_loss: 0.4521\n",
      "  Batch [410/1299] D_loss: -0.6702, G_loss: -1.9975\n",
      "  Batch [420/1299] D_loss: -0.1390, G_loss: 0.0456\n",
      "  Batch [430/1299] D_loss: -0.2168, G_loss: -0.1703\n",
      "  Batch [440/1299] D_loss: -0.0493, G_loss: 0.1303\n",
      "  Batch [450/1299] D_loss: -0.1218, G_loss: 0.3919\n",
      "  Batch [460/1299] D_loss: -0.0887, G_loss: 0.3623\n",
      "  Batch [470/1299] D_loss: -0.1736, G_loss: 0.3428\n",
      "  Batch [480/1299] D_loss: -0.2383, G_loss: 0.2791\n",
      "  Batch [490/1299] D_loss: -0.0198, G_loss: 0.0506\n",
      "  Batch [500/1299] D_loss: -0.0097, G_loss: 0.1255\n",
      "  Batch [510/1299] D_loss: -0.3039, G_loss: 0.1558\n",
      "  Batch [520/1299] D_loss: -0.1168, G_loss: 0.3855\n",
      "  Batch [530/1299] D_loss: -0.0939, G_loss: 0.6064\n",
      "  Batch [540/1299] D_loss: -0.1371, G_loss: 0.6207\n",
      "  Batch [550/1299] D_loss: 0.0366, G_loss: 0.6737\n",
      "  Batch [560/1299] D_loss: -0.0848, G_loss: 0.2965\n",
      "  Batch [570/1299] D_loss: -0.0253, G_loss: 0.2623\n",
      "  Batch [580/1299] D_loss: -0.0799, G_loss: -0.0047\n",
      "  Batch [590/1299] D_loss: -1.7796, G_loss: -5.4351\n",
      "  Batch [600/1299] D_loss: -0.0349, G_loss: 0.1230\n",
      "  Batch [610/1299] D_loss: -0.0384, G_loss: 0.1952\n",
      "  Batch [620/1299] D_loss: -0.0362, G_loss: 0.2772\n",
      "  Batch [630/1299] D_loss: -0.0373, G_loss: 0.3125\n",
      "  Batch [640/1299] D_loss: 0.0265, G_loss: 0.2815\n",
      "  Batch [650/1299] D_loss: -0.0498, G_loss: 0.2313\n",
      "  Batch [660/1299] D_loss: -0.2036, G_loss: -1.8694\n",
      "  Batch [670/1299] D_loss: 0.0119, G_loss: 0.1137\n",
      "  Batch [680/1299] D_loss: -0.0558, G_loss: 0.1759\n",
      "  Batch [690/1299] D_loss: -0.1640, G_loss: 0.2558\n",
      "  Batch [700/1299] D_loss: -0.1235, G_loss: 0.3417\n",
      "  Batch [710/1299] D_loss: -0.1235, G_loss: 0.1812\n",
      "  Batch [720/1299] D_loss: -0.5884, G_loss: -0.2547\n",
      "  Batch [730/1299] D_loss: -0.3761, G_loss: 0.0585\n",
      "  Batch [740/1299] D_loss: -0.4916, G_loss: 0.0944\n",
      "  Batch [750/1299] D_loss: -0.0870, G_loss: 0.1116\n",
      "  Batch [760/1299] D_loss: -0.0288, G_loss: 0.2507\n",
      "  Batch [770/1299] D_loss: -0.0578, G_loss: 0.3908\n",
      "  Batch [780/1299] D_loss: -0.0172, G_loss: 0.3962\n",
      "  Batch [790/1299] D_loss: -0.0774, G_loss: 0.3406\n",
      "  Batch [800/1299] D_loss: -1.0589, G_loss: -2.5012\n",
      "  Batch [810/1299] D_loss: -0.1051, G_loss: 0.0474\n",
      "  Batch [820/1299] D_loss: -0.7408, G_loss: -0.6295\n",
      "  Batch [830/1299] D_loss: -0.0852, G_loss: 0.2518\n",
      "  Batch [840/1299] D_loss: -0.0787, G_loss: 0.3803\n",
      "  Batch [850/1299] D_loss: 0.0717, G_loss: 0.3914\n",
      "  Batch [860/1299] D_loss: -0.1190, G_loss: 0.3599\n",
      "  Batch [870/1299] D_loss: -0.0705, G_loss: 0.3104\n",
      "  Batch [880/1299] D_loss: -0.6217, G_loss: -0.2454\n",
      "  Batch [890/1299] D_loss: -0.2707, G_loss: -0.1798\n",
      "  Batch [900/1299] D_loss: -0.1715, G_loss: 0.0393\n",
      "  Batch [910/1299] D_loss: -0.1798, G_loss: 0.0728\n",
      "  Batch [920/1299] D_loss: -0.3983, G_loss: 0.2013\n",
      "  Batch [930/1299] D_loss: -0.3929, G_loss: 0.1159\n",
      "  Batch [940/1299] D_loss: -0.0437, G_loss: 0.2777\n",
      "  Batch [950/1299] D_loss: -0.3784, G_loss: 0.2669\n",
      "  Batch [960/1299] D_loss: -0.0560, G_loss: 0.2901\n",
      "  Batch [970/1299] D_loss: -0.4239, G_loss: 0.2635\n",
      "  Batch [980/1299] D_loss: -0.5929, G_loss: 0.1006\n",
      "  Batch [990/1299] D_loss: -2.2001, G_loss: -1.6876\n",
      "  Batch [1000/1299] D_loss: -0.8888, G_loss: -0.0539\n",
      "  Batch [1010/1299] D_loss: -0.0979, G_loss: -0.0381\n",
      "  Batch [1020/1299] D_loss: 0.1292, G_loss: 0.0813\n",
      "  Batch [1030/1299] D_loss: -1.0183, G_loss: 0.1275\n",
      "  Batch [1040/1299] D_loss: -0.3538, G_loss: 0.1833\n",
      "  Batch [1050/1299] D_loss: -0.0938, G_loss: 0.1455\n",
      "  Batch [1060/1299] D_loss: -0.2913, G_loss: 0.6812\n",
      "  Batch [1070/1299] D_loss: -0.5472, G_loss: 1.1691\n",
      "  Batch [1080/1299] D_loss: -0.2650, G_loss: 1.2350\n",
      "  Batch [1090/1299] D_loss: -0.2988, G_loss: 0.9714\n",
      "  Batch [1100/1299] D_loss: -0.1151, G_loss: 0.7241\n",
      "  Batch [1110/1299] D_loss: -0.0331, G_loss: 0.4878\n",
      "  Batch [1120/1299] D_loss: -0.0654, G_loss: 0.2806\n",
      "  Batch [1130/1299] D_loss: 0.1928, G_loss: 0.0512\n",
      "  Batch [1140/1299] D_loss: -1.0973, G_loss: -1.7751\n",
      "  Batch [1150/1299] D_loss: -0.9977, G_loss: -0.3935\n",
      "  Batch [1160/1299] D_loss: -0.1482, G_loss: -0.0063\n",
      "  Batch [1170/1299] D_loss: -0.3954, G_loss: -0.7712\n",
      "  Batch [1180/1299] D_loss: -0.1466, G_loss: 0.3565\n",
      "  Batch [1190/1299] D_loss: -0.2258, G_loss: 0.5810\n",
      "  Batch [1200/1299] D_loss: -0.1552, G_loss: 0.5917\n",
      "  Batch [1210/1299] D_loss: -0.2047, G_loss: 0.5580\n",
      "  Batch [1220/1299] D_loss: -0.2166, G_loss: 0.4967\n",
      "  Batch [1230/1299] D_loss: 0.0478, G_loss: 0.2360\n",
      "  Batch [1240/1299] D_loss: -0.6355, G_loss: -1.0470\n",
      "  Batch [1250/1299] D_loss: -0.3046, G_loss: -0.1710\n",
      "  Batch [1260/1299] D_loss: -2.7538, G_loss: -1.2591\n",
      "  Batch [1270/1299] D_loss: -0.2769, G_loss: -0.3045\n",
      "  Batch [1280/1299] D_loss: -0.3932, G_loss: 0.1085\n",
      "  Batch [1290/1299] D_loss: -0.4092, G_loss: 0.0226\n",
      "\n",
      "Epoch 9 Summary:\n",
      "  Average D_loss: -0.1689\n",
      "  Average G_loss: -0.0714\n",
      "\n",
      "Epoch [10/100]\n",
      "  Batch [0/1299] D_loss: -0.2001, G_loss: 0.0276\n",
      "  Batch [10/1299] D_loss: 0.0550, G_loss: 0.2668\n",
      "  Batch [20/1299] D_loss: -0.0749, G_loss: 0.4217\n",
      "  Batch [30/1299] D_loss: -0.3563, G_loss: 0.6171\n",
      "  Batch [40/1299] D_loss: -0.0673, G_loss: 0.5634\n",
      "  Batch [50/1299] D_loss: -0.1725, G_loss: 0.5914\n",
      "  Batch [60/1299] D_loss: 0.0280, G_loss: 0.4038\n",
      "  Batch [70/1299] D_loss: -1.1758, G_loss: -1.9146\n",
      "  Batch [80/1299] D_loss: -0.0149, G_loss: 0.1264\n",
      "  Batch [90/1299] D_loss: -0.0002, G_loss: 0.2063\n",
      "  Batch [100/1299] D_loss: -0.0094, G_loss: 0.2129\n",
      "  Batch [110/1299] D_loss: 0.0528, G_loss: 0.1445\n",
      "  Batch [120/1299] D_loss: -1.3168, G_loss: -2.0920\n",
      "  Batch [130/1299] D_loss: -0.9857, G_loss: -2.0932\n",
      "  Batch [140/1299] D_loss: -0.2968, G_loss: 0.1060\n",
      "  Batch [150/1299] D_loss: -0.7070, G_loss: 0.0600\n",
      "  Batch [160/1299] D_loss: -0.0853, G_loss: 0.2536\n",
      "  Batch [170/1299] D_loss: -0.1846, G_loss: 0.1484\n",
      "  Batch [180/1299] D_loss: -0.5371, G_loss: -0.0378\n",
      "  Batch [190/1299] D_loss: -0.0594, G_loss: 0.1597\n",
      "  Batch [200/1299] D_loss: -0.0472, G_loss: 0.5075\n",
      "  Batch [210/1299] D_loss: -0.1832, G_loss: 0.4616\n",
      "  Batch [220/1299] D_loss: -0.1615, G_loss: 0.5698\n",
      "  Batch [230/1299] D_loss: -0.0242, G_loss: 0.4882\n",
      "  Batch [240/1299] D_loss: -0.0481, G_loss: 0.0074\n",
      "  Batch [250/1299] D_loss: 0.0138, G_loss: 0.0583\n",
      "  Batch [260/1299] D_loss: 0.0092, G_loss: 0.2294\n",
      "  Batch [270/1299] D_loss: -0.0527, G_loss: 0.3354\n",
      "  Batch [280/1299] D_loss: -0.1282, G_loss: 0.4118\n",
      "  Batch [290/1299] D_loss: 0.0008, G_loss: 0.4722\n",
      "  Batch [300/1299] D_loss: -0.0536, G_loss: 0.2780\n",
      "  Batch [310/1299] D_loss: -1.6244, G_loss: -0.0350\n",
      "  Batch [320/1299] D_loss: -1.0591, G_loss: -1.6356\n",
      "  Batch [330/1299] D_loss: -0.8590, G_loss: -2.1173\n",
      "  Batch [340/1299] D_loss: -0.8895, G_loss: -0.9452\n",
      "  Batch [350/1299] D_loss: -0.0945, G_loss: 0.2956\n",
      "  Batch [360/1299] D_loss: -0.0755, G_loss: 0.5329\n",
      "  Batch [370/1299] D_loss: 0.0376, G_loss: 0.6592\n",
      "  Batch [380/1299] D_loss: -0.1160, G_loss: 0.5442\n",
      "  Batch [390/1299] D_loss: 0.0003, G_loss: 0.4037\n",
      "  Batch [400/1299] D_loss: -0.2389, G_loss: 0.0966\n",
      "  Batch [410/1299] D_loss: -0.6329, G_loss: -0.2910\n",
      "  Batch [420/1299] D_loss: -0.0470, G_loss: 0.0864\n",
      "  Batch [430/1299] D_loss: -0.1102, G_loss: 0.2177\n",
      "  Batch [440/1299] D_loss: -0.1849, G_loss: 0.3993\n",
      "  Batch [450/1299] D_loss: -0.1178, G_loss: 0.4859\n",
      "  Batch [460/1299] D_loss: -0.1638, G_loss: 0.5156\n",
      "  Batch [470/1299] D_loss: 0.0464, G_loss: 0.0926\n",
      "  Batch [480/1299] D_loss: -1.2300, G_loss: -2.3405\n",
      "  Batch [490/1299] D_loss: -1.0702, G_loss: -1.1908\n",
      "  Batch [500/1299] D_loss: -0.6406, G_loss: 0.0904\n",
      "  Batch [510/1299] D_loss: -0.2819, G_loss: 0.2582\n",
      "  Batch [520/1299] D_loss: -0.3375, G_loss: 0.2711\n",
      "  Batch [530/1299] D_loss: -0.0329, G_loss: 0.2914\n",
      "  Batch [540/1299] D_loss: -0.0540, G_loss: 0.3133\n",
      "  Batch [550/1299] D_loss: -0.1122, G_loss: 0.2807\n",
      "  Batch [560/1299] D_loss: -0.1342, G_loss: -0.2336\n",
      "  Batch [570/1299] D_loss: -0.5371, G_loss: -1.0554\n",
      "  Batch [580/1299] D_loss: -1.7055, G_loss: -0.1846\n",
      "  Batch [590/1299] D_loss: -0.1409, G_loss: 0.0762\n",
      "  Batch [600/1299] D_loss: -0.0475, G_loss: 0.2958\n",
      "  Batch [610/1299] D_loss: -0.0678, G_loss: 0.5910\n",
      "  Batch [620/1299] D_loss: -0.1863, G_loss: 0.6733\n",
      "  Batch [630/1299] D_loss: -0.2210, G_loss: 0.6668\n",
      "  Batch [640/1299] D_loss: -0.1227, G_loss: 0.6603\n",
      "  Batch [650/1299] D_loss: 0.0129, G_loss: 0.3470\n",
      "  Batch [660/1299] D_loss: -0.8875, G_loss: -3.1137\n",
      "  Batch [670/1299] D_loss: -0.5611, G_loss: 0.1102\n",
      "  Batch [680/1299] D_loss: -0.0261, G_loss: 0.1959\n",
      "  Batch [690/1299] D_loss: 0.0607, G_loss: 0.3197\n",
      "  Batch [700/1299] D_loss: -0.0671, G_loss: 0.4449\n",
      "  Batch [710/1299] D_loss: -0.2121, G_loss: 0.4325\n",
      "  Batch [720/1299] D_loss: -0.0434, G_loss: 0.2985\n",
      "  Batch [730/1299] D_loss: -1.3178, G_loss: -2.1177\n",
      "  Batch [740/1299] D_loss: -0.2877, G_loss: -0.3400\n",
      "  Batch [750/1299] D_loss: -0.1001, G_loss: 0.1816\n",
      "  Batch [760/1299] D_loss: -0.3264, G_loss: 0.1519\n",
      "  Batch [770/1299] D_loss: -0.4386, G_loss: -0.2450\n",
      "  Batch [780/1299] D_loss: -0.0830, G_loss: 0.3024\n",
      "  Batch [790/1299] D_loss: -0.1326, G_loss: 0.5052\n",
      "  Batch [800/1299] D_loss: -0.1364, G_loss: 0.6035\n",
      "  Batch [810/1299] D_loss: -0.1341, G_loss: 0.5707\n",
      "  Batch [820/1299] D_loss: -0.1162, G_loss: 0.2722\n",
      "  Batch [830/1299] D_loss: -0.2151, G_loss: -0.0050\n",
      "  Batch [840/1299] D_loss: -0.0746, G_loss: 0.0587\n",
      "  Batch [850/1299] D_loss: -1.1923, G_loss: -0.6897\n",
      "  Batch [860/1299] D_loss: -0.4682, G_loss: 0.0678\n",
      "  Batch [870/1299] D_loss: -0.3605, G_loss: 0.1517\n",
      "  Batch [880/1299] D_loss: -0.0969, G_loss: 0.2184\n",
      "  Batch [890/1299] D_loss: -0.0792, G_loss: 0.3014\n",
      "  Batch [900/1299] D_loss: -0.0962, G_loss: 0.2757\n",
      "  Batch [910/1299] D_loss: -0.0115, G_loss: 0.2789\n",
      "  Batch [920/1299] D_loss: -0.7588, G_loss: 0.0142\n",
      "  Batch [930/1299] D_loss: -1.2086, G_loss: -2.1271\n",
      "  Batch [940/1299] D_loss: -0.1009, G_loss: 0.1629\n",
      "  Batch [950/1299] D_loss: -0.2138, G_loss: 0.4586\n",
      "  Batch [960/1299] D_loss: -0.0416, G_loss: 0.6161\n",
      "  Batch [970/1299] D_loss: -0.2789, G_loss: 0.6722\n",
      "  Batch [980/1299] D_loss: -0.0623, G_loss: 0.5576\n",
      "  Batch [990/1299] D_loss: -0.0151, G_loss: 0.2932\n",
      "  Batch [1000/1299] D_loss: -0.2229, G_loss: -0.2211\n",
      "  Batch [1010/1299] D_loss: -0.0262, G_loss: 0.1950\n",
      "  Batch [1020/1299] D_loss: -0.0623, G_loss: 0.3190\n",
      "  Batch [1030/1299] D_loss: -0.0758, G_loss: 0.4144\n",
      "  Batch [1040/1299] D_loss: -0.1573, G_loss: 0.4666\n",
      "  Batch [1050/1299] D_loss: -0.1256, G_loss: 0.3014\n",
      "  Batch [1060/1299] D_loss: -0.2817, G_loss: -0.6221\n",
      "  Batch [1070/1299] D_loss: -0.1200, G_loss: 0.1916\n",
      "  Batch [1080/1299] D_loss: -0.0244, G_loss: 0.2589\n",
      "  Batch [1090/1299] D_loss: -0.0539, G_loss: 0.3033\n",
      "  Batch [1100/1299] D_loss: -0.0512, G_loss: 0.3353\n",
      "  Batch [1110/1299] D_loss: -0.0836, G_loss: 0.2513\n",
      "  Batch [1120/1299] D_loss: -0.0225, G_loss: 0.0809\n",
      "  Batch [1130/1299] D_loss: 0.0089, G_loss: 0.1258\n",
      "  Batch [1140/1299] D_loss: 0.0040, G_loss: 0.1922\n",
      "  Batch [1150/1299] D_loss: -0.0587, G_loss: 0.2099\n",
      "  Batch [1160/1299] D_loss: -0.0621, G_loss: 0.1934\n",
      "  Batch [1170/1299] D_loss: 0.0243, G_loss: -1.3281\n",
      "  Batch [1180/1299] D_loss: -0.1075, G_loss: -0.0128\n",
      "  Batch [1190/1299] D_loss: -0.1059, G_loss: 0.0462\n",
      "  Batch [1200/1299] D_loss: -0.0596, G_loss: 0.3459\n",
      "  Batch [1210/1299] D_loss: -0.1247, G_loss: 0.4084\n",
      "  Batch [1220/1299] D_loss: -0.0466, G_loss: 0.4756\n",
      "  Batch [1230/1299] D_loss: -0.1004, G_loss: 0.4903\n",
      "  Batch [1240/1299] D_loss: -0.0295, G_loss: 0.2838\n",
      "  Batch [1250/1299] D_loss: -0.0766, G_loss: 0.2345\n",
      "  Batch [1260/1299] D_loss: -0.0286, G_loss: -0.0105\n",
      "  Batch [1270/1299] D_loss: 0.0116, G_loss: 0.0573\n",
      "  Batch [1280/1299] D_loss: -0.0400, G_loss: 0.0846\n",
      "  Batch [1290/1299] D_loss: -1.8696, G_loss: -1.8775\n",
      "\n",
      "Epoch 10 Summary:\n",
      "  Average D_loss: -0.1412\n",
      "  Average G_loss: -0.0431\n",
      "\n",
      "Epoch [11/100]\n",
      "  Batch [0/1299] D_loss: 0.0220, G_loss: 0.1445\n",
      "  Batch [10/1299] D_loss: -0.1622, G_loss: 0.4090\n",
      "  Batch [20/1299] D_loss: -0.0941, G_loss: 0.4022\n",
      "  Batch [30/1299] D_loss: -0.1149, G_loss: 0.5187\n",
      "  Batch [40/1299] D_loss: 0.0203, G_loss: 0.2744\n",
      "  Batch [50/1299] D_loss: -0.0309, G_loss: 0.1759\n",
      "  Batch [60/1299] D_loss: -0.3282, G_loss: -0.6169\n",
      "  Batch [70/1299] D_loss: -0.0471, G_loss: 0.1878\n",
      "  Batch [80/1299] D_loss: -0.0234, G_loss: 0.3652\n",
      "  Batch [90/1299] D_loss: -0.2214, G_loss: 0.5388\n",
      "  Batch [100/1299] D_loss: -0.1795, G_loss: 0.5787\n",
      "  Batch [110/1299] D_loss: -0.0982, G_loss: 0.4830\n",
      "  Batch [120/1299] D_loss: -0.0528, G_loss: 0.1722\n",
      "  Batch [130/1299] D_loss: -0.6943, G_loss: -0.1033\n",
      "  Batch [140/1299] D_loss: -1.0850, G_loss: -0.5406\n",
      "  Batch [150/1299] D_loss: -0.9215, G_loss: -0.0333\n",
      "  Batch [160/1299] D_loss: -0.9361, G_loss: -0.6403\n",
      "  Batch [170/1299] D_loss: -0.0359, G_loss: 0.1615\n",
      "  Batch [180/1299] D_loss: -0.1641, G_loss: 0.2553\n",
      "  Batch [190/1299] D_loss: -0.0375, G_loss: 0.3448\n",
      "  Batch [200/1299] D_loss: -0.2009, G_loss: 0.6768\n",
      "  Batch [210/1299] D_loss: -0.1197, G_loss: 0.4889\n",
      "  Batch [220/1299] D_loss: -0.0169, G_loss: 0.4603\n",
      "  Batch [230/1299] D_loss: -0.3953, G_loss: -0.1920\n",
      "  Batch [240/1299] D_loss: -0.2969, G_loss: -0.1660\n",
      "  Batch [250/1299] D_loss: -1.4006, G_loss: -1.2638\n",
      "  Batch [260/1299] D_loss: -0.1873, G_loss: 0.1319\n",
      "  Batch [270/1299] D_loss: -0.3191, G_loss: 0.2039\n",
      "  Batch [280/1299] D_loss: -0.0994, G_loss: 0.3942\n",
      "  Batch [290/1299] D_loss: -0.0937, G_loss: 0.5339\n",
      "  Batch [300/1299] D_loss: -0.0821, G_loss: 0.6879\n",
      "  Batch [310/1299] D_loss: -0.0830, G_loss: 0.5677\n",
      "  Batch [320/1299] D_loss: -0.1918, G_loss: 0.5233\n",
      "  Batch [330/1299] D_loss: 0.0369, G_loss: 0.2347\n",
      "  Batch [340/1299] D_loss: -0.5923, G_loss: -0.9506\n",
      "  Batch [350/1299] D_loss: -0.4895, G_loss: -0.6451\n",
      "  Batch [360/1299] D_loss: -0.7849, G_loss: -0.1922\n",
      "  Batch [370/1299] D_loss: 0.0081, G_loss: 0.2631\n",
      "  Batch [380/1299] D_loss: -0.0493, G_loss: 0.3306\n",
      "  Batch [390/1299] D_loss: -0.0480, G_loss: 0.3531\n",
      "  Batch [400/1299] D_loss: -0.6904, G_loss: 0.0420\n",
      "  Batch [410/1299] D_loss: -0.7115, G_loss: 0.1383\n",
      "  Batch [420/1299] D_loss: -0.3890, G_loss: -0.0213\n",
      "  Batch [430/1299] D_loss: -0.0370, G_loss: 0.1151\n",
      "  Batch [440/1299] D_loss: -0.1604, G_loss: 0.6181\n",
      "  Batch [450/1299] D_loss: -0.0991, G_loss: 0.4896\n",
      "  Batch [460/1299] D_loss: -0.0547, G_loss: 0.4831\n",
      "  Batch [470/1299] D_loss: -0.0313, G_loss: 0.5051\n",
      "  Batch [480/1299] D_loss: -0.0039, G_loss: 0.3943\n",
      "  Batch [490/1299] D_loss: -0.1221, G_loss: 0.2397\n",
      "  Batch [500/1299] D_loss: 0.0128, G_loss: 0.0401\n",
      "  Batch [510/1299] D_loss: -0.2906, G_loss: -1.2569\n",
      "  Batch [520/1299] D_loss: -0.2034, G_loss: -0.4181\n",
      "  Batch [530/1299] D_loss: -0.2256, G_loss: 0.1332\n",
      "  Batch [540/1299] D_loss: -0.1840, G_loss: 0.1287\n",
      "  Batch [550/1299] D_loss: -0.0175, G_loss: 0.1969\n",
      "  Batch [560/1299] D_loss: -0.0182, G_loss: 0.2833\n",
      "  Batch [570/1299] D_loss: 0.0360, G_loss: 0.5528\n",
      "  Batch [580/1299] D_loss: -0.1303, G_loss: 0.3789\n",
      "  Batch [590/1299] D_loss: -0.0792, G_loss: 0.2795\n",
      "  Batch [600/1299] D_loss: 0.0318, G_loss: 0.2487\n",
      "  Batch [610/1299] D_loss: -0.2146, G_loss: -0.2054\n",
      "  Batch [620/1299] D_loss: -0.0456, G_loss: 0.1249\n",
      "  Batch [630/1299] D_loss: -0.0774, G_loss: 0.3216\n",
      "  Batch [640/1299] D_loss: -0.0349, G_loss: 0.3132\n",
      "  Batch [650/1299] D_loss: 0.0038, G_loss: 0.2847\n",
      "  Batch [660/1299] D_loss: -2.7756, G_loss: -7.5348\n",
      "  Batch [670/1299] D_loss: -0.1955, G_loss: 0.0126\n",
      "  Batch [680/1299] D_loss: -0.0406, G_loss: 0.3281\n",
      "  Batch [690/1299] D_loss: -0.0571, G_loss: 0.4658\n",
      "  Batch [700/1299] D_loss: -0.2723, G_loss: 0.6201\n",
      "  Batch [710/1299] D_loss: -0.2283, G_loss: 0.5979\n",
      "  Batch [720/1299] D_loss: -0.0555, G_loss: 0.3033\n",
      "  Batch [730/1299] D_loss: 0.0509, G_loss: 0.2217\n",
      "  Batch [740/1299] D_loss: -0.6229, G_loss: -1.1019\n",
      "  Batch [750/1299] D_loss: -0.2355, G_loss: -0.2544\n",
      "  Batch [760/1299] D_loss: -0.5178, G_loss: 0.0980\n",
      "  Batch [770/1299] D_loss: -0.1535, G_loss: 0.0635\n",
      "  Batch [780/1299] D_loss: 0.0464, G_loss: 0.2523\n",
      "  Batch [790/1299] D_loss: -0.2061, G_loss: 0.4130\n",
      "  Batch [800/1299] D_loss: 0.0118, G_loss: 0.4777\n",
      "  Batch [810/1299] D_loss: -0.0895, G_loss: 0.4018\n",
      "  Batch [820/1299] D_loss: -0.2802, G_loss: 0.3756\n",
      "  Batch [830/1299] D_loss: -0.0473, G_loss: 0.1461\n",
      "  Batch [840/1299] D_loss: -0.0459, G_loss: 0.1324\n",
      "  Batch [850/1299] D_loss: -0.4275, G_loss: -0.0136\n",
      "  Batch [860/1299] D_loss: -1.6356, G_loss: -0.4444\n",
      "  Batch [870/1299] D_loss: -0.1600, G_loss: 0.2537\n",
      "  Batch [880/1299] D_loss: -0.0208, G_loss: 0.2678\n",
      "  Batch [890/1299] D_loss: 0.0430, G_loss: 0.2297\n",
      "  Batch [900/1299] D_loss: -0.2157, G_loss: 0.2441\n",
      "  Batch [910/1299] D_loss: -0.1906, G_loss: 0.1934\n",
      "  Batch [920/1299] D_loss: 0.0400, G_loss: 0.1673\n",
      "  Batch [930/1299] D_loss: 0.0202, G_loss: 0.3110\n",
      "  Batch [940/1299] D_loss: 0.0362, G_loss: 0.2740\n",
      "  Batch [950/1299] D_loss: -0.0663, G_loss: 0.3383\n",
      "  Batch [960/1299] D_loss: -0.4176, G_loss: -0.1124\n",
      "  Batch [970/1299] D_loss: -0.3528, G_loss: -0.4167\n",
      "  Batch [980/1299] D_loss: -1.4280, G_loss: -0.1137\n",
      "  Batch [990/1299] D_loss: -0.5117, G_loss: -0.3180\n",
      "  Batch [1000/1299] D_loss: -0.2540, G_loss: 0.0844\n",
      "  Batch [1010/1299] D_loss: -1.9218, G_loss: -3.6370\n",
      "  Batch [1020/1299] D_loss: -0.0240, G_loss: 0.2276\n",
      "  Batch [1030/1299] D_loss: -0.2234, G_loss: 0.5130\n",
      "  Batch [1040/1299] D_loss: -0.3928, G_loss: 0.7378\n",
      "  Batch [1050/1299] D_loss: 0.0152, G_loss: 0.7141\n",
      "  Batch [1060/1299] D_loss: -0.0624, G_loss: 0.5662\n",
      "  Batch [1070/1299] D_loss: -0.1041, G_loss: 0.4609\n",
      "  Batch [1080/1299] D_loss: 0.0487, G_loss: 0.2410\n",
      "  Batch [1090/1299] D_loss: -0.1656, G_loss: 0.0813\n",
      "  Batch [1100/1299] D_loss: -0.9356, G_loss: -1.9894\n",
      "  Batch [1110/1299] D_loss: -0.0011, G_loss: 0.0762\n",
      "  Batch [1120/1299] D_loss: -0.2944, G_loss: 0.0324\n",
      "  Batch [1130/1299] D_loss: -0.0213, G_loss: 0.2788\n",
      "  Batch [1140/1299] D_loss: -0.1095, G_loss: 0.3755\n",
      "  Batch [1150/1299] D_loss: -0.0780, G_loss: 0.3522\n",
      "  Batch [1160/1299] D_loss: -0.8511, G_loss: -3.1028\n",
      "  Batch [1170/1299] D_loss: -0.1927, G_loss: 0.1483\n",
      "  Batch [1180/1299] D_loss: -0.0406, G_loss: 0.2204\n",
      "  Batch [1190/1299] D_loss: -0.0560, G_loss: 0.4809\n",
      "  Batch [1200/1299] D_loss: -0.0871, G_loss: 0.5561\n",
      "  Batch [1210/1299] D_loss: -0.1869, G_loss: 0.7985\n",
      "  Batch [1220/1299] D_loss: -0.1162, G_loss: 0.5237\n",
      "  Batch [1230/1299] D_loss: 0.0857, G_loss: 0.4575\n",
      "  Batch [1240/1299] D_loss: -0.0607, G_loss: 0.2699\n",
      "  Batch [1250/1299] D_loss: -0.8102, G_loss: -1.9205\n",
      "  Batch [1260/1299] D_loss: -0.0064, G_loss: 0.0406\n",
      "  Batch [1270/1299] D_loss: -0.0828, G_loss: -0.0622\n",
      "  Batch [1280/1299] D_loss: -0.7259, G_loss: -0.6154\n",
      "  Batch [1290/1299] D_loss: -0.3046, G_loss: 0.0221\n",
      "\n",
      "Epoch 11 Summary:\n",
      "  Average D_loss: -0.1575\n",
      "  Average G_loss: -0.0744\n",
      "\n",
      "Epoch [12/100]\n",
      "  Batch [0/1299] D_loss: -1.2668, G_loss: -0.3814\n",
      "  Batch [10/1299] D_loss: -0.0887, G_loss: 0.1419\n",
      "  Batch [20/1299] D_loss: -0.0257, G_loss: 0.1943\n",
      "  Batch [30/1299] D_loss: -0.2235, G_loss: 0.3464\n",
      "  Batch [40/1299] D_loss: -0.1514, G_loss: 0.4823\n",
      "  Batch [50/1299] D_loss: -0.0441, G_loss: 0.4228\n",
      "  Batch [60/1299] D_loss: -0.0533, G_loss: 0.2833\n",
      "  Batch [70/1299] D_loss: 0.0117, G_loss: 0.0471\n",
      "  Batch [80/1299] D_loss: -0.0760, G_loss: 0.0992\n",
      "  Batch [90/1299] D_loss: -0.0297, G_loss: 0.1981\n",
      "  Batch [100/1299] D_loss: -0.1373, G_loss: 0.3326\n",
      "  Batch [110/1299] D_loss: -0.0505, G_loss: 0.5107\n",
      "  Batch [120/1299] D_loss: -0.0867, G_loss: 0.3474\n",
      "  Batch [130/1299] D_loss: -0.1915, G_loss: 0.3098\n",
      "  Batch [140/1299] D_loss: -2.2102, G_loss: -4.1979\n",
      "  Batch [150/1299] D_loss: -0.4722, G_loss: -1.1609\n",
      "  Batch [160/1299] D_loss: -0.4345, G_loss: -0.0609\n",
      "  Batch [170/1299] D_loss: -0.0508, G_loss: 0.1724\n",
      "  Batch [180/1299] D_loss: -0.3884, G_loss: -0.4724\n",
      "  Batch [190/1299] D_loss: -0.0756, G_loss: 0.1228\n",
      "  Batch [200/1299] D_loss: -0.0914, G_loss: 0.3190\n",
      "  Batch [210/1299] D_loss: -0.1852, G_loss: 0.4851\n",
      "  Batch [220/1299] D_loss: -0.1089, G_loss: 0.5858\n",
      "  Batch [230/1299] D_loss: 0.0085, G_loss: 0.3869\n",
      "  Batch [240/1299] D_loss: 0.0182, G_loss: 0.2665\n",
      "  Batch [250/1299] D_loss: -0.0351, G_loss: 0.3588\n",
      "  Batch [260/1299] D_loss: -0.3540, G_loss: -0.3435\n",
      "  Batch [270/1299] D_loss: -0.4218, G_loss: -0.1300\n",
      "  Batch [280/1299] D_loss: -0.1459, G_loss: 0.1303\n",
      "  Batch [290/1299] D_loss: -0.1946, G_loss: 0.1973\n",
      "  Batch [300/1299] D_loss: -0.1647, G_loss: 0.4284\n",
      "  Batch [310/1299] D_loss: -0.0425, G_loss: 0.6202\n",
      "  Batch [320/1299] D_loss: -0.0766, G_loss: 0.4642\n",
      "  Batch [330/1299] D_loss: -0.0566, G_loss: 0.5193\n",
      "  Batch [340/1299] D_loss: 0.0112, G_loss: 0.2743\n",
      "  Batch [350/1299] D_loss: -1.9116, G_loss: -1.3944\n",
      "  Batch [360/1299] D_loss: -0.3050, G_loss: -0.2255\n",
      "  Batch [370/1299] D_loss: -0.2914, G_loss: -0.0440\n",
      "  Batch [380/1299] D_loss: -0.0108, G_loss: 0.3137\n",
      "  Batch [390/1299] D_loss: -0.1247, G_loss: 0.4526\n",
      "  Batch [400/1299] D_loss: -0.1889, G_loss: 0.4549\n",
      "  Batch [410/1299] D_loss: 0.0826, G_loss: 0.3559\n",
      "  Batch [420/1299] D_loss: -0.0221, G_loss: 0.1768\n",
      "  Batch [430/1299] D_loss: -1.1533, G_loss: -3.2329\n",
      "  Batch [440/1299] D_loss: -0.0920, G_loss: 0.0711\n",
      "  Batch [450/1299] D_loss: -0.5820, G_loss: -0.2835\n",
      "  Batch [460/1299] D_loss: -0.0311, G_loss: 0.2779\n",
      "  Batch [470/1299] D_loss: -0.5493, G_loss: 0.0722\n",
      "  Batch [480/1299] D_loss: -0.0589, G_loss: 0.1259\n",
      "  Batch [490/1299] D_loss: 0.0250, G_loss: 0.3346\n",
      "  Batch [500/1299] D_loss: -0.0880, G_loss: 0.5619\n",
      "  Batch [510/1299] D_loss: 0.0157, G_loss: 0.6072\n",
      "  Batch [520/1299] D_loss: 0.0721, G_loss: 0.5107\n",
      "  Batch [530/1299] D_loss: -0.1145, G_loss: 0.3546\n",
      "  Batch [540/1299] D_loss: -2.9871, G_loss: -2.7347\n",
      "  Batch [550/1299] D_loss: -0.1074, G_loss: 0.1491\n",
      "  Batch [560/1299] D_loss: -0.0129, G_loss: 0.1971\n",
      "  Batch [570/1299] D_loss: -0.0338, G_loss: 0.3004\n",
      "  Batch [580/1299] D_loss: -0.0806, G_loss: 0.3246\n",
      "  Batch [590/1299] D_loss: -0.3101, G_loss: -0.2814\n",
      "  Batch [600/1299] D_loss: -0.5078, G_loss: 0.0587\n",
      "  Batch [610/1299] D_loss: -0.0512, G_loss: 0.3371\n",
      "  Batch [620/1299] D_loss: -0.2418, G_loss: 0.7350\n",
      "  Batch [630/1299] D_loss: -0.2295, G_loss: 0.6992\n",
      "  Batch [640/1299] D_loss: -0.0146, G_loss: 0.5631\n",
      "  Batch [650/1299] D_loss: -0.0593, G_loss: 0.2534\n",
      "  Batch [660/1299] D_loss: -0.0009, G_loss: 0.1887\n",
      "  Batch [670/1299] D_loss: -2.1065, G_loss: -1.2111\n",
      "  Batch [680/1299] D_loss: -0.1566, G_loss: 0.1027\n",
      "  Batch [690/1299] D_loss: -0.6108, G_loss: -0.0925\n",
      "  Batch [700/1299] D_loss: -0.1823, G_loss: -0.1030\n",
      "  Batch [710/1299] D_loss: -0.0366, G_loss: 0.2965\n",
      "  Batch [720/1299] D_loss: -0.1431, G_loss: 0.5339\n",
      "  Batch [730/1299] D_loss: -0.1108, G_loss: 0.6486\n",
      "  Batch [740/1299] D_loss: -0.0940, G_loss: 0.5387\n",
      "  Batch [750/1299] D_loss: 0.0864, G_loss: 0.3865\n",
      "  Batch [760/1299] D_loss: -0.2446, G_loss: -2.1864\n",
      "  Batch [770/1299] D_loss: -0.6872, G_loss: -1.6115\n",
      "  Batch [780/1299] D_loss: -0.1504, G_loss: 0.3021\n",
      "  Batch [790/1299] D_loss: -0.0487, G_loss: 0.4724\n",
      "  Batch [800/1299] D_loss: -0.0394, G_loss: 0.5027\n",
      "  Batch [810/1299] D_loss: -0.2042, G_loss: 0.5568\n",
      "  Batch [820/1299] D_loss: -0.0763, G_loss: 0.4895\n",
      "  Batch [830/1299] D_loss: -0.0004, G_loss: 0.3281\n",
      "  Batch [840/1299] D_loss: -0.0324, G_loss: 0.0446\n",
      "  Batch [850/1299] D_loss: -0.0684, G_loss: 0.0177\n",
      "  Batch [860/1299] D_loss: -1.2928, G_loss: -2.9886\n",
      "  Batch [870/1299] D_loss: -0.0780, G_loss: 0.2064\n",
      "  Batch [880/1299] D_loss: -0.0635, G_loss: 0.3236\n",
      "  Batch [890/1299] D_loss: -0.5686, G_loss: 0.5681\n",
      "  Batch [900/1299] D_loss: -0.1632, G_loss: 0.5719\n",
      "  Batch [910/1299] D_loss: -0.0183, G_loss: 0.5645\n",
      "  Batch [920/1299] D_loss: -0.0820, G_loss: 0.2593\n",
      "  Batch [930/1299] D_loss: -0.1339, G_loss: 0.0236\n",
      "  Batch [940/1299] D_loss: -0.0445, G_loss: 0.0790\n",
      "  Batch [950/1299] D_loss: -0.8251, G_loss: -0.2296\n",
      "  Batch [960/1299] D_loss: -0.0882, G_loss: 0.2586\n",
      "  Batch [970/1299] D_loss: -0.0797, G_loss: 0.4250\n",
      "  Batch [980/1299] D_loss: -0.0368, G_loss: 0.6619\n",
      "  Batch [990/1299] D_loss: -0.0164, G_loss: 0.4427\n",
      "  Batch [1000/1299] D_loss: -0.0520, G_loss: 0.3154\n",
      "  Batch [1010/1299] D_loss: -0.2923, G_loss: -0.8337\n",
      "  Batch [1020/1299] D_loss: -1.9873, G_loss: -3.5282\n",
      "  Batch [1030/1299] D_loss: -1.1740, G_loss: -1.1149\n",
      "  Batch [1040/1299] D_loss: -0.0898, G_loss: 0.1359\n",
      "  Batch [1050/1299] D_loss: -0.0186, G_loss: 0.2912\n",
      "  Batch [1060/1299] D_loss: -0.0603, G_loss: 0.4019\n",
      "  Batch [1070/1299] D_loss: -0.0850, G_loss: 0.3614\n",
      "  Batch [1080/1299] D_loss: -0.0685, G_loss: 0.3515\n",
      "  Batch [1090/1299] D_loss: -0.1414, G_loss: 0.2886\n",
      "  Batch [1100/1299] D_loss: -0.4605, G_loss: -0.0176\n",
      "  Batch [1110/1299] D_loss: -0.3386, G_loss: -0.3293\n",
      "  Batch [1120/1299] D_loss: -0.0102, G_loss: 0.1377\n",
      "  Batch [1130/1299] D_loss: -0.2072, G_loss: -0.1952\n",
      "  Batch [1140/1299] D_loss: -0.0809, G_loss: 0.2254\n",
      "  Batch [1150/1299] D_loss: 0.0243, G_loss: 0.1519\n",
      "  Batch [1160/1299] D_loss: -0.0203, G_loss: 0.1793\n",
      "  Batch [1170/1299] D_loss: -2.3206, G_loss: -3.0510\n",
      "  Batch [1180/1299] D_loss: -0.0088, G_loss: 0.1118\n",
      "  Batch [1190/1299] D_loss: -0.0149, G_loss: 0.2236\n",
      "  Batch [1200/1299] D_loss: -0.1901, G_loss: 0.4243\n",
      "  Batch [1210/1299] D_loss: -0.1073, G_loss: 0.4441\n",
      "  Batch [1220/1299] D_loss: -0.1509, G_loss: 0.3840\n",
      "  Batch [1230/1299] D_loss: -0.0520, G_loss: 0.2204\n",
      "  Batch [1240/1299] D_loss: -0.0269, G_loss: 0.2684\n",
      "  Batch [1250/1299] D_loss: -0.3004, G_loss: -0.0929\n",
      "  Batch [1260/1299] D_loss: -0.3174, G_loss: -0.5405\n",
      "  Batch [1270/1299] D_loss: -0.6588, G_loss: -0.8102\n",
      "  Batch [1280/1299] D_loss: -0.1353, G_loss: 0.0107\n",
      "  Batch [1290/1299] D_loss: -0.1425, G_loss: 0.2977\n",
      "\n",
      "Epoch 12 Summary:\n",
      "  Average D_loss: -0.1335\n",
      "  Average G_loss: -0.0503\n",
      "\n",
      "Epoch [13/100]\n",
      "  Batch [0/1299] D_loss: -0.1228, G_loss: 0.4750\n",
      "  Batch [10/1299] D_loss: -0.2729, G_loss: 0.3979\n",
      "  Batch [20/1299] D_loss: 0.0383, G_loss: 0.2788\n",
      "  Batch [30/1299] D_loss: -0.0872, G_loss: 0.2731\n",
      "  Batch [40/1299] D_loss: -0.0145, G_loss: 0.1226\n",
      "  Batch [50/1299] D_loss: -0.0384, G_loss: 0.2664\n",
      "  Batch [60/1299] D_loss: -0.1138, G_loss: 0.4152\n",
      "  Batch [70/1299] D_loss: -0.0138, G_loss: 0.4505\n",
      "  Batch [80/1299] D_loss: -0.0463, G_loss: 0.3011\n",
      "  Batch [90/1299] D_loss: -0.1966, G_loss: 0.0935\n",
      "  Batch [100/1299] D_loss: -0.0041, G_loss: 0.0999\n",
      "  Batch [110/1299] D_loss: -0.0910, G_loss: 0.1306\n",
      "  Batch [120/1299] D_loss: -0.2472, G_loss: -0.8859\n",
      "  Batch [130/1299] D_loss: -0.0943, G_loss: 0.0642\n",
      "  Batch [140/1299] D_loss: -0.1801, G_loss: 0.0529\n",
      "  Batch [150/1299] D_loss: -0.1927, G_loss: 0.0408\n",
      "  Batch [160/1299] D_loss: -0.0391, G_loss: 0.1739\n",
      "  Batch [170/1299] D_loss: -1.2148, G_loss: -1.1061\n",
      "  Batch [180/1299] D_loss: -0.0527, G_loss: 0.2501\n",
      "  Batch [190/1299] D_loss: -0.3379, G_loss: 0.5966\n",
      "  Batch [200/1299] D_loss: -0.1761, G_loss: 0.5189\n",
      "  Batch [210/1299] D_loss: 0.0995, G_loss: 0.6092\n",
      "  Batch [220/1299] D_loss: -0.0002, G_loss: 0.3889\n",
      "  Batch [230/1299] D_loss: -0.0561, G_loss: 0.2428\n",
      "  Batch [240/1299] D_loss: -0.7515, G_loss: -0.4004\n",
      "  Batch [250/1299] D_loss: -0.0292, G_loss: 0.2582\n",
      "  Batch [260/1299] D_loss: -0.1699, G_loss: 0.6415\n",
      "  Batch [270/1299] D_loss: 0.0650, G_loss: 0.6058\n",
      "  Batch [280/1299] D_loss: -0.0693, G_loss: 0.5230\n",
      "  Batch [290/1299] D_loss: -0.0870, G_loss: 0.5410\n",
      "  Batch [300/1299] D_loss: -1.7448, G_loss: -3.1048\n",
      "  Batch [310/1299] D_loss: -0.1560, G_loss: -0.8512\n",
      "  Batch [320/1299] D_loss: -0.0318, G_loss: 0.1340\n",
      "  Batch [330/1299] D_loss: -1.5585, G_loss: -0.0989\n",
      "  Batch [340/1299] D_loss: -0.0268, G_loss: 0.1659\n",
      "  Batch [350/1299] D_loss: -0.0366, G_loss: 0.2857\n",
      "  Batch [360/1299] D_loss: 0.0970, G_loss: 0.3599\n",
      "  Batch [370/1299] D_loss: -0.1059, G_loss: 0.5478\n",
      "  Batch [380/1299] D_loss: 0.0012, G_loss: 0.4872\n",
      "  Batch [390/1299] D_loss: -0.1191, G_loss: 0.3486\n",
      "  Batch [400/1299] D_loss: -1.7340, G_loss: -1.7211\n",
      "  Batch [410/1299] D_loss: -0.0141, G_loss: 0.1214\n",
      "  Batch [420/1299] D_loss: -0.0660, G_loss: 0.1155\n",
      "  Batch [430/1299] D_loss: -0.0887, G_loss: 0.2260\n",
      "  Batch [440/1299] D_loss: -0.9621, G_loss: -3.2901\n",
      "  Batch [450/1299] D_loss: -0.0038, G_loss: 0.1845\n",
      "  Batch [460/1299] D_loss: -0.1727, G_loss: 0.5393\n",
      "  Batch [470/1299] D_loss: 0.0295, G_loss: 0.3853\n",
      "  Batch [480/1299] D_loss: 0.0123, G_loss: 0.4826\n",
      "  Batch [490/1299] D_loss: -0.0554, G_loss: 0.2221\n",
      "  Batch [500/1299] D_loss: -0.4141, G_loss: -0.9370\n",
      "  Batch [510/1299] D_loss: -0.0300, G_loss: 0.2023\n",
      "  Batch [520/1299] D_loss: -0.1486, G_loss: 0.3108\n",
      "  Batch [530/1299] D_loss: -0.0840, G_loss: 0.3949\n",
      "  Batch [540/1299] D_loss: -0.1325, G_loss: 0.4002\n",
      "  Batch [550/1299] D_loss: -0.0127, G_loss: 0.2095\n",
      "  Batch [560/1299] D_loss: -0.1850, G_loss: 0.3554\n",
      "  Batch [570/1299] D_loss: 0.0180, G_loss: 0.0479\n",
      "  Batch [580/1299] D_loss: -0.0101, G_loss: 0.0510\n",
      "  Batch [590/1299] D_loss: -0.0109, G_loss: 0.0691\n",
      "  Batch [600/1299] D_loss: -0.0211, G_loss: 0.0838\n",
      "  Batch [610/1299] D_loss: -0.0377, G_loss: 0.1196\n",
      "  Batch [620/1299] D_loss: -1.0943, G_loss: -0.9930\n",
      "  Batch [630/1299] D_loss: -0.0453, G_loss: 0.1689\n",
      "  Batch [640/1299] D_loss: -0.1669, G_loss: 0.4444\n",
      "  Batch [650/1299] D_loss: -0.0648, G_loss: 0.4447\n",
      "  Batch [660/1299] D_loss: -0.0804, G_loss: 0.2215\n",
      "  Batch [670/1299] D_loss: -0.8954, G_loss: -2.0642\n",
      "  Batch [680/1299] D_loss: -0.2063, G_loss: -0.1217\n",
      "  Batch [690/1299] D_loss: -0.0595, G_loss: 0.1017\n",
      "  Batch [700/1299] D_loss: -0.0799, G_loss: 0.2802\n",
      "  Batch [710/1299] D_loss: -0.1058, G_loss: 0.5784\n",
      "  Batch [720/1299] D_loss: -0.1666, G_loss: 0.5546\n",
      "  Batch [730/1299] D_loss: -0.1752, G_loss: 0.5649\n",
      "  Batch [740/1299] D_loss: -0.0363, G_loss: 0.2661\n",
      "  Batch [750/1299] D_loss: -1.2837, G_loss: -4.6931\n",
      "  Batch [760/1299] D_loss: -1.1085, G_loss: -1.3673\n",
      "  Batch [770/1299] D_loss: -0.2694, G_loss: -0.7955\n",
      "  Batch [780/1299] D_loss: -0.3419, G_loss: -0.1793\n",
      "  Batch [790/1299] D_loss: -0.1047, G_loss: 0.2398\n",
      "  Batch [800/1299] D_loss: -0.1178, G_loss: 0.1218\n",
      "  Batch [810/1299] D_loss: -0.0488, G_loss: 0.3351\n",
      "  Batch [820/1299] D_loss: -0.0743, G_loss: 0.3888\n",
      "  Batch [830/1299] D_loss: -0.0592, G_loss: 0.5084\n",
      "  Batch [840/1299] D_loss: -0.0799, G_loss: 0.3718\n",
      "  Batch [850/1299] D_loss: -4.5941, G_loss: -7.4585\n",
      "  Batch [860/1299] D_loss: -0.0345, G_loss: 0.1398\n",
      "  Batch [870/1299] D_loss: -0.1102, G_loss: 0.2458\n",
      "  Batch [880/1299] D_loss: -0.0401, G_loss: 0.3499\n",
      "  Batch [890/1299] D_loss: 0.0127, G_loss: 0.2439\n",
      "  Batch [900/1299] D_loss: -0.1095, G_loss: 0.0324\n",
      "  Batch [910/1299] D_loss: -0.0184, G_loss: 0.0504\n",
      "  Batch [920/1299] D_loss: -0.5166, G_loss: -0.5156\n",
      "  Batch [930/1299] D_loss: -0.0363, G_loss: 0.1516\n",
      "  Batch [940/1299] D_loss: -0.0900, G_loss: 0.3651\n",
      "  Batch [950/1299] D_loss: -0.3168, G_loss: 0.3757\n",
      "  Batch [960/1299] D_loss: -0.0726, G_loss: 0.4042\n",
      "  Batch [970/1299] D_loss: 0.2193, G_loss: -0.6643\n",
      "  Batch [980/1299] D_loss: -0.2715, G_loss: -0.1609\n",
      "  Batch [990/1299] D_loss: -0.0696, G_loss: 0.1535\n",
      "  Batch [1000/1299] D_loss: -0.1573, G_loss: 0.2817\n",
      "  Batch [1010/1299] D_loss: -0.1583, G_loss: 0.4863\n",
      "  Batch [1020/1299] D_loss: 0.0170, G_loss: 0.5426\n",
      "  Batch [1030/1299] D_loss: -0.0499, G_loss: 0.3820\n",
      "  Batch [1040/1299] D_loss: -0.4766, G_loss: -1.6207\n",
      "  Batch [1050/1299] D_loss: -0.0046, G_loss: 0.1827\n",
      "  Batch [1060/1299] D_loss: -0.0442, G_loss: 0.3592\n",
      "  Batch [1070/1299] D_loss: -0.1029, G_loss: 0.4842\n",
      "  Batch [1080/1299] D_loss: 0.0299, G_loss: 0.4048\n",
      "  Batch [1090/1299] D_loss: 0.0657, G_loss: 0.4910\n",
      "  Batch [1100/1299] D_loss: 0.0457, G_loss: 0.2377\n",
      "  Batch [1110/1299] D_loss: -0.7584, G_loss: -0.4440\n",
      "  Batch [1120/1299] D_loss: -0.4598, G_loss: -1.6848\n",
      "  Batch [1130/1299] D_loss: -0.0851, G_loss: 0.4282\n",
      "  Batch [1140/1299] D_loss: -0.0061, G_loss: 0.5640\n",
      "  Batch [1150/1299] D_loss: -0.0764, G_loss: 0.4737\n",
      "  Batch [1160/1299] D_loss: -0.1087, G_loss: 0.9796\n",
      "  Batch [1170/1299] D_loss: -0.1126, G_loss: 0.3305\n",
      "  Batch [1180/1299] D_loss: -0.1186, G_loss: 0.2414\n",
      "  Batch [1190/1299] D_loss: -1.7978, G_loss: -4.1584\n",
      "  Batch [1200/1299] D_loss: -1.5099, G_loss: -2.2753\n",
      "  Batch [1210/1299] D_loss: -0.1921, G_loss: 0.1132\n",
      "  Batch [1220/1299] D_loss: -0.1618, G_loss: 0.1017\n",
      "  Batch [1230/1299] D_loss: -0.9051, G_loss: -0.5093\n",
      "  Batch [1240/1299] D_loss: -0.3247, G_loss: -0.8517\n",
      "  Batch [1250/1299] D_loss: -0.8932, G_loss: -2.6742\n",
      "  Batch [1260/1299] D_loss: -0.4753, G_loss: -0.2970\n",
      "  Batch [1270/1299] D_loss: -0.0852, G_loss: 0.3509\n",
      "  Batch [1280/1299] D_loss: -0.1426, G_loss: 0.4991\n",
      "  Batch [1290/1299] D_loss: -0.2232, G_loss: 0.6316\n",
      "\n",
      "Epoch 13 Summary:\n",
      "  Average D_loss: -0.1204\n",
      "  Average G_loss: -0.0617\n",
      "\n",
      "Epoch [14/100]\n",
      "  Batch [0/1299] D_loss: 0.0096, G_loss: 0.6541\n",
      "  Batch [10/1299] D_loss: -0.2410, G_loss: 0.5364\n",
      "  Batch [20/1299] D_loss: 0.0546, G_loss: 0.2140\n",
      "  Batch [30/1299] D_loss: -0.7417, G_loss: -0.3447\n",
      "  Batch [40/1299] D_loss: -0.3638, G_loss: -0.0248\n",
      "  Batch [50/1299] D_loss: -0.0580, G_loss: 0.1811\n",
      "  Batch [60/1299] D_loss: -0.3149, G_loss: 0.2092\n",
      "  Batch [70/1299] D_loss: -0.3024, G_loss: 0.1776\n",
      "  Batch [80/1299] D_loss: -0.0931, G_loss: 0.3455\n",
      "  Batch [90/1299] D_loss: 0.0251, G_loss: 0.2517\n",
      "  Batch [100/1299] D_loss: -0.6360, G_loss: 0.1048\n",
      "  Batch [110/1299] D_loss: -0.1250, G_loss: 0.3501\n",
      "  Batch [120/1299] D_loss: -0.1103, G_loss: 0.3533\n",
      "  Batch [130/1299] D_loss: -0.0917, G_loss: 0.3659\n",
      "  Batch [140/1299] D_loss: -1.2545, G_loss: -2.0032\n",
      "  Batch [150/1299] D_loss: -0.1701, G_loss: 0.0543\n",
      "  Batch [160/1299] D_loss: -0.2981, G_loss: -0.0021\n",
      "  Batch [170/1299] D_loss: -0.5168, G_loss: -0.4974\n",
      "  Batch [180/1299] D_loss: -0.1999, G_loss: 0.1610\n",
      "  Batch [190/1299] D_loss: -0.2855, G_loss: 0.0593\n",
      "  Batch [200/1299] D_loss: -0.6229, G_loss: -1.2442\n",
      "  Batch [210/1299] D_loss: -0.0688, G_loss: 0.3189\n",
      "  Batch [220/1299] D_loss: -0.1488, G_loss: 0.5252\n",
      "  Batch [230/1299] D_loss: 0.0155, G_loss: 0.7784\n",
      "  Batch [240/1299] D_loss: -0.5213, G_loss: 0.5615\n",
      "  Batch [250/1299] D_loss: -0.0257, G_loss: 0.7848\n",
      "  Batch [260/1299] D_loss: -0.0080, G_loss: 0.3031\n",
      "  Batch [270/1299] D_loss: -3.2898, G_loss: -7.3723\n",
      "  Batch [280/1299] D_loss: -1.0465, G_loss: -0.3243\n",
      "  Batch [290/1299] D_loss: 0.0125, G_loss: 0.2076\n",
      "  Batch [300/1299] D_loss: -0.0205, G_loss: 0.2879\n",
      "  Batch [310/1299] D_loss: 0.0198, G_loss: 0.2999\n",
      "  Batch [320/1299] D_loss: -0.0896, G_loss: 0.4012\n",
      "  Batch [330/1299] D_loss: 0.0040, G_loss: 0.3855\n",
      "  Batch [340/1299] D_loss: -0.0945, G_loss: 0.2501\n",
      "  Batch [350/1299] D_loss: -0.0237, G_loss: -0.0566\n",
      "  Batch [360/1299] D_loss: 0.0432, G_loss: 0.1565\n",
      "  Batch [370/1299] D_loss: 0.0494, G_loss: 0.1365\n",
      "  Batch [380/1299] D_loss: -0.0153, G_loss: 0.1305\n",
      "  Batch [390/1299] D_loss: -0.0731, G_loss: 0.2102\n",
      "  Batch [400/1299] D_loss: -1.0825, G_loss: -1.7381\n",
      "  Batch [410/1299] D_loss: -0.0307, G_loss: 0.1152\n",
      "  Batch [420/1299] D_loss: -0.0361, G_loss: 0.2499\n",
      "  Batch [430/1299] D_loss: -0.0058, G_loss: 0.3911\n",
      "  Batch [440/1299] D_loss: -0.1155, G_loss: 0.4647\n",
      "  Batch [450/1299] D_loss: -0.7570, G_loss: -4.2134\n",
      "  Batch [460/1299] D_loss: -0.3361, G_loss: 0.0126\n",
      "  Batch [470/1299] D_loss: -0.0386, G_loss: 0.0259\n",
      "  Batch [480/1299] D_loss: -0.0307, G_loss: 0.0982\n",
      "  Batch [490/1299] D_loss: -0.0791, G_loss: 0.2784\n",
      "  Batch [500/1299] D_loss: -0.1726, G_loss: 0.4689\n",
      "  Batch [510/1299] D_loss: -0.1422, G_loss: 0.5173\n",
      "  Batch [520/1299] D_loss: 0.0771, G_loss: 0.2881\n",
      "  Batch [530/1299] D_loss: -2.1813, G_loss: -3.2150\n",
      "  Batch [540/1299] D_loss: -0.3502, G_loss: -0.2315\n",
      "  Batch [550/1299] D_loss: -0.0057, G_loss: 0.2197\n",
      "  Batch [560/1299] D_loss: 0.0129, G_loss: 0.3521\n",
      "  Batch [570/1299] D_loss: -0.0845, G_loss: 0.4403\n",
      "  Batch [580/1299] D_loss: -0.0883, G_loss: 0.3648\n",
      "  Batch [590/1299] D_loss: -0.0290, G_loss: 0.3410\n",
      "  Batch [600/1299] D_loss: -0.0922, G_loss: 0.1351\n",
      "  Batch [610/1299] D_loss: -0.2766, G_loss: -0.3383\n",
      "  Batch [620/1299] D_loss: -0.1314, G_loss: 0.1006\n",
      "  Batch [630/1299] D_loss: -0.7047, G_loss: -0.4606\n",
      "  Batch [640/1299] D_loss: -0.1901, G_loss: 0.3706\n",
      "  Batch [650/1299] D_loss: -0.0626, G_loss: 0.6920\n",
      "  Batch [660/1299] D_loss: -0.0938, G_loss: 0.7062\n",
      "  Batch [670/1299] D_loss: 0.0050, G_loss: 0.5583\n",
      "  Batch [680/1299] D_loss: -0.0083, G_loss: 0.3663\n",
      "  Batch [690/1299] D_loss: -0.0977, G_loss: -0.1860\n",
      "  Batch [700/1299] D_loss: -3.9668, G_loss: -2.9986\n",
      "  Batch [710/1299] D_loss: -1.3568, G_loss: -0.7289\n",
      "  Batch [720/1299] D_loss: -1.4687, G_loss: -1.7278\n",
      "  Batch [730/1299] D_loss: -0.1540, G_loss: 0.1286\n",
      "  Batch [740/1299] D_loss: -0.1271, G_loss: 0.3858\n",
      "  Batch [750/1299] D_loss: 0.0198, G_loss: 0.4498\n",
      "  Batch [760/1299] D_loss: -0.1302, G_loss: 0.4233\n",
      "  Batch [770/1299] D_loss: -0.0108, G_loss: 0.3857\n",
      "  Batch [780/1299] D_loss: -0.0619, G_loss: 0.4746\n",
      "  Batch [790/1299] D_loss: -0.0203, G_loss: 0.3571\n",
      "  Batch [800/1299] D_loss: -0.7933, G_loss: -2.7818\n",
      "  Batch [810/1299] D_loss: -1.5385, G_loss: -0.6520\n",
      "  Batch [820/1299] D_loss: -0.3182, G_loss: -0.1620\n",
      "  Batch [830/1299] D_loss: -0.0369, G_loss: 0.2469\n",
      "  Batch [840/1299] D_loss: -0.1373, G_loss: 0.5099\n",
      "  Batch [850/1299] D_loss: -0.1771, G_loss: 0.4831\n",
      "  Batch [860/1299] D_loss: -0.2154, G_loss: 0.4922\n",
      "  Batch [870/1299] D_loss: -0.1632, G_loss: 0.6367\n",
      "  Batch [880/1299] D_loss: 0.0068, G_loss: 0.3724\n",
      "  Batch [890/1299] D_loss: -0.4710, G_loss: -2.8618\n",
      "  Batch [900/1299] D_loss: 0.0167, G_loss: 0.1026\n",
      "  Batch [910/1299] D_loss: -0.3883, G_loss: -0.8210\n",
      "  Batch [920/1299] D_loss: -1.9991, G_loss: 0.0308\n",
      "  Batch [930/1299] D_loss: -0.0989, G_loss: 0.2131\n",
      "  Batch [940/1299] D_loss: 0.0341, G_loss: 0.3568\n",
      "  Batch [950/1299] D_loss: -0.0103, G_loss: 0.4696\n",
      "  Batch [960/1299] D_loss: -0.0384, G_loss: 0.4219\n",
      "  Batch [970/1299] D_loss: -0.0552, G_loss: 0.2854\n",
      "  Batch [980/1299] D_loss: -0.0003, G_loss: 0.4288\n",
      "  Batch [990/1299] D_loss: -4.2305, G_loss: -8.4577\n",
      "  Batch [1000/1299] D_loss: -0.0123, G_loss: 0.2886\n",
      "  Batch [1010/1299] D_loss: -0.0823, G_loss: 0.3829\n",
      "  Batch [1020/1299] D_loss: -0.1126, G_loss: 0.4555\n",
      "  Batch [1030/1299] D_loss: 0.0985, G_loss: 0.3154\n",
      "  Batch [1040/1299] D_loss: -1.2677, G_loss: -2.9928\n",
      "  Batch [1050/1299] D_loss: -0.4342, G_loss: -0.6604\n",
      "  Batch [1060/1299] D_loss: -0.0618, G_loss: 0.2866\n",
      "  Batch [1070/1299] D_loss: -0.0555, G_loss: 0.4801\n",
      "  Batch [1080/1299] D_loss: -0.0614, G_loss: 0.3828\n",
      "  Batch [1090/1299] D_loss: 0.0366, G_loss: 0.3659\n",
      "  Batch [1100/1299] D_loss: -0.1717, G_loss: 0.4257\n",
      "  Batch [1110/1299] D_loss: -0.1031, G_loss: 0.3652\n",
      "  Batch [1120/1299] D_loss: -0.1113, G_loss: 0.0408\n",
      "  Batch [1130/1299] D_loss: -1.1262, G_loss: -2.7628\n",
      "  Batch [1140/1299] D_loss: -0.7245, G_loss: -0.4377\n",
      "  Batch [1150/1299] D_loss: -0.1439, G_loss: 0.1436\n",
      "  Batch [1160/1299] D_loss: -0.1138, G_loss: 0.0549\n",
      "  Batch [1170/1299] D_loss: -0.1964, G_loss: 0.4485\n",
      "  Batch [1180/1299] D_loss: -0.1476, G_loss: 0.3247\n",
      "  Batch [1190/1299] D_loss: -0.1391, G_loss: 0.5738\n",
      "  Batch [1200/1299] D_loss: 0.0372, G_loss: 0.4980\n",
      "  Batch [1210/1299] D_loss: -0.1958, G_loss: 0.4040\n",
      "  Batch [1220/1299] D_loss: -0.0061, G_loss: 0.2127\n",
      "  Batch [1230/1299] D_loss: -0.0506, G_loss: 0.0500\n",
      "  Batch [1240/1299] D_loss: -0.4978, G_loss: -0.0760\n",
      "  Batch [1250/1299] D_loss: -0.0802, G_loss: 0.2676\n",
      "  Batch [1260/1299] D_loss: -0.0256, G_loss: 0.4895\n",
      "  Batch [1270/1299] D_loss: 0.0585, G_loss: 0.5577\n",
      "  Batch [1280/1299] D_loss: -0.1153, G_loss: 0.4877\n",
      "  Batch [1290/1299] D_loss: 0.0816, G_loss: 0.4194\n",
      "\n",
      "Epoch 14 Summary:\n",
      "  Average D_loss: -0.1278\n",
      "  Average G_loss: -0.0506\n",
      "\n",
      "Epoch [15/100]\n",
      "  Batch [0/1299] D_loss: -0.0738, G_loss: 0.3151\n",
      "  Batch [10/1299] D_loss: -1.3939, G_loss: -0.8126\n",
      "  Batch [20/1299] D_loss: -0.0347, G_loss: 0.1758\n",
      "  Batch [30/1299] D_loss: -0.0696, G_loss: 0.4138\n",
      "  Batch [40/1299] D_loss: -0.0194, G_loss: 0.4573\n",
      "  Batch [50/1299] D_loss: -0.0631, G_loss: 0.5129\n",
      "  Batch [60/1299] D_loss: -0.0453, G_loss: 0.2984\n",
      "  Batch [70/1299] D_loss: -0.0512, G_loss: 0.1140\n",
      "  Batch [80/1299] D_loss: -0.0727, G_loss: 0.0534\n",
      "  Batch [90/1299] D_loss: -0.9672, G_loss: -0.5340\n",
      "  Batch [100/1299] D_loss: -0.5775, G_loss: -1.2704\n",
      "  Batch [110/1299] D_loss: -0.5743, G_loss: 0.1040\n",
      "  Batch [120/1299] D_loss: -0.1003, G_loss: 0.3734\n",
      "  Batch [130/1299] D_loss: -0.0737, G_loss: 0.4539\n",
      "  Batch [140/1299] D_loss: 0.0650, G_loss: 0.7170\n",
      "  Batch [150/1299] D_loss: -0.2272, G_loss: 0.6858\n",
      "  Batch [160/1299] D_loss: -0.0060, G_loss: 0.5574\n",
      "  Batch [170/1299] D_loss: -0.1177, G_loss: 0.4540\n",
      "  Batch [180/1299] D_loss: -1.4272, G_loss: -2.4928\n",
      "  Batch [190/1299] D_loss: -0.0526, G_loss: 0.1278\n",
      "  Batch [200/1299] D_loss: -0.1352, G_loss: 0.3812\n",
      "  Batch [210/1299] D_loss: -0.0595, G_loss: 0.6909\n",
      "  Batch [220/1299] D_loss: -0.0695, G_loss: 0.5458\n",
      "  Batch [230/1299] D_loss: 0.0207, G_loss: 0.5093\n",
      "  Batch [240/1299] D_loss: 0.0565, G_loss: 0.4267\n",
      "  Batch [250/1299] D_loss: 0.0187, G_loss: 0.1893\n",
      "  Batch [260/1299] D_loss: -1.7094, G_loss: -1.9736\n",
      "  Batch [270/1299] D_loss: -1.0848, G_loss: -1.2813\n",
      "  Batch [280/1299] D_loss: -0.6360, G_loss: -0.6268\n",
      "  Batch [290/1299] D_loss: -0.0309, G_loss: 0.2511\n",
      "  Batch [300/1299] D_loss: -0.0351, G_loss: 0.3325\n",
      "  Batch [310/1299] D_loss: -0.1345, G_loss: 0.4585\n",
      "  Batch [320/1299] D_loss: -0.1107, G_loss: 0.4205\n",
      "  Batch [330/1299] D_loss: 0.0005, G_loss: 0.3830\n",
      "  Batch [340/1299] D_loss: -0.0587, G_loss: 0.1443\n",
      "  Batch [350/1299] D_loss: -1.1838, G_loss: -0.9879\n",
      "  Batch [360/1299] D_loss: -0.8697, G_loss: -1.5022\n",
      "  Batch [370/1299] D_loss: -0.2605, G_loss: -0.1135\n",
      "  Batch [380/1299] D_loss: -0.1180, G_loss: 0.3186\n",
      "  Batch [390/1299] D_loss: -0.1124, G_loss: 0.4170\n",
      "  Batch [400/1299] D_loss: -0.3486, G_loss: 0.7645\n",
      "  Batch [410/1299] D_loss: -0.0392, G_loss: 0.6582\n",
      "  Batch [420/1299] D_loss: 0.0244, G_loss: 0.4584\n",
      "  Batch [430/1299] D_loss: -0.0939, G_loss: 0.4009\n",
      "  Batch [440/1299] D_loss: -0.1518, G_loss: -3.5823\n",
      "  Batch [450/1299] D_loss: -0.0249, G_loss: 0.1322\n",
      "  Batch [460/1299] D_loss: -0.0402, G_loss: 0.2552\n",
      "  Batch [470/1299] D_loss: -0.0564, G_loss: 0.4123\n",
      "  Batch [480/1299] D_loss: -0.0385, G_loss: 0.4236\n",
      "  Batch [490/1299] D_loss: -0.1981, G_loss: 0.4052\n",
      "  Batch [500/1299] D_loss: -0.0561, G_loss: 0.3229\n",
      "  Batch [510/1299] D_loss: -0.1289, G_loss: 0.3869\n",
      "  Batch [520/1299] D_loss: -0.0184, G_loss: -0.0134\n",
      "  Batch [530/1299] D_loss: 0.0072, G_loss: 0.0736\n",
      "  Batch [540/1299] D_loss: -0.0283, G_loss: 0.1028\n",
      "  Batch [550/1299] D_loss: -0.0249, G_loss: 0.1647\n",
      "  Batch [560/1299] D_loss: -0.1176, G_loss: -0.3394\n",
      "  Batch [570/1299] D_loss: -0.0038, G_loss: 0.2459\n",
      "  Batch [580/1299] D_loss: 0.0565, G_loss: 0.3749\n",
      "  Batch [590/1299] D_loss: -0.0578, G_loss: 0.3314\n",
      "  Batch [600/1299] D_loss: -0.0063, G_loss: 0.3263\n",
      "  Batch [610/1299] D_loss: -0.0433, G_loss: 0.2683\n",
      "  Batch [620/1299] D_loss: -0.3328, G_loss: -0.9441\n",
      "  Batch [630/1299] D_loss: -0.0034, G_loss: 0.0526\n",
      "  Batch [640/1299] D_loss: -0.5455, G_loss: -0.2182\n",
      "  Batch [650/1299] D_loss: -0.2028, G_loss: 0.2593\n",
      "  Batch [660/1299] D_loss: -0.0106, G_loss: 0.3457\n",
      "  Batch [670/1299] D_loss: -0.0646, G_loss: 0.3276\n",
      "  Batch [680/1299] D_loss: -0.0191, G_loss: 0.2487\n",
      "  Batch [690/1299] D_loss: -0.0131, G_loss: 0.2644\n",
      "  Batch [700/1299] D_loss: -0.1543, G_loss: 0.0372\n",
      "  Batch [710/1299] D_loss: -0.4750, G_loss: -0.4416\n",
      "  Batch [720/1299] D_loss: -0.1384, G_loss: -0.0814\n",
      "  Batch [730/1299] D_loss: -1.4306, G_loss: -1.2045\n",
      "  Batch [740/1299] D_loss: -0.0428, G_loss: 0.1706\n",
      "  Batch [750/1299] D_loss: -0.1158, G_loss: 0.4266\n",
      "  Batch [760/1299] D_loss: 0.0093, G_loss: 0.3983\n",
      "  Batch [770/1299] D_loss: -0.1065, G_loss: 0.3986\n",
      "  Batch [780/1299] D_loss: 0.0894, G_loss: 0.1640\n",
      "  Batch [790/1299] D_loss: -0.6486, G_loss: -1.4893\n",
      "  Batch [800/1299] D_loss: -0.0940, G_loss: 0.1948\n",
      "  Batch [810/1299] D_loss: -0.1380, G_loss: 0.3778\n",
      "  Batch [820/1299] D_loss: -0.0308, G_loss: 0.3141\n",
      "  Batch [830/1299] D_loss: -0.0218, G_loss: 0.2019\n",
      "  Batch [840/1299] D_loss: -0.0041, G_loss: 0.1230\n",
      "  Batch [850/1299] D_loss: -1.5591, G_loss: -3.0409\n",
      "  Batch [860/1299] D_loss: -0.0504, G_loss: 0.3204\n",
      "  Batch [870/1299] D_loss: -0.3353, G_loss: -0.3203\n",
      "  Batch [880/1299] D_loss: -0.0312, G_loss: 0.3586\n",
      "  Batch [890/1299] D_loss: -0.0270, G_loss: 0.3997\n",
      "  Batch [900/1299] D_loss: -0.0887, G_loss: 0.4348\n",
      "  Batch [910/1299] D_loss: -0.0466, G_loss: 0.2940\n",
      "  Batch [920/1299] D_loss: -0.0255, G_loss: 0.1713\n",
      "  Batch [930/1299] D_loss: -0.7793, G_loss: -0.9615\n",
      "  Batch [940/1299] D_loss: -1.1377, G_loss: -2.2577\n",
      "  Batch [950/1299] D_loss: -1.2423, G_loss: -1.3983\n",
      "  Batch [960/1299] D_loss: -0.2745, G_loss: 0.1596\n",
      "  Batch [970/1299] D_loss: -0.0570, G_loss: 0.4404\n",
      "  Batch [980/1299] D_loss: -0.2147, G_loss: 0.6076\n",
      "  Batch [990/1299] D_loss: 0.0119, G_loss: 0.7192\n",
      "  Batch [1000/1299] D_loss: 0.1633, G_loss: 0.5777\n",
      "  Batch [1010/1299] D_loss: -0.0191, G_loss: 0.2176\n",
      "  Batch [1020/1299] D_loss: -0.0372, G_loss: 0.1856\n",
      "  Batch [1030/1299] D_loss: -0.4429, G_loss: -0.4144\n",
      "  Batch [1040/1299] D_loss: -0.3027, G_loss: -0.1973\n",
      "  Batch [1050/1299] D_loss: -0.0217, G_loss: 0.1568\n",
      "  Batch [1060/1299] D_loss: -0.0286, G_loss: 0.2490\n",
      "  Batch [1070/1299] D_loss: -0.0709, G_loss: 0.2413\n",
      "  Batch [1080/1299] D_loss: -0.4770, G_loss: 0.0258\n",
      "  Batch [1090/1299] D_loss: -0.3877, G_loss: -0.0176\n",
      "  Batch [1100/1299] D_loss: -0.6405, G_loss: -0.0876\n",
      "  Batch [1110/1299] D_loss: -0.6790, G_loss: -1.6845\n",
      "  Batch [1120/1299] D_loss: -0.0153, G_loss: 0.2290\n",
      "  Batch [1130/1299] D_loss: -0.0287, G_loss: 0.5222\n",
      "  Batch [1140/1299] D_loss: -0.1039, G_loss: 0.7181\n",
      "  Batch [1150/1299] D_loss: -0.0716, G_loss: 0.6627\n",
      "  Batch [1160/1299] D_loss: -0.2831, G_loss: 0.5827\n",
      "  Batch [1170/1299] D_loss: -0.0259, G_loss: 0.3261\n",
      "  Batch [1180/1299] D_loss: -0.0843, G_loss: 0.4021\n",
      "  Batch [1190/1299] D_loss: -0.9387, G_loss: -1.7833\n",
      "  Batch [1200/1299] D_loss: -0.8612, G_loss: -1.7194\n",
      "  Batch [1210/1299] D_loss: -0.5164, G_loss: -0.2155\n",
      "  Batch [1220/1299] D_loss: -0.1421, G_loss: 0.1216\n",
      "  Batch [1230/1299] D_loss: -0.2186, G_loss: 0.1508\n",
      "  Batch [1240/1299] D_loss: -0.0999, G_loss: 0.2525\n",
      "  Batch [1250/1299] D_loss: -0.0179, G_loss: 0.2404\n",
      "  Batch [1260/1299] D_loss: -0.8828, G_loss: -1.3480\n",
      "  Batch [1270/1299] D_loss: -0.1101, G_loss: 0.4123\n",
      "  Batch [1280/1299] D_loss: -0.1755, G_loss: 0.6138\n",
      "  Batch [1290/1299] D_loss: 0.0167, G_loss: 0.4975\n",
      "\n",
      "Epoch 15 Summary:\n",
      "  Average D_loss: -0.1284\n",
      "  Average G_loss: -0.0502\n",
      "\n",
      "Epoch [16/100]\n",
      "  Batch [0/1299] D_loss: -0.1168, G_loss: 0.5793\n",
      "  Batch [10/1299] D_loss: -0.0067, G_loss: 0.2316\n",
      "  Batch [20/1299] D_loss: -0.3081, G_loss: -1.5794\n",
      "  Batch [30/1299] D_loss: -0.2606, G_loss: -0.0892\n",
      "  Batch [40/1299] D_loss: -0.6973, G_loss: 0.0352\n",
      "  Batch [50/1299] D_loss: -0.0678, G_loss: 0.3571\n",
      "  Batch [60/1299] D_loss: -0.1310, G_loss: 0.4248\n",
      "  Batch [70/1299] D_loss: -0.0309, G_loss: 0.5096\n",
      "  Batch [80/1299] D_loss: -0.0921, G_loss: 0.4331\n",
      "  Batch [90/1299] D_loss: -0.0838, G_loss: 0.1865\n",
      "  Batch [100/1299] D_loss: -0.5689, G_loss: -0.0089\n",
      "  Batch [110/1299] D_loss: -0.0655, G_loss: 0.0858\n",
      "  Batch [120/1299] D_loss: -0.2956, G_loss: -0.5853\n",
      "  Batch [130/1299] D_loss: -0.1346, G_loss: 0.1411\n",
      "  Batch [140/1299] D_loss: -0.0884, G_loss: 0.2950\n",
      "  Batch [150/1299] D_loss: -0.1028, G_loss: 0.6227\n",
      "  Batch [160/1299] D_loss: -0.1429, G_loss: 0.5403\n",
      "  Batch [170/1299] D_loss: -0.0476, G_loss: 0.5577\n",
      "  Batch [180/1299] D_loss: -0.0378, G_loss: 0.4765\n",
      "  Batch [190/1299] D_loss: -0.1154, G_loss: 0.4977\n",
      "  Batch [200/1299] D_loss: -0.5256, G_loss: -1.1617\n",
      "  Batch [210/1299] D_loss: -0.2224, G_loss: 0.0414\n",
      "  Batch [220/1299] D_loss: -0.3344, G_loss: 0.1570\n",
      "  Batch [230/1299] D_loss: -0.0568, G_loss: 0.3864\n",
      "  Batch [240/1299] D_loss: -0.0266, G_loss: 0.3307\n",
      "  Batch [250/1299] D_loss: -0.5706, G_loss: -0.6404\n",
      "  Batch [260/1299] D_loss: -0.5191, G_loss: -0.3031\n",
      "  Batch [270/1299] D_loss: -0.2293, G_loss: 0.0626\n",
      "  Batch [280/1299] D_loss: -0.0288, G_loss: 0.1656\n",
      "  Batch [290/1299] D_loss: -0.1388, G_loss: 0.3799\n",
      "  Batch [300/1299] D_loss: -0.0520, G_loss: 0.5636\n",
      "  Batch [310/1299] D_loss: 0.0272, G_loss: 0.5394\n",
      "  Batch [320/1299] D_loss: -3.2185, G_loss: -0.2430\n",
      "  Batch [330/1299] D_loss: -0.2179, G_loss: -0.1514\n",
      "  Batch [340/1299] D_loss: -0.0973, G_loss: 0.1499\n",
      "  Batch [350/1299] D_loss: -0.1372, G_loss: 0.2482\n",
      "  Batch [360/1299] D_loss: -0.0552, G_loss: 0.3677\n",
      "  Batch [370/1299] D_loss: -0.1057, G_loss: 0.4426\n",
      "  Batch [380/1299] D_loss: -0.1390, G_loss: 0.4803\n",
      "  Batch [390/1299] D_loss: -0.0765, G_loss: 0.2295\n",
      "  Batch [400/1299] D_loss: -2.8412, G_loss: -6.4447\n",
      "  Batch [410/1299] D_loss: -0.1906, G_loss: -0.1119\n",
      "  Batch [420/1299] D_loss: -0.5451, G_loss: -0.1557\n",
      "  Batch [430/1299] D_loss: -0.0805, G_loss: 0.0823\n",
      "  Batch [440/1299] D_loss: -0.1140, G_loss: 0.3375\n",
      "  Batch [450/1299] D_loss: -0.1505, G_loss: 0.4588\n",
      "  Batch [460/1299] D_loss: -0.1783, G_loss: 0.6207\n",
      "  Batch [470/1299] D_loss: -0.1886, G_loss: 0.6401\n",
      "  Batch [480/1299] D_loss: -0.0240, G_loss: 0.3064\n",
      "  Batch [490/1299] D_loss: -0.0573, G_loss: 0.1002\n",
      "  Batch [500/1299] D_loss: -2.1032, G_loss: -2.2381\n",
      "  Batch [510/1299] D_loss: -0.0165, G_loss: 0.1800\n",
      "  Batch [520/1299] D_loss: -0.0444, G_loss: 0.2529\n",
      "  Batch [530/1299] D_loss: 0.0283, G_loss: 0.2893\n",
      "  Batch [540/1299] D_loss: -0.1162, G_loss: 0.4021\n",
      "  Batch [550/1299] D_loss: -0.0361, G_loss: 0.1741\n",
      "  Batch [560/1299] D_loss: -0.0552, G_loss: 0.2290\n",
      "  Batch [570/1299] D_loss: -0.4022, G_loss: -0.9188\n",
      "  Batch [580/1299] D_loss: -0.0579, G_loss: 0.1278\n",
      "  Batch [590/1299] D_loss: -0.0841, G_loss: 0.3118\n",
      "  Batch [600/1299] D_loss: 0.0294, G_loss: 0.3603\n",
      "  Batch [610/1299] D_loss: -0.0830, G_loss: 0.3453\n",
      "  Batch [620/1299] D_loss: -0.0671, G_loss: 0.2946\n",
      "  Batch [630/1299] D_loss: -0.0362, G_loss: 0.0798\n",
      "  Batch [640/1299] D_loss: -1.0911, G_loss: -0.4146\n",
      "  Batch [650/1299] D_loss: -0.3237, G_loss: -0.1688\n",
      "  Batch [660/1299] D_loss: -0.6017, G_loss: -0.4244\n",
      "  Batch [670/1299] D_loss: -0.0900, G_loss: 0.1190\n",
      "  Batch [680/1299] D_loss: -0.0587, G_loss: 0.3904\n",
      "  Batch [690/1299] D_loss: -0.1639, G_loss: 0.4750\n",
      "  Batch [700/1299] D_loss: -0.2154, G_loss: 0.5216\n",
      "  Batch [710/1299] D_loss: -0.1230, G_loss: 0.5154\n",
      "  Batch [720/1299] D_loss: -0.1191, G_loss: 0.2422\n",
      "  Batch [730/1299] D_loss: -0.0212, G_loss: 0.1272\n",
      "  Batch [740/1299] D_loss: 0.0085, G_loss: 0.1240\n",
      "  Batch [750/1299] D_loss: -0.2289, G_loss: 0.0836\n",
      "  Batch [760/1299] D_loss: -0.0846, G_loss: 0.1641\n",
      "  Batch [770/1299] D_loss: -0.0713, G_loss: 0.2917\n",
      "  Batch [780/1299] D_loss: -0.0724, G_loss: 0.5221\n",
      "  Batch [790/1299] D_loss: 0.0004, G_loss: 0.4031\n",
      "  Batch [800/1299] D_loss: -0.0261, G_loss: 0.2797\n",
      "  Batch [810/1299] D_loss: 0.2882, G_loss: -2.1238\n",
      "  Batch [820/1299] D_loss: -0.4482, G_loss: -0.0963\n",
      "  Batch [830/1299] D_loss: 0.0081, G_loss: 0.1747\n",
      "  Batch [840/1299] D_loss: 0.0346, G_loss: 0.1654\n",
      "  Batch [850/1299] D_loss: -0.0203, G_loss: 0.2924\n",
      "  Batch [860/1299] D_loss: -0.0763, G_loss: 0.2346\n",
      "  Batch [870/1299] D_loss: -0.0834, G_loss: 0.0540\n",
      "  Batch [880/1299] D_loss: -0.9288, G_loss: -0.0735\n",
      "  Batch [890/1299] D_loss: -0.0024, G_loss: 0.2290\n",
      "  Batch [900/1299] D_loss: -0.0515, G_loss: 0.3011\n",
      "  Batch [910/1299] D_loss: -0.0406, G_loss: 0.3798\n",
      "  Batch [920/1299] D_loss: 0.0103, G_loss: 0.3893\n",
      "  Batch [930/1299] D_loss: -0.0313, G_loss: 0.2294\n",
      "  Batch [940/1299] D_loss: -0.1261, G_loss: -0.0921\n",
      "  Batch [950/1299] D_loss: -0.8526, G_loss: -0.4809\n",
      "  Batch [960/1299] D_loss: -0.1169, G_loss: 0.1249\n",
      "  Batch [970/1299] D_loss: -0.1808, G_loss: 0.1824\n",
      "  Batch [980/1299] D_loss: -0.1232, G_loss: 0.1062\n",
      "  Batch [990/1299] D_loss: -0.4575, G_loss: -0.4263\n",
      "  Batch [1000/1299] D_loss: -0.0244, G_loss: 0.1421\n",
      "  Batch [1010/1299] D_loss: -0.1096, G_loss: 0.3098\n",
      "  Batch [1020/1299] D_loss: -0.0297, G_loss: 0.6537\n",
      "  Batch [1030/1299] D_loss: -0.1674, G_loss: 0.5120\n",
      "  Batch [1040/1299] D_loss: -0.1556, G_loss: 0.4822\n",
      "  Batch [1050/1299] D_loss: -0.1130, G_loss: 0.2529\n",
      "  Batch [1060/1299] D_loss: -0.5662, G_loss: -0.3664\n",
      "  Batch [1070/1299] D_loss: -0.0396, G_loss: 0.1315\n",
      "  Batch [1080/1299] D_loss: -0.0561, G_loss: 0.2835\n",
      "  Batch [1090/1299] D_loss: -0.0933, G_loss: 0.2510\n",
      "  Batch [1100/1299] D_loss: -0.1496, G_loss: 0.4553\n",
      "  Batch [1110/1299] D_loss: -0.0227, G_loss: 0.2656\n",
      "  Batch [1120/1299] D_loss: -1.1227, G_loss: -0.7212\n",
      "  Batch [1130/1299] D_loss: -0.2537, G_loss: 0.1105\n",
      "  Batch [1140/1299] D_loss: -1.2973, G_loss: -0.4147\n",
      "  Batch [1150/1299] D_loss: 0.0049, G_loss: 0.1000\n",
      "  Batch [1160/1299] D_loss: -0.0988, G_loss: 0.2516\n",
      "  Batch [1170/1299] D_loss: -0.1267, G_loss: 0.3602\n",
      "  Batch [1180/1299] D_loss: -0.0864, G_loss: 0.5225\n",
      "  Batch [1190/1299] D_loss: -0.0680, G_loss: 0.3923\n",
      "  Batch [1200/1299] D_loss: -0.0484, G_loss: 0.4411\n",
      "  Batch [1210/1299] D_loss: -0.9879, G_loss: -4.5066\n",
      "  Batch [1220/1299] D_loss: -0.4952, G_loss: -0.3742\n",
      "  Batch [1230/1299] D_loss: -0.0369, G_loss: 0.0922\n",
      "  Batch [1240/1299] D_loss: -0.0976, G_loss: 0.2906\n",
      "  Batch [1250/1299] D_loss: -0.1423, G_loss: 0.4944\n",
      "  Batch [1260/1299] D_loss: -0.0955, G_loss: 0.5570\n",
      "  Batch [1270/1299] D_loss: -0.0654, G_loss: 0.5023\n",
      "  Batch [1280/1299] D_loss: -0.0683, G_loss: 0.4422\n",
      "  Batch [1290/1299] D_loss: -0.0705, G_loss: 0.2606\n",
      "\n",
      "Epoch 16 Summary:\n",
      "  Average D_loss: -0.1336\n",
      "  Average G_loss: -0.0524\n",
      "\n",
      "Epoch [17/100]\n",
      "  Batch [0/1299] D_loss: -0.7311, G_loss: -1.0831\n",
      "  Batch [10/1299] D_loss: 0.0126, G_loss: 0.0681\n",
      "  Batch [20/1299] D_loss: -0.5873, G_loss: -2.1570\n",
      "  Batch [30/1299] D_loss: -0.9810, G_loss: -0.3245\n",
      "  Batch [40/1299] D_loss: -0.2572, G_loss: -0.0743\n",
      "  Batch [50/1299] D_loss: -0.0116, G_loss: 0.3405\n",
      "  Batch [60/1299] D_loss: -0.1045, G_loss: 0.4897\n",
      "  Batch [70/1299] D_loss: -0.0481, G_loss: 0.7113\n",
      "  Batch [80/1299] D_loss: -0.1751, G_loss: 0.6207\n",
      "  Batch [90/1299] D_loss: -0.0450, G_loss: 0.3556\n",
      "  Batch [100/1299] D_loss: 0.0197, G_loss: 0.2172\n",
      "  Batch [110/1299] D_loss: -1.2045, G_loss: -3.4779\n",
      "  Batch [120/1299] D_loss: -0.4434, G_loss: -0.1806\n",
      "  Batch [130/1299] D_loss: 0.0050, G_loss: 0.3541\n",
      "  Batch [140/1299] D_loss: -0.1392, G_loss: 0.4475\n",
      "  Batch [150/1299] D_loss: 0.0038, G_loss: 0.3978\n",
      "  Batch [160/1299] D_loss: 0.0291, G_loss: 0.3258\n",
      "  Batch [170/1299] D_loss: -0.0904, G_loss: 0.4047\n",
      "  Batch [180/1299] D_loss: -0.1190, G_loss: 0.0892\n",
      "  Batch [190/1299] D_loss: 0.0363, G_loss: 0.0940\n",
      "  Batch [200/1299] D_loss: -0.0927, G_loss: 0.3748\n",
      "  Batch [210/1299] D_loss: -0.1915, G_loss: 0.5102\n",
      "  Batch [220/1299] D_loss: -0.0202, G_loss: 0.6226\n",
      "  Batch [230/1299] D_loss: 0.0216, G_loss: 0.4607\n",
      "  Batch [240/1299] D_loss: -0.0929, G_loss: 0.3152\n",
      "  Batch [250/1299] D_loss: -0.1599, G_loss: -0.1986\n",
      "  Batch [260/1299] D_loss: -0.0928, G_loss: 0.0943\n",
      "  Batch [270/1299] D_loss: -0.0080, G_loss: 0.1399\n",
      "  Batch [280/1299] D_loss: -0.1223, G_loss: 0.2853\n",
      "  Batch [290/1299] D_loss: -0.0926, G_loss: 0.4529\n",
      "  Batch [300/1299] D_loss: 0.0606, G_loss: 0.3321\n",
      "  Batch [310/1299] D_loss: 0.0269, G_loss: 0.3805\n",
      "  Batch [320/1299] D_loss: -0.0380, G_loss: 0.3264\n",
      "  Batch [330/1299] D_loss: -2.2411, G_loss: -4.5970\n",
      "  Batch [340/1299] D_loss: -0.0079, G_loss: 0.0818\n",
      "  Batch [350/1299] D_loss: -0.0010, G_loss: 0.1600\n",
      "  Batch [360/1299] D_loss: -0.0742, G_loss: 0.3058\n",
      "  Batch [370/1299] D_loss: -0.2209, G_loss: 0.5751\n",
      "  Batch [380/1299] D_loss: -0.0549, G_loss: 0.4501\n",
      "  Batch [390/1299] D_loss: -0.1205, G_loss: 0.3509\n",
      "  Batch [400/1299] D_loss: -0.2904, G_loss: -0.8092\n",
      "  Batch [410/1299] D_loss: -0.7002, G_loss: -0.2507\n",
      "  Batch [420/1299] D_loss: -0.0927, G_loss: 0.1117\n",
      "  Batch [430/1299] D_loss: -0.0550, G_loss: 0.2982\n",
      "  Batch [440/1299] D_loss: -0.0541, G_loss: 0.3795\n",
      "  Batch [450/1299] D_loss: -0.0615, G_loss: 0.4608\n",
      "  Batch [460/1299] D_loss: -0.0810, G_loss: 0.4833\n",
      "  Batch [470/1299] D_loss: 0.0336, G_loss: 0.2302\n",
      "  Batch [480/1299] D_loss: -0.0470, G_loss: 0.2148\n",
      "  Batch [490/1299] D_loss: -0.2686, G_loss: -0.1932\n",
      "  Batch [500/1299] D_loss: -0.0780, G_loss: 0.2178\n",
      "  Batch [510/1299] D_loss: 0.0248, G_loss: 0.2422\n",
      "  Batch [520/1299] D_loss: -0.0831, G_loss: 0.2033\n",
      "  Batch [530/1299] D_loss: -0.0483, G_loss: 0.1471\n",
      "  Batch [540/1299] D_loss: -0.2299, G_loss: -0.0536\n",
      "  Batch [550/1299] D_loss: -0.9776, G_loss: -0.2847\n",
      "  Batch [560/1299] D_loss: -0.0971, G_loss: 0.1351\n",
      "  Batch [570/1299] D_loss: -0.1080, G_loss: 0.3672\n",
      "  Batch [580/1299] D_loss: -0.2107, G_loss: 0.5749\n",
      "  Batch [590/1299] D_loss: -0.0607, G_loss: 0.6171\n",
      "  Batch [600/1299] D_loss: -0.0628, G_loss: 0.4975\n",
      "  Batch [610/1299] D_loss: 0.0048, G_loss: 0.3761\n",
      "  Batch [620/1299] D_loss: -0.0552, G_loss: 0.2162\n",
      "  Batch [630/1299] D_loss: -0.3529, G_loss: -0.0818\n",
      "  Batch [640/1299] D_loss: -0.0339, G_loss: 0.1407\n",
      "  Batch [650/1299] D_loss: -0.1751, G_loss: 0.4631\n",
      "  Batch [660/1299] D_loss: -0.0383, G_loss: 0.4059\n",
      "  Batch [670/1299] D_loss: -0.0307, G_loss: 0.3928\n",
      "  Batch [680/1299] D_loss: -0.8090, G_loss: -3.3726\n",
      "  Batch [690/1299] D_loss: -1.0790, G_loss: -2.1908\n",
      "  Batch [700/1299] D_loss: -0.5665, G_loss: -0.7886\n",
      "  Batch [710/1299] D_loss: -0.0849, G_loss: 0.3407\n",
      "  Batch [720/1299] D_loss: -0.1019, G_loss: 0.5935\n",
      "  Batch [730/1299] D_loss: -0.1429, G_loss: 0.5923\n",
      "  Batch [740/1299] D_loss: -0.1362, G_loss: 0.3949\n",
      "  Batch [750/1299] D_loss: -0.0355, G_loss: 0.3575\n",
      "  Batch [760/1299] D_loss: -0.0174, G_loss: 0.2418\n",
      "  Batch [770/1299] D_loss: -0.0259, G_loss: 0.0990\n",
      "  Batch [780/1299] D_loss: -0.0044, G_loss: 0.1493\n",
      "  Batch [790/1299] D_loss: -0.1493, G_loss: 0.2741\n",
      "  Batch [800/1299] D_loss: 0.0207, G_loss: 0.3457\n",
      "  Batch [810/1299] D_loss: -0.0829, G_loss: 0.2732\n",
      "  Batch [820/1299] D_loss: -0.0215, G_loss: 0.2288\n",
      "  Batch [830/1299] D_loss: -1.2326, G_loss: -3.6862\n",
      "  Batch [840/1299] D_loss: -1.3616, G_loss: -0.3296\n",
      "  Batch [850/1299] D_loss: -0.0612, G_loss: 0.1501\n",
      "  Batch [860/1299] D_loss: -0.1827, G_loss: 0.1089\n",
      "  Batch [870/1299] D_loss: -0.8391, G_loss: -2.8513\n",
      "  Batch [880/1299] D_loss: -1.2933, G_loss: 0.1367\n",
      "  Batch [890/1299] D_loss: -0.0962, G_loss: 0.3017\n",
      "  Batch [900/1299] D_loss: -0.1743, G_loss: 0.5545\n",
      "  Batch [910/1299] D_loss: -0.1441, G_loss: 0.8136\n",
      "  Batch [920/1299] D_loss: -0.1518, G_loss: 0.5771\n",
      "  Batch [930/1299] D_loss: -0.0811, G_loss: 0.3127\n",
      "  Batch [940/1299] D_loss: -0.0632, G_loss: 0.2343\n",
      "  Batch [950/1299] D_loss: -1.0886, G_loss: -1.5548\n",
      "  Batch [960/1299] D_loss: -0.9577, G_loss: -2.2937\n",
      "  Batch [970/1299] D_loss: -0.0276, G_loss: 0.0837\n",
      "  Batch [980/1299] D_loss: 0.0040, G_loss: 0.1957\n",
      "  Batch [990/1299] D_loss: -0.1330, G_loss: 0.2841\n",
      "  Batch [1000/1299] D_loss: -0.0566, G_loss: 0.3596\n",
      "  Batch [1010/1299] D_loss: -0.0159, G_loss: 0.2710\n",
      "  Batch [1020/1299] D_loss: -0.9068, G_loss: -2.2599\n",
      "  Batch [1030/1299] D_loss: -0.3801, G_loss: -0.1481\n",
      "  Batch [1040/1299] D_loss: -0.5301, G_loss: -0.1425\n",
      "  Batch [1050/1299] D_loss: -0.0571, G_loss: 0.1658\n",
      "  Batch [1060/1299] D_loss: -0.0676, G_loss: 0.3057\n",
      "  Batch [1070/1299] D_loss: -0.1389, G_loss: 0.4140\n",
      "  Batch [1080/1299] D_loss: -0.1309, G_loss: 0.5504\n",
      "  Batch [1090/1299] D_loss: -0.0822, G_loss: 0.4690\n",
      "  Batch [1100/1299] D_loss: -0.0977, G_loss: 0.4358\n",
      "  Batch [1110/1299] D_loss: -0.0576, G_loss: 0.2915\n",
      "  Batch [1120/1299] D_loss: -0.8535, G_loss: -1.0423\n",
      "  Batch [1130/1299] D_loss: -0.3139, G_loss: -0.4168\n",
      "  Batch [1140/1299] D_loss: -0.0361, G_loss: 0.1612\n",
      "  Batch [1150/1299] D_loss: -0.0921, G_loss: 0.2970\n",
      "  Batch [1160/1299] D_loss: -0.1109, G_loss: 0.3566\n",
      "  Batch [1170/1299] D_loss: 0.0586, G_loss: 0.3389\n",
      "  Batch [1180/1299] D_loss: -0.0593, G_loss: 0.4370\n",
      "  Batch [1190/1299] D_loss: -0.2037, G_loss: 0.4289\n",
      "  Batch [1200/1299] D_loss: -3.4151, G_loss: -8.4015\n",
      "  Batch [1210/1299] D_loss: -0.0913, G_loss: 0.0669\n",
      "  Batch [1220/1299] D_loss: -0.2912, G_loss: -0.0675\n",
      "  Batch [1230/1299] D_loss: -0.2614, G_loss: -0.2419\n",
      "  Batch [1240/1299] D_loss: -0.4006, G_loss: -1.1337\n",
      "  Batch [1250/1299] D_loss: -0.7385, G_loss: 0.0285\n",
      "  Batch [1260/1299] D_loss: -0.0475, G_loss: 0.2421\n",
      "  Batch [1270/1299] D_loss: -0.0353, G_loss: 0.5457\n",
      "  Batch [1280/1299] D_loss: -0.1540, G_loss: 0.6090\n",
      "  Batch [1290/1299] D_loss: -0.4195, G_loss: 0.6205\n",
      "\n",
      "Epoch 17 Summary:\n",
      "  Average D_loss: -0.1271\n",
      "  Average G_loss: -0.0626\n",
      "\n",
      "Epoch [18/100]\n",
      "  Batch [0/1299] D_loss: -0.0005, G_loss: 0.5943\n",
      "  Batch [10/1299] D_loss: 0.0485, G_loss: 0.4689\n",
      "  Batch [20/1299] D_loss: -0.0106, G_loss: 0.3063\n",
      "  Batch [30/1299] D_loss: -0.0723, G_loss: 0.0863\n",
      "  Batch [40/1299] D_loss: -0.0270, G_loss: 0.1941\n",
      "  Batch [50/1299] D_loss: -0.0475, G_loss: 0.3558\n",
      "  Batch [60/1299] D_loss: -0.1511, G_loss: 0.4898\n",
      "  Batch [70/1299] D_loss: -0.0687, G_loss: 0.3296\n",
      "  Batch [80/1299] D_loss: -0.4336, G_loss: -1.8185\n",
      "  Batch [90/1299] D_loss: -0.4205, G_loss: -0.3363\n",
      "  Batch [100/1299] D_loss: -0.2586, G_loss: -0.0249\n",
      "  Batch [110/1299] D_loss: -0.2907, G_loss: 0.1147\n",
      "  Batch [120/1299] D_loss: -0.0567, G_loss: 0.2026\n",
      "  Batch [130/1299] D_loss: 0.0170, G_loss: 0.2823\n",
      "  Batch [140/1299] D_loss: -0.0562, G_loss: 0.4080\n",
      "  Batch [150/1299] D_loss: -0.1991, G_loss: 0.5629\n",
      "  Batch [160/1299] D_loss: -0.0293, G_loss: 0.4071\n",
      "  Batch [170/1299] D_loss: -0.1005, G_loss: 0.2384\n",
      "  Batch [180/1299] D_loss: -0.6934, G_loss: -2.3999\n",
      "  Batch [190/1299] D_loss: 0.0178, G_loss: 0.1336\n",
      "  Batch [200/1299] D_loss: -0.0482, G_loss: 0.1991\n",
      "  Batch [210/1299] D_loss: -0.0682, G_loss: 0.2802\n",
      "  Batch [220/1299] D_loss: -0.0651, G_loss: 0.3827\n",
      "  Batch [230/1299] D_loss: -0.0765, G_loss: 0.2508\n",
      "  Batch [240/1299] D_loss: -1.4661, G_loss: -2.8782\n",
      "  Batch [250/1299] D_loss: -0.9309, G_loss: 0.0035\n",
      "  Batch [260/1299] D_loss: -0.2614, G_loss: 0.0992\n",
      "  Batch [270/1299] D_loss: -0.6485, G_loss: -0.3488\n",
      "  Batch [280/1299] D_loss: -0.0517, G_loss: 0.3081\n",
      "  Batch [290/1299] D_loss: 0.0454, G_loss: 0.4277\n",
      "  Batch [300/1299] D_loss: -0.0550, G_loss: 0.7081\n",
      "  Batch [310/1299] D_loss: -0.1323, G_loss: 0.5388\n",
      "  Batch [320/1299] D_loss: 0.0072, G_loss: 0.3940\n",
      "  Batch [330/1299] D_loss: -0.2553, G_loss: 0.2323\n",
      "  Batch [340/1299] D_loss: -1.4494, G_loss: -2.0551\n",
      "  Batch [350/1299] D_loss: -0.0515, G_loss: 0.0747\n",
      "  Batch [360/1299] D_loss: -1.3184, G_loss: -4.0511\n",
      "  Batch [370/1299] D_loss: -0.5101, G_loss: -0.2929\n",
      "  Batch [380/1299] D_loss: -0.3809, G_loss: 0.0685\n",
      "  Batch [390/1299] D_loss: -0.0386, G_loss: 0.2281\n",
      "  Batch [400/1299] D_loss: -0.1045, G_loss: 0.3814\n",
      "  Batch [410/1299] D_loss: 0.0179, G_loss: 0.3955\n",
      "  Batch [420/1299] D_loss: -0.1700, G_loss: 0.5196\n",
      "  Batch [430/1299] D_loss: 0.0856, G_loss: 0.5073\n",
      "  Batch [440/1299] D_loss: -0.0645, G_loss: 0.4971\n",
      "  Batch [450/1299] D_loss: -1.9218, G_loss: -7.8501\n",
      "  Batch [460/1299] D_loss: -1.4704, G_loss: -1.4912\n",
      "  Batch [470/1299] D_loss: -0.2329, G_loss: 0.1602\n",
      "  Batch [480/1299] D_loss: 0.0580, G_loss: 0.3797\n",
      "  Batch [490/1299] D_loss: -0.1341, G_loss: 0.3651\n",
      "  Batch [500/1299] D_loss: -0.1173, G_loss: 0.6538\n",
      "  Batch [510/1299] D_loss: -0.0529, G_loss: 0.4080\n",
      "  Batch [520/1299] D_loss: -0.0666, G_loss: 0.1645\n",
      "  Batch [530/1299] D_loss: -0.4873, G_loss: -1.0979\n",
      "  Batch [540/1299] D_loss: -0.1593, G_loss: 0.0510\n",
      "  Batch [550/1299] D_loss: -0.0427, G_loss: 0.1148\n",
      "  Batch [560/1299] D_loss: -0.0180, G_loss: 0.1948\n",
      "  Batch [570/1299] D_loss: -0.1428, G_loss: 0.4107\n",
      "  Batch [580/1299] D_loss: 0.0355, G_loss: 0.5441\n",
      "  Batch [590/1299] D_loss: -0.1572, G_loss: 0.6838\n",
      "  Batch [600/1299] D_loss: -0.1931, G_loss: 0.3883\n",
      "  Batch [610/1299] D_loss: -2.9180, G_loss: -6.4381\n",
      "  Batch [620/1299] D_loss: -1.1919, G_loss: -1.7890\n",
      "  Batch [630/1299] D_loss: 0.0088, G_loss: 0.1248\n",
      "  Batch [640/1299] D_loss: -0.0056, G_loss: 0.1915\n",
      "  Batch [650/1299] D_loss: 0.0356, G_loss: 0.2499\n",
      "  Batch [660/1299] D_loss: -0.0553, G_loss: 0.2948\n",
      "  Batch [670/1299] D_loss: -0.0714, G_loss: 0.4137\n",
      "  Batch [680/1299] D_loss: 0.0007, G_loss: 0.1804\n",
      "  Batch [690/1299] D_loss: -0.5921, G_loss: 0.0528\n",
      "  Batch [700/1299] D_loss: -0.0708, G_loss: 0.1834\n",
      "  Batch [710/1299] D_loss: -0.0489, G_loss: 0.1642\n",
      "  Batch [720/1299] D_loss: -0.0408, G_loss: 0.3206\n",
      "  Batch [730/1299] D_loss: -0.1307, G_loss: 0.3811\n",
      "  Batch [740/1299] D_loss: -0.0492, G_loss: 0.6106\n",
      "  Batch [750/1299] D_loss: -0.0711, G_loss: 0.5064\n",
      "  Batch [760/1299] D_loss: -0.0364, G_loss: 0.4294\n",
      "  Batch [770/1299] D_loss: -0.8238, G_loss: -0.2263\n",
      "  Batch [780/1299] D_loss: -0.9380, G_loss: 0.0555\n",
      "  Batch [790/1299] D_loss: -0.2180, G_loss: -0.1650\n",
      "  Batch [800/1299] D_loss: -0.0186, G_loss: 0.1848\n",
      "  Batch [810/1299] D_loss: -0.0312, G_loss: 0.1303\n",
      "  Batch [820/1299] D_loss: 0.0328, G_loss: 0.2809\n",
      "  Batch [830/1299] D_loss: -0.6115, G_loss: -0.4316\n",
      "  Batch [840/1299] D_loss: -0.6636, G_loss: -0.0125\n",
      "  Batch [850/1299] D_loss: 0.0197, G_loss: 0.2357\n",
      "  Batch [860/1299] D_loss: -0.1347, G_loss: 0.3419\n",
      "  Batch [870/1299] D_loss: 0.0399, G_loss: 0.4130\n",
      "  Batch [880/1299] D_loss: -0.0562, G_loss: 0.4822\n",
      "  Batch [890/1299] D_loss: 0.0369, G_loss: 0.3520\n",
      "  Batch [900/1299] D_loss: -0.6899, G_loss: -1.2704\n",
      "  Batch [910/1299] D_loss: -1.0061, G_loss: -1.9903\n",
      "  Batch [920/1299] D_loss: -0.0466, G_loss: 0.0840\n",
      "  Batch [930/1299] D_loss: -0.0852, G_loss: 0.2986\n",
      "  Batch [940/1299] D_loss: -0.1940, G_loss: 0.4655\n",
      "  Batch [950/1299] D_loss: -0.0187, G_loss: 0.3581\n",
      "  Batch [960/1299] D_loss: -0.1265, G_loss: 0.3788\n",
      "  Batch [970/1299] D_loss: -0.0043, G_loss: 0.2734\n",
      "  Batch [980/1299] D_loss: -0.0017, G_loss: 0.2440\n",
      "  Batch [990/1299] D_loss: 0.0332, G_loss: 0.1117\n",
      "  Batch [1000/1299] D_loss: -0.0160, G_loss: 0.1509\n",
      "  Batch [1010/1299] D_loss: 0.0295, G_loss: 0.1729\n",
      "  Batch [1020/1299] D_loss: -0.0039, G_loss: 0.2581\n",
      "  Batch [1030/1299] D_loss: -0.0472, G_loss: 0.0391\n",
      "  Batch [1040/1299] D_loss: -0.0260, G_loss: 0.1345\n",
      "  Batch [1050/1299] D_loss: -0.0567, G_loss: 0.3042\n",
      "  Batch [1060/1299] D_loss: 0.0462, G_loss: 0.4797\n",
      "  Batch [1070/1299] D_loss: -0.0813, G_loss: 0.3202\n",
      "  Batch [1080/1299] D_loss: -0.2393, G_loss: 0.5500\n",
      "  Batch [1090/1299] D_loss: -1.1156, G_loss: -0.0182\n",
      "  Batch [1100/1299] D_loss: -0.8627, G_loss: -2.1322\n",
      "  Batch [1110/1299] D_loss: -0.6012, G_loss: -1.2407\n",
      "  Batch [1120/1299] D_loss: -0.7974, G_loss: -0.0307\n",
      "  Batch [1130/1299] D_loss: -0.0413, G_loss: 0.0024\n",
      "  Batch [1140/1299] D_loss: -0.2463, G_loss: 0.0800\n",
      "  Batch [1150/1299] D_loss: -0.1674, G_loss: 0.0018\n",
      "  Batch [1160/1299] D_loss: -0.4674, G_loss: -1.1705\n",
      "  Batch [1170/1299] D_loss: -0.5011, G_loss: -0.3017\n",
      "  Batch [1180/1299] D_loss: -0.1830, G_loss: 0.0887\n",
      "  Batch [1190/1299] D_loss: -0.5360, G_loss: -0.0277\n",
      "  Batch [1200/1299] D_loss: -0.0497, G_loss: 0.3592\n",
      "  Batch [1210/1299] D_loss: -0.0696, G_loss: 0.5259\n",
      "  Batch [1220/1299] D_loss: -0.1059, G_loss: 0.6961\n",
      "  Batch [1230/1299] D_loss: -0.0474, G_loss: 0.7315\n",
      "  Batch [1240/1299] D_loss: -0.0534, G_loss: 0.5811\n",
      "  Batch [1250/1299] D_loss: -0.2373, G_loss: 0.4764\n",
      "  Batch [1260/1299] D_loss: -0.2785, G_loss: 0.1118\n",
      "  Batch [1270/1299] D_loss: -0.0296, G_loss: 0.0955\n",
      "  Batch [1280/1299] D_loss: -0.0829, G_loss: 0.0684\n",
      "  Batch [1290/1299] D_loss: -0.0894, G_loss: 0.2204\n",
      "\n",
      "Epoch 18 Summary:\n",
      "  Average D_loss: -0.1568\n",
      "  Average G_loss: -0.0649\n",
      "\n",
      "Epoch [19/100]\n",
      "  Batch [0/1299] D_loss: -0.0020, G_loss: 0.3425\n",
      "  Batch [10/1299] D_loss: -0.1315, G_loss: 0.4041\n",
      "  Batch [20/1299] D_loss: -0.0207, G_loss: 0.4388\n",
      "  Batch [30/1299] D_loss: -0.1288, G_loss: 0.3346\n",
      "  Batch [40/1299] D_loss: -1.0556, G_loss: -2.3573\n",
      "  Batch [50/1299] D_loss: -0.5055, G_loss: 0.0714\n",
      "  Batch [60/1299] D_loss: -0.1457, G_loss: 0.2274\n",
      "  Batch [70/1299] D_loss: -0.2603, G_loss: 0.1905\n",
      "  Batch [80/1299] D_loss: -0.0489, G_loss: 0.2377\n",
      "  Batch [90/1299] D_loss: -0.2903, G_loss: 0.2022\n",
      "  Batch [100/1299] D_loss: -0.0207, G_loss: 0.1800\n",
      "  Batch [110/1299] D_loss: -0.1392, G_loss: 0.3747\n",
      "  Batch [120/1299] D_loss: -1.8467, G_loss: -0.5446\n",
      "  Batch [130/1299] D_loss: -0.2882, G_loss: 0.1225\n",
      "  Batch [140/1299] D_loss: -0.0389, G_loss: 0.1249\n",
      "  Batch [150/1299] D_loss: -0.0250, G_loss: 0.0880\n",
      "  Batch [160/1299] D_loss: -0.1173, G_loss: 0.2801\n",
      "  Batch [170/1299] D_loss: -0.1526, G_loss: 0.3989\n",
      "  Batch [180/1299] D_loss: -0.1004, G_loss: 0.4902\n",
      "  Batch [190/1299] D_loss: 0.0051, G_loss: 0.3214\n",
      "  Batch [200/1299] D_loss: -0.0313, G_loss: 0.1737\n",
      "  Batch [210/1299] D_loss: -1.3867, G_loss: -1.4323\n",
      "  Batch [220/1299] D_loss: -0.3693, G_loss: -0.2427\n",
      "  Batch [230/1299] D_loss: -0.0772, G_loss: 0.1078\n",
      "  Batch [240/1299] D_loss: -1.3495, G_loss: -1.2009\n",
      "  Batch [250/1299] D_loss: -1.0112, G_loss: -0.8299\n",
      "  Batch [260/1299] D_loss: -0.0249, G_loss: 0.1469\n",
      "  Batch [270/1299] D_loss: -0.0472, G_loss: 0.2805\n",
      "  Batch [280/1299] D_loss: -0.0523, G_loss: 0.3110\n",
      "  Batch [290/1299] D_loss: 0.0511, G_loss: 0.5023\n",
      "  Batch [300/1299] D_loss: -0.0434, G_loss: 0.4627\n",
      "  Batch [310/1299] D_loss: 0.0474, G_loss: 0.2636\n",
      "  Batch [320/1299] D_loss: -0.1462, G_loss: 0.3890\n",
      "  Batch [330/1299] D_loss: -0.0855, G_loss: 0.0752\n",
      "  Batch [340/1299] D_loss: -0.5304, G_loss: -1.2086\n",
      "  Batch [350/1299] D_loss: -0.1005, G_loss: 0.1066\n",
      "  Batch [360/1299] D_loss: -0.1376, G_loss: 0.1771\n",
      "  Batch [370/1299] D_loss: -0.0208, G_loss: 0.3829\n",
      "  Batch [380/1299] D_loss: -0.0326, G_loss: 0.4464\n",
      "  Batch [390/1299] D_loss: -0.0622, G_loss: 0.5497\n",
      "  Batch [400/1299] D_loss: -0.0696, G_loss: 0.4731\n",
      "  Batch [410/1299] D_loss: -3.6947, G_loss: -5.2979\n",
      "  Batch [420/1299] D_loss: -0.0495, G_loss: 0.1971\n",
      "  Batch [430/1299] D_loss: -0.0270, G_loss: 0.3438\n",
      "  Batch [440/1299] D_loss: -0.1018, G_loss: 0.4471\n",
      "  Batch [450/1299] D_loss: 0.0230, G_loss: 0.3934\n",
      "  Batch [460/1299] D_loss: -0.0749, G_loss: 0.3404\n",
      "  Batch [470/1299] D_loss: -0.3960, G_loss: -1.4263\n",
      "  Batch [480/1299] D_loss: -0.3318, G_loss: -0.0261\n",
      "  Batch [490/1299] D_loss: -0.0989, G_loss: 0.3256\n",
      "  Batch [500/1299] D_loss: -0.0841, G_loss: 0.4169\n",
      "  Batch [510/1299] D_loss: 0.0692, G_loss: 0.3192\n",
      "  Batch [520/1299] D_loss: -0.1118, G_loss: 0.3730\n",
      "  Batch [530/1299] D_loss: -0.0240, G_loss: 0.4591\n",
      "  Batch [540/1299] D_loss: -0.0331, G_loss: 0.4144\n",
      "  Batch [550/1299] D_loss: -0.0293, G_loss: 0.0702\n",
      "  Batch [560/1299] D_loss: -0.6588, G_loss: -0.1831\n",
      "  Batch [570/1299] D_loss: -0.2435, G_loss: 0.0152\n",
      "  Batch [580/1299] D_loss: -0.1985, G_loss: 0.0286\n",
      "  Batch [590/1299] D_loss: -0.0334, G_loss: 0.3868\n",
      "  Batch [600/1299] D_loss: -0.1487, G_loss: 0.5052\n",
      "  Batch [610/1299] D_loss: -0.1835, G_loss: 0.5384\n",
      "  Batch [620/1299] D_loss: -0.0351, G_loss: 0.4282\n",
      "  Batch [630/1299] D_loss: -2.1997, G_loss: -0.5502\n",
      "  Batch [640/1299] D_loss: -0.0280, G_loss: 0.0730\n",
      "  Batch [650/1299] D_loss: -1.3863, G_loss: 0.0134\n",
      "  Batch [660/1299] D_loss: -0.1854, G_loss: 0.2297\n",
      "  Batch [670/1299] D_loss: -0.0766, G_loss: 0.2854\n",
      "  Batch [680/1299] D_loss: -0.1694, G_loss: 0.4991\n",
      "  Batch [690/1299] D_loss: -0.2248, G_loss: 0.5043\n",
      "  Batch [700/1299] D_loss: -0.1036, G_loss: 0.5057\n",
      "  Batch [710/1299] D_loss: -0.0156, G_loss: 0.2945\n",
      "  Batch [720/1299] D_loss: -0.7724, G_loss: -1.0637\n",
      "  Batch [730/1299] D_loss: -0.1018, G_loss: 0.0663\n",
      "  Batch [740/1299] D_loss: -0.0827, G_loss: 0.2791\n",
      "  Batch [750/1299] D_loss: -0.0463, G_loss: 0.4297\n",
      "  Batch [760/1299] D_loss: -0.0606, G_loss: 0.3693\n",
      "  Batch [770/1299] D_loss: -0.0223, G_loss: 0.4585\n",
      "  Batch [780/1299] D_loss: -0.0463, G_loss: 0.1799\n",
      "  Batch [790/1299] D_loss: -0.2052, G_loss: -0.2039\n",
      "  Batch [800/1299] D_loss: -0.0126, G_loss: 0.1723\n",
      "  Batch [810/1299] D_loss: -0.1451, G_loss: 0.0668\n",
      "  Batch [820/1299] D_loss: -0.0723, G_loss: 0.2873\n",
      "  Batch [830/1299] D_loss: -0.1095, G_loss: 0.4403\n",
      "  Batch [840/1299] D_loss: -0.2211, G_loss: 0.5352\n",
      "  Batch [850/1299] D_loss: -0.0766, G_loss: 0.4876\n",
      "  Batch [860/1299] D_loss: -0.0232, G_loss: 0.5302\n",
      "  Batch [870/1299] D_loss: -0.0148, G_loss: 0.2425\n",
      "  Batch [880/1299] D_loss: -1.0133, G_loss: -5.1348\n",
      "  Batch [890/1299] D_loss: -0.4449, G_loss: 0.0988\n",
      "  Batch [900/1299] D_loss: -0.0910, G_loss: 0.1778\n",
      "  Batch [910/1299] D_loss: -0.0680, G_loss: 0.1533\n",
      "  Batch [920/1299] D_loss: -0.0578, G_loss: 0.2911\n",
      "  Batch [930/1299] D_loss: -0.1170, G_loss: 0.4625\n",
      "  Batch [940/1299] D_loss: -0.1015, G_loss: 0.6620\n",
      "  Batch [950/1299] D_loss: -0.0320, G_loss: 0.5004\n",
      "  Batch [960/1299] D_loss: -0.1752, G_loss: 0.5027\n",
      "  Batch [970/1299] D_loss: 0.0137, G_loss: 0.2980\n",
      "  Batch [980/1299] D_loss: -0.6021, G_loss: -0.5506\n",
      "  Batch [990/1299] D_loss: -0.0200, G_loss: 0.1065\n",
      "  Batch [1000/1299] D_loss: -0.0486, G_loss: 0.2164\n",
      "  Batch [1010/1299] D_loss: -0.1069, G_loss: 0.3506\n",
      "  Batch [1020/1299] D_loss: -0.0971, G_loss: 0.2766\n",
      "  Batch [1030/1299] D_loss: -0.1896, G_loss: 0.3537\n",
      "  Batch [1040/1299] D_loss: -0.9855, G_loss: -2.6269\n",
      "  Batch [1050/1299] D_loss: -1.2501, G_loss: -0.5378\n",
      "  Batch [1060/1299] D_loss: -0.1368, G_loss: 0.2452\n",
      "  Batch [1070/1299] D_loss: -0.1482, G_loss: 0.3836\n",
      "  Batch [1080/1299] D_loss: -0.0658, G_loss: 0.4980\n",
      "  Batch [1090/1299] D_loss: -0.1639, G_loss: 0.5487\n",
      "  Batch [1100/1299] D_loss: -0.0433, G_loss: 0.2073\n",
      "  Batch [1110/1299] D_loss: -0.4406, G_loss: -0.0096\n",
      "  Batch [1120/1299] D_loss: -0.4567, G_loss: -0.7308\n",
      "  Batch [1130/1299] D_loss: -0.0247, G_loss: 0.0868\n",
      "  Batch [1140/1299] D_loss: -0.0097, G_loss: 0.2278\n",
      "  Batch [1150/1299] D_loss: -0.1053, G_loss: 0.3742\n",
      "  Batch [1160/1299] D_loss: -0.0690, G_loss: 0.3782\n",
      "  Batch [1170/1299] D_loss: -0.1103, G_loss: 0.3660\n",
      "  Batch [1180/1299] D_loss: -1.9663, G_loss: -3.0865\n",
      "  Batch [1190/1299] D_loss: -0.0444, G_loss: -0.1675\n",
      "  Batch [1200/1299] D_loss: -0.6796, G_loss: -1.5783\n",
      "  Batch [1210/1299] D_loss: 0.0453, G_loss: 0.0776\n",
      "  Batch [1220/1299] D_loss: -0.0653, G_loss: 0.2120\n",
      "  Batch [1230/1299] D_loss: -0.0694, G_loss: 0.5960\n",
      "  Batch [1240/1299] D_loss: -0.1124, G_loss: 0.5822\n",
      "  Batch [1250/1299] D_loss: -0.0605, G_loss: 0.4406\n",
      "  Batch [1260/1299] D_loss: 0.0705, G_loss: 0.2535\n",
      "  Batch [1270/1299] D_loss: -0.6460, G_loss: -1.9135\n",
      "  Batch [1280/1299] D_loss: -0.4699, G_loss: -0.9915\n",
      "  Batch [1290/1299] D_loss: -0.2535, G_loss: -0.0941\n",
      "\n",
      "Epoch 19 Summary:\n",
      "  Average D_loss: -0.1476\n",
      "  Average G_loss: -0.0549\n",
      "\n",
      "Epoch [20/100]\n",
      "  Batch [0/1299] D_loss: -0.1490, G_loss: -0.0414\n",
      "  Batch [10/1299] D_loss: -0.0339, G_loss: 0.1475\n",
      "  Batch [20/1299] D_loss: -0.7921, G_loss: -0.2247\n",
      "  Batch [30/1299] D_loss: -0.6742, G_loss: -0.0500\n",
      "  Batch [40/1299] D_loss: -0.0607, G_loss: 0.2759\n",
      "  Batch [50/1299] D_loss: -0.2315, G_loss: 0.5000\n",
      "  Batch [60/1299] D_loss: -0.1707, G_loss: 0.5387\n",
      "  Batch [70/1299] D_loss: -0.1490, G_loss: 0.6009\n",
      "  Batch [80/1299] D_loss: 0.0309, G_loss: 0.6711\n",
      "  Batch [90/1299] D_loss: 0.0169, G_loss: 0.3723\n",
      "  Batch [100/1299] D_loss: -1.7906, G_loss: -0.7678\n",
      "  Batch [110/1299] D_loss: -0.3172, G_loss: -0.5480\n",
      "  Batch [120/1299] D_loss: -0.0032, G_loss: 0.0965\n",
      "  Batch [130/1299] D_loss: -0.0510, G_loss: 0.0927\n",
      "  Batch [140/1299] D_loss: -0.7211, G_loss: -0.0118\n",
      "  Batch [150/1299] D_loss: 0.0075, G_loss: 0.1797\n",
      "  Batch [160/1299] D_loss: -0.2095, G_loss: 0.4232\n",
      "  Batch [170/1299] D_loss: 0.0607, G_loss: 0.4679\n",
      "  Batch [180/1299] D_loss: -0.1599, G_loss: 0.5941\n",
      "  Batch [190/1299] D_loss: 0.0130, G_loss: 0.5563\n",
      "  Batch [200/1299] D_loss: -0.0984, G_loss: 0.3331\n",
      "  Batch [210/1299] D_loss: -3.0265, G_loss: -4.9722\n",
      "  Batch [220/1299] D_loss: -0.0090, G_loss: 0.1136\n",
      "  Batch [230/1299] D_loss: -0.0751, G_loss: 0.2856\n",
      "  Batch [240/1299] D_loss: -0.0574, G_loss: 0.4136\n",
      "  Batch [250/1299] D_loss: -0.0270, G_loss: 0.3617\n",
      "  Batch [260/1299] D_loss: -0.5983, G_loss: -0.6253\n",
      "  Batch [270/1299] D_loss: -0.3400, G_loss: 0.0180\n",
      "  Batch [280/1299] D_loss: -2.0174, G_loss: -2.3978\n",
      "  Batch [290/1299] D_loss: -0.0117, G_loss: 0.1121\n",
      "  Batch [300/1299] D_loss: -0.0625, G_loss: 0.2769\n",
      "  Batch [310/1299] D_loss: 0.0079, G_loss: 0.3454\n",
      "  Batch [320/1299] D_loss: -0.0151, G_loss: 0.2728\n",
      "  Batch [330/1299] D_loss: -0.0859, G_loss: 0.3395\n",
      "  Batch [340/1299] D_loss: -0.0645, G_loss: 0.2471\n",
      "  Batch [350/1299] D_loss: -0.6056, G_loss: -0.5773\n",
      "  Batch [360/1299] D_loss: -0.3081, G_loss: -0.4028\n",
      "  Batch [370/1299] D_loss: -0.0005, G_loss: 0.1758\n",
      "  Batch [380/1299] D_loss: -0.1169, G_loss: 0.2680\n",
      "  Batch [390/1299] D_loss: -0.1040, G_loss: 0.4577\n",
      "  Batch [400/1299] D_loss: -0.0763, G_loss: 0.4116\n",
      "  Batch [410/1299] D_loss: -0.0275, G_loss: 0.3114\n",
      "  Batch [420/1299] D_loss: -0.1724, G_loss: 0.0188\n",
      "  Batch [430/1299] D_loss: 0.0040, G_loss: 0.0904\n",
      "  Batch [440/1299] D_loss: 0.0200, G_loss: 0.0838\n",
      "  Batch [450/1299] D_loss: 0.0133, G_loss: 0.0887\n",
      "  Batch [460/1299] D_loss: -0.0296, G_loss: 0.0524\n",
      "  Batch [470/1299] D_loss: -0.0050, G_loss: 0.2182\n",
      "  Batch [480/1299] D_loss: -0.0031, G_loss: 0.0374\n",
      "  Batch [490/1299] D_loss: -0.0779, G_loss: 0.0383\n",
      "  Batch [500/1299] D_loss: -0.7538, G_loss: -0.6137\n",
      "  Batch [510/1299] D_loss: -0.0638, G_loss: 0.2912\n",
      "  Batch [520/1299] D_loss: -0.0358, G_loss: 0.3680\n",
      "  Batch [530/1299] D_loss: -0.0770, G_loss: 0.5928\n",
      "  Batch [540/1299] D_loss: -0.2182, G_loss: 0.6054\n",
      "  Batch [550/1299] D_loss: -0.0668, G_loss: 0.5392\n",
      "  Batch [560/1299] D_loss: -0.2414, G_loss: -0.8393\n",
      "  Batch [570/1299] D_loss: -0.3649, G_loss: -0.1093\n",
      "  Batch [580/1299] D_loss: -0.4421, G_loss: -0.1122\n",
      "  Batch [590/1299] D_loss: -0.0259, G_loss: 0.1348\n",
      "  Batch [600/1299] D_loss: 0.0063, G_loss: 0.2826\n",
      "  Batch [610/1299] D_loss: -0.9666, G_loss: -1.3082\n",
      "  Batch [620/1299] D_loss: -0.2213, G_loss: -0.2980\n",
      "  Batch [630/1299] D_loss: -0.1777, G_loss: 0.1126\n",
      "  Batch [640/1299] D_loss: -0.1164, G_loss: -0.1459\n",
      "  Batch [650/1299] D_loss: -0.0200, G_loss: 0.1927\n",
      "  Batch [660/1299] D_loss: -0.0346, G_loss: 0.3764\n",
      "  Batch [670/1299] D_loss: -0.0842, G_loss: 0.4420\n",
      "  Batch [680/1299] D_loss: 0.0047, G_loss: 0.5017\n",
      "  Batch [690/1299] D_loss: 0.1141, G_loss: 0.3881\n",
      "  Batch [700/1299] D_loss: -0.1562, G_loss: 0.4223\n",
      "  Batch [710/1299] D_loss: -4.4324, G_loss: -9.4702\n",
      "  Batch [720/1299] D_loss: -0.0148, G_loss: 0.1092\n",
      "  Batch [730/1299] D_loss: -0.4501, G_loss: -0.5181\n",
      "  Batch [740/1299] D_loss: -2.1251, G_loss: -3.7231\n",
      "  Batch [750/1299] D_loss: -0.1521, G_loss: -0.0273\n",
      "  Batch [760/1299] D_loss: -0.5562, G_loss: 0.0298\n",
      "  Batch [770/1299] D_loss: -0.0727, G_loss: -0.1017\n",
      "  Batch [780/1299] D_loss: -0.4269, G_loss: -0.0038\n",
      "  Batch [790/1299] D_loss: -0.5441, G_loss: -0.2229\n",
      "  Batch [800/1299] D_loss: 0.0109, G_loss: 0.2645\n",
      "  Batch [810/1299] D_loss: -0.2281, G_loss: 0.5859\n",
      "  Batch [820/1299] D_loss: -0.1832, G_loss: 0.6634\n",
      "  Batch [830/1299] D_loss: -0.2203, G_loss: 0.8594\n",
      "  Batch [840/1299] D_loss: -0.0757, G_loss: 0.4736\n",
      "  Batch [850/1299] D_loss: -0.0184, G_loss: 0.2762\n",
      "  Batch [860/1299] D_loss: -0.3702, G_loss: -0.6911\n",
      "  Batch [870/1299] D_loss: -0.1775, G_loss: -0.1092\n",
      "  Batch [880/1299] D_loss: -0.0582, G_loss: 0.2529\n",
      "  Batch [890/1299] D_loss: -0.1190, G_loss: 0.3800\n",
      "  Batch [900/1299] D_loss: -0.1313, G_loss: 0.4748\n",
      "  Batch [910/1299] D_loss: -0.1437, G_loss: 0.4350\n",
      "  Batch [920/1299] D_loss: 0.0185, G_loss: 0.3194\n",
      "  Batch [930/1299] D_loss: -2.3404, G_loss: -2.1115\n",
      "  Batch [940/1299] D_loss: -0.3410, G_loss: -0.1441\n",
      "  Batch [950/1299] D_loss: -1.4563, G_loss: -2.3019\n",
      "  Batch [960/1299] D_loss: -0.4783, G_loss: 0.1219\n",
      "  Batch [970/1299] D_loss: -0.0142, G_loss: 0.3124\n",
      "  Batch [980/1299] D_loss: -0.0580, G_loss: 0.3738\n",
      "  Batch [990/1299] D_loss: -0.0930, G_loss: 0.3784\n",
      "  Batch [1000/1299] D_loss: -0.1089, G_loss: 0.2881\n",
      "  Batch [1010/1299] D_loss: -0.5445, G_loss: -1.2984\n",
      "  Batch [1020/1299] D_loss: -0.4390, G_loss: -0.0969\n",
      "  Batch [1030/1299] D_loss: -0.1046, G_loss: 0.4246\n",
      "  Batch [1040/1299] D_loss: -0.0699, G_loss: 0.4239\n",
      "  Batch [1050/1299] D_loss: -0.1394, G_loss: 0.3078\n",
      "  Batch [1060/1299] D_loss: -2.3818, G_loss: -2.7719\n",
      "  Batch [1070/1299] D_loss: -0.0017, G_loss: 0.1864\n",
      "  Batch [1080/1299] D_loss: 0.0470, G_loss: 0.2838\n",
      "  Batch [1090/1299] D_loss: -0.0343, G_loss: 0.3190\n",
      "  Batch [1100/1299] D_loss: -0.0817, G_loss: 0.4818\n",
      "  Batch [1110/1299] D_loss: -0.0445, G_loss: 0.4003\n",
      "  Batch [1120/1299] D_loss: -0.1512, G_loss: 0.0493\n",
      "  Batch [1130/1299] D_loss: -0.0107, G_loss: 0.1762\n",
      "  Batch [1140/1299] D_loss: -0.1575, G_loss: 0.1645\n",
      "  Batch [1150/1299] D_loss: -0.0168, G_loss: 0.1072\n",
      "  Batch [1160/1299] D_loss: 0.0090, G_loss: 0.2572\n",
      "  Batch [1170/1299] D_loss: -0.1711, G_loss: 0.3746\n",
      "  Batch [1180/1299] D_loss: -0.0083, G_loss: 0.3458\n",
      "  Batch [1190/1299] D_loss: -0.0365, G_loss: 0.2820\n",
      "  Batch [1200/1299] D_loss: 0.0128, G_loss: 0.2560\n",
      "  Batch [1210/1299] D_loss: -0.1713, G_loss: -0.2012\n",
      "  Batch [1220/1299] D_loss: 0.0070, G_loss: 0.0447\n",
      "  Batch [1230/1299] D_loss: -0.0180, G_loss: 0.1005\n",
      "  Batch [1240/1299] D_loss: -0.0225, G_loss: 0.1030\n",
      "  Batch [1250/1299] D_loss: -1.0233, G_loss: -0.8796\n",
      "  Batch [1260/1299] D_loss: -0.1646, G_loss: -0.0447\n",
      "  Batch [1270/1299] D_loss: -0.0685, G_loss: 0.2874\n",
      "  Batch [1280/1299] D_loss: -0.0224, G_loss: 0.3325\n",
      "  Batch [1290/1299] D_loss: -1.9468, G_loss: -1.5984\n",
      "\n",
      "Epoch 20 Summary:\n",
      "  Average D_loss: -0.1269\n",
      "  Average G_loss: -0.0453\n",
      "\n",
      "Epoch [21/100]\n",
      "  Batch [0/1299] D_loss: -0.3703, G_loss: -0.0103\n",
      "  Batch [10/1299] D_loss: -0.3085, G_loss: -0.1927\n",
      "  Batch [20/1299] D_loss: -0.0495, G_loss: 0.1208\n",
      "  Batch [30/1299] D_loss: -0.3711, G_loss: -0.1076\n",
      "  Batch [40/1299] D_loss: -0.0303, G_loss: 0.2223\n",
      "  Batch [50/1299] D_loss: -0.1042, G_loss: 0.4276\n",
      "  Batch [60/1299] D_loss: -0.0681, G_loss: 0.4335\n",
      "  Batch [70/1299] D_loss: -0.2262, G_loss: 0.5499\n",
      "  Batch [80/1299] D_loss: -0.1114, G_loss: 0.4960\n",
      "  Batch [90/1299] D_loss: -0.0755, G_loss: 0.2635\n",
      "  Batch [100/1299] D_loss: -0.5746, G_loss: -0.3439\n",
      "  Batch [110/1299] D_loss: -0.1869, G_loss: 0.0668\n",
      "  Batch [120/1299] D_loss: -0.1339, G_loss: 0.0907\n",
      "  Batch [130/1299] D_loss: 0.0300, G_loss: 0.2683\n",
      "  Batch [140/1299] D_loss: -0.0531, G_loss: 0.3117\n",
      "  Batch [150/1299] D_loss: -0.0395, G_loss: 0.3501\n",
      "  Batch [160/1299] D_loss: -0.1619, G_loss: 0.2773\n",
      "  Batch [170/1299] D_loss: -1.6806, G_loss: -4.5961\n",
      "  Batch [180/1299] D_loss: -0.6994, G_loss: -0.0057\n",
      "  Batch [190/1299] D_loss: -0.1237, G_loss: 0.1755\n",
      "  Batch [200/1299] D_loss: -0.0925, G_loss: 0.3486\n",
      "  Batch [210/1299] D_loss: -0.0929, G_loss: 0.4349\n",
      "  Batch [220/1299] D_loss: -0.1149, G_loss: 0.6044\n",
      "  Batch [230/1299] D_loss: 0.0664, G_loss: 0.3520\n",
      "  Batch [240/1299] D_loss: -0.0306, G_loss: 0.3551\n",
      "  Batch [250/1299] D_loss: -0.0945, G_loss: 0.2028\n",
      "  Batch [260/1299] D_loss: -0.7701, G_loss: -0.8529\n",
      "  Batch [270/1299] D_loss: -0.1896, G_loss: -0.0391\n",
      "  Batch [280/1299] D_loss: -0.0537, G_loss: 0.1540\n",
      "  Batch [290/1299] D_loss: -0.0190, G_loss: 0.2461\n",
      "  Batch [300/1299] D_loss: -0.0463, G_loss: 0.3084\n",
      "  Batch [310/1299] D_loss: -0.1254, G_loss: 0.3735\n",
      "  Batch [320/1299] D_loss: -0.0645, G_loss: 0.3371\n",
      "  Batch [330/1299] D_loss: -0.1595, G_loss: 0.1129\n",
      "  Batch [340/1299] D_loss: -0.9453, G_loss: -0.3650\n",
      "  Batch [350/1299] D_loss: 0.0269, G_loss: 0.2527\n",
      "  Batch [360/1299] D_loss: -0.0852, G_loss: 0.4076\n",
      "  Batch [370/1299] D_loss: -0.0464, G_loss: 0.4915\n",
      "  Batch [380/1299] D_loss: -0.1207, G_loss: 0.4158\n",
      "  Batch [390/1299] D_loss: -0.1007, G_loss: 0.3734\n",
      "  Batch [400/1299] D_loss: -2.7961, G_loss: -2.3979\n",
      "  Batch [410/1299] D_loss: -0.0081, G_loss: 0.0784\n",
      "  Batch [420/1299] D_loss: 0.0081, G_loss: 0.2352\n",
      "  Batch [430/1299] D_loss: -0.1310, G_loss: 0.2988\n",
      "  Batch [440/1299] D_loss: -0.0781, G_loss: 0.3471\n",
      "  Batch [450/1299] D_loss: -0.1037, G_loss: 0.2984\n",
      "  Batch [460/1299] D_loss: -0.5596, G_loss: -1.9256\n",
      "  Batch [470/1299] D_loss: -0.9919, G_loss: -0.8199\n",
      "  Batch [480/1299] D_loss: -0.0268, G_loss: 0.1797\n",
      "  Batch [490/1299] D_loss: -0.0828, G_loss: 0.3961\n",
      "  Batch [500/1299] D_loss: -0.0902, G_loss: 0.4550\n",
      "  Batch [510/1299] D_loss: -0.0959, G_loss: 0.3797\n",
      "  Batch [520/1299] D_loss: -0.0036, G_loss: 0.2709\n",
      "  Batch [530/1299] D_loss: -0.1430, G_loss: -0.1508\n",
      "  Batch [540/1299] D_loss: -1.8307, G_loss: -2.7835\n",
      "  Batch [550/1299] D_loss: -0.0532, G_loss: 0.2125\n",
      "  Batch [560/1299] D_loss: -0.0599, G_loss: 0.3426\n",
      "  Batch [570/1299] D_loss: -0.1624, G_loss: 0.3365\n",
      "  Batch [580/1299] D_loss: -0.1536, G_loss: 0.5050\n",
      "  Batch [590/1299] D_loss: -0.0995, G_loss: 0.4505\n",
      "  Batch [600/1299] D_loss: -0.1237, G_loss: 0.3227\n",
      "  Batch [610/1299] D_loss: -1.4995, G_loss: -1.2360\n",
      "  Batch [620/1299] D_loss: -0.1813, G_loss: 0.1761\n",
      "  Batch [630/1299] D_loss: -0.4639, G_loss: -0.8769\n",
      "  Batch [640/1299] D_loss: -0.3530, G_loss: -0.0619\n",
      "  Batch [650/1299] D_loss: -0.9578, G_loss: -2.1765\n",
      "  Batch [660/1299] D_loss: -0.0074, G_loss: 0.1581\n",
      "  Batch [670/1299] D_loss: -0.0743, G_loss: 0.3660\n",
      "  Batch [680/1299] D_loss: -0.0609, G_loss: 0.6265\n",
      "  Batch [690/1299] D_loss: -0.2080, G_loss: 0.5791\n",
      "  Batch [700/1299] D_loss: -0.1598, G_loss: 0.4818\n",
      "  Batch [710/1299] D_loss: -0.1426, G_loss: 0.3724\n",
      "  Batch [720/1299] D_loss: -1.7984, G_loss: -0.2200\n",
      "  Batch [730/1299] D_loss: -0.4022, G_loss: 0.0914\n",
      "  Batch [740/1299] D_loss: -0.1761, G_loss: 0.0294\n",
      "  Batch [750/1299] D_loss: -0.0176, G_loss: 0.2798\n",
      "  Batch [760/1299] D_loss: -0.0361, G_loss: 0.2721\n",
      "  Batch [770/1299] D_loss: 0.0199, G_loss: 0.3700\n",
      "  Batch [780/1299] D_loss: -0.0058, G_loss: 0.4099\n",
      "  Batch [790/1299] D_loss: -0.0852, G_loss: 0.3414\n",
      "  Batch [800/1299] D_loss: -0.0167, G_loss: 0.2687\n",
      "  Batch [810/1299] D_loss: -1.0851, G_loss: -1.9635\n",
      "  Batch [820/1299] D_loss: -0.1888, G_loss: -0.4666\n",
      "  Batch [830/1299] D_loss: -0.0357, G_loss: 0.1329\n",
      "  Batch [840/1299] D_loss: -0.0233, G_loss: 0.2833\n",
      "  Batch [850/1299] D_loss: -0.0145, G_loss: 0.2551\n",
      "  Batch [860/1299] D_loss: -0.0591, G_loss: 0.4380\n",
      "  Batch [870/1299] D_loss: -0.0840, G_loss: 0.4024\n",
      "  Batch [880/1299] D_loss: 0.7468, G_loss: -3.3888\n",
      "  Batch [890/1299] D_loss: -0.0275, G_loss: 0.1602\n",
      "  Batch [900/1299] D_loss: -0.0209, G_loss: 0.2297\n",
      "  Batch [910/1299] D_loss: -0.0895, G_loss: 0.2557\n",
      "  Batch [920/1299] D_loss: -0.1053, G_loss: 0.3288\n",
      "  Batch [930/1299] D_loss: 0.0358, G_loss: 0.1870\n",
      "  Batch [940/1299] D_loss: -0.0373, G_loss: 0.2000\n",
      "  Batch [950/1299] D_loss: -0.1425, G_loss: 0.1157\n",
      "  Batch [960/1299] D_loss: -0.0297, G_loss: 0.1325\n",
      "  Batch [970/1299] D_loss: -0.5286, G_loss: -1.5800\n",
      "  Batch [980/1299] D_loss: -0.0273, G_loss: 0.1371\n",
      "  Batch [990/1299] D_loss: -0.0411, G_loss: 0.1597\n",
      "  Batch [1000/1299] D_loss: -0.0943, G_loss: 0.2912\n",
      "  Batch [1010/1299] D_loss: -0.0885, G_loss: 0.3201\n",
      "  Batch [1020/1299] D_loss: -0.0973, G_loss: 0.2184\n",
      "  Batch [1030/1299] D_loss: -0.2294, G_loss: -0.4357\n",
      "  Batch [1040/1299] D_loss: -0.0529, G_loss: 0.1801\n",
      "  Batch [1050/1299] D_loss: -0.0136, G_loss: 0.2588\n",
      "  Batch [1060/1299] D_loss: -0.0991, G_loss: 0.3975\n",
      "  Batch [1070/1299] D_loss: -0.0037, G_loss: 0.3199\n",
      "  Batch [1080/1299] D_loss: 0.0813, G_loss: 0.3231\n",
      "  Batch [1090/1299] D_loss: -2.0888, G_loss: -3.2855\n",
      "  Batch [1100/1299] D_loss: 0.0073, G_loss: 0.0533\n",
      "  Batch [1110/1299] D_loss: -0.0212, G_loss: 0.0740\n",
      "  Batch [1120/1299] D_loss: -0.0507, G_loss: 0.0952\n",
      "  Batch [1130/1299] D_loss: -0.1615, G_loss: -0.1951\n",
      "  Batch [1140/1299] D_loss: -0.1560, G_loss: 0.1519\n",
      "  Batch [1150/1299] D_loss: -1.8926, G_loss: -2.7590\n",
      "  Batch [1160/1299] D_loss: -0.0624, G_loss: 0.4163\n",
      "  Batch [1170/1299] D_loss: -0.0941, G_loss: 0.4975\n",
      "  Batch [1180/1299] D_loss: -0.1518, G_loss: 0.6615\n",
      "  Batch [1190/1299] D_loss: -0.0463, G_loss: 0.4849\n",
      "  Batch [1200/1299] D_loss: -0.1733, G_loss: 0.5516\n",
      "  Batch [1210/1299] D_loss: -0.0749, G_loss: 0.2935\n",
      "  Batch [1220/1299] D_loss: 0.2440, G_loss: -0.9502\n",
      "  Batch [1230/1299] D_loss: -0.2708, G_loss: -0.6757\n",
      "  Batch [1240/1299] D_loss: -0.0293, G_loss: 0.2018\n",
      "  Batch [1250/1299] D_loss: -0.0652, G_loss: 0.3583\n",
      "  Batch [1260/1299] D_loss: -0.0534, G_loss: 0.4054\n",
      "  Batch [1270/1299] D_loss: -0.0374, G_loss: 0.3629\n",
      "  Batch [1280/1299] D_loss: -0.0818, G_loss: 0.4205\n",
      "  Batch [1290/1299] D_loss: -0.0631, G_loss: 0.2606\n",
      "\n",
      "Epoch 21 Summary:\n",
      "  Average D_loss: -0.1319\n",
      "  Average G_loss: -0.0643\n",
      "\n",
      "Epoch [22/100]\n",
      "  Batch [0/1299] D_loss: -0.6835, G_loss: -1.8273\n",
      "  Batch [10/1299] D_loss: -0.7088, G_loss: -0.4362\n",
      "  Batch [20/1299] D_loss: -0.2136, G_loss: 0.0704\n",
      "  Batch [30/1299] D_loss: -0.0668, G_loss: 0.5023\n",
      "  Batch [40/1299] D_loss: -0.0761, G_loss: 0.4913\n",
      "  Batch [50/1299] D_loss: -0.1516, G_loss: 0.4985\n",
      "  Batch [60/1299] D_loss: -0.0591, G_loss: 0.4133\n",
      "  Batch [70/1299] D_loss: 0.0368, G_loss: 0.2973\n",
      "  Batch [80/1299] D_loss: -0.3169, G_loss: 0.0507\n",
      "  Batch [90/1299] D_loss: -0.1538, G_loss: 0.0584\n",
      "  Batch [100/1299] D_loss: -0.6947, G_loss: -1.1879\n",
      "  Batch [110/1299] D_loss: -0.3755, G_loss: -0.2485\n",
      "  Batch [120/1299] D_loss: -0.5357, G_loss: -0.1359\n",
      "  Batch [130/1299] D_loss: -0.3418, G_loss: -0.2844\n",
      "  Batch [140/1299] D_loss: -0.9836, G_loss: -0.6349\n",
      "  Batch [150/1299] D_loss: -0.1074, G_loss: 0.3404\n",
      "  Batch [160/1299] D_loss: -0.1118, G_loss: 0.6187\n",
      "  Batch [170/1299] D_loss: -0.2461, G_loss: 0.8695\n",
      "  Batch [180/1299] D_loss: -0.1563, G_loss: 0.6648\n",
      "  Batch [190/1299] D_loss: -0.1011, G_loss: 0.5344\n",
      "  Batch [200/1299] D_loss: -0.1101, G_loss: 0.4356\n",
      "  Batch [210/1299] D_loss: -0.4812, G_loss: -0.6164\n",
      "  Batch [220/1299] D_loss: -0.4851, G_loss: -0.6299\n",
      "  Batch [230/1299] D_loss: -0.8290, G_loss: -0.9694\n",
      "  Batch [240/1299] D_loss: -0.7226, G_loss: -0.0428\n",
      "  Batch [250/1299] D_loss: -0.6042, G_loss: -0.6901\n",
      "  Batch [260/1299] D_loss: -0.0027, G_loss: 0.3506\n",
      "  Batch [270/1299] D_loss: -0.1525, G_loss: 0.5030\n",
      "  Batch [280/1299] D_loss: -0.1802, G_loss: 0.7105\n",
      "  Batch [290/1299] D_loss: -0.0751, G_loss: 0.5067\n",
      "  Batch [300/1299] D_loss: -0.0552, G_loss: 0.4165\n",
      "  Batch [310/1299] D_loss: 0.0419, G_loss: 0.3091\n",
      "  Batch [320/1299] D_loss: -0.0950, G_loss: 0.2742\n",
      "  Batch [330/1299] D_loss: 0.0118, G_loss: 0.0539\n",
      "  Batch [340/1299] D_loss: -0.3152, G_loss: -0.6649\n",
      "  Batch [350/1299] D_loss: -1.3723, G_loss: -0.0302\n",
      "  Batch [360/1299] D_loss: -0.5145, G_loss: -1.0830\n",
      "  Batch [370/1299] D_loss: -0.0431, G_loss: 0.2398\n",
      "  Batch [380/1299] D_loss: -0.1939, G_loss: 0.4120\n",
      "  Batch [390/1299] D_loss: -0.1345, G_loss: 0.5482\n",
      "  Batch [400/1299] D_loss: -0.1057, G_loss: 0.4006\n",
      "  Batch [410/1299] D_loss: -0.0267, G_loss: 0.4571\n",
      "  Batch [420/1299] D_loss: -0.1226, G_loss: 0.3755\n",
      "  Batch [430/1299] D_loss: -1.8262, G_loss: -2.7107\n",
      "  Batch [440/1299] D_loss: -0.6024, G_loss: -1.0096\n",
      "  Batch [450/1299] D_loss: -0.0459, G_loss: 0.2590\n",
      "  Batch [460/1299] D_loss: -0.1096, G_loss: 0.3492\n",
      "  Batch [470/1299] D_loss: -0.1534, G_loss: 0.3598\n",
      "  Batch [480/1299] D_loss: -0.0650, G_loss: 0.3613\n",
      "  Batch [490/1299] D_loss: -0.0046, G_loss: 0.3178\n",
      "  Batch [500/1299] D_loss: -2.8039, G_loss: -0.9123\n",
      "  Batch [510/1299] D_loss: -0.0988, G_loss: 0.0593\n",
      "  Batch [520/1299] D_loss: -0.0386, G_loss: 0.1057\n",
      "  Batch [530/1299] D_loss: -0.0205, G_loss: 0.2377\n",
      "  Batch [540/1299] D_loss: 0.0250, G_loss: 0.3068\n",
      "  Batch [550/1299] D_loss: -0.1292, G_loss: 0.3183\n",
      "  Batch [560/1299] D_loss: -0.0145, G_loss: 0.2036\n",
      "  Batch [570/1299] D_loss: -1.0064, G_loss: -2.1785\n",
      "  Batch [580/1299] D_loss: -0.0687, G_loss: 0.1942\n",
      "  Batch [590/1299] D_loss: -0.0224, G_loss: 0.2477\n",
      "  Batch [600/1299] D_loss: -0.0972, G_loss: 0.2521\n",
      "  Batch [610/1299] D_loss: -0.0331, G_loss: 0.2000\n",
      "  Batch [620/1299] D_loss: -3.8911, G_loss: -4.4454\n",
      "  Batch [630/1299] D_loss: -0.0273, G_loss: 0.1289\n",
      "  Batch [640/1299] D_loss: -0.0487, G_loss: 0.2956\n",
      "  Batch [650/1299] D_loss: -0.0233, G_loss: 0.2021\n",
      "  Batch [660/1299] D_loss: -0.0296, G_loss: 0.2915\n",
      "  Batch [670/1299] D_loss: -0.1394, G_loss: -1.6355\n",
      "  Batch [680/1299] D_loss: -0.0727, G_loss: 0.1735\n",
      "  Batch [690/1299] D_loss: -0.0315, G_loss: 0.3335\n",
      "  Batch [700/1299] D_loss: -0.1570, G_loss: 0.3388\n",
      "  Batch [710/1299] D_loss: -0.0359, G_loss: 0.4035\n",
      "  Batch [720/1299] D_loss: 0.0391, G_loss: 0.2635\n",
      "  Batch [730/1299] D_loss: -2.3533, G_loss: -4.2384\n",
      "  Batch [740/1299] D_loss: -0.2738, G_loss: -0.4347\n",
      "  Batch [750/1299] D_loss: -0.5586, G_loss: -1.4261\n",
      "  Batch [760/1299] D_loss: 0.0201, G_loss: 0.1657\n",
      "  Batch [770/1299] D_loss: -0.0185, G_loss: 0.2722\n",
      "  Batch [780/1299] D_loss: -0.0518, G_loss: 0.4706\n",
      "  Batch [790/1299] D_loss: -0.0215, G_loss: 0.3432\n",
      "  Batch [800/1299] D_loss: -0.9812, G_loss: -1.3620\n",
      "  Batch [810/1299] D_loss: -0.0209, G_loss: 0.1104\n",
      "  Batch [820/1299] D_loss: -1.2234, G_loss: -1.8395\n",
      "  Batch [830/1299] D_loss: -0.3288, G_loss: -0.2741\n",
      "  Batch [840/1299] D_loss: -0.0071, G_loss: 0.1572\n",
      "  Batch [850/1299] D_loss: -0.0279, G_loss: 0.1958\n",
      "  Batch [860/1299] D_loss: -0.1753, G_loss: 0.0106\n",
      "  Batch [870/1299] D_loss: -0.4038, G_loss: -0.0689\n",
      "  Batch [880/1299] D_loss: -0.0306, G_loss: 0.2569\n",
      "  Batch [890/1299] D_loss: -0.0078, G_loss: 0.4198\n",
      "  Batch [900/1299] D_loss: -0.0502, G_loss: 0.4022\n",
      "  Batch [910/1299] D_loss: -0.0719, G_loss: 0.4203\n",
      "  Batch [920/1299] D_loss: -0.0549, G_loss: 0.3189\n",
      "  Batch [930/1299] D_loss: -1.1394, G_loss: -1.0669\n",
      "  Batch [940/1299] D_loss: -0.4823, G_loss: -1.3416\n",
      "  Batch [950/1299] D_loss: -0.0221, G_loss: 0.1571\n",
      "  Batch [960/1299] D_loss: -0.0483, G_loss: 0.2094\n",
      "  Batch [970/1299] D_loss: -0.0375, G_loss: 0.3233\n",
      "  Batch [980/1299] D_loss: 0.0172, G_loss: 0.2439\n",
      "  Batch [990/1299] D_loss: -0.0860, G_loss: 0.2653\n",
      "  Batch [1000/1299] D_loss: -0.4137, G_loss: -4.0411\n",
      "  Batch [1010/1299] D_loss: -0.0364, G_loss: 0.0871\n",
      "  Batch [1020/1299] D_loss: -0.6488, G_loss: -0.7869\n",
      "  Batch [1030/1299] D_loss: -0.3570, G_loss: -0.0665\n",
      "  Batch [1040/1299] D_loss: -0.1416, G_loss: 0.3849\n",
      "  Batch [1050/1299] D_loss: -0.1115, G_loss: 0.5897\n",
      "  Batch [1060/1299] D_loss: -0.0929, G_loss: 0.6725\n",
      "  Batch [1070/1299] D_loss: -0.0038, G_loss: 0.3158\n",
      "  Batch [1080/1299] D_loss: -1.6717, G_loss: -5.9050\n",
      "  Batch [1090/1299] D_loss: -0.0246, G_loss: 0.0629\n",
      "  Batch [1100/1299] D_loss: -0.8013, G_loss: -1.4532\n",
      "  Batch [1110/1299] D_loss: -0.1349, G_loss: 0.1053\n",
      "  Batch [1120/1299] D_loss: -0.2045, G_loss: 0.0643\n",
      "  Batch [1130/1299] D_loss: -0.0602, G_loss: 0.2407\n",
      "  Batch [1140/1299] D_loss: -0.1945, G_loss: 0.4621\n",
      "  Batch [1150/1299] D_loss: -0.1274, G_loss: 0.7060\n",
      "  Batch [1160/1299] D_loss: -0.1497, G_loss: 0.6646\n",
      "  Batch [1170/1299] D_loss: -0.1107, G_loss: 0.4247\n",
      "  Batch [1180/1299] D_loss: 0.0107, G_loss: 0.1291\n",
      "  Batch [1190/1299] D_loss: -0.8908, G_loss: -1.5127\n",
      "  Batch [1200/1299] D_loss: -0.0206, G_loss: 0.2355\n",
      "  Batch [1210/1299] D_loss: -0.0416, G_loss: 0.4135\n",
      "  Batch [1220/1299] D_loss: -0.1151, G_loss: 0.3983\n",
      "  Batch [1230/1299] D_loss: -0.0343, G_loss: 0.3886\n",
      "  Batch [1240/1299] D_loss: -0.0122, G_loss: 0.4137\n",
      "  Batch [1250/1299] D_loss: -0.0450, G_loss: 0.2532\n",
      "  Batch [1260/1299] D_loss: -1.0655, G_loss: -2.6383\n",
      "  Batch [1270/1299] D_loss: -0.0215, G_loss: 0.0310\n",
      "  Batch [1280/1299] D_loss: 0.0002, G_loss: 0.0582\n",
      "  Batch [1290/1299] D_loss: -0.7328, G_loss: -1.7115\n",
      "\n",
      "Epoch 22 Summary:\n",
      "  Average D_loss: -0.1247\n",
      "  Average G_loss: -0.0530\n",
      "\n",
      "Epoch [23/100]\n",
      "  Batch [0/1299] D_loss: -0.0079, G_loss: 0.0909\n",
      "  Batch [10/1299] D_loss: -0.0400, G_loss: 0.1096\n",
      "  Batch [20/1299] D_loss: -0.3408, G_loss: -0.0199\n",
      "  Batch [30/1299] D_loss: -0.0779, G_loss: 0.3436\n",
      "  Batch [40/1299] D_loss: -0.1594, G_loss: 0.4647\n",
      "  Batch [50/1299] D_loss: -0.1093, G_loss: 0.6238\n",
      "  Batch [60/1299] D_loss: -0.0262, G_loss: 0.5636\n",
      "  Batch [70/1299] D_loss: -0.0972, G_loss: 0.2802\n",
      "  Batch [80/1299] D_loss: -1.2421, G_loss: -2.1203\n",
      "  Batch [90/1299] D_loss: -0.0963, G_loss: 0.0856\n",
      "  Batch [100/1299] D_loss: -1.3363, G_loss: -2.2081\n",
      "  Batch [110/1299] D_loss: -0.8998, G_loss: -1.4018\n",
      "  Batch [120/1299] D_loss: -0.1009, G_loss: -0.0918\n",
      "  Batch [130/1299] D_loss: -0.1594, G_loss: -0.1732\n",
      "  Batch [140/1299] D_loss: -0.6555, G_loss: 0.1020\n",
      "  Batch [150/1299] D_loss: -0.0771, G_loss: 0.3582\n",
      "  Batch [160/1299] D_loss: -0.1688, G_loss: 0.5997\n",
      "  Batch [170/1299] D_loss: -0.2851, G_loss: 0.7969\n",
      "  Batch [180/1299] D_loss: -0.0252, G_loss: 0.7365\n",
      "  Batch [190/1299] D_loss: -0.1008, G_loss: 0.4365\n",
      "  Batch [200/1299] D_loss: -0.1337, G_loss: 0.3742\n",
      "  Batch [210/1299] D_loss: -1.7665, G_loss: -2.2408\n",
      "  Batch [220/1299] D_loss: -0.9539, G_loss: -1.7741\n",
      "  Batch [230/1299] D_loss: -0.0164, G_loss: 0.1840\n",
      "  Batch [240/1299] D_loss: -0.8844, G_loss: -1.1101\n",
      "  Batch [250/1299] D_loss: -0.0422, G_loss: 0.1756\n",
      "  Batch [260/1299] D_loss: -0.1502, G_loss: 0.4116\n",
      "  Batch [270/1299] D_loss: -0.0810, G_loss: 0.6354\n",
      "  Batch [280/1299] D_loss: -0.1176, G_loss: 0.6150\n",
      "  Batch [290/1299] D_loss: -0.0835, G_loss: 0.4672\n",
      "  Batch [300/1299] D_loss: -0.0279, G_loss: 0.3423\n",
      "  Batch [310/1299] D_loss: 0.0465, G_loss: 0.1207\n",
      "  Batch [320/1299] D_loss: -0.0227, G_loss: 0.2527\n",
      "  Batch [330/1299] D_loss: -0.0397, G_loss: 0.1477\n",
      "  Batch [340/1299] D_loss: -0.2724, G_loss: 0.1298\n",
      "  Batch [350/1299] D_loss: 0.0147, G_loss: 0.1178\n",
      "  Batch [360/1299] D_loss: -0.0536, G_loss: 0.1425\n",
      "  Batch [370/1299] D_loss: -0.0579, G_loss: 0.1228\n",
      "  Batch [380/1299] D_loss: -0.0127, G_loss: 0.2440\n",
      "  Batch [390/1299] D_loss: -0.0589, G_loss: 0.4111\n",
      "  Batch [400/1299] D_loss: -0.1714, G_loss: 0.6473\n",
      "  Batch [410/1299] D_loss: -0.2100, G_loss: 0.4874\n",
      "  Batch [420/1299] D_loss: -0.1398, G_loss: 0.4238\n",
      "  Batch [430/1299] D_loss: -0.8076, G_loss: -2.6139\n",
      "  Batch [440/1299] D_loss: -0.5435, G_loss: -0.2614\n",
      "  Batch [450/1299] D_loss: -0.0129, G_loss: 0.1575\n",
      "  Batch [460/1299] D_loss: -0.0369, G_loss: 0.3335\n",
      "  Batch [470/1299] D_loss: -0.0853, G_loss: 0.4495\n",
      "  Batch [480/1299] D_loss: 0.0396, G_loss: 0.3707\n",
      "  Batch [490/1299] D_loss: -0.0612, G_loss: 0.3054\n",
      "  Batch [500/1299] D_loss: -1.8592, G_loss: -2.4681\n",
      "  Batch [510/1299] D_loss: -1.7945, G_loss: -2.9946\n",
      "  Batch [520/1299] D_loss: -0.0738, G_loss: 0.1622\n",
      "  Batch [530/1299] D_loss: 0.0372, G_loss: 0.3016\n",
      "  Batch [540/1299] D_loss: -0.0550, G_loss: 0.4577\n",
      "  Batch [550/1299] D_loss: 0.0888, G_loss: 0.4692\n",
      "  Batch [560/1299] D_loss: -0.0817, G_loss: 0.4222\n",
      "  Batch [570/1299] D_loss: -0.0812, G_loss: 0.3601\n",
      "  Batch [580/1299] D_loss: -0.0772, G_loss: 0.1928\n",
      "  Batch [590/1299] D_loss: -0.0209, G_loss: 0.1681\n",
      "  Batch [600/1299] D_loss: -0.0684, G_loss: 0.2744\n",
      "  Batch [610/1299] D_loss: -0.1145, G_loss: 0.3307\n",
      "  Batch [620/1299] D_loss: 0.0632, G_loss: 0.4007\n",
      "  Batch [630/1299] D_loss: 0.0054, G_loss: 0.2137\n",
      "  Batch [640/1299] D_loss: -0.8164, G_loss: -0.5237\n",
      "  Batch [650/1299] D_loss: 0.0011, G_loss: 0.2176\n",
      "  Batch [660/1299] D_loss: 0.0153, G_loss: 0.2341\n",
      "  Batch [670/1299] D_loss: -0.0571, G_loss: 0.2955\n",
      "  Batch [680/1299] D_loss: -0.1031, G_loss: 0.3426\n",
      "  Batch [690/1299] D_loss: 0.0229, G_loss: 0.3112\n",
      "  Batch [700/1299] D_loss: -0.8966, G_loss: -1.9919\n",
      "  Batch [710/1299] D_loss: -0.0276, G_loss: 0.0784\n",
      "  Batch [720/1299] D_loss: -0.1216, G_loss: 0.0325\n",
      "  Batch [730/1299] D_loss: -0.6881, G_loss: -1.0663\n",
      "  Batch [740/1299] D_loss: -0.1444, G_loss: 0.2396\n",
      "  Batch [750/1299] D_loss: -0.1607, G_loss: 0.4939\n",
      "  Batch [760/1299] D_loss: -0.1133, G_loss: 0.6323\n",
      "  Batch [770/1299] D_loss: -0.1714, G_loss: 0.5600\n",
      "  Batch [780/1299] D_loss: 0.0938, G_loss: 0.4054\n",
      "  Batch [790/1299] D_loss: -0.0332, G_loss: 0.2913\n",
      "  Batch [800/1299] D_loss: 0.0083, G_loss: 0.1205\n",
      "  Batch [810/1299] D_loss: 0.0013, G_loss: 0.0787\n",
      "  Batch [820/1299] D_loss: 0.0012, G_loss: 0.0484\n",
      "  Batch [830/1299] D_loss: -0.1872, G_loss: -0.4619\n",
      "  Batch [840/1299] D_loss: 0.0097, G_loss: 0.1106\n",
      "  Batch [850/1299] D_loss: -0.0721, G_loss: 0.1868\n",
      "  Batch [860/1299] D_loss: -0.0522, G_loss: 0.2522\n",
      "  Batch [870/1299] D_loss: -0.1497, G_loss: -0.6291\n",
      "  Batch [880/1299] D_loss: 0.0151, G_loss: 0.1119\n",
      "  Batch [890/1299] D_loss: -0.0704, G_loss: 0.2134\n",
      "  Batch [900/1299] D_loss: -0.0619, G_loss: 0.3719\n",
      "  Batch [910/1299] D_loss: -0.0834, G_loss: 0.4137\n",
      "  Batch [920/1299] D_loss: -0.2526, G_loss: 0.4724\n",
      "  Batch [930/1299] D_loss: -0.2035, G_loss: 0.3238\n",
      "  Batch [940/1299] D_loss: -1.5317, G_loss: -2.6342\n",
      "  Batch [950/1299] D_loss: -0.0523, G_loss: 0.1163\n",
      "  Batch [960/1299] D_loss: -0.0327, G_loss: 0.2364\n",
      "  Batch [970/1299] D_loss: -0.0090, G_loss: 0.3417\n",
      "  Batch [980/1299] D_loss: -0.0475, G_loss: 0.3395\n",
      "  Batch [990/1299] D_loss: -0.0051, G_loss: 0.2875\n",
      "  Batch [1000/1299] D_loss: -0.0290, G_loss: 0.1725\n",
      "  Batch [1010/1299] D_loss: -0.0419, G_loss: 0.0526\n",
      "  Batch [1020/1299] D_loss: -0.2353, G_loss: -0.0839\n",
      "  Batch [1030/1299] D_loss: -0.7389, G_loss: -1.7128\n",
      "  Batch [1040/1299] D_loss: -0.0525, G_loss: 0.2295\n",
      "  Batch [1050/1299] D_loss: -0.0984, G_loss: 0.4293\n",
      "  Batch [1060/1299] D_loss: -0.0709, G_loss: 0.3971\n",
      "  Batch [1070/1299] D_loss: -0.0668, G_loss: 0.4964\n",
      "  Batch [1080/1299] D_loss: -0.0184, G_loss: 0.3306\n",
      "  Batch [1090/1299] D_loss: -0.0062, G_loss: 0.0626\n",
      "  Batch [1100/1299] D_loss: -1.2831, G_loss: -1.3254\n",
      "  Batch [1110/1299] D_loss: -0.5536, G_loss: -0.3305\n",
      "  Batch [1120/1299] D_loss: -0.0840, G_loss: 0.4031\n",
      "  Batch [1130/1299] D_loss: -0.0639, G_loss: 0.3474\n",
      "  Batch [1140/1299] D_loss: -0.0407, G_loss: 0.3634\n",
      "  Batch [1150/1299] D_loss: -0.0751, G_loss: 0.3516\n",
      "  Batch [1160/1299] D_loss: -0.0277, G_loss: 0.2172\n",
      "  Batch [1170/1299] D_loss: -1.3908, G_loss: -2.8637\n",
      "  Batch [1180/1299] D_loss: 0.0031, G_loss: 0.2001\n",
      "  Batch [1190/1299] D_loss: 0.0359, G_loss: 0.2081\n",
      "  Batch [1200/1299] D_loss: -0.0571, G_loss: 0.2896\n",
      "  Batch [1210/1299] D_loss: -0.0827, G_loss: 0.2307\n",
      "  Batch [1220/1299] D_loss: 0.0262, G_loss: 0.2767\n",
      "  Batch [1230/1299] D_loss: -1.3857, G_loss: -0.8628\n",
      "  Batch [1240/1299] D_loss: -0.0688, G_loss: -0.0230\n",
      "  Batch [1250/1299] D_loss: -0.3682, G_loss: -0.0283\n",
      "  Batch [1260/1299] D_loss: -0.0285, G_loss: 0.1652\n",
      "  Batch [1270/1299] D_loss: -0.0741, G_loss: 0.3412\n",
      "  Batch [1280/1299] D_loss: -0.1882, G_loss: 0.4718\n",
      "  Batch [1290/1299] D_loss: -0.0462, G_loss: 0.5325\n",
      "\n",
      "Epoch 23 Summary:\n",
      "  Average D_loss: -0.1172\n",
      "  Average G_loss: -0.0490\n",
      "\n",
      "Epoch [24/100]\n",
      "  Batch [0/1299] D_loss: -0.0941, G_loss: 0.4062\n",
      "  Batch [10/1299] D_loss: -0.0049, G_loss: 0.2826\n",
      "  Batch [20/1299] D_loss: -3.5098, G_loss: -5.2863\n",
      "  Batch [30/1299] D_loss: 0.0072, G_loss: 0.1346\n",
      "  Batch [40/1299] D_loss: 0.0110, G_loss: 0.1294\n",
      "  Batch [50/1299] D_loss: -0.0615, G_loss: 0.2581\n",
      "  Batch [60/1299] D_loss: -0.0216, G_loss: 0.2495\n",
      "  Batch [70/1299] D_loss: -0.0184, G_loss: 0.2087\n",
      "  Batch [80/1299] D_loss: -0.0428, G_loss: -0.1621\n",
      "  Batch [90/1299] D_loss: -0.0605, G_loss: -0.0826\n",
      "  Batch [100/1299] D_loss: -0.2539, G_loss: 0.0584\n",
      "  Batch [110/1299] D_loss: -0.2337, G_loss: -0.2341\n",
      "  Batch [120/1299] D_loss: -0.1330, G_loss: -0.3316\n",
      "  Batch [130/1299] D_loss: -0.0725, G_loss: 0.3968\n",
      "  Batch [140/1299] D_loss: -0.0136, G_loss: 0.4446\n",
      "  Batch [150/1299] D_loss: -0.0003, G_loss: 0.4946\n",
      "  Batch [160/1299] D_loss: -0.0669, G_loss: 0.5400\n",
      "  Batch [170/1299] D_loss: 0.0446, G_loss: 0.2760\n",
      "  Batch [180/1299] D_loss: -0.3308, G_loss: -0.5740\n",
      "  Batch [190/1299] D_loss: -0.0911, G_loss: 0.0760\n",
      "  Batch [200/1299] D_loss: -0.0576, G_loss: 0.2452\n",
      "  Batch [210/1299] D_loss: -0.0748, G_loss: 0.3365\n",
      "  Batch [220/1299] D_loss: -0.1334, G_loss: 0.4783\n",
      "  Batch [230/1299] D_loss: -0.0198, G_loss: 0.3956\n",
      "  Batch [240/1299] D_loss: -0.1251, G_loss: 0.2812\n",
      "  Batch [250/1299] D_loss: -1.0470, G_loss: -2.6220\n",
      "  Batch [260/1299] D_loss: -0.9072, G_loss: -0.7492\n",
      "  Batch [270/1299] D_loss: -1.1637, G_loss: -1.5348\n",
      "  Batch [280/1299] D_loss: -0.9461, G_loss: -0.1784\n",
      "  Batch [290/1299] D_loss: -0.2914, G_loss: 0.0455\n",
      "  Batch [300/1299] D_loss: -0.0803, G_loss: 0.3331\n",
      "  Batch [310/1299] D_loss: -0.0482, G_loss: 0.2982\n",
      "  Batch [320/1299] D_loss: -0.1501, G_loss: 0.5410\n",
      "  Batch [330/1299] D_loss: -0.0589, G_loss: 0.5697\n",
      "  Batch [340/1299] D_loss: -0.0228, G_loss: 0.3190\n",
      "  Batch [350/1299] D_loss: -1.0628, G_loss: -0.5101\n",
      "  Batch [360/1299] D_loss: -0.1056, G_loss: -0.0405\n",
      "  Batch [370/1299] D_loss: -0.0651, G_loss: -0.0543\n",
      "  Batch [380/1299] D_loss: -0.5382, G_loss: 0.0308\n",
      "  Batch [390/1299] D_loss: -0.3839, G_loss: -0.5818\n",
      "  Batch [400/1299] D_loss: -0.0901, G_loss: 0.2454\n",
      "  Batch [410/1299] D_loss: -0.1985, G_loss: 0.6191\n",
      "  Batch [420/1299] D_loss: -0.1041, G_loss: 0.5775\n",
      "  Batch [430/1299] D_loss: -0.1424, G_loss: 0.7029\n",
      "  Batch [440/1299] D_loss: -0.1731, G_loss: 0.5986\n",
      "  Batch [450/1299] D_loss: -5.2904, G_loss: -7.6104\n",
      "  Batch [460/1299] D_loss: -0.1545, G_loss: 0.1271\n",
      "  Batch [470/1299] D_loss: -0.0006, G_loss: 0.0976\n",
      "  Batch [480/1299] D_loss: -0.2653, G_loss: -0.3515\n",
      "  Batch [490/1299] D_loss: -0.0356, G_loss: 0.2081\n",
      "  Batch [500/1299] D_loss: -0.1298, G_loss: 0.4167\n",
      "  Batch [510/1299] D_loss: -0.2014, G_loss: 0.5692\n",
      "  Batch [520/1299] D_loss: -0.2746, G_loss: 0.6321\n",
      "  Batch [530/1299] D_loss: -0.1563, G_loss: 0.6608\n",
      "  Batch [540/1299] D_loss: -0.0534, G_loss: 0.3415\n",
      "  Batch [550/1299] D_loss: -0.0828, G_loss: -1.4461\n",
      "  Batch [560/1299] D_loss: -0.4329, G_loss: -0.5938\n",
      "  Batch [570/1299] D_loss: 0.0307, G_loss: 0.3019\n",
      "  Batch [580/1299] D_loss: -0.0626, G_loss: 0.2285\n",
      "  Batch [590/1299] D_loss: -0.1061, G_loss: 0.3138\n",
      "  Batch [600/1299] D_loss: -2.3277, G_loss: -4.8965\n",
      "  Batch [610/1299] D_loss: -0.6957, G_loss: -1.3021\n",
      "  Batch [620/1299] D_loss: -0.0685, G_loss: 0.2095\n",
      "  Batch [630/1299] D_loss: -0.1868, G_loss: 0.4982\n",
      "  Batch [640/1299] D_loss: -0.1137, G_loss: 0.5135\n",
      "  Batch [650/1299] D_loss: -0.0581, G_loss: 0.6272\n",
      "  Batch [660/1299] D_loss: -0.1372, G_loss: 0.5428\n",
      "  Batch [670/1299] D_loss: -0.1225, G_loss: 0.4733\n",
      "  Batch [680/1299] D_loss: -0.8527, G_loss: -0.9138\n",
      "  Batch [690/1299] D_loss: -0.2396, G_loss: 0.1316\n",
      "  Batch [700/1299] D_loss: -0.0381, G_loss: 0.1256\n",
      "  Batch [710/1299] D_loss: -0.4121, G_loss: -1.3282\n",
      "  Batch [720/1299] D_loss: -0.2264, G_loss: 0.0626\n",
      "  Batch [730/1299] D_loss: 0.0143, G_loss: 0.0903\n",
      "  Batch [740/1299] D_loss: -0.1076, G_loss: 0.2966\n",
      "  Batch [750/1299] D_loss: -0.0901, G_loss: 0.5199\n",
      "  Batch [760/1299] D_loss: 0.0474, G_loss: 0.6655\n",
      "  Batch [770/1299] D_loss: -0.0545, G_loss: 0.5807\n",
      "  Batch [780/1299] D_loss: -0.0152, G_loss: 0.3320\n",
      "  Batch [790/1299] D_loss: -4.3923, G_loss: -4.9536\n",
      "  Batch [800/1299] D_loss: -0.1357, G_loss: 0.0257\n",
      "  Batch [810/1299] D_loss: -0.2529, G_loss: -0.0154\n",
      "  Batch [820/1299] D_loss: -0.3303, G_loss: 0.0089\n",
      "  Batch [830/1299] D_loss: -0.0413, G_loss: 0.1433\n",
      "  Batch [840/1299] D_loss: -0.1514, G_loss: 0.1546\n",
      "  Batch [850/1299] D_loss: -0.0141, G_loss: 0.3550\n",
      "  Batch [860/1299] D_loss: -0.0347, G_loss: 0.5158\n",
      "  Batch [870/1299] D_loss: -0.1060, G_loss: 0.4804\n",
      "  Batch [880/1299] D_loss: -0.0670, G_loss: 0.4049\n",
      "  Batch [890/1299] D_loss: -0.0918, G_loss: 0.3774\n",
      "  Batch [900/1299] D_loss: -0.2034, G_loss: -0.2766\n",
      "  Batch [910/1299] D_loss: -0.6620, G_loss: -1.2332\n",
      "  Batch [920/1299] D_loss: -0.0679, G_loss: 0.1457\n",
      "  Batch [930/1299] D_loss: -0.0559, G_loss: 0.2222\n",
      "  Batch [940/1299] D_loss: -0.0619, G_loss: 0.4065\n",
      "  Batch [950/1299] D_loss: -0.1787, G_loss: 0.4757\n",
      "  Batch [960/1299] D_loss: -0.0509, G_loss: 0.4782\n",
      "  Batch [970/1299] D_loss: -0.0832, G_loss: 0.3754\n",
      "  Batch [980/1299] D_loss: 0.0163, G_loss: 0.1547\n",
      "  Batch [990/1299] D_loss: -1.2704, G_loss: -2.0703\n",
      "  Batch [1000/1299] D_loss: -1.7562, G_loss: -1.0275\n",
      "  Batch [1010/1299] D_loss: -0.2213, G_loss: 0.0927\n",
      "  Batch [1020/1299] D_loss: -0.1163, G_loss: 0.0765\n",
      "  Batch [1030/1299] D_loss: -1.4865, G_loss: -0.1650\n",
      "  Batch [1040/1299] D_loss: -0.4609, G_loss: -0.0182\n",
      "  Batch [1050/1299] D_loss: -0.0445, G_loss: 0.1029\n",
      "  Batch [1060/1299] D_loss: -0.1527, G_loss: 0.1444\n",
      "  Batch [1070/1299] D_loss: -0.0729, G_loss: 0.1836\n",
      "  Batch [1080/1299] D_loss: -0.1194, G_loss: 0.4328\n",
      "  Batch [1090/1299] D_loss: -0.1651, G_loss: 0.4953\n",
      "  Batch [1100/1299] D_loss: -0.2048, G_loss: 0.4488\n",
      "  Batch [1110/1299] D_loss: -0.1099, G_loss: 0.4520\n",
      "  Batch [1120/1299] D_loss: -0.0180, G_loss: 0.0602\n",
      "  Batch [1130/1299] D_loss: -0.1464, G_loss: 0.1547\n",
      "  Batch [1140/1299] D_loss: -1.2906, G_loss: -1.1060\n",
      "  Batch [1150/1299] D_loss: -0.2457, G_loss: 0.0268\n",
      "  Batch [1160/1299] D_loss: -0.0454, G_loss: 0.2160\n",
      "  Batch [1170/1299] D_loss: -0.1501, G_loss: 0.4383\n",
      "  Batch [1180/1299] D_loss: -0.1318, G_loss: 0.3998\n",
      "  Batch [1190/1299] D_loss: -0.0396, G_loss: 0.3683\n",
      "  Batch [1200/1299] D_loss: -0.0588, G_loss: 0.3455\n",
      "  Batch [1210/1299] D_loss: -0.3978, G_loss: -0.0746\n",
      "  Batch [1220/1299] D_loss: -0.1316, G_loss: 0.1719\n",
      "  Batch [1230/1299] D_loss: -0.0317, G_loss: 0.0666\n",
      "  Batch [1240/1299] D_loss: -0.4767, G_loss: 0.0261\n",
      "  Batch [1250/1299] D_loss: -0.1480, G_loss: -0.0232\n",
      "  Batch [1260/1299] D_loss: -0.6154, G_loss: -0.5962\n",
      "  Batch [1270/1299] D_loss: -0.4617, G_loss: -0.1153\n",
      "  Batch [1280/1299] D_loss: -1.3414, G_loss: 0.1112\n",
      "  Batch [1290/1299] D_loss: -0.7854, G_loss: -0.2611\n",
      "\n",
      "Epoch 24 Summary:\n",
      "  Average D_loss: -0.1545\n",
      "  Average G_loss: -0.0957\n",
      "\n",
      "Epoch [25/100]\n",
      "  Batch [0/1299] D_loss: -0.3043, G_loss: -0.0507\n",
      "  Batch [10/1299] D_loss: -0.8836, G_loss: 0.1677\n",
      "  Batch [20/1299] D_loss: -0.2805, G_loss: 0.1268\n",
      "  Batch [30/1299] D_loss: -0.1079, G_loss: 0.5762\n",
      "  Batch [40/1299] D_loss: -0.2626, G_loss: 0.6670\n",
      "  Batch [50/1299] D_loss: 0.0847, G_loss: 0.5548\n",
      "  Batch [60/1299] D_loss: -0.1342, G_loss: 0.5641\n",
      "  Batch [70/1299] D_loss: -0.0224, G_loss: 0.4142\n",
      "  Batch [80/1299] D_loss: -1.7829, G_loss: -4.3796\n",
      "  Batch [90/1299] D_loss: -0.0294, G_loss: 0.1362\n",
      "  Batch [100/1299] D_loss: -0.0792, G_loss: 0.2748\n",
      "  Batch [110/1299] D_loss: -0.0258, G_loss: 0.4626\n",
      "  Batch [120/1299] D_loss: -0.0708, G_loss: 0.5383\n",
      "  Batch [130/1299] D_loss: -0.1553, G_loss: 0.3827\n",
      "  Batch [140/1299] D_loss: -5.2382, G_loss: -5.8489\n",
      "  Batch [150/1299] D_loss: -0.0246, G_loss: 0.1598\n",
      "  Batch [160/1299] D_loss: -0.0228, G_loss: 0.2442\n",
      "  Batch [170/1299] D_loss: -0.0276, G_loss: 0.1730\n",
      "  Batch [180/1299] D_loss: -1.3346, G_loss: -0.3973\n",
      "  Batch [190/1299] D_loss: -0.2873, G_loss: -0.7540\n",
      "  Batch [200/1299] D_loss: -0.0264, G_loss: 0.1694\n",
      "  Batch [210/1299] D_loss: -0.0447, G_loss: 0.3678\n",
      "  Batch [220/1299] D_loss: -0.2287, G_loss: 0.4236\n",
      "  Batch [230/1299] D_loss: -0.0596, G_loss: 0.3634\n",
      "  Batch [240/1299] D_loss: -0.0215, G_loss: 0.3998\n",
      "  Batch [250/1299] D_loss: 0.0155, G_loss: 0.1261\n",
      "  Batch [260/1299] D_loss: -0.5014, G_loss: -0.3743\n",
      "  Batch [270/1299] D_loss: -0.0101, G_loss: 0.2112\n",
      "  Batch [280/1299] D_loss: -0.1014, G_loss: 0.3865\n",
      "  Batch [290/1299] D_loss: -0.1472, G_loss: 0.4296\n",
      "  Batch [300/1299] D_loss: 0.0234, G_loss: 0.2459\n",
      "  Batch [310/1299] D_loss: -0.0609, G_loss: 0.1829\n",
      "  Batch [320/1299] D_loss: -0.0705, G_loss: 0.1218\n",
      "  Batch [330/1299] D_loss: -1.7885, G_loss: -3.4129\n",
      "  Batch [340/1299] D_loss: -0.2776, G_loss: -0.1389\n",
      "  Batch [350/1299] D_loss: -0.1886, G_loss: -0.3387\n",
      "  Batch [360/1299] D_loss: -0.6100, G_loss: -1.5649\n",
      "  Batch [370/1299] D_loss: -0.0575, G_loss: 0.2269\n",
      "  Batch [380/1299] D_loss: -0.2220, G_loss: 0.5232\n",
      "  Batch [390/1299] D_loss: -0.2240, G_loss: 0.6244\n",
      "  Batch [400/1299] D_loss: -0.0132, G_loss: 0.5247\n",
      "  Batch [410/1299] D_loss: -0.1023, G_loss: 0.2979\n",
      "  Batch [420/1299] D_loss: -0.3275, G_loss: -1.3752\n",
      "  Batch [430/1299] D_loss: -0.7150, G_loss: -0.2336\n",
      "  Batch [440/1299] D_loss: -0.1514, G_loss: 0.0759\n",
      "  Batch [450/1299] D_loss: -0.0837, G_loss: 0.1300\n",
      "  Batch [460/1299] D_loss: -0.2083, G_loss: 0.1400\n",
      "  Batch [470/1299] D_loss: -0.5322, G_loss: -0.0447\n",
      "  Batch [480/1299] D_loss: -0.0224, G_loss: 0.1583\n",
      "  Batch [490/1299] D_loss: -0.3206, G_loss: -0.0591\n",
      "  Batch [500/1299] D_loss: -0.0386, G_loss: 0.3216\n",
      "  Batch [510/1299] D_loss: -0.1081, G_loss: 0.4048\n",
      "  Batch [520/1299] D_loss: -0.1943, G_loss: 0.6651\n",
      "  Batch [530/1299] D_loss: -0.1678, G_loss: 0.5862\n",
      "  Batch [540/1299] D_loss: 0.0281, G_loss: 0.4407\n",
      "  Batch [550/1299] D_loss: -0.0455, G_loss: 0.3426\n",
      "  Batch [560/1299] D_loss: -0.4444, G_loss: -0.5755\n",
      "  Batch [570/1299] D_loss: -0.0331, G_loss: 0.1532\n",
      "  Batch [580/1299] D_loss: -0.0949, G_loss: 0.2902\n",
      "  Batch [590/1299] D_loss: -0.0682, G_loss: 0.4606\n",
      "  Batch [600/1299] D_loss: -0.0434, G_loss: 0.4276\n",
      "  Batch [610/1299] D_loss: -0.0276, G_loss: 0.4157\n",
      "  Batch [620/1299] D_loss: -0.0205, G_loss: 0.3924\n",
      "  Batch [630/1299] D_loss: -0.0092, G_loss: -0.0223\n",
      "  Batch [640/1299] D_loss: -0.0386, G_loss: 0.0718\n",
      "  Batch [650/1299] D_loss: -0.0199, G_loss: 0.1061\n",
      "  Batch [660/1299] D_loss: -0.0577, G_loss: 0.2385\n",
      "  Batch [670/1299] D_loss: -0.0387, G_loss: 0.3045\n",
      "  Batch [680/1299] D_loss: -0.0188, G_loss: 0.3970\n",
      "  Batch [690/1299] D_loss: 0.0249, G_loss: 0.3805\n",
      "  Batch [700/1299] D_loss: -0.0227, G_loss: 0.2368\n",
      "  Batch [710/1299] D_loss: -1.5661, G_loss: -0.2427\n",
      "  Batch [720/1299] D_loss: -2.5304, G_loss: -2.9438\n",
      "  Batch [730/1299] D_loss: -0.0705, G_loss: 0.0778\n",
      "  Batch [740/1299] D_loss: -0.1136, G_loss: -0.0380\n",
      "  Batch [750/1299] D_loss: -0.0423, G_loss: 0.2568\n",
      "  Batch [760/1299] D_loss: -0.0580, G_loss: 0.3491\n",
      "  Batch [770/1299] D_loss: -0.1314, G_loss: 0.4427\n",
      "  Batch [780/1299] D_loss: -0.0123, G_loss: 0.5041\n",
      "  Batch [790/1299] D_loss: 0.0161, G_loss: 0.3513\n",
      "  Batch [800/1299] D_loss: -0.4405, G_loss: -0.1967\n",
      "  Batch [810/1299] D_loss: -0.4905, G_loss: -0.3166\n",
      "  Batch [820/1299] D_loss: -1.5617, G_loss: -3.3131\n",
      "  Batch [830/1299] D_loss: -0.0882, G_loss: 0.3509\n",
      "  Batch [840/1299] D_loss: -0.2447, G_loss: 0.5216\n",
      "  Batch [850/1299] D_loss: -0.0622, G_loss: 0.6063\n",
      "  Batch [860/1299] D_loss: -0.0868, G_loss: 0.4642\n",
      "  Batch [870/1299] D_loss: -0.1116, G_loss: 0.5980\n",
      "  Batch [880/1299] D_loss: -0.1187, G_loss: 0.4788\n",
      "  Batch [890/1299] D_loss: 0.0325, G_loss: 0.1438\n",
      "  Batch [900/1299] D_loss: -1.5472, G_loss: -3.2794\n",
      "  Batch [910/1299] D_loss: -0.0883, G_loss: 0.1587\n",
      "  Batch [920/1299] D_loss: -0.1002, G_loss: 0.3120\n",
      "  Batch [930/1299] D_loss: -0.1269, G_loss: 0.4293\n",
      "  Batch [940/1299] D_loss: -0.0613, G_loss: 0.4397\n",
      "  Batch [950/1299] D_loss: -0.0481, G_loss: 0.4472\n",
      "  Batch [960/1299] D_loss: -2.3348, G_loss: -1.9626\n",
      "  Batch [970/1299] D_loss: 0.0496, G_loss: 0.0727\n",
      "  Batch [980/1299] D_loss: 0.0188, G_loss: 0.1363\n",
      "  Batch [990/1299] D_loss: -0.1325, G_loss: -0.5929\n",
      "  Batch [1000/1299] D_loss: -0.0138, G_loss: 0.1456\n",
      "  Batch [1010/1299] D_loss: -0.1015, G_loss: 0.3692\n",
      "  Batch [1020/1299] D_loss: -0.1457, G_loss: 0.5621\n",
      "  Batch [1030/1299] D_loss: -0.0334, G_loss: 0.4014\n",
      "  Batch [1040/1299] D_loss: -0.1036, G_loss: 0.3001\n",
      "  Batch [1050/1299] D_loss: -0.0780, G_loss: -0.1900\n",
      "  Batch [1060/1299] D_loss: -1.1011, G_loss: -2.0208\n",
      "  Batch [1070/1299] D_loss: -0.1228, G_loss: 0.0727\n",
      "  Batch [1080/1299] D_loss: -0.0278, G_loss: 0.1975\n",
      "  Batch [1090/1299] D_loss: -0.0566, G_loss: 0.2987\n",
      "  Batch [1100/1299] D_loss: -0.0879, G_loss: 0.3921\n",
      "  Batch [1110/1299] D_loss: -0.1449, G_loss: 0.3421\n",
      "  Batch [1120/1299] D_loss: -0.0505, G_loss: 0.2156\n",
      "  Batch [1130/1299] D_loss: 0.0246, G_loss: -0.5539\n",
      "  Batch [1140/1299] D_loss: -0.2117, G_loss: 0.0602\n",
      "  Batch [1150/1299] D_loss: -0.5200, G_loss: -1.1412\n",
      "  Batch [1160/1299] D_loss: -0.2350, G_loss: 0.0926\n",
      "  Batch [1170/1299] D_loss: -0.4706, G_loss: -0.0049\n",
      "  Batch [1180/1299] D_loss: -0.0800, G_loss: 0.0921\n",
      "  Batch [1190/1299] D_loss: -0.1026, G_loss: 0.2216\n",
      "  Batch [1200/1299] D_loss: -0.0246, G_loss: 0.3305\n",
      "  Batch [1210/1299] D_loss: -0.1846, G_loss: 0.5108\n",
      "  Batch [1220/1299] D_loss: -0.1301, G_loss: 0.4845\n",
      "  Batch [1230/1299] D_loss: -0.0140, G_loss: 0.3281\n",
      "  Batch [1240/1299] D_loss: -0.0286, G_loss: 0.2419\n",
      "  Batch [1250/1299] D_loss: -0.9658, G_loss: -0.8219\n",
      "  Batch [1260/1299] D_loss: -0.7050, G_loss: -1.3740\n",
      "  Batch [1270/1299] D_loss: -0.1602, G_loss: 0.0514\n",
      "  Batch [1280/1299] D_loss: -0.1700, G_loss: 0.0912\n",
      "  Batch [1290/1299] D_loss: -0.0650, G_loss: 0.2908\n",
      "\n",
      "Epoch 25 Summary:\n",
      "  Average D_loss: -0.1370\n",
      "  Average G_loss: -0.0439\n",
      "\n",
      "Epoch [26/100]\n",
      "  Batch [0/1299] D_loss: -0.0851, G_loss: 0.4605\n",
      "  Batch [10/1299] D_loss: -0.1904, G_loss: 0.6237\n",
      "  Batch [20/1299] D_loss: -0.0105, G_loss: 0.4482\n",
      "  Batch [30/1299] D_loss: -0.0872, G_loss: 0.4249\n",
      "  Batch [40/1299] D_loss: 0.0010, G_loss: 0.1065\n",
      "  Batch [50/1299] D_loss: 0.0062, G_loss: 0.1289\n",
      "  Batch [60/1299] D_loss: 0.0090, G_loss: 0.1406\n",
      "  Batch [70/1299] D_loss: -0.0544, G_loss: 0.2731\n",
      "  Batch [80/1299] D_loss: -0.0027, G_loss: 0.3448\n",
      "  Batch [90/1299] D_loss: -0.0701, G_loss: 0.3092\n",
      "  Batch [100/1299] D_loss: -0.0568, G_loss: 0.0940\n",
      "  Batch [110/1299] D_loss: 0.0028, G_loss: 0.2557\n",
      "  Batch [120/1299] D_loss: -0.0861, G_loss: 0.2716\n",
      "  Batch [130/1299] D_loss: -0.0663, G_loss: 0.2747\n",
      "  Batch [140/1299] D_loss: -0.0882, G_loss: 0.3205\n",
      "  Batch [150/1299] D_loss: -2.1541, G_loss: -4.4705\n",
      "  Batch [160/1299] D_loss: -0.0212, G_loss: 0.1246\n",
      "  Batch [170/1299] D_loss: -0.7130, G_loss: 0.0917\n",
      "  Batch [180/1299] D_loss: 0.0229, G_loss: 0.0853\n",
      "  Batch [190/1299] D_loss: -0.0580, G_loss: 0.1515\n",
      "  Batch [200/1299] D_loss: -0.0666, G_loss: 0.2425\n",
      "  Batch [210/1299] D_loss: -1.2399, G_loss: -2.1291\n",
      "  Batch [220/1299] D_loss: -0.0406, G_loss: 0.2071\n",
      "  Batch [230/1299] D_loss: -0.1017, G_loss: 0.5011\n",
      "  Batch [240/1299] D_loss: -0.0707, G_loss: 0.4460\n",
      "  Batch [250/1299] D_loss: -0.1345, G_loss: 0.4448\n",
      "  Batch [260/1299] D_loss: -0.1221, G_loss: 0.3105\n",
      "  Batch [270/1299] D_loss: -0.3880, G_loss: -1.4312\n",
      "  Batch [280/1299] D_loss: -0.0587, G_loss: 0.0035\n",
      "  Batch [290/1299] D_loss: -0.5014, G_loss: 0.0801\n",
      "  Batch [300/1299] D_loss: -0.1198, G_loss: 0.3611\n",
      "  Batch [310/1299] D_loss: -0.0898, G_loss: 0.4065\n",
      "  Batch [320/1299] D_loss: -0.0676, G_loss: 0.5491\n",
      "  Batch [330/1299] D_loss: -0.0657, G_loss: 0.4559\n",
      "  Batch [340/1299] D_loss: 0.0018, G_loss: 0.2918\n",
      "  Batch [350/1299] D_loss: -0.1313, G_loss: -0.7388\n",
      "  Batch [360/1299] D_loss: -0.3467, G_loss: -0.9845\n",
      "  Batch [370/1299] D_loss: -0.0100, G_loss: 0.1962\n",
      "  Batch [380/1299] D_loss: -0.1264, G_loss: 0.3982\n",
      "  Batch [390/1299] D_loss: -0.0738, G_loss: 0.5038\n",
      "  Batch [400/1299] D_loss: -0.0932, G_loss: 0.4112\n",
      "  Batch [410/1299] D_loss: -0.1135, G_loss: 0.3712\n",
      "  Batch [420/1299] D_loss: -0.3925, G_loss: -1.6519\n",
      "  Batch [430/1299] D_loss: 0.0041, G_loss: 0.1526\n",
      "  Batch [440/1299] D_loss: -0.0208, G_loss: 0.2771\n",
      "  Batch [450/1299] D_loss: -0.0074, G_loss: 0.3318\n",
      "  Batch [460/1299] D_loss: 0.0163, G_loss: 0.2551\n",
      "  Batch [470/1299] D_loss: -0.0750, G_loss: 0.2822\n",
      "  Batch [480/1299] D_loss: -0.1296, G_loss: -0.3666\n",
      "  Batch [490/1299] D_loss: -0.1138, G_loss: -0.0851\n",
      "  Batch [500/1299] D_loss: -0.6963, G_loss: -0.8189\n",
      "  Batch [510/1299] D_loss: -0.2693, G_loss: 0.0008\n",
      "  Batch [520/1299] D_loss: -0.3582, G_loss: 0.0063\n",
      "  Batch [530/1299] D_loss: -0.1093, G_loss: 0.2249\n",
      "  Batch [540/1299] D_loss: -0.0378, G_loss: 0.3725\n",
      "  Batch [550/1299] D_loss: -0.1619, G_loss: 0.5145\n",
      "  Batch [560/1299] D_loss: -0.1699, G_loss: 0.7127\n",
      "  Batch [570/1299] D_loss: 0.0945, G_loss: 0.5011\n",
      "  Batch [580/1299] D_loss: -0.0917, G_loss: 0.4222\n",
      "  Batch [590/1299] D_loss: -0.0412, G_loss: 0.1591\n",
      "  Batch [600/1299] D_loss: -0.0144, G_loss: 0.0438\n",
      "  Batch [610/1299] D_loss: -0.1935, G_loss: 0.0342\n",
      "  Batch [620/1299] D_loss: -0.0693, G_loss: 0.2829\n",
      "  Batch [630/1299] D_loss: -0.1045, G_loss: 0.3584\n",
      "  Batch [640/1299] D_loss: -0.1020, G_loss: 0.4734\n",
      "  Batch [650/1299] D_loss: -0.0466, G_loss: 0.4392\n",
      "  Batch [660/1299] D_loss: -0.0158, G_loss: 0.3038\n",
      "  Batch [670/1299] D_loss: -0.0289, G_loss: 0.2275\n",
      "  Batch [680/1299] D_loss: 0.0111, G_loss: 0.1125\n",
      "  Batch [690/1299] D_loss: -0.0331, G_loss: 0.1971\n",
      "  Batch [700/1299] D_loss: -0.1369, G_loss: 0.2094\n",
      "  Batch [710/1299] D_loss: -0.0156, G_loss: 0.0239\n",
      "  Batch [720/1299] D_loss: -1.0337, G_loss: -0.0579\n",
      "  Batch [730/1299] D_loss: -0.2308, G_loss: 0.1029\n",
      "  Batch [740/1299] D_loss: 0.0403, G_loss: 0.1443\n",
      "  Batch [750/1299] D_loss: 0.0105, G_loss: 0.2842\n",
      "  Batch [760/1299] D_loss: -0.0738, G_loss: 0.4459\n",
      "  Batch [770/1299] D_loss: -0.0321, G_loss: 0.5141\n",
      "  Batch [780/1299] D_loss: -0.2428, G_loss: 0.5696\n",
      "  Batch [790/1299] D_loss: -0.0371, G_loss: 0.3785\n",
      "  Batch [800/1299] D_loss: -3.1036, G_loss: -2.1231\n",
      "  Batch [810/1299] D_loss: -0.4346, G_loss: -0.7453\n",
      "  Batch [820/1299] D_loss: -0.4604, G_loss: 0.1022\n",
      "  Batch [830/1299] D_loss: -1.0468, G_loss: -2.9056\n",
      "  Batch [840/1299] D_loss: -0.2246, G_loss: 0.0305\n",
      "  Batch [850/1299] D_loss: -0.0224, G_loss: 0.2050\n",
      "  Batch [860/1299] D_loss: -0.1541, G_loss: 0.3533\n",
      "  Batch [870/1299] D_loss: -0.2503, G_loss: 0.4906\n",
      "  Batch [880/1299] D_loss: -0.0287, G_loss: 0.2740\n",
      "  Batch [890/1299] D_loss: -0.0310, G_loss: 0.3315\n",
      "  Batch [900/1299] D_loss: -1.2890, G_loss: -2.5563\n",
      "  Batch [910/1299] D_loss: -0.0427, G_loss: 0.1155\n",
      "  Batch [920/1299] D_loss: -1.2183, G_loss: -2.1069\n",
      "  Batch [930/1299] D_loss: 0.0003, G_loss: 0.1714\n",
      "  Batch [940/1299] D_loss: -0.0642, G_loss: 0.2417\n",
      "  Batch [950/1299] D_loss: -0.0409, G_loss: 0.3561\n",
      "  Batch [960/1299] D_loss: 0.0216, G_loss: 0.2642\n",
      "  Batch [970/1299] D_loss: -1.0053, G_loss: -4.6299\n",
      "  Batch [980/1299] D_loss: -0.2074, G_loss: -0.0296\n",
      "  Batch [990/1299] D_loss: -0.0368, G_loss: 0.2290\n",
      "  Batch [1000/1299] D_loss: -0.0123, G_loss: 0.4212\n",
      "  Batch [1010/1299] D_loss: -0.1370, G_loss: 0.4962\n",
      "  Batch [1020/1299] D_loss: -0.0812, G_loss: 0.3097\n",
      "  Batch [1030/1299] D_loss: -0.0194, G_loss: 0.2035\n",
      "  Batch [1040/1299] D_loss: -0.0240, G_loss: 0.0784\n",
      "  Batch [1050/1299] D_loss: -1.5285, G_loss: -2.4929\n",
      "  Batch [1060/1299] D_loss: -0.0501, G_loss: 0.1823\n",
      "  Batch [1070/1299] D_loss: -0.0312, G_loss: 0.3860\n",
      "  Batch [1080/1299] D_loss: -0.0685, G_loss: 0.4736\n",
      "  Batch [1090/1299] D_loss: -0.1348, G_loss: 0.4557\n",
      "  Batch [1100/1299] D_loss: -0.0911, G_loss: 0.3616\n",
      "  Batch [1110/1299] D_loss: -0.2981, G_loss: -0.1793\n",
      "  Batch [1120/1299] D_loss: -1.1558, G_loss: -1.2712\n",
      "  Batch [1130/1299] D_loss: -0.9000, G_loss: -0.7141\n",
      "  Batch [1140/1299] D_loss: -0.0993, G_loss: 0.0787\n",
      "  Batch [1150/1299] D_loss: -0.1523, G_loss: 0.1624\n",
      "  Batch [1160/1299] D_loss: -1.8352, G_loss: -1.5069\n",
      "  Batch [1170/1299] D_loss: -0.1037, G_loss: 0.0550\n",
      "  Batch [1180/1299] D_loss: -0.0699, G_loss: 0.2887\n",
      "  Batch [1190/1299] D_loss: -0.0369, G_loss: 0.4355\n",
      "  Batch [1200/1299] D_loss: -0.1039, G_loss: 0.5505\n",
      "  Batch [1210/1299] D_loss: 0.0190, G_loss: 0.3883\n",
      "  Batch [1220/1299] D_loss: -0.0416, G_loss: 0.3317\n",
      "  Batch [1230/1299] D_loss: -2.5037, G_loss: -2.6070\n",
      "  Batch [1240/1299] D_loss: -0.1794, G_loss: 0.0783\n",
      "  Batch [1250/1299] D_loss: -0.1189, G_loss: -0.1543\n",
      "  Batch [1260/1299] D_loss: -0.0772, G_loss: 0.1547\n",
      "  Batch [1270/1299] D_loss: -0.9877, G_loss: -0.2290\n",
      "  Batch [1280/1299] D_loss: -0.3363, G_loss: 0.1068\n",
      "  Batch [1290/1299] D_loss: -0.0385, G_loss: 0.1174\n",
      "\n",
      "Epoch 26 Summary:\n",
      "  Average D_loss: -0.1187\n",
      "  Average G_loss: -0.0610\n",
      "\n",
      "Epoch [27/100]\n",
      "  Batch [0/1299] D_loss: -0.1907, G_loss: 0.1951\n",
      "  Batch [10/1299] D_loss: -0.6585, G_loss: 0.1426\n",
      "  Batch [20/1299] D_loss: -0.6689, G_loss: -0.0866\n",
      "  Batch [30/1299] D_loss: -1.6720, G_loss: -0.6644\n",
      "  Batch [40/1299] D_loss: -0.0471, G_loss: 0.1374\n",
      "  Batch [50/1299] D_loss: -0.0662, G_loss: 0.3172\n",
      "  Batch [60/1299] D_loss: -0.2385, G_loss: 0.5414\n",
      "  Batch [70/1299] D_loss: -0.0496, G_loss: 0.5648\n",
      "  Batch [80/1299] D_loss: -0.1369, G_loss: 0.6843\n",
      "  Batch [90/1299] D_loss: -0.0411, G_loss: 0.6897\n",
      "  Batch [100/1299] D_loss: -0.0449, G_loss: 0.6239\n",
      "  Batch [110/1299] D_loss: -0.0163, G_loss: 0.2357\n",
      "  Batch [120/1299] D_loss: -1.5781, G_loss: -0.2112\n",
      "  Batch [130/1299] D_loss: -0.1850, G_loss: 0.0651\n",
      "  Batch [140/1299] D_loss: -0.8435, G_loss: -1.1778\n",
      "  Batch [150/1299] D_loss: -0.1199, G_loss: 0.0973\n",
      "  Batch [160/1299] D_loss: -0.1973, G_loss: -0.0833\n",
      "  Batch [170/1299] D_loss: -0.3090, G_loss: 0.0455\n",
      "  Batch [180/1299] D_loss: -0.7833, G_loss: 0.0846\n",
      "  Batch [190/1299] D_loss: -0.0266, G_loss: 0.0833\n",
      "  Batch [200/1299] D_loss: -0.0868, G_loss: 0.3001\n",
      "  Batch [210/1299] D_loss: -0.1442, G_loss: 0.4821\n",
      "  Batch [220/1299] D_loss: -0.2474, G_loss: 0.6251\n",
      "  Batch [230/1299] D_loss: -0.0831, G_loss: 0.5180\n",
      "  Batch [240/1299] D_loss: -0.0131, G_loss: 0.4031\n",
      "  Batch [250/1299] D_loss: -0.1583, G_loss: 0.2789\n",
      "  Batch [260/1299] D_loss: -0.1758, G_loss: -0.1060\n",
      "  Batch [270/1299] D_loss: -0.0019, G_loss: 0.1670\n",
      "  Batch [280/1299] D_loss: -1.3811, G_loss: -0.0211\n",
      "  Batch [290/1299] D_loss: -0.9386, G_loss: -0.9980\n",
      "  Batch [300/1299] D_loss: -0.2117, G_loss: -0.0200\n",
      "  Batch [310/1299] D_loss: -0.3618, G_loss: 0.0918\n",
      "  Batch [320/1299] D_loss: -0.7207, G_loss: 0.1702\n",
      "  Batch [330/1299] D_loss: -0.1217, G_loss: 0.3083\n",
      "  Batch [340/1299] D_loss: -0.1630, G_loss: 0.4725\n",
      "  Batch [350/1299] D_loss: -0.1585, G_loss: 0.7254\n",
      "  Batch [360/1299] D_loss: -0.1200, G_loss: 0.5887\n",
      "  Batch [370/1299] D_loss: 0.0304, G_loss: 0.4600\n",
      "  Batch [380/1299] D_loss: -0.0189, G_loss: 0.3163\n",
      "  Batch [390/1299] D_loss: -0.1393, G_loss: 0.2011\n",
      "  Batch [400/1299] D_loss: -1.6552, G_loss: -1.1480\n",
      "  Batch [410/1299] D_loss: -0.0930, G_loss: 0.2799\n",
      "  Batch [420/1299] D_loss: -0.0747, G_loss: 0.3532\n",
      "  Batch [430/1299] D_loss: -0.1279, G_loss: 0.3842\n",
      "  Batch [440/1299] D_loss: -0.1268, G_loss: 0.4278\n",
      "  Batch [450/1299] D_loss: -0.0253, G_loss: 0.4158\n",
      "  Batch [460/1299] D_loss: -3.0632, G_loss: -2.1840\n",
      "  Batch [470/1299] D_loss: 0.0089, G_loss: 0.2176\n",
      "  Batch [480/1299] D_loss: -0.0152, G_loss: 0.2399\n",
      "  Batch [490/1299] D_loss: -0.0206, G_loss: 0.2634\n",
      "  Batch [500/1299] D_loss: -0.0069, G_loss: 0.2454\n",
      "  Batch [510/1299] D_loss: -0.0736, G_loss: 0.2267\n",
      "  Batch [520/1299] D_loss: -0.4842, G_loss: -0.2827\n",
      "  Batch [530/1299] D_loss: -0.5012, G_loss: -0.8801\n",
      "  Batch [540/1299] D_loss: -0.0580, G_loss: 0.2317\n",
      "  Batch [550/1299] D_loss: -0.1626, G_loss: 0.5284\n",
      "  Batch [560/1299] D_loss: -0.1286, G_loss: 0.6443\n",
      "  Batch [570/1299] D_loss: -0.1381, G_loss: 0.5210\n",
      "  Batch [580/1299] D_loss: 0.0040, G_loss: 0.5122\n",
      "  Batch [590/1299] D_loss: -0.0112, G_loss: 0.4429\n",
      "  Batch [600/1299] D_loss: -4.0234, G_loss: -8.3374\n",
      "  Batch [610/1299] D_loss: -1.9227, G_loss: -2.6189\n",
      "  Batch [620/1299] D_loss: -0.0111, G_loss: 0.1554\n",
      "  Batch [630/1299] D_loss: -0.0265, G_loss: 0.2490\n",
      "  Batch [640/1299] D_loss: -0.0292, G_loss: 0.3011\n",
      "  Batch [650/1299] D_loss: -0.1495, G_loss: 0.3678\n",
      "  Batch [660/1299] D_loss: -0.1187, G_loss: 0.4046\n",
      "  Batch [670/1299] D_loss: -1.6684, G_loss: -4.0293\n",
      "  Batch [680/1299] D_loss: -1.7177, G_loss: -2.6073\n",
      "  Batch [690/1299] D_loss: -0.1395, G_loss: 0.1484\n",
      "  Batch [700/1299] D_loss: -0.0388, G_loss: 0.2200\n",
      "  Batch [710/1299] D_loss: -0.0627, G_loss: 0.3413\n",
      "  Batch [720/1299] D_loss: -0.0791, G_loss: 0.4838\n",
      "  Batch [730/1299] D_loss: -0.1248, G_loss: 0.4362\n",
      "  Batch [740/1299] D_loss: -0.0313, G_loss: 0.3375\n",
      "  Batch [750/1299] D_loss: -2.8104, G_loss: -2.7672\n",
      "  Batch [760/1299] D_loss: -1.5503, G_loss: -1.3400\n",
      "  Batch [770/1299] D_loss: -0.0258, G_loss: 0.2281\n",
      "  Batch [780/1299] D_loss: -0.1075, G_loss: 0.2860\n",
      "  Batch [790/1299] D_loss: -0.1440, G_loss: 0.5287\n",
      "  Batch [800/1299] D_loss: -0.1152, G_loss: 0.5133\n",
      "  Batch [810/1299] D_loss: -0.0094, G_loss: 0.5251\n",
      "  Batch [820/1299] D_loss: 0.0791, G_loss: 0.4174\n",
      "  Batch [830/1299] D_loss: -0.0772, G_loss: 0.2724\n",
      "  Batch [840/1299] D_loss: -1.8005, G_loss: -0.5718\n",
      "  Batch [850/1299] D_loss: -0.2796, G_loss: -0.3723\n",
      "  Batch [860/1299] D_loss: -0.0938, G_loss: 0.0324\n",
      "  Batch [870/1299] D_loss: -0.0221, G_loss: 0.2077\n",
      "  Batch [880/1299] D_loss: -0.1145, G_loss: 0.4159\n",
      "  Batch [890/1299] D_loss: -0.0617, G_loss: 0.3998\n",
      "  Batch [900/1299] D_loss: -0.1634, G_loss: 0.6120\n",
      "  Batch [910/1299] D_loss: -0.1527, G_loss: 0.8152\n",
      "  Batch [920/1299] D_loss: -0.1101, G_loss: 0.4939\n",
      "  Batch [930/1299] D_loss: -2.2668, G_loss: -4.2255\n",
      "  Batch [940/1299] D_loss: -0.1101, G_loss: 0.0739\n",
      "  Batch [950/1299] D_loss: -0.5162, G_loss: -0.1963\n",
      "  Batch [960/1299] D_loss: -0.9070, G_loss: -1.0072\n",
      "  Batch [970/1299] D_loss: -0.0267, G_loss: 0.1997\n",
      "  Batch [980/1299] D_loss: -0.0166, G_loss: 0.2250\n",
      "  Batch [990/1299] D_loss: -0.0880, G_loss: 0.3590\n",
      "  Batch [1000/1299] D_loss: -0.1121, G_loss: 0.3194\n",
      "  Batch [1010/1299] D_loss: -0.0986, G_loss: 0.2994\n",
      "  Batch [1020/1299] D_loss: -0.8213, G_loss: -1.3651\n",
      "  Batch [1030/1299] D_loss: -0.1785, G_loss: 0.1899\n",
      "  Batch [1040/1299] D_loss: -0.0269, G_loss: 0.2735\n",
      "  Batch [1050/1299] D_loss: -0.1588, G_loss: 0.4011\n",
      "  Batch [1060/1299] D_loss: -0.0022, G_loss: 0.2432\n",
      "  Batch [1070/1299] D_loss: -2.5726, G_loss: -1.6445\n",
      "  Batch [1080/1299] D_loss: -0.2790, G_loss: -0.8644\n",
      "  Batch [1090/1299] D_loss: -0.1417, G_loss: 0.0298\n",
      "  Batch [1100/1299] D_loss: -0.2247, G_loss: 0.0484\n",
      "  Batch [1110/1299] D_loss: -0.2651, G_loss: 0.1057\n",
      "  Batch [1120/1299] D_loss: -0.4622, G_loss: 0.0207\n",
      "  Batch [1130/1299] D_loss: -0.2108, G_loss: 0.3697\n",
      "  Batch [1140/1299] D_loss: -0.1327, G_loss: 0.6018\n",
      "  Batch [1150/1299] D_loss: -0.1182, G_loss: 0.6643\n",
      "  Batch [1160/1299] D_loss: -0.0611, G_loss: 0.5065\n",
      "  Batch [1170/1299] D_loss: -0.0039, G_loss: 0.5019\n",
      "  Batch [1180/1299] D_loss: -1.9970, G_loss: -6.9343\n",
      "  Batch [1190/1299] D_loss: -0.6559, G_loss: 0.0307\n",
      "  Batch [1200/1299] D_loss: -0.1121, G_loss: 0.0673\n",
      "  Batch [1210/1299] D_loss: -0.1978, G_loss: -0.3011\n",
      "  Batch [1220/1299] D_loss: -0.0489, G_loss: 0.2235\n",
      "  Batch [1230/1299] D_loss: -0.1429, G_loss: 0.2797\n",
      "  Batch [1240/1299] D_loss: -0.1526, G_loss: 0.5554\n",
      "  Batch [1250/1299] D_loss: -0.1891, G_loss: 0.8000\n",
      "  Batch [1260/1299] D_loss: -0.1153, G_loss: 0.4807\n",
      "  Batch [1270/1299] D_loss: -0.0837, G_loss: 0.4091\n",
      "  Batch [1280/1299] D_loss: -2.3980, G_loss: -3.1429\n",
      "  Batch [1290/1299] D_loss: -0.3884, G_loss: -0.2288\n",
      "\n",
      "Epoch 27 Summary:\n",
      "  Average D_loss: -0.1727\n",
      "  Average G_loss: -0.0636\n",
      "\n",
      "Epoch [28/100]\n",
      "  Batch [0/1299] D_loss: 0.1067, G_loss: -0.0060\n",
      "  Batch [10/1299] D_loss: -0.0144, G_loss: 0.0797\n",
      "  Batch [20/1299] D_loss: -0.0156, G_loss: 0.1268\n",
      "  Batch [30/1299] D_loss: -0.0829, G_loss: 0.2441\n",
      "  Batch [40/1299] D_loss: -0.0712, G_loss: 0.4034\n",
      "  Batch [50/1299] D_loss: -0.1981, G_loss: 0.4566\n",
      "  Batch [60/1299] D_loss: -0.1748, G_loss: 0.3528\n",
      "  Batch [70/1299] D_loss: -0.5633, G_loss: -0.6282\n",
      "  Batch [80/1299] D_loss: -0.4268, G_loss: -0.2255\n",
      "  Batch [90/1299] D_loss: -0.8865, G_loss: -1.7725\n",
      "  Batch [100/1299] D_loss: -0.9834, G_loss: -0.4275\n",
      "  Batch [110/1299] D_loss: -0.0918, G_loss: 0.2883\n",
      "  Batch [120/1299] D_loss: -0.0998, G_loss: 0.5056\n",
      "  Batch [130/1299] D_loss: -0.0819, G_loss: 0.6541\n",
      "  Batch [140/1299] D_loss: -0.0371, G_loss: 0.6317\n",
      "  Batch [150/1299] D_loss: -0.0163, G_loss: 0.5625\n",
      "  Batch [160/1299] D_loss: -0.0180, G_loss: 0.4420\n",
      "  Batch [170/1299] D_loss: 0.0024, G_loss: 0.1861\n",
      "  Batch [180/1299] D_loss: -0.8878, G_loss: -1.0869\n",
      "  Batch [190/1299] D_loss: -0.8307, G_loss: -0.3797\n",
      "  Batch [200/1299] D_loss: -0.3525, G_loss: -0.1539\n",
      "  Batch [210/1299] D_loss: -0.2652, G_loss: -0.0238\n",
      "  Batch [220/1299] D_loss: -0.8284, G_loss: -2.1122\n",
      "  Batch [230/1299] D_loss: 0.0017, G_loss: 0.2243\n",
      "  Batch [240/1299] D_loss: -0.0592, G_loss: 0.3969\n",
      "  Batch [250/1299] D_loss: 0.0017, G_loss: 0.3805\n",
      "  Batch [260/1299] D_loss: -0.0645, G_loss: 0.5965\n",
      "  Batch [270/1299] D_loss: -0.2081, G_loss: 0.5654\n",
      "  Batch [280/1299] D_loss: -0.0477, G_loss: 0.2434\n",
      "  Batch [290/1299] D_loss: -0.1454, G_loss: 0.2265\n",
      "  Batch [300/1299] D_loss: -0.6903, G_loss: -1.6619\n",
      "  Batch [310/1299] D_loss: -0.1143, G_loss: 0.0583\n",
      "  Batch [320/1299] D_loss: -0.0742, G_loss: 0.0185\n",
      "  Batch [330/1299] D_loss: -0.0025, G_loss: 0.1512\n",
      "  Batch [340/1299] D_loss: 0.0019, G_loss: 0.2919\n",
      "  Batch [350/1299] D_loss: -0.0927, G_loss: 0.4676\n",
      "  Batch [360/1299] D_loss: -0.1917, G_loss: 0.5215\n",
      "  Batch [370/1299] D_loss: -0.5241, G_loss: -0.6730\n",
      "  Batch [380/1299] D_loss: -0.9313, G_loss: -1.0174\n",
      "  Batch [390/1299] D_loss: -0.0054, G_loss: 0.1622\n",
      "  Batch [400/1299] D_loss: -0.0844, G_loss: 0.3142\n",
      "  Batch [410/1299] D_loss: -0.0752, G_loss: 0.4589\n",
      "  Batch [420/1299] D_loss: -0.0309, G_loss: 0.4245\n",
      "  Batch [430/1299] D_loss: -0.0287, G_loss: 0.3836\n",
      "  Batch [440/1299] D_loss: -0.0073, G_loss: 0.2866\n",
      "  Batch [450/1299] D_loss: -0.4132, G_loss: -0.2416\n",
      "  Batch [460/1299] D_loss: 0.0068, G_loss: 0.1360\n",
      "  Batch [470/1299] D_loss: -1.2737, G_loss: -0.9573\n",
      "  Batch [480/1299] D_loss: -0.1211, G_loss: 0.0510\n",
      "  Batch [490/1299] D_loss: -0.0274, G_loss: 0.2388\n",
      "  Batch [500/1299] D_loss: -0.0092, G_loss: 0.3501\n",
      "  Batch [510/1299] D_loss: -0.1002, G_loss: 0.3515\n",
      "  Batch [520/1299] D_loss: -0.1199, G_loss: 0.4653\n",
      "  Batch [530/1299] D_loss: -0.0956, G_loss: 0.3335\n",
      "  Batch [540/1299] D_loss: -0.9785, G_loss: -1.1939\n",
      "  Batch [550/1299] D_loss: -1.1916, G_loss: -0.3235\n",
      "  Batch [560/1299] D_loss: -0.9194, G_loss: -0.2301\n",
      "  Batch [570/1299] D_loss: -0.1728, G_loss: -0.3053\n",
      "  Batch [580/1299] D_loss: -0.0297, G_loss: 0.0732\n",
      "  Batch [590/1299] D_loss: -0.0626, G_loss: 0.2739\n",
      "  Batch [600/1299] D_loss: -0.1017, G_loss: 0.3689\n",
      "  Batch [610/1299] D_loss: -0.0724, G_loss: 0.4854\n",
      "  Batch [620/1299] D_loss: -0.0158, G_loss: 0.4699\n",
      "  Batch [630/1299] D_loss: -0.0185, G_loss: 0.5470\n",
      "  Batch [640/1299] D_loss: -5.3014, G_loss: -5.8518\n",
      "  Batch [650/1299] D_loss: -0.8445, G_loss: -0.0381\n",
      "  Batch [660/1299] D_loss: -0.0644, G_loss: 0.2060\n",
      "  Batch [670/1299] D_loss: -0.5117, G_loss: -0.4523\n",
      "  Batch [680/1299] D_loss: -0.0886, G_loss: 0.2561\n",
      "  Batch [690/1299] D_loss: 0.0216, G_loss: 0.3912\n",
      "  Batch [700/1299] D_loss: -0.0860, G_loss: 0.4194\n",
      "  Batch [710/1299] D_loss: -0.0630, G_loss: 0.2939\n",
      "  Batch [720/1299] D_loss: -0.0445, G_loss: 0.2382\n",
      "  Batch [730/1299] D_loss: -0.2536, G_loss: 0.0401\n",
      "  Batch [740/1299] D_loss: -0.0583, G_loss: 0.0732\n",
      "  Batch [750/1299] D_loss: 0.0238, G_loss: 0.1303\n",
      "  Batch [760/1299] D_loss: 0.0100, G_loss: 0.1330\n",
      "  Batch [770/1299] D_loss: -0.0536, G_loss: 0.1894\n",
      "  Batch [780/1299] D_loss: -0.0423, G_loss: 0.2711\n",
      "  Batch [790/1299] D_loss: -0.0390, G_loss: 0.3253\n",
      "  Batch [800/1299] D_loss: 0.0141, G_loss: 0.2764\n",
      "  Batch [810/1299] D_loss: -0.0720, G_loss: 0.1107\n",
      "  Batch [820/1299] D_loss: -0.2113, G_loss: 0.0441\n",
      "  Batch [830/1299] D_loss: -0.2401, G_loss: -0.1271\n",
      "  Batch [840/1299] D_loss: -0.0606, G_loss: 0.2440\n",
      "  Batch [850/1299] D_loss: -0.1330, G_loss: 0.3684\n",
      "  Batch [860/1299] D_loss: -0.1293, G_loss: 0.5093\n",
      "  Batch [870/1299] D_loss: -0.1106, G_loss: 0.4653\n",
      "  Batch [880/1299] D_loss: 0.0547, G_loss: 0.6501\n",
      "  Batch [890/1299] D_loss: 0.0009, G_loss: 0.2764\n",
      "  Batch [900/1299] D_loss: -0.9331, G_loss: -3.5882\n",
      "  Batch [910/1299] D_loss: -0.0032, G_loss: 0.1345\n",
      "  Batch [920/1299] D_loss: -0.0994, G_loss: 0.2909\n",
      "  Batch [930/1299] D_loss: -0.0902, G_loss: 0.3233\n",
      "  Batch [940/1299] D_loss: -0.0608, G_loss: 0.3283\n",
      "  Batch [950/1299] D_loss: -0.0092, G_loss: 0.3060\n",
      "  Batch [960/1299] D_loss: -1.0460, G_loss: -1.3462\n",
      "  Batch [970/1299] D_loss: -0.0285, G_loss: 0.1636\n",
      "  Batch [980/1299] D_loss: -0.1016, G_loss: 0.2917\n",
      "  Batch [990/1299] D_loss: -0.0670, G_loss: 0.3704\n",
      "  Batch [1000/1299] D_loss: -0.1776, G_loss: 0.3841\n",
      "  Batch [1010/1299] D_loss: -0.1424, G_loss: 0.4332\n",
      "  Batch [1020/1299] D_loss: -0.0513, G_loss: 0.2604\n",
      "  Batch [1030/1299] D_loss: -0.3137, G_loss: -0.2653\n",
      "  Batch [1040/1299] D_loss: -0.1719, G_loss: -0.1351\n",
      "  Batch [1050/1299] D_loss: -0.0349, G_loss: 0.1415\n",
      "  Batch [1060/1299] D_loss: -0.1063, G_loss: 0.2055\n",
      "  Batch [1070/1299] D_loss: -0.3512, G_loss: 0.0453\n",
      "  Batch [1080/1299] D_loss: -0.0517, G_loss: 0.1255\n",
      "  Batch [1090/1299] D_loss: -1.4228, G_loss: -1.7070\n",
      "  Batch [1100/1299] D_loss: -0.6370, G_loss: 0.0507\n",
      "  Batch [1110/1299] D_loss: -0.0951, G_loss: 0.2942\n",
      "  Batch [1120/1299] D_loss: -0.1232, G_loss: 0.5555\n",
      "  Batch [1130/1299] D_loss: -0.1227, G_loss: 0.7098\n",
      "  Batch [1140/1299] D_loss: -0.2581, G_loss: 0.7369\n",
      "  Batch [1150/1299] D_loss: -0.0898, G_loss: 0.6540\n",
      "  Batch [1160/1299] D_loss: -0.0484, G_loss: 0.4611\n",
      "  Batch [1170/1299] D_loss: -0.1586, G_loss: 0.3864\n",
      "  Batch [1180/1299] D_loss: -0.3789, G_loss: -0.0769\n",
      "  Batch [1190/1299] D_loss: -0.8651, G_loss: -1.0719\n",
      "  Batch [1200/1299] D_loss: -0.0509, G_loss: 0.2266\n",
      "  Batch [1210/1299] D_loss: -0.0884, G_loss: 0.3199\n",
      "  Batch [1220/1299] D_loss: -0.0318, G_loss: 0.3831\n",
      "  Batch [1230/1299] D_loss: -0.1172, G_loss: 0.3432\n",
      "  Batch [1240/1299] D_loss: -0.0182, G_loss: 0.3739\n",
      "  Batch [1250/1299] D_loss: -0.0485, G_loss: 0.2780\n",
      "  Batch [1260/1299] D_loss: -0.5765, G_loss: -1.2802\n",
      "  Batch [1270/1299] D_loss: -0.0048, G_loss: 0.0349\n",
      "  Batch [1280/1299] D_loss: -0.0926, G_loss: 0.1274\n",
      "  Batch [1290/1299] D_loss: -1.0900, G_loss: -2.9330\n",
      "\n",
      "Epoch 28 Summary:\n",
      "  Average D_loss: -0.1547\n",
      "  Average G_loss: -0.0900\n",
      "\n",
      "Epoch [29/100]\n",
      "  Batch [0/1299] D_loss: -0.0356, G_loss: 0.1622\n",
      "  Batch [10/1299] D_loss: -0.1111, G_loss: 0.3931\n",
      "  Batch [20/1299] D_loss: -0.1153, G_loss: 0.6424\n",
      "  Batch [30/1299] D_loss: -0.1686, G_loss: 0.7466\n",
      "  Batch [40/1299] D_loss: -0.3551, G_loss: 0.6572\n",
      "  Batch [50/1299] D_loss: -0.2113, G_loss: 0.5939\n",
      "  Batch [60/1299] D_loss: -0.1339, G_loss: 0.3517\n",
      "  Batch [70/1299] D_loss: -0.8066, G_loss: -3.2014\n",
      "  Batch [80/1299] D_loss: -0.1953, G_loss: -0.3287\n",
      "  Batch [90/1299] D_loss: -0.0272, G_loss: 0.0984\n",
      "  Batch [100/1299] D_loss: -0.0609, G_loss: 0.1280\n",
      "  Batch [110/1299] D_loss: -0.1095, G_loss: 0.3227\n",
      "  Batch [120/1299] D_loss: -0.0850, G_loss: 0.5622\n",
      "  Batch [130/1299] D_loss: -0.0826, G_loss: 0.5417\n",
      "  Batch [140/1299] D_loss: -0.0892, G_loss: 0.5132\n",
      "  Batch [150/1299] D_loss: -0.0323, G_loss: 0.2894\n",
      "  Batch [160/1299] D_loss: -1.7833, G_loss: -4.5088\n",
      "  Batch [170/1299] D_loss: -0.1648, G_loss: 0.0926\n",
      "  Batch [180/1299] D_loss: -0.0917, G_loss: 0.2183\n",
      "  Batch [190/1299] D_loss: -0.0648, G_loss: 0.4358\n",
      "  Batch [200/1299] D_loss: -0.1719, G_loss: 0.4752\n",
      "  Batch [210/1299] D_loss: -0.0249, G_loss: 0.5370\n",
      "  Batch [220/1299] D_loss: -0.0386, G_loss: 0.2841\n",
      "  Batch [230/1299] D_loss: -0.0690, G_loss: 0.1973\n",
      "  Batch [240/1299] D_loss: -1.4039, G_loss: -1.7199\n",
      "  Batch [250/1299] D_loss: -0.1772, G_loss: 0.0543\n",
      "  Batch [260/1299] D_loss: -0.6387, G_loss: -0.3903\n",
      "  Batch [270/1299] D_loss: -1.0084, G_loss: -2.0780\n",
      "  Batch [280/1299] D_loss: -0.0200, G_loss: 0.2965\n",
      "  Batch [290/1299] D_loss: -0.1076, G_loss: 0.4924\n",
      "  Batch [300/1299] D_loss: -0.1849, G_loss: 0.6220\n",
      "  Batch [310/1299] D_loss: -0.1261, G_loss: 0.6191\n",
      "  Batch [320/1299] D_loss: -0.1314, G_loss: 0.5416\n",
      "  Batch [330/1299] D_loss: -0.0361, G_loss: 0.2172\n",
      "  Batch [340/1299] D_loss: -0.3916, G_loss: -0.5366\n",
      "  Batch [350/1299] D_loss: -0.0008, G_loss: 0.1566\n",
      "  Batch [360/1299] D_loss: -0.0401, G_loss: 0.1997\n",
      "  Batch [370/1299] D_loss: -0.1140, G_loss: 0.3096\n",
      "  Batch [380/1299] D_loss: -0.2422, G_loss: 0.3424\n",
      "  Batch [390/1299] D_loss: -2.9237, G_loss: -1.2375\n",
      "  Batch [400/1299] D_loss: -0.0349, G_loss: 0.1215\n",
      "  Batch [410/1299] D_loss: 0.0240, G_loss: 0.1608\n",
      "  Batch [420/1299] D_loss: -0.0904, G_loss: -0.4147\n",
      "  Batch [430/1299] D_loss: -0.0671, G_loss: 0.1092\n",
      "  Batch [440/1299] D_loss: -0.3827, G_loss: -0.2584\n",
      "  Batch [450/1299] D_loss: 0.0010, G_loss: 0.1186\n",
      "  Batch [460/1299] D_loss: -0.0839, G_loss: 0.3814\n",
      "  Batch [470/1299] D_loss: -0.2080, G_loss: 0.5252\n",
      "  Batch [480/1299] D_loss: -0.0308, G_loss: 0.4960\n",
      "  Batch [490/1299] D_loss: -0.0573, G_loss: 0.3614\n",
      "  Batch [500/1299] D_loss: -0.1208, G_loss: 0.1883\n",
      "  Batch [510/1299] D_loss: -0.4965, G_loss: -0.8073\n",
      "  Batch [520/1299] D_loss: -0.4995, G_loss: -0.0398\n",
      "  Batch [530/1299] D_loss: -0.2297, G_loss: -0.1980\n",
      "  Batch [540/1299] D_loss: -0.9187, G_loss: -0.7160\n",
      "  Batch [550/1299] D_loss: -0.1069, G_loss: 0.1304\n",
      "  Batch [560/1299] D_loss: -1.0316, G_loss: -0.4336\n",
      "  Batch [570/1299] D_loss: -0.5865, G_loss: -0.2386\n",
      "  Batch [580/1299] D_loss: -0.0937, G_loss: 0.2671\n",
      "  Batch [590/1299] D_loss: -0.1055, G_loss: 0.4955\n",
      "  Batch [600/1299] D_loss: -0.1775, G_loss: 0.6196\n",
      "  Batch [610/1299] D_loss: -0.1979, G_loss: 0.8738\n",
      "  Batch [620/1299] D_loss: 0.0361, G_loss: 0.4260\n",
      "  Batch [630/1299] D_loss: -3.3323, G_loss: -0.9824\n",
      "  Batch [640/1299] D_loss: -0.0320, G_loss: 0.1618\n",
      "  Batch [650/1299] D_loss: -0.6846, G_loss: -0.4884\n",
      "  Batch [660/1299] D_loss: -0.3073, G_loss: -0.4103\n",
      "  Batch [670/1299] D_loss: -0.8580, G_loss: -0.9765\n",
      "  Batch [680/1299] D_loss: -0.4537, G_loss: -0.0074\n",
      "  Batch [690/1299] D_loss: -1.6030, G_loss: -0.5627\n",
      "  Batch [700/1299] D_loss: -0.0660, G_loss: 0.3759\n",
      "  Batch [710/1299] D_loss: -0.1153, G_loss: 0.6829\n",
      "  Batch [720/1299] D_loss: -0.1941, G_loss: 0.8526\n",
      "  Batch [730/1299] D_loss: 0.0058, G_loss: 0.6166\n",
      "  Batch [740/1299] D_loss: -0.0131, G_loss: 0.3659\n",
      "  Batch [750/1299] D_loss: -1.4361, G_loss: -1.5618\n",
      "  Batch [760/1299] D_loss: -0.2152, G_loss: 0.0199\n",
      "  Batch [770/1299] D_loss: -0.4533, G_loss: -0.5825\n",
      "  Batch [780/1299] D_loss: -0.5325, G_loss: -2.0846\n",
      "  Batch [790/1299] D_loss: -0.4575, G_loss: -0.1915\n",
      "  Batch [800/1299] D_loss: -0.0809, G_loss: 0.1992\n",
      "  Batch [810/1299] D_loss: -0.0226, G_loss: 0.3395\n",
      "  Batch [820/1299] D_loss: -0.0410, G_loss: 0.4571\n",
      "  Batch [830/1299] D_loss: -0.0450, G_loss: 0.4769\n",
      "  Batch [840/1299] D_loss: -0.0129, G_loss: 0.3586\n",
      "  Batch [850/1299] D_loss: -0.0288, G_loss: 0.3333\n",
      "  Batch [860/1299] D_loss: -2.4885, G_loss: -5.0019\n",
      "  Batch [870/1299] D_loss: -0.0055, G_loss: 0.1429\n",
      "  Batch [880/1299] D_loss: -0.0894, G_loss: 0.2808\n",
      "  Batch [890/1299] D_loss: -0.0539, G_loss: 0.3734\n",
      "  Batch [900/1299] D_loss: -0.0223, G_loss: 0.2333\n",
      "  Batch [910/1299] D_loss: -0.0493, G_loss: 0.2092\n",
      "  Batch [920/1299] D_loss: -0.2039, G_loss: -0.7072\n",
      "  Batch [930/1299] D_loss: -1.4718, G_loss: -1.7227\n",
      "  Batch [940/1299] D_loss: -0.1737, G_loss: 0.2576\n",
      "  Batch [950/1299] D_loss: -0.6188, G_loss: -0.0324\n",
      "  Batch [960/1299] D_loss: -0.0866, G_loss: 0.1211\n",
      "  Batch [970/1299] D_loss: -0.0216, G_loss: 0.2315\n",
      "  Batch [980/1299] D_loss: -0.0121, G_loss: 0.3958\n",
      "  Batch [990/1299] D_loss: -0.0568, G_loss: 0.4544\n",
      "  Batch [1000/1299] D_loss: -0.0524, G_loss: 0.3933\n",
      "  Batch [1010/1299] D_loss: 0.0066, G_loss: 0.2667\n",
      "  Batch [1020/1299] D_loss: -0.3043, G_loss: -2.0159\n",
      "  Batch [1030/1299] D_loss: 0.0139, G_loss: 0.2419\n",
      "  Batch [1040/1299] D_loss: -0.0795, G_loss: 0.3729\n",
      "  Batch [1050/1299] D_loss: -0.0833, G_loss: 0.4973\n",
      "  Batch [1060/1299] D_loss: -0.0742, G_loss: 0.3779\n",
      "  Batch [1070/1299] D_loss: -0.0361, G_loss: 0.2135\n",
      "  Batch [1080/1299] D_loss: -0.8725, G_loss: -1.0972\n",
      "  Batch [1090/1299] D_loss: -0.1262, G_loss: 0.1499\n",
      "  Batch [1100/1299] D_loss: -0.1038, G_loss: 0.3188\n",
      "  Batch [1110/1299] D_loss: 0.0026, G_loss: 0.3895\n",
      "  Batch [1120/1299] D_loss: -0.1198, G_loss: 0.5471\n",
      "  Batch [1130/1299] D_loss: -0.0571, G_loss: 0.2531\n",
      "  Batch [1140/1299] D_loss: -0.0639, G_loss: 0.3125\n",
      "  Batch [1150/1299] D_loss: -0.8355, G_loss: -1.5590\n",
      "  Batch [1160/1299] D_loss: -1.2296, G_loss: -1.0916\n",
      "  Batch [1170/1299] D_loss: -0.1316, G_loss: 0.0533\n",
      "  Batch [1180/1299] D_loss: -0.0061, G_loss: 0.2569\n",
      "  Batch [1190/1299] D_loss: -0.0560, G_loss: 0.2813\n",
      "  Batch [1200/1299] D_loss: -0.1677, G_loss: 0.3788\n",
      "  Batch [1210/1299] D_loss: -0.0708, G_loss: 0.4024\n",
      "  Batch [1220/1299] D_loss: -1.8940, G_loss: -4.1479\n",
      "  Batch [1230/1299] D_loss: -0.9241, G_loss: -2.0424\n",
      "  Batch [1240/1299] D_loss: -0.8455, G_loss: 0.0041\n",
      "  Batch [1250/1299] D_loss: -0.2708, G_loss: -0.3756\n",
      "  Batch [1260/1299] D_loss: -0.0441, G_loss: 0.3080\n",
      "  Batch [1270/1299] D_loss: -0.0445, G_loss: 0.3669\n",
      "  Batch [1280/1299] D_loss: -0.0862, G_loss: 0.4781\n",
      "  Batch [1290/1299] D_loss: -0.1049, G_loss: 0.4847\n",
      "\n",
      "Epoch 29 Summary:\n",
      "  Average D_loss: -0.1479\n",
      "  Average G_loss: -0.0321\n",
      "\n",
      "Epoch [30/100]\n",
      "  Batch [0/1299] D_loss: 0.0125, G_loss: 0.3418\n",
      "  Batch [10/1299] D_loss: -0.0443, G_loss: 0.2255\n",
      "  Batch [20/1299] D_loss: -0.7356, G_loss: -1.2524\n",
      "  Batch [30/1299] D_loss: -0.3004, G_loss: -0.5551\n",
      "  Batch [40/1299] D_loss: -0.1133, G_loss: 0.0756\n",
      "  Batch [50/1299] D_loss: -0.0473, G_loss: 0.1746\n",
      "  Batch [60/1299] D_loss: -0.0941, G_loss: 0.2002\n",
      "  Batch [70/1299] D_loss: -1.1644, G_loss: 0.0229\n",
      "  Batch [80/1299] D_loss: -0.0406, G_loss: 0.1641\n",
      "  Batch [90/1299] D_loss: -0.0766, G_loss: 0.3329\n",
      "  Batch [100/1299] D_loss: -0.1932, G_loss: 0.4167\n",
      "  Batch [110/1299] D_loss: -0.0452, G_loss: 0.4821\n",
      "  Batch [120/1299] D_loss: -0.0353, G_loss: 0.4381\n",
      "  Batch [130/1299] D_loss: -0.0045, G_loss: 0.3598\n",
      "  Batch [140/1299] D_loss: -0.0754, G_loss: 0.2360\n",
      "  Batch [150/1299] D_loss: -0.0137, G_loss: 0.1269\n",
      "  Batch [160/1299] D_loss: 0.0119, G_loss: 0.0946\n",
      "  Batch [170/1299] D_loss: -0.0423, G_loss: 0.1106\n",
      "  Batch [180/1299] D_loss: -0.6993, G_loss: -0.3290\n",
      "  Batch [190/1299] D_loss: -0.0507, G_loss: -0.0427\n",
      "  Batch [200/1299] D_loss: -0.8690, G_loss: -0.1976\n",
      "  Batch [210/1299] D_loss: -0.2859, G_loss: -0.3601\n",
      "  Batch [220/1299] D_loss: -0.1382, G_loss: 0.2700\n",
      "  Batch [230/1299] D_loss: -0.1670, G_loss: 0.3859\n",
      "  Batch [240/1299] D_loss: -0.1650, G_loss: 0.6228\n",
      "  Batch [250/1299] D_loss: -0.0497, G_loss: 0.5309\n",
      "  Batch [260/1299] D_loss: -0.0276, G_loss: 0.3208\n",
      "  Batch [270/1299] D_loss: -0.6196, G_loss: -1.0266\n",
      "  Batch [280/1299] D_loss: -0.1275, G_loss: -0.0812\n",
      "  Batch [290/1299] D_loss: -0.0323, G_loss: 0.1167\n",
      "  Batch [300/1299] D_loss: -0.0899, G_loss: 0.2489\n",
      "  Batch [310/1299] D_loss: -0.1228, G_loss: 0.3199\n",
      "  Batch [320/1299] D_loss: -0.1208, G_loss: 0.3261\n",
      "  Batch [330/1299] D_loss: 0.0192, G_loss: 0.2137\n",
      "  Batch [340/1299] D_loss: -3.1705, G_loss: -5.6489\n",
      "  Batch [350/1299] D_loss: 0.0075, G_loss: 0.0780\n",
      "  Batch [360/1299] D_loss: -0.0252, G_loss: 0.2091\n",
      "  Batch [370/1299] D_loss: -0.0462, G_loss: 0.3703\n",
      "  Batch [380/1299] D_loss: -0.1287, G_loss: 0.3795\n",
      "  Batch [390/1299] D_loss: -0.0600, G_loss: 0.3317\n",
      "  Batch [400/1299] D_loss: -0.0005, G_loss: 0.2835\n",
      "  Batch [410/1299] D_loss: -0.9081, G_loss: -0.7831\n",
      "  Batch [420/1299] D_loss: -0.8938, G_loss: -1.0974\n",
      "  Batch [430/1299] D_loss: -0.2142, G_loss: 0.0890\n",
      "  Batch [440/1299] D_loss: -0.3138, G_loss: -0.0071\n",
      "  Batch [450/1299] D_loss: -0.5499, G_loss: 0.0807\n",
      "  Batch [460/1299] D_loss: -0.6480, G_loss: -0.2997\n",
      "  Batch [470/1299] D_loss: -0.0947, G_loss: 0.2347\n",
      "  Batch [480/1299] D_loss: -0.0479, G_loss: 0.3344\n",
      "  Batch [490/1299] D_loss: -0.1425, G_loss: 0.4958\n",
      "  Batch [500/1299] D_loss: -0.0222, G_loss: 0.4599\n",
      "  Batch [510/1299] D_loss: -0.0360, G_loss: 0.3896\n",
      "  Batch [520/1299] D_loss: -0.0429, G_loss: 0.2944\n",
      "  Batch [530/1299] D_loss: -1.5368, G_loss: -1.7312\n",
      "  Batch [540/1299] D_loss: 0.0089, G_loss: 0.1207\n",
      "  Batch [550/1299] D_loss: -0.5235, G_loss: -0.3924\n",
      "  Batch [560/1299] D_loss: -1.8342, G_loss: -0.5995\n",
      "  Batch [570/1299] D_loss: -0.0448, G_loss: 0.1614\n",
      "  Batch [580/1299] D_loss: -0.0711, G_loss: 0.3631\n",
      "  Batch [590/1299] D_loss: -0.1166, G_loss: 0.5803\n",
      "  Batch [600/1299] D_loss: -0.1738, G_loss: 0.5161\n",
      "  Batch [610/1299] D_loss: -0.0097, G_loss: 0.3787\n",
      "  Batch [620/1299] D_loss: -0.0431, G_loss: 0.2317\n",
      "  Batch [630/1299] D_loss: -0.0110, G_loss: 0.0985\n",
      "  Batch [640/1299] D_loss: -0.0760, G_loss: 0.1177\n",
      "  Batch [650/1299] D_loss: -0.1357, G_loss: 0.0890\n",
      "  Batch [660/1299] D_loss: -0.0322, G_loss: 0.2081\n",
      "  Batch [670/1299] D_loss: -0.1679, G_loss: 0.4113\n",
      "  Batch [680/1299] D_loss: -0.1744, G_loss: 0.4661\n",
      "  Batch [690/1299] D_loss: -0.0304, G_loss: 0.3792\n",
      "  Batch [700/1299] D_loss: -0.0513, G_loss: 0.2397\n",
      "  Batch [710/1299] D_loss: -1.0698, G_loss: -0.3810\n",
      "  Batch [720/1299] D_loss: -0.1160, G_loss: -0.1681\n",
      "  Batch [730/1299] D_loss: -0.0502, G_loss: 0.2699\n",
      "  Batch [740/1299] D_loss: -0.0804, G_loss: 0.4442\n",
      "  Batch [750/1299] D_loss: -0.0225, G_loss: 0.4571\n",
      "  Batch [760/1299] D_loss: -0.0080, G_loss: 0.5156\n",
      "  Batch [770/1299] D_loss: -0.0161, G_loss: 0.4557\n",
      "  Batch [780/1299] D_loss: -0.1405, G_loss: 0.1424\n",
      "  Batch [790/1299] D_loss: -0.0547, G_loss: 0.0994\n",
      "  Batch [800/1299] D_loss: -0.0352, G_loss: 0.2062\n",
      "  Batch [810/1299] D_loss: -0.0632, G_loss: 0.3419\n",
      "  Batch [820/1299] D_loss: -0.0122, G_loss: 0.4433\n",
      "  Batch [830/1299] D_loss: 0.0101, G_loss: 0.4220\n",
      "  Batch [840/1299] D_loss: -0.0716, G_loss: 0.3246\n",
      "  Batch [850/1299] D_loss: -0.0980, G_loss: 0.2335\n",
      "  Batch [860/1299] D_loss: -0.0421, G_loss: 0.1055\n",
      "  Batch [870/1299] D_loss: -0.7613, G_loss: -0.0140\n",
      "  Batch [880/1299] D_loss: -0.2929, G_loss: -0.5534\n",
      "  Batch [890/1299] D_loss: 0.0046, G_loss: 0.1610\n",
      "  Batch [900/1299] D_loss: -0.1316, G_loss: 0.2663\n",
      "  Batch [910/1299] D_loss: -0.0850, G_loss: 0.3089\n",
      "  Batch [920/1299] D_loss: -0.0376, G_loss: 0.2606\n",
      "  Batch [930/1299] D_loss: -2.4928, G_loss: -3.0513\n",
      "  Batch [940/1299] D_loss: -0.8647, G_loss: -0.5483\n",
      "  Batch [950/1299] D_loss: -0.0426, G_loss: 0.0721\n",
      "  Batch [960/1299] D_loss: -0.0728, G_loss: 0.1304\n",
      "  Batch [970/1299] D_loss: -0.1006, G_loss: 0.3320\n",
      "  Batch [980/1299] D_loss: -0.1286, G_loss: 0.3786\n",
      "  Batch [990/1299] D_loss: -0.0533, G_loss: 0.5198\n",
      "  Batch [1000/1299] D_loss: -0.0573, G_loss: 0.4051\n",
      "  Batch [1010/1299] D_loss: -0.0283, G_loss: 0.2196\n",
      "  Batch [1020/1299] D_loss: -1.0132, G_loss: -2.0423\n",
      "  Batch [1030/1299] D_loss: -0.9255, G_loss: -0.9445\n",
      "  Batch [1040/1299] D_loss: -0.1422, G_loss: 0.1780\n",
      "  Batch [1050/1299] D_loss: -0.0636, G_loss: 0.4072\n",
      "  Batch [1060/1299] D_loss: 0.0407, G_loss: 0.4685\n",
      "  Batch [1070/1299] D_loss: -0.1380, G_loss: 0.4074\n",
      "  Batch [1080/1299] D_loss: -0.1040, G_loss: 0.3639\n",
      "  Batch [1090/1299] D_loss: 0.0270, G_loss: 0.1665\n",
      "  Batch [1100/1299] D_loss: -0.3990, G_loss: -0.8105\n",
      "  Batch [1110/1299] D_loss: -0.1491, G_loss: -0.5559\n",
      "  Batch [1120/1299] D_loss: -0.0376, G_loss: 0.2397\n",
      "  Batch [1130/1299] D_loss: -0.0453, G_loss: 0.3598\n",
      "  Batch [1140/1299] D_loss: -0.0946, G_loss: 0.4934\n",
      "  Batch [1150/1299] D_loss: 0.0523, G_loss: 0.3747\n",
      "  Batch [1160/1299] D_loss: -0.1113, G_loss: 0.4206\n",
      "  Batch [1170/1299] D_loss: -1.1071, G_loss: -1.5763\n",
      "  Batch [1180/1299] D_loss: -0.0568, G_loss: 0.1951\n",
      "  Batch [1190/1299] D_loss: 0.0663, G_loss: 0.3363\n",
      "  Batch [1200/1299] D_loss: -0.1081, G_loss: 0.4482\n",
      "  Batch [1210/1299] D_loss: -1.6395, G_loss: -1.8051\n",
      "  Batch [1220/1299] D_loss: -0.0780, G_loss: 0.0123\n",
      "  Batch [1230/1299] D_loss: -0.1156, G_loss: 0.0171\n",
      "  Batch [1240/1299] D_loss: -0.0259, G_loss: 0.2899\n",
      "  Batch [1250/1299] D_loss: -0.1161, G_loss: 0.3118\n",
      "  Batch [1260/1299] D_loss: -0.0423, G_loss: 0.3905\n",
      "  Batch [1270/1299] D_loss: -0.1433, G_loss: 0.4783\n",
      "  Batch [1280/1299] D_loss: 0.0108, G_loss: 0.2671\n",
      "  Batch [1290/1299] D_loss: -0.1195, G_loss: -0.1631\n",
      "\n",
      "Epoch 30 Summary:\n",
      "  Average D_loss: -0.1354\n",
      "  Average G_loss: -0.0699\n",
      "\n",
      "Epoch [31/100]\n",
      "  Batch [0/1299] D_loss: -0.0009, G_loss: 0.1195\n",
      "  Batch [10/1299] D_loss: -0.0068, G_loss: 0.2667\n",
      "  Batch [20/1299] D_loss: -0.0361, G_loss: 0.4499\n",
      "  Batch [30/1299] D_loss: -0.0721, G_loss: 0.4281\n",
      "  Batch [40/1299] D_loss: -0.1032, G_loss: 0.3812\n",
      "  Batch [50/1299] D_loss: -1.8585, G_loss: -2.4609\n",
      "  Batch [60/1299] D_loss: -0.0030, G_loss: 0.1080\n",
      "  Batch [70/1299] D_loss: -0.0166, G_loss: 0.2522\n",
      "  Batch [80/1299] D_loss: -0.0109, G_loss: 0.3988\n",
      "  Batch [90/1299] D_loss: -0.0941, G_loss: 0.4648\n",
      "  Batch [100/1299] D_loss: -0.0701, G_loss: 0.5962\n",
      "  Batch [110/1299] D_loss: -0.0444, G_loss: 0.3741\n",
      "  Batch [120/1299] D_loss: -2.4031, G_loss: -3.7267\n",
      "  Batch [130/1299] D_loss: -0.4218, G_loss: 0.0064\n",
      "  Batch [140/1299] D_loss: 0.0106, G_loss: 0.2070\n",
      "  Batch [150/1299] D_loss: 0.0072, G_loss: 0.3348\n",
      "  Batch [160/1299] D_loss: -0.1427, G_loss: 0.4454\n",
      "  Batch [170/1299] D_loss: -0.0975, G_loss: 0.4971\n",
      "  Batch [180/1299] D_loss: 0.0077, G_loss: 0.2509\n",
      "  Batch [190/1299] D_loss: -2.1097, G_loss: -0.7733\n",
      "  Batch [200/1299] D_loss: -0.5568, G_loss: -0.6835\n",
      "  Batch [210/1299] D_loss: -0.6518, G_loss: -0.1431\n",
      "  Batch [220/1299] D_loss: -0.1718, G_loss: 0.0489\n",
      "  Batch [230/1299] D_loss: -0.9807, G_loss: -0.7337\n",
      "  Batch [240/1299] D_loss: -0.1047, G_loss: -0.1279\n",
      "  Batch [250/1299] D_loss: -0.0367, G_loss: 0.3239\n",
      "  Batch [260/1299] D_loss: -0.0766, G_loss: 0.5167\n",
      "  Batch [270/1299] D_loss: -0.0128, G_loss: 0.4393\n",
      "  Batch [280/1299] D_loss: -0.0350, G_loss: 0.4275\n",
      "  Batch [290/1299] D_loss: -0.0338, G_loss: 0.3208\n",
      "  Batch [300/1299] D_loss: -2.3350, G_loss: -4.8804\n",
      "  Batch [310/1299] D_loss: -1.1862, G_loss: -0.8511\n",
      "  Batch [320/1299] D_loss: 0.0064, G_loss: 0.2073\n",
      "  Batch [330/1299] D_loss: -0.0363, G_loss: 0.3038\n",
      "  Batch [340/1299] D_loss: -0.0243, G_loss: 0.3841\n",
      "  Batch [350/1299] D_loss: -0.0940, G_loss: 0.2534\n",
      "  Batch [360/1299] D_loss: -0.1043, G_loss: 0.2593\n",
      "  Batch [370/1299] D_loss: -0.0428, G_loss: 0.1103\n",
      "  Batch [380/1299] D_loss: -0.6098, G_loss: -0.3491\n",
      "  Batch [390/1299] D_loss: -0.0881, G_loss: 0.1457\n",
      "  Batch [400/1299] D_loss: -0.0596, G_loss: 0.2124\n",
      "  Batch [410/1299] D_loss: 0.0068, G_loss: 0.3498\n",
      "  Batch [420/1299] D_loss: -0.0904, G_loss: 0.4857\n",
      "  Batch [430/1299] D_loss: -0.0369, G_loss: 0.5119\n",
      "  Batch [440/1299] D_loss: -0.0509, G_loss: 0.3959\n",
      "  Batch [450/1299] D_loss: -0.2657, G_loss: -0.1588\n",
      "  Batch [460/1299] D_loss: -0.0022, G_loss: 0.0825\n",
      "  Batch [470/1299] D_loss: -0.6929, G_loss: -0.0621\n",
      "  Batch [480/1299] D_loss: -0.4864, G_loss: 0.0840\n",
      "  Batch [490/1299] D_loss: -0.4813, G_loss: -0.1145\n",
      "  Batch [500/1299] D_loss: -0.0519, G_loss: 0.3492\n",
      "  Batch [510/1299] D_loss: -0.1190, G_loss: 0.3597\n",
      "  Batch [520/1299] D_loss: -0.1795, G_loss: 0.5309\n",
      "  Batch [530/1299] D_loss: -0.0424, G_loss: 0.4690\n",
      "  Batch [540/1299] D_loss: -0.0455, G_loss: 0.3216\n",
      "  Batch [550/1299] D_loss: -0.0262, G_loss: 0.2255\n",
      "  Batch [560/1299] D_loss: -0.6996, G_loss: -0.1072\n",
      "  Batch [570/1299] D_loss: -0.5134, G_loss: -0.4427\n",
      "  Batch [580/1299] D_loss: -0.8812, G_loss: -0.4168\n",
      "  Batch [590/1299] D_loss: -0.0763, G_loss: 0.0959\n",
      "  Batch [600/1299] D_loss: -0.5337, G_loss: 0.0130\n",
      "  Batch [610/1299] D_loss: -0.7262, G_loss: -0.0879\n",
      "  Batch [620/1299] D_loss: -0.0774, G_loss: 0.2314\n",
      "  Batch [630/1299] D_loss: -0.1539, G_loss: 0.1162\n",
      "  Batch [640/1299] D_loss: -0.6573, G_loss: -0.0182\n",
      "  Batch [650/1299] D_loss: -1.5497, G_loss: -0.5801\n",
      "  Batch [660/1299] D_loss: -0.0750, G_loss: 0.2174\n",
      "  Batch [670/1299] D_loss: -0.1026, G_loss: 0.5024\n",
      "  Batch [680/1299] D_loss: -0.0659, G_loss: 0.6381\n",
      "  Batch [690/1299] D_loss: -0.0824, G_loss: 0.5603\n",
      "  Batch [700/1299] D_loss: -0.0048, G_loss: 0.5658\n",
      "  Batch [710/1299] D_loss: -1.0810, G_loss: -2.4759\n",
      "  Batch [720/1299] D_loss: -0.0173, G_loss: 0.0515\n",
      "  Batch [730/1299] D_loss: -0.2217, G_loss: 0.0038\n",
      "  Batch [740/1299] D_loss: -0.0201, G_loss: 0.1955\n",
      "  Batch [750/1299] D_loss: -0.0632, G_loss: 0.3710\n",
      "  Batch [760/1299] D_loss: -0.1823, G_loss: 0.6368\n",
      "  Batch [770/1299] D_loss: 0.0024, G_loss: 0.4218\n",
      "  Batch [780/1299] D_loss: -0.0592, G_loss: 0.4314\n",
      "  Batch [790/1299] D_loss: -0.1185, G_loss: 0.2891\n",
      "  Batch [800/1299] D_loss: -2.0627, G_loss: -3.3842\n",
      "  Batch [810/1299] D_loss: 0.0407, G_loss: 0.2509\n",
      "  Batch [820/1299] D_loss: -0.0422, G_loss: 0.3793\n",
      "  Batch [830/1299] D_loss: -0.1050, G_loss: 0.4099\n",
      "  Batch [840/1299] D_loss: -0.1447, G_loss: 0.4828\n",
      "  Batch [850/1299] D_loss: -0.0510, G_loss: 0.3672\n",
      "  Batch [860/1299] D_loss: -0.1457, G_loss: 0.3074\n",
      "  Batch [870/1299] D_loss: -0.0619, G_loss: 0.0953\n",
      "  Batch [880/1299] D_loss: -0.0463, G_loss: 0.1585\n",
      "  Batch [890/1299] D_loss: -0.0681, G_loss: 0.1787\n",
      "  Batch [900/1299] D_loss: -0.0409, G_loss: 0.2196\n",
      "  Batch [910/1299] D_loss: 0.0034, G_loss: 0.2596\n",
      "  Batch [920/1299] D_loss: -0.0582, G_loss: 0.3356\n",
      "  Batch [930/1299] D_loss: -1.9262, G_loss: -2.2855\n",
      "  Batch [940/1299] D_loss: -0.2514, G_loss: -0.2985\n",
      "  Batch [950/1299] D_loss: -0.7217, G_loss: -0.7420\n",
      "  Batch [960/1299] D_loss: 0.0153, G_loss: 0.1137\n",
      "  Batch [970/1299] D_loss: -2.1935, G_loss: -4.8820\n",
      "  Batch [980/1299] D_loss: -1.6348, G_loss: -1.6460\n",
      "  Batch [990/1299] D_loss: -0.0791, G_loss: 0.0565\n",
      "  Batch [1000/1299] D_loss: -0.0980, G_loss: 0.3075\n",
      "  Batch [1010/1299] D_loss: -0.1442, G_loss: 0.5639\n",
      "  Batch [1020/1299] D_loss: -0.0513, G_loss: 0.6178\n",
      "  Batch [1030/1299] D_loss: -0.1284, G_loss: 0.5255\n",
      "  Batch [1040/1299] D_loss: -0.0211, G_loss: 0.3823\n",
      "  Batch [1050/1299] D_loss: -0.0773, G_loss: 0.2183\n",
      "  Batch [1060/1299] D_loss: -0.4893, G_loss: -0.7659\n",
      "  Batch [1070/1299] D_loss: -0.6330, G_loss: -0.5848\n",
      "  Batch [1080/1299] D_loss: -0.0820, G_loss: 0.2619\n",
      "  Batch [1090/1299] D_loss: -0.0380, G_loss: 0.3563\n",
      "  Batch [1100/1299] D_loss: -0.1342, G_loss: 0.5835\n",
      "  Batch [1110/1299] D_loss: -0.0829, G_loss: 0.5158\n",
      "  Batch [1120/1299] D_loss: -0.0208, G_loss: 0.4383\n",
      "  Batch [1130/1299] D_loss: -1.3567, G_loss: -0.2304\n",
      "  Batch [1140/1299] D_loss: -1.4421, G_loss: -2.1328\n",
      "  Batch [1150/1299] D_loss: -2.8934, G_loss: -3.6988\n",
      "  Batch [1160/1299] D_loss: -0.3207, G_loss: -0.2353\n",
      "  Batch [1170/1299] D_loss: -0.3217, G_loss: 0.0714\n",
      "  Batch [1180/1299] D_loss: -0.0436, G_loss: 0.1763\n",
      "  Batch [1190/1299] D_loss: -0.5115, G_loss: -1.4610\n",
      "  Batch [1200/1299] D_loss: -0.0540, G_loss: 0.1346\n",
      "  Batch [1210/1299] D_loss: -0.2055, G_loss: 0.4900\n",
      "  Batch [1220/1299] D_loss: -0.1949, G_loss: 0.6590\n",
      "  Batch [1230/1299] D_loss: -0.0580, G_loss: 0.4954\n",
      "  Batch [1240/1299] D_loss: -0.0919, G_loss: 0.3727\n",
      "  Batch [1250/1299] D_loss: -0.4337, G_loss: -0.5462\n",
      "  Batch [1260/1299] D_loss: -0.6732, G_loss: -0.1015\n",
      "  Batch [1270/1299] D_loss: -0.8820, G_loss: -1.5350\n",
      "  Batch [1280/1299] D_loss: -0.0712, G_loss: 0.1841\n",
      "  Batch [1290/1299] D_loss: -0.0716, G_loss: 0.1880\n",
      "\n",
      "Epoch 31 Summary:\n",
      "  Average D_loss: -0.1440\n",
      "  Average G_loss: -0.0555\n",
      "\n",
      "Epoch [32/100]\n",
      "  Batch [0/1299] D_loss: -0.1257, G_loss: 0.2587\n",
      "  Batch [10/1299] D_loss: -0.0727, G_loss: 0.3979\n",
      "  Batch [20/1299] D_loss: -0.1721, G_loss: 0.3912\n",
      "  Batch [30/1299] D_loss: -0.1046, G_loss: 0.3500\n",
      "  Batch [40/1299] D_loss: -3.2225, G_loss: -2.3886\n",
      "  Batch [50/1299] D_loss: -0.0315, G_loss: 0.1471\n",
      "  Batch [60/1299] D_loss: -0.0508, G_loss: 0.2619\n",
      "  Batch [70/1299] D_loss: -0.1167, G_loss: 0.4073\n",
      "  Batch [80/1299] D_loss: -0.1466, G_loss: 0.4412\n",
      "  Batch [90/1299] D_loss: -0.0972, G_loss: 0.3824\n",
      "  Batch [100/1299] D_loss: -0.9363, G_loss: -1.0096\n",
      "  Batch [110/1299] D_loss: -0.3948, G_loss: -0.2908\n",
      "  Batch [120/1299] D_loss: -0.0919, G_loss: 0.2847\n",
      "  Batch [130/1299] D_loss: -0.1677, G_loss: 0.4577\n",
      "  Batch [140/1299] D_loss: -0.1649, G_loss: 0.7756\n",
      "  Batch [150/1299] D_loss: -0.1325, G_loss: 0.5710\n",
      "  Batch [160/1299] D_loss: -0.0054, G_loss: 0.4137\n",
      "  Batch [170/1299] D_loss: -2.8161, G_loss: -5.1060\n",
      "  Batch [180/1299] D_loss: -0.1369, G_loss: 0.0214\n",
      "  Batch [190/1299] D_loss: -0.2093, G_loss: -0.5346\n",
      "  Batch [200/1299] D_loss: -0.4183, G_loss: 0.1050\n",
      "  Batch [210/1299] D_loss: -0.4607, G_loss: -0.7319\n",
      "  Batch [220/1299] D_loss: -0.6165, G_loss: -0.9965\n",
      "  Batch [230/1299] D_loss: -0.0738, G_loss: 0.2745\n",
      "  Batch [240/1299] D_loss: -0.1122, G_loss: 0.4560\n",
      "  Batch [250/1299] D_loss: -0.0650, G_loss: 0.5074\n",
      "  Batch [260/1299] D_loss: 0.0240, G_loss: 0.5490\n",
      "  Batch [270/1299] D_loss: -0.0058, G_loss: 0.3401\n",
      "  Batch [280/1299] D_loss: -0.0478, G_loss: 0.1999\n",
      "  Batch [290/1299] D_loss: -0.5105, G_loss: -0.9295\n",
      "  Batch [300/1299] D_loss: -0.0495, G_loss: 0.2297\n",
      "  Batch [310/1299] D_loss: -0.0334, G_loss: 0.2867\n",
      "  Batch [320/1299] D_loss: -0.0403, G_loss: 0.2510\n",
      "  Batch [330/1299] D_loss: -1.4002, G_loss: -1.0004\n",
      "  Batch [340/1299] D_loss: 0.0050, G_loss: 0.1171\n",
      "  Batch [350/1299] D_loss: -0.0014, G_loss: 0.1052\n",
      "  Batch [360/1299] D_loss: -1.1020, G_loss: -1.1122\n",
      "  Batch [370/1299] D_loss: -0.6094, G_loss: -0.2429\n",
      "  Batch [380/1299] D_loss: -0.0318, G_loss: 0.3111\n",
      "  Batch [390/1299] D_loss: -0.1092, G_loss: 0.3912\n",
      "  Batch [400/1299] D_loss: -0.1199, G_loss: 0.4966\n",
      "  Batch [410/1299] D_loss: -0.0906, G_loss: 0.4856\n",
      "  Batch [420/1299] D_loss: 0.0359, G_loss: 0.3204\n",
      "  Batch [430/1299] D_loss: -0.1023, G_loss: 0.2475\n",
      "  Batch [440/1299] D_loss: -0.2521, G_loss: -0.4805\n",
      "  Batch [450/1299] D_loss: 0.0157, G_loss: 0.0987\n",
      "  Batch [460/1299] D_loss: -0.0315, G_loss: 0.1402\n",
      "  Batch [470/1299] D_loss: -0.1897, G_loss: 0.0872\n",
      "  Batch [480/1299] D_loss: -0.0924, G_loss: 0.0922\n",
      "  Batch [490/1299] D_loss: -0.0742, G_loss: 0.1887\n",
      "  Batch [500/1299] D_loss: -0.1250, G_loss: 0.5547\n",
      "  Batch [510/1299] D_loss: -0.1105, G_loss: 0.5479\n",
      "  Batch [520/1299] D_loss: -0.0746, G_loss: 0.6204\n",
      "  Batch [530/1299] D_loss: -0.1074, G_loss: 0.5024\n",
      "  Batch [540/1299] D_loss: -0.0113, G_loss: 0.2579\n",
      "  Batch [550/1299] D_loss: -0.4933, G_loss: -0.3992\n",
      "  Batch [560/1299] D_loss: -0.2665, G_loss: -0.1638\n",
      "  Batch [570/1299] D_loss: -1.0775, G_loss: 0.0278\n",
      "  Batch [580/1299] D_loss: -0.6065, G_loss: -0.9344\n",
      "  Batch [590/1299] D_loss: -0.0808, G_loss: 0.2787\n",
      "  Batch [600/1299] D_loss: -0.1301, G_loss: 0.4815\n",
      "  Batch [610/1299] D_loss: -0.1400, G_loss: 0.5163\n",
      "  Batch [620/1299] D_loss: -0.1788, G_loss: 0.4674\n",
      "  Batch [630/1299] D_loss: -0.0339, G_loss: 0.3786\n",
      "  Batch [640/1299] D_loss: -0.0762, G_loss: 0.3171\n",
      "  Batch [650/1299] D_loss: -0.2187, G_loss: 0.1059\n",
      "  Batch [660/1299] D_loss: -0.4021, G_loss: -0.1470\n",
      "  Batch [670/1299] D_loss: -0.1150, G_loss: 0.2726\n",
      "  Batch [680/1299] D_loss: -0.0794, G_loss: 0.3534\n",
      "  Batch [690/1299] D_loss: -0.1405, G_loss: 0.4203\n",
      "  Batch [700/1299] D_loss: -0.0894, G_loss: 0.5009\n",
      "  Batch [710/1299] D_loss: -0.0384, G_loss: 0.2425\n",
      "  Batch [720/1299] D_loss: -1.0012, G_loss: -2.2686\n",
      "  Batch [730/1299] D_loss: -0.0067, G_loss: 0.2266\n",
      "  Batch [740/1299] D_loss: -0.0423, G_loss: 0.2681\n",
      "  Batch [750/1299] D_loss: -2.7729, G_loss: -4.6572\n",
      "  Batch [760/1299] D_loss: -0.1277, G_loss: -0.0955\n",
      "  Batch [770/1299] D_loss: -0.0217, G_loss: 0.2572\n",
      "  Batch [780/1299] D_loss: -0.0359, G_loss: 0.3269\n",
      "  Batch [790/1299] D_loss: -0.0898, G_loss: 0.4317\n",
      "  Batch [800/1299] D_loss: 0.0202, G_loss: 0.2234\n",
      "  Batch [810/1299] D_loss: -1.8734, G_loss: -2.4317\n",
      "  Batch [820/1299] D_loss: -0.1657, G_loss: 0.0233\n",
      "  Batch [830/1299] D_loss: -0.0375, G_loss: 0.0936\n",
      "  Batch [840/1299] D_loss: 0.0098, G_loss: 0.1451\n",
      "  Batch [850/1299] D_loss: -0.0355, G_loss: 0.1780\n",
      "  Batch [860/1299] D_loss: -0.0184, G_loss: 0.2614\n",
      "  Batch [870/1299] D_loss: -0.0555, G_loss: 0.3841\n",
      "  Batch [880/1299] D_loss: -3.5473, G_loss: -4.0268\n",
      "  Batch [890/1299] D_loss: -0.5313, G_loss: -1.0726\n",
      "  Batch [900/1299] D_loss: 0.0060, G_loss: 0.1750\n",
      "  Batch [910/1299] D_loss: -0.1274, G_loss: 0.4551\n",
      "  Batch [920/1299] D_loss: -0.1115, G_loss: 0.5007\n",
      "  Batch [930/1299] D_loss: -0.1182, G_loss: 0.3489\n",
      "  Batch [940/1299] D_loss: -0.0635, G_loss: 0.2541\n",
      "  Batch [950/1299] D_loss: -1.1610, G_loss: -0.6781\n",
      "  Batch [960/1299] D_loss: -0.1177, G_loss: 0.0929\n",
      "  Batch [970/1299] D_loss: -0.2362, G_loss: -0.1105\n",
      "  Batch [980/1299] D_loss: -0.0649, G_loss: 0.2599\n",
      "  Batch [990/1299] D_loss: -0.0851, G_loss: 0.4285\n",
      "  Batch [1000/1299] D_loss: -0.0198, G_loss: 0.5576\n",
      "  Batch [1010/1299] D_loss: -0.0297, G_loss: 0.4508\n",
      "  Batch [1020/1299] D_loss: -0.0421, G_loss: 0.5539\n",
      "  Batch [1030/1299] D_loss: -0.0613, G_loss: 0.2565\n",
      "  Batch [1040/1299] D_loss: 0.0021, G_loss: 0.1227\n",
      "  Batch [1050/1299] D_loss: -0.0333, G_loss: 0.1656\n",
      "  Batch [1060/1299] D_loss: -0.0494, G_loss: 0.2714\n",
      "  Batch [1070/1299] D_loss: -0.6175, G_loss: -0.2891\n",
      "  Batch [1080/1299] D_loss: -0.0015, G_loss: 0.2311\n",
      "  Batch [1090/1299] D_loss: 0.0051, G_loss: 0.3185\n",
      "  Batch [1100/1299] D_loss: -0.0915, G_loss: 0.4885\n",
      "  Batch [1110/1299] D_loss: -0.1194, G_loss: 0.4342\n",
      "  Batch [1120/1299] D_loss: -0.0372, G_loss: 0.2832\n",
      "  Batch [1130/1299] D_loss: -0.2801, G_loss: -0.0319\n",
      "  Batch [1140/1299] D_loss: -0.3234, G_loss: 0.0784\n",
      "  Batch [1150/1299] D_loss: -1.3909, G_loss: -3.3110\n",
      "  Batch [1160/1299] D_loss: 0.0012, G_loss: 0.1811\n",
      "  Batch [1170/1299] D_loss: -0.0295, G_loss: 0.3064\n",
      "  Batch [1180/1299] D_loss: -0.0681, G_loss: 0.3000\n",
      "  Batch [1190/1299] D_loss: -0.0948, G_loss: 0.3715\n",
      "  Batch [1200/1299] D_loss: -0.0810, G_loss: 0.3278\n",
      "  Batch [1210/1299] D_loss: -0.0321, G_loss: 0.2007\n",
      "  Batch [1220/1299] D_loss: -0.8180, G_loss: -0.4406\n",
      "  Batch [1230/1299] D_loss: 0.0194, G_loss: 0.2735\n",
      "  Batch [1240/1299] D_loss: -0.0136, G_loss: 0.2687\n",
      "  Batch [1250/1299] D_loss: -0.0624, G_loss: 0.2938\n",
      "  Batch [1260/1299] D_loss: -0.0837, G_loss: 0.2053\n",
      "  Batch [1270/1299] D_loss: -0.0384, G_loss: -0.0054\n",
      "  Batch [1280/1299] D_loss: -0.7881, G_loss: -0.5953\n",
      "  Batch [1290/1299] D_loss: -0.7408, G_loss: 0.0825\n",
      "\n",
      "Epoch 32 Summary:\n",
      "  Average D_loss: -0.1401\n",
      "  Average G_loss: -0.0570\n",
      "\n",
      "Epoch [33/100]\n",
      "  Batch [0/1299] D_loss: -0.4108, G_loss: -0.0783\n",
      "  Batch [10/1299] D_loss: -0.1878, G_loss: 0.0917\n",
      "  Batch [20/1299] D_loss: -0.6689, G_loss: -0.8547\n",
      "  Batch [30/1299] D_loss: -0.0569, G_loss: 0.3691\n",
      "  Batch [40/1299] D_loss: -0.0917, G_loss: 0.5415\n",
      "  Batch [50/1299] D_loss: -0.1167, G_loss: 0.5732\n",
      "  Batch [60/1299] D_loss: -0.1385, G_loss: 0.6352\n",
      "  Batch [70/1299] D_loss: -0.0610, G_loss: 0.2476\n",
      "  Batch [80/1299] D_loss: -0.0529, G_loss: -0.0683\n",
      "  Batch [90/1299] D_loss: -0.0487, G_loss: 0.0680\n",
      "  Batch [100/1299] D_loss: -0.0145, G_loss: 0.1811\n",
      "  Batch [110/1299] D_loss: -0.0241, G_loss: 0.2664\n",
      "  Batch [120/1299] D_loss: -0.0643, G_loss: 0.2349\n",
      "  Batch [130/1299] D_loss: -2.5088, G_loss: -5.6023\n",
      "  Batch [140/1299] D_loss: -0.0186, G_loss: 0.0733\n",
      "  Batch [150/1299] D_loss: -0.0070, G_loss: 0.1697\n",
      "  Batch [160/1299] D_loss: -0.0320, G_loss: 0.2889\n",
      "  Batch [170/1299] D_loss: -0.1561, G_loss: 0.3873\n",
      "  Batch [180/1299] D_loss: -0.2668, G_loss: 0.3136\n",
      "  Batch [190/1299] D_loss: -0.4041, G_loss: -0.1516\n",
      "  Batch [200/1299] D_loss: 0.0246, G_loss: 0.1264\n",
      "  Batch [210/1299] D_loss: -1.4824, G_loss: -0.4755\n",
      "  Batch [220/1299] D_loss: -0.5562, G_loss: -0.2590\n",
      "  Batch [230/1299] D_loss: -0.4851, G_loss: -0.2636\n",
      "  Batch [240/1299] D_loss: -0.0907, G_loss: 0.2320\n",
      "  Batch [250/1299] D_loss: -0.0928, G_loss: 0.4088\n",
      "  Batch [260/1299] D_loss: -0.1611, G_loss: 0.7047\n",
      "  Batch [270/1299] D_loss: -0.1560, G_loss: 0.7386\n",
      "  Batch [280/1299] D_loss: -0.0408, G_loss: 0.5457\n",
      "  Batch [290/1299] D_loss: -0.1105, G_loss: 0.3742\n",
      "  Batch [300/1299] D_loss: -3.1100, G_loss: -6.0029\n",
      "  Batch [310/1299] D_loss: -0.5884, G_loss: -1.6402\n",
      "  Batch [320/1299] D_loss: -0.0986, G_loss: 0.1125\n",
      "  Batch [330/1299] D_loss: -0.0611, G_loss: 0.1942\n",
      "  Batch [340/1299] D_loss: -0.0686, G_loss: 0.3169\n",
      "  Batch [350/1299] D_loss: -0.0658, G_loss: 0.3884\n",
      "  Batch [360/1299] D_loss: -0.0314, G_loss: 0.4001\n",
      "  Batch [370/1299] D_loss: -0.1394, G_loss: 0.4392\n",
      "  Batch [380/1299] D_loss: 0.0516, G_loss: 0.2015\n",
      "  Batch [390/1299] D_loss: -0.8919, G_loss: -1.5206\n",
      "  Batch [400/1299] D_loss: -0.0225, G_loss: 0.1983\n",
      "  Batch [410/1299] D_loss: -0.0166, G_loss: 0.3134\n",
      "  Batch [420/1299] D_loss: -0.0314, G_loss: 0.4101\n",
      "  Batch [430/1299] D_loss: -0.2076, G_loss: 0.3382\n",
      "  Batch [440/1299] D_loss: -0.8825, G_loss: -0.4001\n",
      "  Batch [450/1299] D_loss: -0.0277, G_loss: 0.0981\n",
      "  Batch [460/1299] D_loss: 0.0477, G_loss: 0.0572\n",
      "  Batch [470/1299] D_loss: -0.3688, G_loss: -0.3405\n",
      "  Batch [480/1299] D_loss: -0.6158, G_loss: -0.3444\n",
      "  Batch [490/1299] D_loss: -0.5392, G_loss: 0.0520\n",
      "  Batch [500/1299] D_loss: -0.0886, G_loss: 0.1766\n",
      "  Batch [510/1299] D_loss: -0.0317, G_loss: 0.4549\n",
      "  Batch [520/1299] D_loss: -0.0999, G_loss: 0.5786\n",
      "  Batch [530/1299] D_loss: -0.0125, G_loss: 0.4610\n",
      "  Batch [540/1299] D_loss: -0.0595, G_loss: 0.3118\n",
      "  Batch [550/1299] D_loss: -0.0341, G_loss: 0.2782\n",
      "  Batch [560/1299] D_loss: -1.1344, G_loss: -2.2759\n",
      "  Batch [570/1299] D_loss: -0.0097, G_loss: 0.1115\n",
      "  Batch [580/1299] D_loss: -0.0491, G_loss: 0.2193\n",
      "  Batch [590/1299] D_loss: -0.0932, G_loss: 0.3531\n",
      "  Batch [600/1299] D_loss: -0.0326, G_loss: 0.2350\n",
      "  Batch [610/1299] D_loss: -0.0368, G_loss: 0.2658\n",
      "  Batch [620/1299] D_loss: -1.2037, G_loss: -0.8484\n",
      "  Batch [630/1299] D_loss: -0.4989, G_loss: 0.1395\n",
      "  Batch [640/1299] D_loss: -0.2502, G_loss: -0.1655\n",
      "  Batch [650/1299] D_loss: -0.0336, G_loss: 0.2842\n",
      "  Batch [660/1299] D_loss: -0.0950, G_loss: 0.2993\n",
      "  Batch [670/1299] D_loss: -0.1460, G_loss: 0.2711\n",
      "  Batch [680/1299] D_loss: -1.0928, G_loss: -1.4915\n",
      "  Batch [690/1299] D_loss: 0.0155, G_loss: 0.1209\n",
      "  Batch [700/1299] D_loss: -0.0326, G_loss: 0.2608\n",
      "  Batch [710/1299] D_loss: -0.0694, G_loss: 0.3429\n",
      "  Batch [720/1299] D_loss: -0.0456, G_loss: 0.3326\n",
      "  Batch [730/1299] D_loss: -0.0149, G_loss: 0.3480\n",
      "  Batch [740/1299] D_loss: -0.0479, G_loss: 0.1673\n",
      "  Batch [750/1299] D_loss: -0.4834, G_loss: -1.0140\n",
      "  Batch [760/1299] D_loss: -0.2701, G_loss: -0.2564\n",
      "  Batch [770/1299] D_loss: -0.0522, G_loss: 0.1864\n",
      "  Batch [780/1299] D_loss: -0.0467, G_loss: 0.2727\n",
      "  Batch [790/1299] D_loss: -0.0288, G_loss: 0.2420\n",
      "  Batch [800/1299] D_loss: -0.4313, G_loss: -0.0280\n",
      "  Batch [810/1299] D_loss: -0.0146, G_loss: 0.1405\n",
      "  Batch [820/1299] D_loss: -0.0713, G_loss: 0.3455\n",
      "  Batch [830/1299] D_loss: -0.0899, G_loss: 0.4952\n",
      "  Batch [840/1299] D_loss: 0.0256, G_loss: 0.3397\n",
      "  Batch [850/1299] D_loss: -0.1367, G_loss: 0.4358\n",
      "  Batch [860/1299] D_loss: -0.0754, G_loss: 0.2671\n",
      "  Batch [870/1299] D_loss: -0.0898, G_loss: 0.0195\n",
      "  Batch [880/1299] D_loss: -0.0359, G_loss: 0.1847\n",
      "  Batch [890/1299] D_loss: -0.0422, G_loss: 0.3225\n",
      "  Batch [900/1299] D_loss: -0.0660, G_loss: 0.3452\n",
      "  Batch [910/1299] D_loss: -0.0408, G_loss: 0.3444\n",
      "  Batch [920/1299] D_loss: -0.0310, G_loss: 0.3028\n",
      "  Batch [930/1299] D_loss: -0.5596, G_loss: -0.1875\n",
      "  Batch [940/1299] D_loss: -0.0163, G_loss: 0.0838\n",
      "  Batch [950/1299] D_loss: -0.0325, G_loss: 0.2705\n",
      "  Batch [960/1299] D_loss: -0.0639, G_loss: 0.2683\n",
      "  Batch [970/1299] D_loss: -0.0808, G_loss: 0.3606\n",
      "  Batch [980/1299] D_loss: -0.1058, G_loss: 0.2805\n",
      "  Batch [990/1299] D_loss: -1.7059, G_loss: -2.0532\n",
      "  Batch [1000/1299] D_loss: -0.6190, G_loss: -0.1599\n",
      "  Batch [1010/1299] D_loss: -0.0379, G_loss: 0.1885\n",
      "  Batch [1020/1299] D_loss: 0.0067, G_loss: 0.3196\n",
      "  Batch [1030/1299] D_loss: -0.0511, G_loss: 0.4098\n",
      "  Batch [1040/1299] D_loss: -0.0314, G_loss: 0.2955\n",
      "  Batch [1050/1299] D_loss: -1.0883, G_loss: -0.0363\n",
      "  Batch [1060/1299] D_loss: -0.4405, G_loss: -0.0142\n",
      "  Batch [1070/1299] D_loss: -0.9377, G_loss: -0.6138\n",
      "  Batch [1080/1299] D_loss: -0.0667, G_loss: 0.1119\n",
      "  Batch [1090/1299] D_loss: -0.0663, G_loss: 0.2879\n",
      "  Batch [1100/1299] D_loss: -0.0509, G_loss: 0.3690\n",
      "  Batch [1110/1299] D_loss: -0.0701, G_loss: 0.3872\n",
      "  Batch [1120/1299] D_loss: -0.0568, G_loss: 0.3007\n",
      "  Batch [1130/1299] D_loss: -0.0436, G_loss: 0.3113\n",
      "  Batch [1140/1299] D_loss: -0.8950, G_loss: -2.0996\n",
      "  Batch [1150/1299] D_loss: -0.6074, G_loss: -0.7015\n",
      "  Batch [1160/1299] D_loss: 0.0076, G_loss: 0.1734\n",
      "  Batch [1170/1299] D_loss: -0.0426, G_loss: 0.2396\n",
      "  Batch [1180/1299] D_loss: -0.0716, G_loss: 0.3083\n",
      "  Batch [1190/1299] D_loss: -0.0717, G_loss: 0.2511\n",
      "  Batch [1200/1299] D_loss: -2.5300, G_loss: -2.6250\n",
      "  Batch [1210/1299] D_loss: -0.0168, G_loss: 0.2254\n",
      "  Batch [1220/1299] D_loss: -0.0429, G_loss: 0.3641\n",
      "  Batch [1230/1299] D_loss: -0.0473, G_loss: 0.3262\n",
      "  Batch [1240/1299] D_loss: -0.0290, G_loss: 0.2448\n",
      "  Batch [1250/1299] D_loss: -1.6615, G_loss: -0.7509\n",
      "  Batch [1260/1299] D_loss: -1.6703, G_loss: -1.8112\n",
      "  Batch [1270/1299] D_loss: -0.0506, G_loss: 0.1606\n",
      "  Batch [1280/1299] D_loss: -0.0645, G_loss: 0.2988\n",
      "  Batch [1290/1299] D_loss: -0.0824, G_loss: 0.4057\n",
      "\n",
      "Epoch 33 Summary:\n",
      "  Average D_loss: -0.1247\n",
      "  Average G_loss: -0.0369\n",
      "\n",
      "Epoch [34/100]\n",
      "  Batch [0/1299] D_loss: -0.0648, G_loss: 0.4033\n",
      "  Batch [10/1299] D_loss: -0.1136, G_loss: 0.3762\n",
      "  Batch [20/1299] D_loss: -0.0084, G_loss: 0.1610\n",
      "  Batch [30/1299] D_loss: 0.0147, G_loss: 0.1046\n",
      "  Batch [40/1299] D_loss: -1.3552, G_loss: -0.8963\n",
      "  Batch [50/1299] D_loss: -1.2397, G_loss: -0.9425\n",
      "  Batch [60/1299] D_loss: -0.2828, G_loss: 0.0553\n",
      "  Batch [70/1299] D_loss: -0.0707, G_loss: 0.2852\n",
      "  Batch [80/1299] D_loss: -0.0331, G_loss: 0.4920\n",
      "  Batch [90/1299] D_loss: -0.1835, G_loss: 0.6552\n",
      "  Batch [100/1299] D_loss: -0.1553, G_loss: 0.5497\n",
      "  Batch [110/1299] D_loss: -0.0509, G_loss: 0.3749\n",
      "  Batch [120/1299] D_loss: -1.7700, G_loss: -5.9631\n",
      "  Batch [130/1299] D_loss: -0.1340, G_loss: 0.0681\n",
      "  Batch [140/1299] D_loss: -0.8491, G_loss: -3.7392\n",
      "  Batch [150/1299] D_loss: -0.1318, G_loss: 0.3451\n",
      "  Batch [160/1299] D_loss: -0.1639, G_loss: 0.5183\n",
      "  Batch [170/1299] D_loss: -0.0836, G_loss: 0.5121\n",
      "  Batch [180/1299] D_loss: -0.0705, G_loss: 0.4827\n",
      "  Batch [190/1299] D_loss: -0.0387, G_loss: 0.4357\n",
      "  Batch [200/1299] D_loss: -0.0479, G_loss: 0.2102\n",
      "  Batch [210/1299] D_loss: 0.0001, G_loss: 0.1216\n",
      "  Batch [220/1299] D_loss: -0.0039, G_loss: 0.1584\n",
      "  Batch [230/1299] D_loss: -0.0207, G_loss: 0.2083\n",
      "  Batch [240/1299] D_loss: -0.0500, G_loss: 0.2670\n",
      "  Batch [250/1299] D_loss: -3.2738, G_loss: -1.2661\n",
      "  Batch [260/1299] D_loss: -0.0198, G_loss: 0.2035\n",
      "  Batch [270/1299] D_loss: -0.0245, G_loss: 0.2373\n",
      "  Batch [280/1299] D_loss: 0.0006, G_loss: 0.1591\n",
      "  Batch [290/1299] D_loss: -2.1822, G_loss: -5.5112\n",
      "  Batch [300/1299] D_loss: -0.0183, G_loss: 0.1226\n",
      "  Batch [310/1299] D_loss: -0.0120, G_loss: 0.2951\n",
      "  Batch [320/1299] D_loss: -0.0155, G_loss: 0.4337\n",
      "  Batch [330/1299] D_loss: -0.0923, G_loss: 0.4452\n",
      "  Batch [340/1299] D_loss: -0.0725, G_loss: 0.3596\n",
      "  Batch [350/1299] D_loss: -0.7814, G_loss: -2.1946\n",
      "  Batch [360/1299] D_loss: -0.0399, G_loss: 0.1202\n",
      "  Batch [370/1299] D_loss: -0.0220, G_loss: 0.1364\n",
      "  Batch [380/1299] D_loss: 0.0058, G_loss: 0.0934\n",
      "  Batch [390/1299] D_loss: -0.0276, G_loss: 0.1617\n",
      "  Batch [400/1299] D_loss: -0.0608, G_loss: 0.2390\n",
      "  Batch [410/1299] D_loss: -1.1450, G_loss: -3.1772\n",
      "  Batch [420/1299] D_loss: -0.5548, G_loss: -0.8878\n",
      "  Batch [430/1299] D_loss: -0.1088, G_loss: 0.0596\n",
      "  Batch [440/1299] D_loss: 0.0304, G_loss: 0.1190\n",
      "  Batch [450/1299] D_loss: -0.1229, G_loss: 0.4134\n",
      "  Batch [460/1299] D_loss: -0.0059, G_loss: 0.4693\n",
      "  Batch [470/1299] D_loss: -0.1237, G_loss: 0.5355\n",
      "  Batch [480/1299] D_loss: -0.0724, G_loss: 0.4283\n",
      "  Batch [490/1299] D_loss: -0.0393, G_loss: 0.2671\n",
      "  Batch [500/1299] D_loss: -0.0620, G_loss: 0.1197\n",
      "  Batch [510/1299] D_loss: -0.4030, G_loss: -0.2275\n",
      "  Batch [520/1299] D_loss: -0.0432, G_loss: 0.2432\n",
      "  Batch [530/1299] D_loss: -0.0299, G_loss: 0.3708\n",
      "  Batch [540/1299] D_loss: -0.1935, G_loss: 0.4464\n",
      "  Batch [550/1299] D_loss: -0.1718, G_loss: 0.4802\n",
      "  Batch [560/1299] D_loss: -1.0237, G_loss: -1.3875\n",
      "  Batch [570/1299] D_loss: -0.0304, G_loss: 0.1019\n",
      "  Batch [580/1299] D_loss: -0.0359, G_loss: 0.2288\n",
      "  Batch [590/1299] D_loss: -0.0933, G_loss: 0.3831\n",
      "  Batch [600/1299] D_loss: -0.0957, G_loss: 0.4781\n",
      "  Batch [610/1299] D_loss: -0.0594, G_loss: 0.4918\n",
      "  Batch [620/1299] D_loss: -0.0492, G_loss: 0.3524\n",
      "  Batch [630/1299] D_loss: -0.0032, G_loss: 0.1297\n",
      "  Batch [640/1299] D_loss: -0.0469, G_loss: 0.1892\n",
      "  Batch [650/1299] D_loss: -0.1083, G_loss: 0.4164\n",
      "  Batch [660/1299] D_loss: -0.2078, G_loss: 0.5241\n",
      "  Batch [670/1299] D_loss: -0.0581, G_loss: 0.6256\n",
      "  Batch [680/1299] D_loss: -0.0292, G_loss: 0.3620\n",
      "  Batch [690/1299] D_loss: -0.6112, G_loss: -0.4769\n",
      "  Batch [700/1299] D_loss: -0.1589, G_loss: -0.2267\n",
      "  Batch [710/1299] D_loss: -0.4493, G_loss: -0.1167\n",
      "  Batch [720/1299] D_loss: 0.0123, G_loss: 0.0665\n",
      "  Batch [730/1299] D_loss: -0.0554, G_loss: 0.2282\n",
      "  Batch [740/1299] D_loss: -0.0973, G_loss: 0.4237\n",
      "  Batch [750/1299] D_loss: -0.0521, G_loss: 0.3759\n",
      "  Batch [760/1299] D_loss: -0.0900, G_loss: 0.4462\n",
      "  Batch [770/1299] D_loss: -0.1034, G_loss: 0.3334\n",
      "  Batch [780/1299] D_loss: -0.0160, G_loss: 0.0286\n",
      "  Batch [790/1299] D_loss: -0.0171, G_loss: 0.0806\n",
      "  Batch [800/1299] D_loss: -0.0123, G_loss: 0.1422\n",
      "  Batch [810/1299] D_loss: -0.0538, G_loss: 0.2008\n",
      "  Batch [820/1299] D_loss: -0.0884, G_loss: 0.2514\n",
      "  Batch [830/1299] D_loss: -0.1239, G_loss: -0.3666\n",
      "  Batch [840/1299] D_loss: -0.0809, G_loss: 0.1571\n",
      "  Batch [850/1299] D_loss: -0.0431, G_loss: 0.3159\n",
      "  Batch [860/1299] D_loss: -0.0165, G_loss: 0.3677\n",
      "  Batch [870/1299] D_loss: -0.0742, G_loss: 0.2607\n",
      "  Batch [880/1299] D_loss: -0.9336, G_loss: -0.6884\n",
      "  Batch [890/1299] D_loss: -0.0632, G_loss: 0.1257\n",
      "  Batch [900/1299] D_loss: -0.6023, G_loss: -0.5668\n",
      "  Batch [910/1299] D_loss: -0.9107, G_loss: -2.3921\n",
      "  Batch [920/1299] D_loss: -0.0459, G_loss: 0.1549\n",
      "  Batch [930/1299] D_loss: -0.0997, G_loss: 0.3301\n",
      "  Batch [940/1299] D_loss: -0.1136, G_loss: 0.4120\n",
      "  Batch [950/1299] D_loss: -0.0476, G_loss: 0.4388\n",
      "  Batch [960/1299] D_loss: -0.0479, G_loss: 0.2935\n",
      "  Batch [970/1299] D_loss: -0.0814, G_loss: 0.1649\n",
      "  Batch [980/1299] D_loss: -0.0044, G_loss: 0.1261\n",
      "  Batch [990/1299] D_loss: -0.0168, G_loss: 0.2240\n",
      "  Batch [1000/1299] D_loss: -0.0687, G_loss: 0.2662\n",
      "  Batch [1010/1299] D_loss: -0.1204, G_loss: 0.3252\n",
      "  Batch [1020/1299] D_loss: -0.0920, G_loss: 0.2756\n",
      "  Batch [1030/1299] D_loss: -0.3799, G_loss: -0.1559\n",
      "  Batch [1040/1299] D_loss: -0.2135, G_loss: 0.0992\n",
      "  Batch [1050/1299] D_loss: -0.0061, G_loss: 0.1555\n",
      "  Batch [1060/1299] D_loss: -0.0692, G_loss: 0.2177\n",
      "  Batch [1070/1299] D_loss: 0.0077, G_loss: 0.3813\n",
      "  Batch [1080/1299] D_loss: -0.0036, G_loss: 0.3505\n",
      "  Batch [1090/1299] D_loss: -0.1284, G_loss: 0.3013\n",
      "  Batch [1100/1299] D_loss: -1.3122, G_loss: -2.8085\n",
      "  Batch [1110/1299] D_loss: -0.4596, G_loss: 0.1362\n",
      "  Batch [1120/1299] D_loss: -0.0363, G_loss: 0.3132\n",
      "  Batch [1130/1299] D_loss: -0.0426, G_loss: 0.2983\n",
      "  Batch [1140/1299] D_loss: -0.0498, G_loss: 0.2088\n",
      "  Batch [1150/1299] D_loss: -0.0725, G_loss: 0.1830\n",
      "  Batch [1160/1299] D_loss: -0.7635, G_loss: -3.0211\n",
      "  Batch [1170/1299] D_loss: -0.6179, G_loss: -1.0568\n",
      "  Batch [1180/1299] D_loss: -0.2217, G_loss: -0.2427\n",
      "  Batch [1190/1299] D_loss: -0.0991, G_loss: 0.1972\n",
      "  Batch [1200/1299] D_loss: -0.0615, G_loss: 0.3972\n",
      "  Batch [1210/1299] D_loss: -0.1027, G_loss: 0.3707\n",
      "  Batch [1220/1299] D_loss: 0.0334, G_loss: 0.4592\n",
      "  Batch [1230/1299] D_loss: 0.0086, G_loss: 0.3764\n",
      "  Batch [1240/1299] D_loss: -2.7750, G_loss: -3.8601\n",
      "  Batch [1250/1299] D_loss: -0.2934, G_loss: -0.1407\n",
      "  Batch [1260/1299] D_loss: -0.6322, G_loss: -1.6359\n",
      "  Batch [1270/1299] D_loss: -0.0953, G_loss: 0.0543\n",
      "  Batch [1280/1299] D_loss: -0.0576, G_loss: 0.2332\n",
      "  Batch [1290/1299] D_loss: -0.1022, G_loss: 0.4197\n",
      "\n",
      "Epoch 34 Summary:\n",
      "  Average D_loss: -0.1147\n",
      "  Average G_loss: -0.0529\n",
      "\n",
      "Epoch [35/100]\n",
      "  Batch [0/1299] D_loss: -0.1816, G_loss: 0.4865\n",
      "  Batch [10/1299] D_loss: -0.1004, G_loss: 0.3445\n",
      "  Batch [20/1299] D_loss: 0.0022, G_loss: 0.3204\n",
      "  Batch [30/1299] D_loss: -2.8712, G_loss: -1.2167\n",
      "  Batch [40/1299] D_loss: -1.2334, G_loss: -2.1848\n",
      "  Batch [50/1299] D_loss: -0.0257, G_loss: 0.1984\n",
      "  Batch [60/1299] D_loss: -0.0846, G_loss: 0.3577\n",
      "  Batch [70/1299] D_loss: 0.0117, G_loss: 0.4111\n",
      "  Batch [80/1299] D_loss: -0.0513, G_loss: 0.4641\n",
      "  Batch [90/1299] D_loss: -0.0297, G_loss: 0.3131\n",
      "  Batch [100/1299] D_loss: 0.0036, G_loss: 0.1510\n",
      "  Batch [110/1299] D_loss: -1.1240, G_loss: -0.1419\n",
      "  Batch [120/1299] D_loss: -0.1393, G_loss: 0.1025\n",
      "  Batch [130/1299] D_loss: -0.0413, G_loss: 0.2961\n",
      "  Batch [140/1299] D_loss: -0.0518, G_loss: 0.2966\n",
      "  Batch [150/1299] D_loss: -0.0633, G_loss: 0.3994\n",
      "  Batch [160/1299] D_loss: 0.0047, G_loss: 0.3203\n",
      "  Batch [170/1299] D_loss: -0.8837, G_loss: -3.3498\n",
      "  Batch [180/1299] D_loss: -0.4820, G_loss: -0.7409\n",
      "  Batch [190/1299] D_loss: -0.2645, G_loss: 0.1277\n",
      "  Batch [200/1299] D_loss: -0.0895, G_loss: 0.3065\n",
      "  Batch [210/1299] D_loss: -0.9590, G_loss: -0.2623\n",
      "  Batch [220/1299] D_loss: -0.0412, G_loss: 0.0088\n",
      "  Batch [230/1299] D_loss: 0.0118, G_loss: 0.1645\n",
      "  Batch [240/1299] D_loss: -0.0722, G_loss: 0.3758\n",
      "  Batch [250/1299] D_loss: -0.0603, G_loss: 0.5469\n",
      "  Batch [260/1299] D_loss: 0.0546, G_loss: 0.5898\n",
      "  Batch [270/1299] D_loss: -0.0457, G_loss: 0.4473\n",
      "  Batch [280/1299] D_loss: -0.0618, G_loss: 0.1728\n",
      "  Batch [290/1299] D_loss: -0.3442, G_loss: -0.6850\n",
      "  Batch [300/1299] D_loss: -0.1243, G_loss: -0.1157\n",
      "  Batch [310/1299] D_loss: -1.0687, G_loss: -0.4311\n",
      "  Batch [320/1299] D_loss: -0.0357, G_loss: 0.1366\n",
      "  Batch [330/1299] D_loss: -0.0920, G_loss: 0.3527\n",
      "  Batch [340/1299] D_loss: -0.0061, G_loss: 0.4226\n",
      "  Batch [350/1299] D_loss: -0.2325, G_loss: 0.5625\n",
      "  Batch [360/1299] D_loss: -0.0739, G_loss: 0.5224\n",
      "  Batch [370/1299] D_loss: -0.0513, G_loss: 0.4223\n",
      "  Batch [380/1299] D_loss: -2.5732, G_loss: -1.7157\n",
      "  Batch [390/1299] D_loss: -0.0050, G_loss: 0.1191\n",
      "  Batch [400/1299] D_loss: -0.0109, G_loss: 0.2344\n",
      "  Batch [410/1299] D_loss: -0.0198, G_loss: 0.3410\n",
      "  Batch [420/1299] D_loss: -0.0426, G_loss: 0.3159\n",
      "  Batch [430/1299] D_loss: -0.1076, G_loss: 0.2440\n",
      "  Batch [440/1299] D_loss: -0.0808, G_loss: 0.1579\n",
      "  Batch [450/1299] D_loss: -0.0613, G_loss: 0.0945\n",
      "  Batch [460/1299] D_loss: -0.0839, G_loss: 0.2005\n",
      "  Batch [470/1299] D_loss: -0.1067, G_loss: 0.2136\n",
      "  Batch [480/1299] D_loss: -0.1612, G_loss: 0.3121\n",
      "  Batch [490/1299] D_loss: 0.0201, G_loss: 0.2768\n",
      "  Batch [500/1299] D_loss: -2.2356, G_loss: -4.5603\n",
      "  Batch [510/1299] D_loss: 0.0076, G_loss: 0.1882\n",
      "  Batch [520/1299] D_loss: -0.1414, G_loss: 0.4488\n",
      "  Batch [530/1299] D_loss: -0.1365, G_loss: 0.5441\n",
      "  Batch [540/1299] D_loss: -0.0033, G_loss: 0.3655\n",
      "  Batch [550/1299] D_loss: -0.0917, G_loss: 0.3966\n",
      "  Batch [560/1299] D_loss: -0.8763, G_loss: -1.5176\n",
      "  Batch [570/1299] D_loss: -0.0815, G_loss: 0.1414\n",
      "  Batch [580/1299] D_loss: -0.1012, G_loss: 0.4005\n",
      "  Batch [590/1299] D_loss: -0.0659, G_loss: 0.4688\n",
      "  Batch [600/1299] D_loss: -0.0621, G_loss: 0.5113\n",
      "  Batch [610/1299] D_loss: -0.0266, G_loss: 0.2922\n",
      "  Batch [620/1299] D_loss: -0.4337, G_loss: 0.0728\n",
      "  Batch [630/1299] D_loss: -0.0119, G_loss: 0.0961\n",
      "  Batch [640/1299] D_loss: -0.0112, G_loss: 0.1740\n",
      "  Batch [650/1299] D_loss: -0.0676, G_loss: 0.3075\n",
      "  Batch [660/1299] D_loss: -0.0467, G_loss: 0.3134\n",
      "  Batch [670/1299] D_loss: 0.0024, G_loss: 0.0180\n",
      "  Batch [680/1299] D_loss: -0.0562, G_loss: 0.2009\n",
      "  Batch [690/1299] D_loss: -0.0482, G_loss: 0.3165\n",
      "  Batch [700/1299] D_loss: 0.0245, G_loss: 0.2390\n",
      "  Batch [710/1299] D_loss: -0.1110, G_loss: 0.3573\n",
      "  Batch [720/1299] D_loss: -0.0614, G_loss: 0.2963\n",
      "  Batch [730/1299] D_loss: -1.0473, G_loss: -1.1117\n",
      "  Batch [740/1299] D_loss: -0.0070, G_loss: 0.1165\n",
      "  Batch [750/1299] D_loss: -0.0314, G_loss: 0.2562\n",
      "  Batch [760/1299] D_loss: -0.0207, G_loss: 0.2904\n",
      "  Batch [770/1299] D_loss: -0.1398, G_loss: 0.2215\n",
      "  Batch [780/1299] D_loss: -2.1628, G_loss: -3.9724\n",
      "  Batch [790/1299] D_loss: -0.0847, G_loss: 0.0308\n",
      "  Batch [800/1299] D_loss: -1.0661, G_loss: -1.6565\n",
      "  Batch [810/1299] D_loss: -0.0346, G_loss: 0.3018\n",
      "  Batch [820/1299] D_loss: -0.0377, G_loss: 0.4703\n",
      "  Batch [830/1299] D_loss: -0.0630, G_loss: 0.4865\n",
      "  Batch [840/1299] D_loss: -0.0321, G_loss: 0.4107\n",
      "  Batch [850/1299] D_loss: -0.0885, G_loss: 0.3467\n",
      "  Batch [860/1299] D_loss: -2.8864, G_loss: -4.4602\n",
      "  Batch [870/1299] D_loss: -0.5920, G_loss: 0.0004\n",
      "  Batch [880/1299] D_loss: -0.0616, G_loss: 0.2073\n",
      "  Batch [890/1299] D_loss: -0.0037, G_loss: 0.2416\n",
      "  Batch [900/1299] D_loss: -0.1118, G_loss: 0.3558\n",
      "  Batch [910/1299] D_loss: -0.0899, G_loss: 0.3106\n",
      "  Batch [920/1299] D_loss: -0.6890, G_loss: -2.5459\n",
      "  Batch [930/1299] D_loss: -0.0255, G_loss: 0.1117\n",
      "  Batch [940/1299] D_loss: -0.8072, G_loss: -0.7530\n",
      "  Batch [950/1299] D_loss: -0.6122, G_loss: -0.3488\n",
      "  Batch [960/1299] D_loss: -1.6269, G_loss: -0.4103\n",
      "  Batch [970/1299] D_loss: -0.0884, G_loss: 0.3077\n",
      "  Batch [980/1299] D_loss: -0.1326, G_loss: 0.4514\n",
      "  Batch [990/1299] D_loss: -0.1119, G_loss: 0.4879\n",
      "  Batch [1000/1299] D_loss: -0.0927, G_loss: 0.4402\n",
      "  Batch [1010/1299] D_loss: -0.0782, G_loss: 0.4492\n",
      "  Batch [1020/1299] D_loss: -1.9926, G_loss: -4.3958\n",
      "  Batch [1030/1299] D_loss: -0.9030, G_loss: -0.4645\n",
      "  Batch [1040/1299] D_loss: -0.2762, G_loss: 0.0942\n",
      "  Batch [1050/1299] D_loss: -0.0749, G_loss: 0.1540\n",
      "  Batch [1060/1299] D_loss: -1.5517, G_loss: -4.2205\n",
      "  Batch [1070/1299] D_loss: -0.1166, G_loss: 0.0760\n",
      "  Batch [1080/1299] D_loss: -0.0355, G_loss: 0.4623\n",
      "  Batch [1090/1299] D_loss: -0.1209, G_loss: 0.6629\n",
      "  Batch [1100/1299] D_loss: 0.0868, G_loss: 0.4478\n",
      "  Batch [1110/1299] D_loss: 0.0374, G_loss: 0.3668\n",
      "  Batch [1120/1299] D_loss: -0.0414, G_loss: 0.5143\n",
      "  Batch [1130/1299] D_loss: -0.0573, G_loss: 0.2709\n",
      "  Batch [1140/1299] D_loss: -0.1125, G_loss: -0.0511\n",
      "  Batch [1150/1299] D_loss: -0.8294, G_loss: -0.0220\n",
      "  Batch [1160/1299] D_loss: -0.3258, G_loss: -0.5170\n",
      "  Batch [1170/1299] D_loss: -0.3564, G_loss: -0.0282\n",
      "  Batch [1180/1299] D_loss: -0.3155, G_loss: 0.0938\n",
      "  Batch [1190/1299] D_loss: -0.0232, G_loss: 0.2629\n",
      "  Batch [1200/1299] D_loss: -0.1000, G_loss: 0.3832\n",
      "  Batch [1210/1299] D_loss: -0.1085, G_loss: 0.4495\n",
      "  Batch [1220/1299] D_loss: -0.0209, G_loss: 0.4728\n",
      "  Batch [1230/1299] D_loss: -0.0776, G_loss: 0.4988\n",
      "  Batch [1240/1299] D_loss: -0.0332, G_loss: 0.2305\n",
      "  Batch [1250/1299] D_loss: -2.2264, G_loss: -7.5221\n",
      "  Batch [1260/1299] D_loss: -0.4873, G_loss: -0.6581\n",
      "  Batch [1270/1299] D_loss: -0.5265, G_loss: -0.1421\n",
      "  Batch [1280/1299] D_loss: 0.0007, G_loss: 0.2673\n",
      "  Batch [1290/1299] D_loss: 0.0063, G_loss: 0.2876\n",
      "\n",
      "Epoch 35 Summary:\n",
      "  Average D_loss: -0.1401\n",
      "  Average G_loss: -0.0583\n",
      "\n",
      "Epoch [36/100]\n",
      "  Batch [0/1299] D_loss: -0.0856, G_loss: 0.4252\n",
      "  Batch [10/1299] D_loss: -0.0645, G_loss: 0.3662\n",
      "  Batch [20/1299] D_loss: -0.1248, G_loss: 0.2751\n",
      "  Batch [30/1299] D_loss: -1.5899, G_loss: -0.6631\n",
      "  Batch [40/1299] D_loss: -0.4530, G_loss: 0.0534\n",
      "  Batch [50/1299] D_loss: -0.3114, G_loss: 0.0096\n",
      "  Batch [60/1299] D_loss: -0.0535, G_loss: 0.2617\n",
      "  Batch [70/1299] D_loss: -0.0132, G_loss: 0.3995\n",
      "  Batch [80/1299] D_loss: -0.0107, G_loss: 0.3956\n",
      "  Batch [90/1299] D_loss: -0.0374, G_loss: 0.4337\n",
      "  Batch [100/1299] D_loss: -0.0137, G_loss: 0.3007\n",
      "  Batch [110/1299] D_loss: -0.6879, G_loss: -1.0920\n",
      "  Batch [120/1299] D_loss: -0.3795, G_loss: -0.1989\n",
      "  Batch [130/1299] D_loss: -0.4284, G_loss: -0.2642\n",
      "  Batch [140/1299] D_loss: -0.0179, G_loss: 0.2544\n",
      "  Batch [150/1299] D_loss: -0.1117, G_loss: 0.4601\n",
      "  Batch [160/1299] D_loss: -0.0796, G_loss: 0.5169\n",
      "  Batch [170/1299] D_loss: -0.0622, G_loss: 0.4169\n",
      "  Batch [180/1299] D_loss: -0.0377, G_loss: 0.2911\n",
      "  Batch [190/1299] D_loss: -0.1036, G_loss: -0.0989\n",
      "  Batch [200/1299] D_loss: -1.6539, G_loss: -1.7880\n",
      "  Batch [210/1299] D_loss: -0.1859, G_loss: 0.0155\n",
      "  Batch [220/1299] D_loss: -0.1445, G_loss: 0.4507\n",
      "  Batch [230/1299] D_loss: 0.0006, G_loss: 0.5493\n",
      "  Batch [240/1299] D_loss: 0.0335, G_loss: 0.5238\n",
      "  Batch [250/1299] D_loss: -0.0203, G_loss: 0.5023\n",
      "  Batch [260/1299] D_loss: -0.0344, G_loss: 0.3322\n",
      "  Batch [270/1299] D_loss: -0.0316, G_loss: 0.1975\n",
      "  Batch [280/1299] D_loss: -1.8404, G_loss: -1.1625\n",
      "  Batch [290/1299] D_loss: -0.2537, G_loss: -0.1175\n",
      "  Batch [300/1299] D_loss: -1.4075, G_loss: -2.0957\n",
      "  Batch [310/1299] D_loss: -0.0181, G_loss: 0.1651\n",
      "  Batch [320/1299] D_loss: -0.0323, G_loss: 0.2807\n",
      "  Batch [330/1299] D_loss: -0.0941, G_loss: 0.4277\n",
      "  Batch [340/1299] D_loss: -0.1020, G_loss: 0.4677\n",
      "  Batch [350/1299] D_loss: -0.0722, G_loss: 0.3745\n",
      "  Batch [360/1299] D_loss: -0.0488, G_loss: 0.2636\n",
      "  Batch [370/1299] D_loss: -2.3024, G_loss: -1.0901\n",
      "  Batch [380/1299] D_loss: -0.0399, G_loss: 0.0248\n",
      "  Batch [390/1299] D_loss: -0.0077, G_loss: 0.1902\n",
      "  Batch [400/1299] D_loss: 0.0018, G_loss: 0.1167\n",
      "  Batch [410/1299] D_loss: -1.8654, G_loss: -1.3396\n",
      "  Batch [420/1299] D_loss: -0.3837, G_loss: -0.4087\n",
      "  Batch [430/1299] D_loss: -0.0393, G_loss: 0.2508\n",
      "  Batch [440/1299] D_loss: -0.0321, G_loss: 0.3517\n",
      "  Batch [450/1299] D_loss: -0.0247, G_loss: 0.3978\n",
      "  Batch [460/1299] D_loss: -0.2141, G_loss: 0.5435\n",
      "  Batch [470/1299] D_loss: -0.2906, G_loss: 0.3594\n",
      "  Batch [480/1299] D_loss: -1.2789, G_loss: -4.0957\n",
      "  Batch [490/1299] D_loss: -0.0257, G_loss: 0.0859\n",
      "  Batch [500/1299] D_loss: -0.0651, G_loss: 0.2192\n",
      "  Batch [510/1299] D_loss: -0.0186, G_loss: 0.3093\n",
      "  Batch [520/1299] D_loss: -0.1321, G_loss: 0.5314\n",
      "  Batch [530/1299] D_loss: -0.0445, G_loss: 0.3631\n",
      "  Batch [540/1299] D_loss: -1.4783, G_loss: -2.3128\n",
      "  Batch [550/1299] D_loss: -0.6574, G_loss: -0.7137\n",
      "  Batch [560/1299] D_loss: -0.0126, G_loss: 0.1874\n",
      "  Batch [570/1299] D_loss: -0.2775, G_loss: -0.1253\n",
      "  Batch [580/1299] D_loss: -0.0661, G_loss: 0.2614\n",
      "  Batch [590/1299] D_loss: -0.0779, G_loss: 0.3641\n",
      "  Batch [600/1299] D_loss: -0.1250, G_loss: 0.5400\n",
      "  Batch [610/1299] D_loss: -0.0719, G_loss: 0.3665\n",
      "  Batch [620/1299] D_loss: -0.0490, G_loss: 0.4036\n",
      "  Batch [630/1299] D_loss: -0.0230, G_loss: 0.1956\n",
      "  Batch [640/1299] D_loss: -0.7192, G_loss: -1.3223\n",
      "  Batch [650/1299] D_loss: -0.9295, G_loss: -1.8966\n",
      "  Batch [660/1299] D_loss: -0.0538, G_loss: 0.1851\n",
      "  Batch [670/1299] D_loss: -0.0388, G_loss: 0.3167\n",
      "  Batch [680/1299] D_loss: -0.0670, G_loss: 0.4030\n",
      "  Batch [690/1299] D_loss: -0.1523, G_loss: 0.4395\n",
      "  Batch [700/1299] D_loss: -0.0217, G_loss: 0.3186\n",
      "  Batch [710/1299] D_loss: -0.0102, G_loss: 0.1977\n",
      "  Batch [720/1299] D_loss: -0.0386, G_loss: 0.0411\n",
      "  Batch [730/1299] D_loss: -0.1104, G_loss: -0.1926\n",
      "  Batch [740/1299] D_loss: -0.8689, G_loss: -0.1592\n",
      "  Batch [750/1299] D_loss: -0.0306, G_loss: 0.2732\n",
      "  Batch [760/1299] D_loss: -0.1823, G_loss: 0.5299\n",
      "  Batch [770/1299] D_loss: -0.0075, G_loss: 0.4696\n",
      "  Batch [780/1299] D_loss: -0.0224, G_loss: 0.4204\n",
      "  Batch [790/1299] D_loss: 0.0600, G_loss: 0.3614\n",
      "  Batch [800/1299] D_loss: -0.9626, G_loss: -3.2269\n",
      "  Batch [810/1299] D_loss: -0.5052, G_loss: 0.0783\n",
      "  Batch [820/1299] D_loss: -0.0501, G_loss: 0.1485\n",
      "  Batch [830/1299] D_loss: -0.4856, G_loss: -0.0593\n",
      "  Batch [840/1299] D_loss: -1.2612, G_loss: -0.9995\n",
      "  Batch [850/1299] D_loss: -0.0694, G_loss: 0.1417\n",
      "  Batch [860/1299] D_loss: -0.0543, G_loss: 0.3930\n",
      "  Batch [870/1299] D_loss: -0.1248, G_loss: 0.4634\n",
      "  Batch [880/1299] D_loss: -0.0707, G_loss: 0.6001\n",
      "  Batch [890/1299] D_loss: 0.0795, G_loss: 0.5121\n",
      "  Batch [900/1299] D_loss: -0.0050, G_loss: 0.3288\n",
      "  Batch [910/1299] D_loss: -0.0461, G_loss: 0.2591\n",
      "  Batch [920/1299] D_loss: -0.0436, G_loss: 0.1146\n",
      "  Batch [930/1299] D_loss: -0.5336, G_loss: -0.3785\n",
      "  Batch [940/1299] D_loss: -0.0414, G_loss: 0.2116\n",
      "  Batch [950/1299] D_loss: -0.0525, G_loss: 0.3505\n",
      "  Batch [960/1299] D_loss: -0.0518, G_loss: 0.4296\n",
      "  Batch [970/1299] D_loss: -0.1771, G_loss: 0.4160\n",
      "  Batch [980/1299] D_loss: -0.0658, G_loss: 0.3513\n",
      "  Batch [990/1299] D_loss: -0.6519, G_loss: -0.9229\n",
      "  Batch [1000/1299] D_loss: -0.8080, G_loss: -0.5214\n",
      "  Batch [1010/1299] D_loss: 0.0016, G_loss: 0.1675\n",
      "  Batch [1020/1299] D_loss: -0.0757, G_loss: 0.2922\n",
      "  Batch [1030/1299] D_loss: -0.0886, G_loss: 0.3565\n",
      "  Batch [1040/1299] D_loss: -0.0421, G_loss: 0.3035\n",
      "  Batch [1050/1299] D_loss: -0.1080, G_loss: 0.2536\n",
      "  Batch [1060/1299] D_loss: -0.0446, G_loss: 0.2059\n",
      "  Batch [1070/1299] D_loss: -0.4850, G_loss: -0.1081\n",
      "  Batch [1080/1299] D_loss: 0.0297, G_loss: 0.1511\n",
      "  Batch [1090/1299] D_loss: -0.1338, G_loss: 0.3435\n",
      "  Batch [1100/1299] D_loss: -0.0493, G_loss: 0.4716\n",
      "  Batch [1110/1299] D_loss: -0.0712, G_loss: 0.5522\n",
      "  Batch [1120/1299] D_loss: -0.0011, G_loss: 0.4088\n",
      "  Batch [1130/1299] D_loss: -0.0592, G_loss: 0.3100\n",
      "  Batch [1140/1299] D_loss: -0.5936, G_loss: -3.3781\n",
      "  Batch [1150/1299] D_loss: -0.0069, G_loss: 0.1121\n",
      "  Batch [1160/1299] D_loss: -0.5550, G_loss: -0.5857\n",
      "  Batch [1170/1299] D_loss: -0.0310, G_loss: 0.1380\n",
      "  Batch [1180/1299] D_loss: -0.0443, G_loss: 0.2762\n",
      "  Batch [1190/1299] D_loss: -0.0318, G_loss: 0.2047\n",
      "  Batch [1200/1299] D_loss: -0.5375, G_loss: -0.0524\n",
      "  Batch [1210/1299] D_loss: -0.0425, G_loss: 0.2740\n",
      "  Batch [1220/1299] D_loss: -0.0768, G_loss: 0.4133\n",
      "  Batch [1230/1299] D_loss: -0.1626, G_loss: 0.4455\n",
      "  Batch [1240/1299] D_loss: 0.0074, G_loss: 0.4011\n",
      "  Batch [1250/1299] D_loss: -0.0130, G_loss: 0.3280\n",
      "  Batch [1260/1299] D_loss: -0.0958, G_loss: 0.2844\n",
      "  Batch [1270/1299] D_loss: -0.5650, G_loss: -1.5295\n",
      "  Batch [1280/1299] D_loss: -0.1890, G_loss: -0.0811\n",
      "  Batch [1290/1299] D_loss: -0.2094, G_loss: 0.0103\n",
      "\n",
      "Epoch 36 Summary:\n",
      "  Average D_loss: -0.1575\n",
      "  Average G_loss: -0.0676\n",
      "\n",
      "Epoch [37/100]\n",
      "  Batch [0/1299] D_loss: -0.1984, G_loss: 0.2026\n",
      "  Batch [10/1299] D_loss: -0.1061, G_loss: 0.3727\n",
      "  Batch [20/1299] D_loss: -0.0425, G_loss: 0.4904\n",
      "  Batch [30/1299] D_loss: -0.0631, G_loss: 0.5046\n",
      "  Batch [40/1299] D_loss: -0.0618, G_loss: 0.3904\n",
      "  Batch [50/1299] D_loss: -1.7248, G_loss: -4.0710\n",
      "  Batch [60/1299] D_loss: -0.1620, G_loss: 0.0836\n",
      "  Batch [70/1299] D_loss: -0.2894, G_loss: 0.0896\n",
      "  Batch [80/1299] D_loss: -0.5313, G_loss: -0.7509\n",
      "  Batch [90/1299] D_loss: -0.0653, G_loss: 0.1451\n",
      "  Batch [100/1299] D_loss: -0.1456, G_loss: 0.4544\n",
      "  Batch [110/1299] D_loss: -0.0626, G_loss: 0.6021\n",
      "  Batch [120/1299] D_loss: -0.0823, G_loss: 0.5859\n",
      "  Batch [130/1299] D_loss: -0.1184, G_loss: 0.4183\n",
      "  Batch [140/1299] D_loss: 0.0159, G_loss: 0.2869\n",
      "  Batch [150/1299] D_loss: -0.5602, G_loss: -1.8641\n",
      "  Batch [160/1299] D_loss: -0.5139, G_loss: -0.9912\n",
      "  Batch [170/1299] D_loss: -0.6526, G_loss: -0.6901\n",
      "  Batch [180/1299] D_loss: -0.0264, G_loss: 0.1750\n",
      "  Batch [190/1299] D_loss: -0.0189, G_loss: 0.2867\n",
      "  Batch [200/1299] D_loss: -0.0813, G_loss: 0.5129\n",
      "  Batch [210/1299] D_loss: -0.1900, G_loss: 0.5119\n",
      "  Batch [220/1299] D_loss: -0.1997, G_loss: 0.4727\n",
      "  Batch [230/1299] D_loss: -0.1164, G_loss: 0.4488\n",
      "  Batch [240/1299] D_loss: -1.5158, G_loss: -2.9571\n",
      "  Batch [250/1299] D_loss: -0.2933, G_loss: -0.3636\n",
      "  Batch [260/1299] D_loss: -0.0256, G_loss: 0.1745\n",
      "  Batch [270/1299] D_loss: -0.0810, G_loss: 0.3204\n",
      "  Batch [280/1299] D_loss: -0.1312, G_loss: 0.2527\n",
      "  Batch [290/1299] D_loss: 0.0136, G_loss: 0.3470\n",
      "  Batch [300/1299] D_loss: 0.0002, G_loss: 0.3911\n",
      "  Batch [310/1299] D_loss: -2.2337, G_loss: -4.6803\n",
      "  Batch [320/1299] D_loss: -0.0467, G_loss: 0.1989\n",
      "  Batch [330/1299] D_loss: -0.0234, G_loss: 0.3216\n",
      "  Batch [340/1299] D_loss: -0.1123, G_loss: 0.4158\n",
      "  Batch [350/1299] D_loss: -0.0318, G_loss: 0.2145\n",
      "  Batch [360/1299] D_loss: -0.1940, G_loss: -1.0565\n",
      "  Batch [370/1299] D_loss: -0.0529, G_loss: 0.1584\n",
      "  Batch [380/1299] D_loss: -0.1014, G_loss: 0.3533\n",
      "  Batch [390/1299] D_loss: -0.0340, G_loss: 0.4611\n",
      "  Batch [400/1299] D_loss: -0.0123, G_loss: 0.3336\n",
      "  Batch [410/1299] D_loss: -0.1044, G_loss: 0.3485\n",
      "  Batch [420/1299] D_loss: -0.4495, G_loss: -0.2293\n",
      "  Batch [430/1299] D_loss: -0.7419, G_loss: -1.9451\n",
      "  Batch [440/1299] D_loss: -0.0532, G_loss: 0.2710\n",
      "  Batch [450/1299] D_loss: -0.1044, G_loss: 0.4394\n",
      "  Batch [460/1299] D_loss: -0.0280, G_loss: 0.4837\n",
      "  Batch [470/1299] D_loss: -0.0831, G_loss: 0.4739\n",
      "  Batch [480/1299] D_loss: -0.0347, G_loss: 0.2935\n",
      "  Batch [490/1299] D_loss: -5.0866, G_loss: -6.7749\n",
      "  Batch [500/1299] D_loss: -0.0260, G_loss: 0.0930\n",
      "  Batch [510/1299] D_loss: 0.0128, G_loss: 0.1527\n",
      "  Batch [520/1299] D_loss: -0.0932, G_loss: 0.1123\n",
      "  Batch [530/1299] D_loss: -0.8123, G_loss: -0.7925\n",
      "  Batch [540/1299] D_loss: -0.6992, G_loss: -0.1687\n",
      "  Batch [550/1299] D_loss: -0.8667, G_loss: -2.2872\n",
      "  Batch [560/1299] D_loss: -0.0205, G_loss: 0.1703\n",
      "  Batch [570/1299] D_loss: -0.0852, G_loss: 0.4506\n",
      "  Batch [580/1299] D_loss: -0.1247, G_loss: 0.5935\n",
      "  Batch [590/1299] D_loss: -0.1411, G_loss: 0.5876\n",
      "  Batch [600/1299] D_loss: -0.0796, G_loss: 0.5205\n",
      "  Batch [610/1299] D_loss: -0.0601, G_loss: 0.4549\n",
      "  Batch [620/1299] D_loss: -0.0156, G_loss: 0.3650\n",
      "  Batch [630/1299] D_loss: -1.3089, G_loss: -2.2413\n",
      "  Batch [640/1299] D_loss: -0.0189, G_loss: 0.1544\n",
      "  Batch [650/1299] D_loss: -0.0441, G_loss: 0.2048\n",
      "  Batch [660/1299] D_loss: 0.0290, G_loss: 0.1811\n",
      "  Batch [670/1299] D_loss: -0.0361, G_loss: 0.2055\n",
      "  Batch [680/1299] D_loss: -0.0492, G_loss: 0.1998\n",
      "  Batch [690/1299] D_loss: -0.0762, G_loss: 0.0347\n",
      "  Batch [700/1299] D_loss: -0.8325, G_loss: -1.1999\n",
      "  Batch [710/1299] D_loss: -0.0346, G_loss: 0.2767\n",
      "  Batch [720/1299] D_loss: -0.0863, G_loss: 0.2994\n",
      "  Batch [730/1299] D_loss: -0.0204, G_loss: 0.4211\n",
      "  Batch [740/1299] D_loss: -0.1235, G_loss: 0.4929\n",
      "  Batch [750/1299] D_loss: 0.0579, G_loss: 0.2896\n",
      "  Batch [760/1299] D_loss: -0.0561, G_loss: 0.2353\n",
      "  Batch [770/1299] D_loss: -0.0254, G_loss: 0.0471\n",
      "  Batch [780/1299] D_loss: -0.0199, G_loss: 0.1154\n",
      "  Batch [790/1299] D_loss: -0.0619, G_loss: 0.2501\n",
      "  Batch [800/1299] D_loss: -0.1954, G_loss: 0.3080\n",
      "  Batch [810/1299] D_loss: 0.0030, G_loss: 0.2154\n",
      "  Batch [820/1299] D_loss: 0.0056, G_loss: 0.1843\n",
      "  Batch [830/1299] D_loss: -0.0126, G_loss: 0.3113\n",
      "  Batch [840/1299] D_loss: -0.0906, G_loss: 0.4051\n",
      "  Batch [850/1299] D_loss: 0.0331, G_loss: 0.3478\n",
      "  Batch [860/1299] D_loss: -0.0266, G_loss: 0.2614\n",
      "  Batch [870/1299] D_loss: -0.5303, G_loss: -0.9132\n",
      "  Batch [880/1299] D_loss: -0.9116, G_loss: -1.7153\n",
      "  Batch [890/1299] D_loss: -0.0449, G_loss: -0.0301\n",
      "  Batch [900/1299] D_loss: -0.4811, G_loss: 0.0740\n",
      "  Batch [910/1299] D_loss: -0.0239, G_loss: 0.2483\n",
      "  Batch [920/1299] D_loss: -0.0779, G_loss: 0.3920\n",
      "  Batch [930/1299] D_loss: -0.1132, G_loss: 0.5187\n",
      "  Batch [940/1299] D_loss: -0.1992, G_loss: 0.5380\n",
      "  Batch [950/1299] D_loss: -0.0491, G_loss: 0.4577\n",
      "  Batch [960/1299] D_loss: -2.2135, G_loss: -3.2457\n",
      "  Batch [970/1299] D_loss: -0.2409, G_loss: -0.6319\n",
      "  Batch [980/1299] D_loss: -0.0170, G_loss: 0.1525\n",
      "  Batch [990/1299] D_loss: -0.0678, G_loss: 0.3006\n",
      "  Batch [1000/1299] D_loss: -0.0705, G_loss: 0.3328\n",
      "  Batch [1010/1299] D_loss: 0.0293, G_loss: 0.2627\n",
      "  Batch [1020/1299] D_loss: -4.8350, G_loss: -3.3452\n",
      "  Batch [1030/1299] D_loss: -0.0275, G_loss: 0.2178\n",
      "  Batch [1040/1299] D_loss: -0.0216, G_loss: 0.2318\n",
      "  Batch [1050/1299] D_loss: -0.0147, G_loss: 0.2696\n",
      "  Batch [1060/1299] D_loss: -0.0383, G_loss: 0.2092\n",
      "  Batch [1070/1299] D_loss: -0.2638, G_loss: -0.1654\n",
      "  Batch [1080/1299] D_loss: -0.9767, G_loss: -0.4185\n",
      "  Batch [1090/1299] D_loss: 0.0373, G_loss: 0.0998\n",
      "  Batch [1100/1299] D_loss: -0.0538, G_loss: 0.1256\n",
      "  Batch [1110/1299] D_loss: -0.0716, G_loss: 0.3557\n",
      "  Batch [1120/1299] D_loss: -0.0516, G_loss: 0.4374\n",
      "  Batch [1130/1299] D_loss: 0.0083, G_loss: 0.4620\n",
      "  Batch [1140/1299] D_loss: -0.2215, G_loss: 0.5127\n",
      "  Batch [1150/1299] D_loss: -0.0212, G_loss: 0.3544\n",
      "  Batch [1160/1299] D_loss: -1.5312, G_loss: -3.9210\n",
      "  Batch [1170/1299] D_loss: -0.0299, G_loss: 0.1342\n",
      "  Batch [1180/1299] D_loss: -0.0395, G_loss: 0.2562\n",
      "  Batch [1190/1299] D_loss: -0.1038, G_loss: 0.4113\n",
      "  Batch [1200/1299] D_loss: -0.1374, G_loss: 0.5462\n",
      "  Batch [1210/1299] D_loss: -0.0039, G_loss: 0.4312\n",
      "  Batch [1220/1299] D_loss: -0.0347, G_loss: 0.3864\n",
      "  Batch [1230/1299] D_loss: -2.1919, G_loss: -5.3670\n",
      "  Batch [1240/1299] D_loss: -0.7705, G_loss: -0.4331\n",
      "  Batch [1250/1299] D_loss: -0.0529, G_loss: 0.1071\n",
      "  Batch [1260/1299] D_loss: -0.0727, G_loss: 0.3369\n",
      "  Batch [1270/1299] D_loss: -0.1651, G_loss: 0.4821\n",
      "  Batch [1280/1299] D_loss: 0.0250, G_loss: 0.4092\n",
      "  Batch [1290/1299] D_loss: -0.0675, G_loss: 0.4436\n",
      "\n",
      "Epoch 37 Summary:\n",
      "  Average D_loss: -0.1272\n",
      "  Average G_loss: -0.0483\n",
      "\n",
      "Epoch [38/100]\n",
      "  Batch [0/1299] D_loss: -0.0105, G_loss: 0.2615\n",
      "  Batch [10/1299] D_loss: -1.1338, G_loss: -1.5279\n",
      "  Batch [20/1299] D_loss: -0.2557, G_loss: -0.0876\n",
      "  Batch [30/1299] D_loss: -0.7176, G_loss: -1.6156\n",
      "  Batch [40/1299] D_loss: -0.0619, G_loss: 0.2140\n",
      "  Batch [50/1299] D_loss: -0.0642, G_loss: 0.3742\n",
      "  Batch [60/1299] D_loss: -0.0383, G_loss: 0.4871\n",
      "  Batch [70/1299] D_loss: -0.1716, G_loss: 0.5368\n",
      "  Batch [80/1299] D_loss: -0.0963, G_loss: 0.3754\n",
      "  Batch [90/1299] D_loss: -0.0195, G_loss: 0.3505\n",
      "  Batch [100/1299] D_loss: -0.0076, G_loss: 0.1388\n",
      "  Batch [110/1299] D_loss: 0.0091, G_loss: 0.3249\n",
      "  Batch [120/1299] D_loss: -0.0349, G_loss: 0.4741\n",
      "  Batch [130/1299] D_loss: -0.0652, G_loss: 0.3268\n",
      "  Batch [140/1299] D_loss: -0.0643, G_loss: 0.2818\n",
      "  Batch [150/1299] D_loss: -0.0656, G_loss: 0.2103\n",
      "  Batch [160/1299] D_loss: -0.3532, G_loss: -0.1261\n",
      "  Batch [170/1299] D_loss: -0.0211, G_loss: 0.3134\n",
      "  Batch [180/1299] D_loss: -0.1527, G_loss: 0.4239\n",
      "  Batch [190/1299] D_loss: -0.1018, G_loss: 0.4520\n",
      "  Batch [200/1299] D_loss: -0.0747, G_loss: 0.4510\n",
      "  Batch [210/1299] D_loss: -0.0313, G_loss: 0.2887\n",
      "  Batch [220/1299] D_loss: -0.0963, G_loss: -1.5790\n",
      "  Batch [230/1299] D_loss: -0.5705, G_loss: -1.1946\n",
      "  Batch [240/1299] D_loss: -0.9460, G_loss: -1.3178\n",
      "  Batch [250/1299] D_loss: -0.3627, G_loss: -0.3820\n",
      "  Batch [260/1299] D_loss: -0.1069, G_loss: 0.3543\n",
      "  Batch [270/1299] D_loss: -0.2110, G_loss: 0.4669\n",
      "  Batch [280/1299] D_loss: -0.0587, G_loss: 0.5264\n",
      "  Batch [290/1299] D_loss: -0.1326, G_loss: 0.5113\n",
      "  Batch [300/1299] D_loss: 0.0131, G_loss: 0.2500\n",
      "  Batch [310/1299] D_loss: -0.8797, G_loss: -1.2385\n",
      "  Batch [320/1299] D_loss: -0.1122, G_loss: 0.1062\n",
      "  Batch [330/1299] D_loss: -0.0697, G_loss: 0.0340\n",
      "  Batch [340/1299] D_loss: -0.1041, G_loss: 0.3219\n",
      "  Batch [350/1299] D_loss: -0.0791, G_loss: 0.4419\n",
      "  Batch [360/1299] D_loss: -0.0145, G_loss: 0.4965\n",
      "  Batch [370/1299] D_loss: -0.1013, G_loss: 0.5733\n",
      "  Batch [380/1299] D_loss: -0.0992, G_loss: 0.3267\n",
      "  Batch [390/1299] D_loss: -0.1995, G_loss: -3.0691\n",
      "  Batch [400/1299] D_loss: -0.0800, G_loss: 0.1283\n",
      "  Batch [410/1299] D_loss: -1.3088, G_loss: -2.5154\n",
      "  Batch [420/1299] D_loss: -0.0512, G_loss: 0.2166\n",
      "  Batch [430/1299] D_loss: -0.0323, G_loss: 0.3018\n",
      "  Batch [440/1299] D_loss: -0.0221, G_loss: 0.3155\n",
      "  Batch [450/1299] D_loss: -0.0204, G_loss: 0.2743\n",
      "  Batch [460/1299] D_loss: -0.0344, G_loss: 0.1759\n",
      "  Batch [470/1299] D_loss: -0.8513, G_loss: -0.7062\n",
      "  Batch [480/1299] D_loss: -0.0012, G_loss: 0.2116\n",
      "  Batch [490/1299] D_loss: -0.0267, G_loss: 0.2736\n",
      "  Batch [500/1299] D_loss: -0.0580, G_loss: 0.2665\n",
      "  Batch [510/1299] D_loss: -1.9853, G_loss: -3.0539\n",
      "  Batch [520/1299] D_loss: -0.0092, G_loss: 0.1144\n",
      "  Batch [530/1299] D_loss: -0.0318, G_loss: 0.2680\n",
      "  Batch [540/1299] D_loss: -0.0928, G_loss: 0.3944\n",
      "  Batch [550/1299] D_loss: -0.1257, G_loss: 0.4077\n",
      "  Batch [560/1299] D_loss: -0.1361, G_loss: 0.2794\n",
      "  Batch [570/1299] D_loss: -0.0927, G_loss: 0.1712\n",
      "  Batch [580/1299] D_loss: -0.4602, G_loss: 0.0271\n",
      "  Batch [590/1299] D_loss: -0.7215, G_loss: -0.1408\n",
      "  Batch [600/1299] D_loss: -0.9866, G_loss: -0.5160\n",
      "  Batch [610/1299] D_loss: -0.4327, G_loss: -0.0905\n",
      "  Batch [620/1299] D_loss: -0.5178, G_loss: 0.0839\n",
      "  Batch [630/1299] D_loss: -0.3183, G_loss: 0.0689\n",
      "  Batch [640/1299] D_loss: -1.5146, G_loss: -0.7187\n",
      "  Batch [650/1299] D_loss: -0.0123, G_loss: 0.2041\n",
      "  Batch [660/1299] D_loss: -0.0738, G_loss: 0.4478\n",
      "  Batch [670/1299] D_loss: 0.0113, G_loss: 0.5925\n",
      "  Batch [680/1299] D_loss: -0.0785, G_loss: 0.5556\n",
      "  Batch [690/1299] D_loss: -0.0010, G_loss: 0.3803\n",
      "  Batch [700/1299] D_loss: -0.0607, G_loss: 0.2367\n",
      "  Batch [710/1299] D_loss: -0.7904, G_loss: -0.9120\n",
      "  Batch [720/1299] D_loss: -0.0206, G_loss: 0.0988\n",
      "  Batch [730/1299] D_loss: -0.0184, G_loss: 0.1195\n",
      "  Batch [740/1299] D_loss: -0.0965, G_loss: 0.2628\n",
      "  Batch [750/1299] D_loss: 0.0254, G_loss: 0.3459\n",
      "  Batch [760/1299] D_loss: -0.1215, G_loss: 0.4349\n",
      "  Batch [770/1299] D_loss: -0.0915, G_loss: 0.3120\n",
      "  Batch [780/1299] D_loss: -1.8514, G_loss: -2.0683\n",
      "  Batch [790/1299] D_loss: -0.0733, G_loss: 0.0894\n",
      "  Batch [800/1299] D_loss: -0.2820, G_loss: 0.0874\n",
      "  Batch [810/1299] D_loss: -1.2104, G_loss: -0.1507\n",
      "  Batch [820/1299] D_loss: -0.2095, G_loss: -0.1542\n",
      "  Batch [830/1299] D_loss: -0.0738, G_loss: 0.3631\n",
      "  Batch [840/1299] D_loss: -0.1141, G_loss: 0.4403\n",
      "  Batch [850/1299] D_loss: -0.1299, G_loss: 0.5938\n",
      "  Batch [860/1299] D_loss: -0.0308, G_loss: 0.4138\n",
      "  Batch [870/1299] D_loss: -0.0877, G_loss: 0.4775\n",
      "  Batch [880/1299] D_loss: -0.0351, G_loss: 0.2929\n",
      "  Batch [890/1299] D_loss: -1.3884, G_loss: -0.5333\n",
      "  Batch [900/1299] D_loss: -1.4427, G_loss: -1.8495\n",
      "  Batch [910/1299] D_loss: -0.9632, G_loss: -0.3417\n",
      "  Batch [920/1299] D_loss: -0.0548, G_loss: 0.2334\n",
      "  Batch [930/1299] D_loss: -0.0848, G_loss: 0.4814\n",
      "  Batch [940/1299] D_loss: -0.0205, G_loss: 0.4494\n",
      "  Batch [950/1299] D_loss: -0.1563, G_loss: 0.4521\n",
      "  Batch [960/1299] D_loss: -0.0763, G_loss: 0.4436\n",
      "  Batch [970/1299] D_loss: -1.7940, G_loss: -2.9287\n",
      "  Batch [980/1299] D_loss: -1.0273, G_loss: -1.4874\n",
      "  Batch [990/1299] D_loss: -0.2133, G_loss: -0.0097\n",
      "  Batch [1000/1299] D_loss: -0.2579, G_loss: -0.1896\n",
      "  Batch [1010/1299] D_loss: -1.1614, G_loss: -0.2452\n",
      "  Batch [1020/1299] D_loss: -0.1244, G_loss: -0.0024\n",
      "  Batch [1030/1299] D_loss: -0.0713, G_loss: 0.1926\n",
      "  Batch [1040/1299] D_loss: -0.0638, G_loss: 0.4012\n",
      "  Batch [1050/1299] D_loss: -0.0739, G_loss: 0.6462\n",
      "  Batch [1060/1299] D_loss: -0.1316, G_loss: 0.5931\n",
      "  Batch [1070/1299] D_loss: -0.0708, G_loss: 0.4591\n",
      "  Batch [1080/1299] D_loss: -0.2513, G_loss: 0.3508\n",
      "  Batch [1090/1299] D_loss: -0.8161, G_loss: -1.6139\n",
      "  Batch [1100/1299] D_loss: -0.5917, G_loss: 0.0539\n",
      "  Batch [1110/1299] D_loss: -0.0584, G_loss: 0.2370\n",
      "  Batch [1120/1299] D_loss: -0.0534, G_loss: 0.4064\n",
      "  Batch [1130/1299] D_loss: -0.1979, G_loss: 0.5589\n",
      "  Batch [1140/1299] D_loss: -0.2099, G_loss: 0.4871\n",
      "  Batch [1150/1299] D_loss: -0.1233, G_loss: 0.5031\n",
      "  Batch [1160/1299] D_loss: -0.0941, G_loss: 0.4656\n",
      "  Batch [1170/1299] D_loss: -0.0337, G_loss: 0.2137\n",
      "  Batch [1180/1299] D_loss: -0.9068, G_loss: -0.6289\n",
      "  Batch [1190/1299] D_loss: -0.6398, G_loss: -0.0084\n",
      "  Batch [1200/1299] D_loss: -0.2108, G_loss: -0.0017\n",
      "  Batch [1210/1299] D_loss: -0.1091, G_loss: 0.3151\n",
      "  Batch [1220/1299] D_loss: -0.0889, G_loss: 0.4539\n",
      "  Batch [1230/1299] D_loss: -0.1589, G_loss: 0.5775\n",
      "  Batch [1240/1299] D_loss: -0.1295, G_loss: 0.5115\n",
      "  Batch [1250/1299] D_loss: -0.0468, G_loss: 0.2789\n",
      "  Batch [1260/1299] D_loss: -0.6528, G_loss: -1.2841\n",
      "  Batch [1270/1299] D_loss: 0.0186, G_loss: 0.1442\n",
      "  Batch [1280/1299] D_loss: -0.0390, G_loss: 0.2866\n",
      "  Batch [1290/1299] D_loss: -0.0327, G_loss: 0.4005\n",
      "\n",
      "Epoch 38 Summary:\n",
      "  Average D_loss: -0.1498\n",
      "  Average G_loss: -0.0598\n",
      "\n",
      "Epoch [39/100]\n",
      "  Batch [0/1299] D_loss: -0.0523, G_loss: 0.3467\n",
      "  Batch [10/1299] D_loss: -0.5340, G_loss: -4.4066\n",
      "  Batch [20/1299] D_loss: -0.0740, G_loss: 0.1785\n",
      "  Batch [30/1299] D_loss: -0.0729, G_loss: 0.2290\n",
      "  Batch [40/1299] D_loss: -0.0730, G_loss: 0.3395\n",
      "  Batch [50/1299] D_loss: -0.0532, G_loss: 0.2991\n",
      "  Batch [60/1299] D_loss: -0.0554, G_loss: 0.2842\n",
      "  Batch [70/1299] D_loss: -1.7177, G_loss: -4.0708\n",
      "  Batch [80/1299] D_loss: -0.7544, G_loss: -1.1868\n",
      "  Batch [90/1299] D_loss: -0.0430, G_loss: 0.1222\n",
      "  Batch [100/1299] D_loss: -0.0218, G_loss: 0.2200\n",
      "  Batch [110/1299] D_loss: -0.0435, G_loss: 0.3359\n",
      "  Batch [120/1299] D_loss: -0.0604, G_loss: 0.4012\n",
      "  Batch [130/1299] D_loss: -0.0252, G_loss: 0.2154\n",
      "  Batch [140/1299] D_loss: -0.3585, G_loss: 0.0171\n",
      "  Batch [150/1299] D_loss: -0.4902, G_loss: -0.3666\n",
      "  Batch [160/1299] D_loss: -0.0049, G_loss: 0.0643\n",
      "  Batch [170/1299] D_loss: -0.0116, G_loss: 0.2133\n",
      "  Batch [180/1299] D_loss: -0.0910, G_loss: 0.3004\n",
      "  Batch [190/1299] D_loss: -0.0483, G_loss: 0.4211\n",
      "  Batch [200/1299] D_loss: -0.1035, G_loss: 0.4907\n",
      "  Batch [210/1299] D_loss: 0.0501, G_loss: 0.2996\n",
      "  Batch [220/1299] D_loss: -0.0076, G_loss: 0.0280\n",
      "  Batch [230/1299] D_loss: -0.2052, G_loss: -0.3723\n",
      "  Batch [240/1299] D_loss: -0.2217, G_loss: -0.8234\n",
      "  Batch [250/1299] D_loss: -0.1628, G_loss: 0.0825\n",
      "  Batch [260/1299] D_loss: -0.0978, G_loss: 0.3245\n",
      "  Batch [270/1299] D_loss: -0.1347, G_loss: 0.4091\n",
      "  Batch [280/1299] D_loss: -0.0859, G_loss: 0.5527\n",
      "  Batch [290/1299] D_loss: -0.0337, G_loss: 0.4326\n",
      "  Batch [300/1299] D_loss: 0.0171, G_loss: 0.2899\n",
      "  Batch [310/1299] D_loss: -0.0345, G_loss: 0.1894\n",
      "  Batch [320/1299] D_loss: -0.8244, G_loss: -1.0244\n",
      "  Batch [330/1299] D_loss: -0.0384, G_loss: 0.1436\n",
      "  Batch [340/1299] D_loss: -0.0655, G_loss: 0.3279\n",
      "  Batch [350/1299] D_loss: -0.0063, G_loss: 0.3704\n",
      "  Batch [360/1299] D_loss: -0.0151, G_loss: 0.4240\n",
      "  Batch [370/1299] D_loss: -0.0497, G_loss: 0.3251\n",
      "  Batch [380/1299] D_loss: -0.0486, G_loss: 0.2337\n",
      "  Batch [390/1299] D_loss: -0.2237, G_loss: -0.3195\n",
      "  Batch [400/1299] D_loss: -1.1254, G_loss: -1.9750\n",
      "  Batch [410/1299] D_loss: -0.8203, G_loss: -0.2738\n",
      "  Batch [420/1299] D_loss: -0.1281, G_loss: 0.3788\n",
      "  Batch [430/1299] D_loss: -0.0791, G_loss: 0.5217\n",
      "  Batch [440/1299] D_loss: -0.1296, G_loss: 0.4451\n",
      "  Batch [450/1299] D_loss: -0.0624, G_loss: 0.4642\n",
      "  Batch [460/1299] D_loss: -0.1261, G_loss: 0.1284\n",
      "  Batch [470/1299] D_loss: -0.0303, G_loss: 0.0833\n",
      "  Batch [480/1299] D_loss: -0.0261, G_loss: 0.1193\n",
      "  Batch [490/1299] D_loss: -0.5834, G_loss: -0.4470\n",
      "  Batch [500/1299] D_loss: -0.3234, G_loss: 0.0648\n",
      "  Batch [510/1299] D_loss: -0.8147, G_loss: -1.1390\n",
      "  Batch [520/1299] D_loss: -0.5324, G_loss: -0.7823\n",
      "  Batch [530/1299] D_loss: -0.4115, G_loss: 0.0613\n",
      "  Batch [540/1299] D_loss: -0.1152, G_loss: 0.2391\n",
      "  Batch [550/1299] D_loss: -0.1758, G_loss: 0.4661\n",
      "  Batch [560/1299] D_loss: -0.0100, G_loss: 0.5022\n",
      "  Batch [570/1299] D_loss: -0.0725, G_loss: 0.5451\n",
      "  Batch [580/1299] D_loss: -0.0074, G_loss: 0.3507\n",
      "  Batch [590/1299] D_loss: -1.5816, G_loss: -1.9916\n",
      "  Batch [600/1299] D_loss: -0.0893, G_loss: 0.0967\n",
      "  Batch [610/1299] D_loss: -0.1508, G_loss: 0.1065\n",
      "  Batch [620/1299] D_loss: -0.6333, G_loss: -0.0998\n",
      "  Batch [630/1299] D_loss: -0.0371, G_loss: 0.2118\n",
      "  Batch [640/1299] D_loss: -0.1224, G_loss: 0.3980\n",
      "  Batch [650/1299] D_loss: -0.1498, G_loss: 0.6878\n",
      "  Batch [660/1299] D_loss: -0.1777, G_loss: 0.7153\n",
      "  Batch [670/1299] D_loss: -0.1473, G_loss: 0.6662\n",
      "  Batch [680/1299] D_loss: -0.0716, G_loss: 0.5212\n",
      "  Batch [690/1299] D_loss: -0.0452, G_loss: 0.0107\n",
      "  Batch [700/1299] D_loss: -0.9954, G_loss: -0.4334\n",
      "  Batch [710/1299] D_loss: -0.4772, G_loss: -0.2088\n",
      "  Batch [720/1299] D_loss: -0.1253, G_loss: 0.2567\n",
      "  Batch [730/1299] D_loss: -0.0622, G_loss: 0.3723\n",
      "  Batch [740/1299] D_loss: -0.0460, G_loss: 0.3702\n",
      "  Batch [750/1299] D_loss: -0.0050, G_loss: 0.3378\n",
      "  Batch [760/1299] D_loss: -0.0173, G_loss: 0.3020\n",
      "  Batch [770/1299] D_loss: -0.3300, G_loss: -0.0556\n",
      "  Batch [780/1299] D_loss: 0.0418, G_loss: 0.1682\n",
      "  Batch [790/1299] D_loss: 0.0043, G_loss: 0.3524\n",
      "  Batch [800/1299] D_loss: -0.0056, G_loss: 0.3503\n",
      "  Batch [810/1299] D_loss: -1.0196, G_loss: -1.8851\n",
      "  Batch [820/1299] D_loss: -0.0383, G_loss: 0.1946\n",
      "  Batch [830/1299] D_loss: -0.0203, G_loss: 0.2366\n",
      "  Batch [840/1299] D_loss: 0.0282, G_loss: 0.3509\n",
      "  Batch [850/1299] D_loss: -0.0158, G_loss: 0.2915\n",
      "  Batch [860/1299] D_loss: -0.9349, G_loss: -0.2617\n",
      "  Batch [870/1299] D_loss: -0.0020, G_loss: 0.1017\n",
      "  Batch [880/1299] D_loss: -0.0319, G_loss: 0.1503\n",
      "  Batch [890/1299] D_loss: -0.0184, G_loss: 0.2395\n",
      "  Batch [900/1299] D_loss: -0.4943, G_loss: -0.1987\n",
      "  Batch [910/1299] D_loss: -0.0233, G_loss: 0.2125\n",
      "  Batch [920/1299] D_loss: -0.0665, G_loss: 0.2571\n",
      "  Batch [930/1299] D_loss: -0.0204, G_loss: 0.3815\n",
      "  Batch [940/1299] D_loss: -0.0328, G_loss: 0.3100\n",
      "  Batch [950/1299] D_loss: -0.2012, G_loss: 0.0082\n",
      "  Batch [960/1299] D_loss: -0.2439, G_loss: -0.3703\n",
      "  Batch [970/1299] D_loss: -0.8999, G_loss: -0.8534\n",
      "  Batch [980/1299] D_loss: -0.1204, G_loss: 0.2826\n",
      "  Batch [990/1299] D_loss: -0.0990, G_loss: 0.4011\n",
      "  Batch [1000/1299] D_loss: -0.1362, G_loss: 0.3939\n",
      "  Batch [1010/1299] D_loss: -0.0796, G_loss: 0.4259\n",
      "  Batch [1020/1299] D_loss: -0.1413, G_loss: 0.3503\n",
      "  Batch [1030/1299] D_loss: -1.4824, G_loss: -0.6119\n",
      "  Batch [1040/1299] D_loss: 0.0112, G_loss: 0.0748\n",
      "  Batch [1050/1299] D_loss: 0.0014, G_loss: 0.0908\n",
      "  Batch [1060/1299] D_loss: -1.0970, G_loss: -2.9957\n",
      "  Batch [1070/1299] D_loss: -0.0162, G_loss: 0.1566\n",
      "  Batch [1080/1299] D_loss: -0.0498, G_loss: 0.2546\n",
      "  Batch [1090/1299] D_loss: -0.0908, G_loss: 0.4819\n",
      "  Batch [1100/1299] D_loss: -0.1609, G_loss: 0.5502\n",
      "  Batch [1110/1299] D_loss: -0.1078, G_loss: 0.4458\n",
      "  Batch [1120/1299] D_loss: -0.0421, G_loss: 0.2477\n",
      "  Batch [1130/1299] D_loss: -0.4668, G_loss: -0.4445\n",
      "  Batch [1140/1299] D_loss: -0.1373, G_loss: -0.0430\n",
      "  Batch [1150/1299] D_loss: -1.0519, G_loss: -1.0035\n",
      "  Batch [1160/1299] D_loss: -0.3798, G_loss: -0.0068\n",
      "  Batch [1170/1299] D_loss: -0.0481, G_loss: 0.3947\n",
      "  Batch [1180/1299] D_loss: -0.1147, G_loss: 0.5595\n",
      "  Batch [1190/1299] D_loss: -0.0260, G_loss: 0.6001\n",
      "  Batch [1200/1299] D_loss: -0.1232, G_loss: 0.6380\n",
      "  Batch [1210/1299] D_loss: 0.0636, G_loss: 0.4013\n",
      "  Batch [1220/1299] D_loss: -0.0089, G_loss: 0.1942\n",
      "  Batch [1230/1299] D_loss: -0.4379, G_loss: -0.6149\n",
      "  Batch [1240/1299] D_loss: -0.8740, G_loss: 0.0901\n",
      "  Batch [1250/1299] D_loss: -0.4877, G_loss: 0.0285\n",
      "  Batch [1260/1299] D_loss: -1.4802, G_loss: -1.2677\n",
      "  Batch [1270/1299] D_loss: -0.1467, G_loss: 0.2917\n",
      "  Batch [1280/1299] D_loss: -0.0616, G_loss: 0.4729\n",
      "  Batch [1290/1299] D_loss: 0.0633, G_loss: 0.4795\n",
      "\n",
      "Epoch 39 Summary:\n",
      "  Average D_loss: -0.1342\n",
      "  Average G_loss: -0.0642\n",
      "\n",
      "Epoch [40/100]\n",
      "  Batch [0/1299] D_loss: 0.0096, G_loss: 0.4197\n",
      "  Batch [10/1299] D_loss: -0.0560, G_loss: 0.4001\n",
      "  Batch [20/1299] D_loss: -0.1016, G_loss: 0.4195\n",
      "  Batch [30/1299] D_loss: -0.0058, G_loss: 0.2333\n",
      "  Batch [40/1299] D_loss: -0.5037, G_loss: -2.7486\n",
      "  Batch [50/1299] D_loss: -0.6776, G_loss: -0.3144\n",
      "  Batch [60/1299] D_loss: -0.7249, G_loss: -0.1968\n",
      "  Batch [70/1299] D_loss: -0.6174, G_loss: 0.0742\n",
      "  Batch [80/1299] D_loss: -0.6066, G_loss: -0.6778\n",
      "  Batch [90/1299] D_loss: -0.2725, G_loss: 0.0675\n",
      "  Batch [100/1299] D_loss: -0.1035, G_loss: 0.4336\n",
      "  Batch [110/1299] D_loss: -0.0327, G_loss: 0.6004\n",
      "  Batch [120/1299] D_loss: -0.1065, G_loss: 0.6205\n",
      "  Batch [130/1299] D_loss: -0.1416, G_loss: 0.3213\n",
      "  Batch [140/1299] D_loss: -0.0847, G_loss: 0.2184\n",
      "  Batch [150/1299] D_loss: -0.4318, G_loss: -0.2536\n",
      "  Batch [160/1299] D_loss: -1.8371, G_loss: -0.5445\n",
      "  Batch [170/1299] D_loss: -0.1098, G_loss: 0.0756\n",
      "  Batch [180/1299] D_loss: -0.9726, G_loss: -1.3049\n",
      "  Batch [190/1299] D_loss: -0.0500, G_loss: 0.3001\n",
      "  Batch [200/1299] D_loss: -0.1964, G_loss: 0.4724\n",
      "  Batch [210/1299] D_loss: -0.1649, G_loss: 0.6102\n",
      "  Batch [220/1299] D_loss: -0.0381, G_loss: 0.6009\n",
      "  Batch [230/1299] D_loss: -0.1060, G_loss: 0.5701\n",
      "  Batch [240/1299] D_loss: -0.0938, G_loss: 0.3858\n",
      "  Batch [250/1299] D_loss: -0.2540, G_loss: 0.2158\n",
      "  Batch [260/1299] D_loss: -0.0751, G_loss: 0.0076\n",
      "  Batch [270/1299] D_loss: -0.3246, G_loss: -0.0350\n",
      "  Batch [280/1299] D_loss: -0.1577, G_loss: 0.0528\n",
      "  Batch [290/1299] D_loss: -0.1414, G_loss: 0.3175\n",
      "  Batch [300/1299] D_loss: -0.4910, G_loss: 0.0674\n",
      "  Batch [310/1299] D_loss: -0.1013, G_loss: 0.1475\n",
      "  Batch [320/1299] D_loss: -0.1143, G_loss: 0.4631\n",
      "  Batch [330/1299] D_loss: -0.0378, G_loss: 0.5123\n",
      "  Batch [340/1299] D_loss: -0.0761, G_loss: 0.5751\n",
      "  Batch [350/1299] D_loss: -0.0888, G_loss: 0.5179\n",
      "  Batch [360/1299] D_loss: 0.0958, G_loss: 0.4637\n",
      "  Batch [370/1299] D_loss: -2.5522, G_loss: -5.2596\n",
      "  Batch [380/1299] D_loss: -0.0188, G_loss: 0.1304\n",
      "  Batch [390/1299] D_loss: -0.1031, G_loss: 0.3523\n",
      "  Batch [400/1299] D_loss: -0.1101, G_loss: 0.4613\n",
      "  Batch [410/1299] D_loss: -0.0483, G_loss: 0.5637\n",
      "  Batch [420/1299] D_loss: -0.0390, G_loss: 0.3729\n",
      "  Batch [430/1299] D_loss: -0.0394, G_loss: 0.2782\n",
      "  Batch [440/1299] D_loss: -3.5068, G_loss: -8.3838\n",
      "  Batch [450/1299] D_loss: -0.1290, G_loss: 0.1393\n",
      "  Batch [460/1299] D_loss: -0.8350, G_loss: -1.7874\n",
      "  Batch [470/1299] D_loss: -0.3223, G_loss: -0.2288\n",
      "  Batch [480/1299] D_loss: -0.8856, G_loss: -0.3472\n",
      "  Batch [490/1299] D_loss: -0.4993, G_loss: -0.4377\n",
      "  Batch [500/1299] D_loss: -0.3232, G_loss: 0.0386\n",
      "  Batch [510/1299] D_loss: -0.1544, G_loss: -0.2479\n",
      "  Batch [520/1299] D_loss: -0.1894, G_loss: 0.1434\n",
      "  Batch [530/1299] D_loss: -0.0252, G_loss: 0.3187\n",
      "  Batch [540/1299] D_loss: -0.0770, G_loss: 0.4650\n",
      "  Batch [550/1299] D_loss: -0.2575, G_loss: 0.6521\n",
      "  Batch [560/1299] D_loss: -0.1206, G_loss: 0.8035\n",
      "  Batch [570/1299] D_loss: -0.1347, G_loss: 0.5469\n",
      "  Batch [580/1299] D_loss: -0.1143, G_loss: 0.4026\n",
      "  Batch [590/1299] D_loss: -2.5208, G_loss: -5.1863\n",
      "  Batch [600/1299] D_loss: -0.1705, G_loss: -0.0912\n",
      "  Batch [610/1299] D_loss: -0.4260, G_loss: -0.2531\n",
      "  Batch [620/1299] D_loss: -0.1167, G_loss: -0.1064\n",
      "  Batch [630/1299] D_loss: -0.2996, G_loss: 0.0740\n",
      "  Batch [640/1299] D_loss: -0.1638, G_loss: 0.0273\n",
      "  Batch [650/1299] D_loss: -0.0932, G_loss: 0.1464\n",
      "  Batch [660/1299] D_loss: -0.1468, G_loss: 0.1673\n",
      "  Batch [670/1299] D_loss: -1.1246, G_loss: -1.4584\n",
      "  Batch [680/1299] D_loss: -0.6670, G_loss: 0.0302\n",
      "  Batch [690/1299] D_loss: -0.1010, G_loss: 0.3419\n",
      "  Batch [700/1299] D_loss: -0.1304, G_loss: 0.4635\n",
      "  Batch [710/1299] D_loss: -0.1467, G_loss: 0.5998\n",
      "  Batch [720/1299] D_loss: -0.0821, G_loss: 0.4662\n",
      "  Batch [730/1299] D_loss: -0.0741, G_loss: 0.3376\n",
      "  Batch [740/1299] D_loss: -0.4732, G_loss: -0.2528\n",
      "  Batch [750/1299] D_loss: -1.1597, G_loss: -0.0166\n",
      "  Batch [760/1299] D_loss: -0.0008, G_loss: 0.1387\n",
      "  Batch [770/1299] D_loss: 0.0054, G_loss: 0.2277\n",
      "  Batch [780/1299] D_loss: -0.0071, G_loss: 0.3597\n",
      "  Batch [790/1299] D_loss: -0.0739, G_loss: 0.3117\n",
      "  Batch [800/1299] D_loss: -0.0184, G_loss: 0.2974\n",
      "  Batch [810/1299] D_loss: -4.4675, G_loss: -6.6902\n",
      "  Batch [820/1299] D_loss: -0.0448, G_loss: 0.2364\n",
      "  Batch [830/1299] D_loss: 0.0338, G_loss: 0.3890\n",
      "  Batch [840/1299] D_loss: -0.1495, G_loss: 0.4811\n",
      "  Batch [850/1299] D_loss: -0.0038, G_loss: 0.2556\n",
      "  Batch [860/1299] D_loss: -0.3110, G_loss: -0.2289\n",
      "  Batch [870/1299] D_loss: -1.3454, G_loss: -0.9880\n",
      "  Batch [880/1299] D_loss: -0.7566, G_loss: -0.7725\n",
      "  Batch [890/1299] D_loss: -0.1018, G_loss: 0.3492\n",
      "  Batch [900/1299] D_loss: -0.1835, G_loss: 0.6464\n",
      "  Batch [910/1299] D_loss: -0.0215, G_loss: 0.6200\n",
      "  Batch [920/1299] D_loss: -0.0287, G_loss: 0.6100\n",
      "  Batch [930/1299] D_loss: -0.0760, G_loss: 0.5909\n",
      "  Batch [940/1299] D_loss: -0.0571, G_loss: 0.4970\n",
      "  Batch [950/1299] D_loss: -0.0330, G_loss: 0.2994\n",
      "  Batch [960/1299] D_loss: -1.3553, G_loss: -2.1236\n",
      "  Batch [970/1299] D_loss: -2.1633, G_loss: -3.8286\n",
      "  Batch [980/1299] D_loss: -0.4952, G_loss: -0.1546\n",
      "  Batch [990/1299] D_loss: -0.4028, G_loss: 0.0312\n",
      "  Batch [1000/1299] D_loss: -0.4348, G_loss: -0.3424\n",
      "  Batch [1010/1299] D_loss: -0.9350, G_loss: 0.0223\n",
      "  Batch [1020/1299] D_loss: -0.0785, G_loss: 0.2112\n",
      "  Batch [1030/1299] D_loss: -0.6781, G_loss: -0.2986\n",
      "  Batch [1040/1299] D_loss: -0.2710, G_loss: 0.1663\n",
      "  Batch [1050/1299] D_loss: -0.0422, G_loss: 0.3714\n",
      "  Batch [1060/1299] D_loss: -0.0910, G_loss: 0.5729\n",
      "  Batch [1070/1299] D_loss: -0.0919, G_loss: 0.7398\n",
      "  Batch [1080/1299] D_loss: -0.1474, G_loss: 0.7239\n",
      "  Batch [1090/1299] D_loss: -0.1901, G_loss: 0.6046\n",
      "  Batch [1100/1299] D_loss: -0.0816, G_loss: 0.6646\n",
      "  Batch [1110/1299] D_loss: -0.1820, G_loss: 0.2741\n",
      "  Batch [1120/1299] D_loss: -0.8196, G_loss: -0.5679\n",
      "  Batch [1130/1299] D_loss: -0.3148, G_loss: -0.2088\n",
      "  Batch [1140/1299] D_loss: -2.0561, G_loss: -2.4935\n",
      "  Batch [1150/1299] D_loss: 0.0015, G_loss: 0.1974\n",
      "  Batch [1160/1299] D_loss: -0.0874, G_loss: 0.2519\n",
      "  Batch [1170/1299] D_loss: -0.1135, G_loss: 0.3232\n",
      "  Batch [1180/1299] D_loss: -0.1284, G_loss: 0.3189\n",
      "  Batch [1190/1299] D_loss: -0.1603, G_loss: 0.3721\n",
      "  Batch [1200/1299] D_loss: -0.2721, G_loss: -0.2768\n",
      "  Batch [1210/1299] D_loss: 0.0143, G_loss: 0.1648\n",
      "  Batch [1220/1299] D_loss: -0.0852, G_loss: 0.2821\n",
      "  Batch [1230/1299] D_loss: -0.0148, G_loss: 0.2860\n",
      "  Batch [1240/1299] D_loss: -0.0892, G_loss: 0.3689\n",
      "  Batch [1250/1299] D_loss: 0.0093, G_loss: 0.2955\n",
      "  Batch [1260/1299] D_loss: -0.0159, G_loss: 0.2455\n",
      "  Batch [1270/1299] D_loss: 0.0340, G_loss: 0.1502\n",
      "  Batch [1280/1299] D_loss: -0.3551, G_loss: -0.3665\n",
      "  Batch [1290/1299] D_loss: -1.1445, G_loss: -0.3805\n",
      "\n",
      "Epoch 40 Summary:\n",
      "  Average D_loss: -0.1804\n",
      "  Average G_loss: -0.0936\n",
      "\n",
      "Epoch [41/100]\n",
      "  Batch [0/1299] D_loss: -0.1179, G_loss: 0.1868\n",
      "  Batch [10/1299] D_loss: -0.0797, G_loss: 0.2352\n",
      "  Batch [20/1299] D_loss: -0.0829, G_loss: 0.3886\n",
      "  Batch [30/1299] D_loss: -0.0632, G_loss: 0.4474\n",
      "  Batch [40/1299] D_loss: -0.0231, G_loss: 0.4067\n",
      "  Batch [50/1299] D_loss: -2.2807, G_loss: -0.3611\n",
      "  Batch [60/1299] D_loss: -0.1985, G_loss: -0.0108\n",
      "  Batch [70/1299] D_loss: -0.0651, G_loss: 0.2100\n",
      "  Batch [80/1299] D_loss: -0.1535, G_loss: 0.5503\n",
      "  Batch [90/1299] D_loss: -0.1724, G_loss: 0.7051\n",
      "  Batch [100/1299] D_loss: 0.0139, G_loss: 0.3898\n",
      "  Batch [110/1299] D_loss: -0.1121, G_loss: 0.3415\n",
      "  Batch [120/1299] D_loss: -0.0492, G_loss: 0.3106\n",
      "  Batch [130/1299] D_loss: -0.7080, G_loss: -0.6969\n",
      "  Batch [140/1299] D_loss: -0.2796, G_loss: -0.1370\n",
      "  Batch [150/1299] D_loss: -0.0309, G_loss: 0.1730\n",
      "  Batch [160/1299] D_loss: 0.0394, G_loss: 0.2486\n",
      "  Batch [170/1299] D_loss: -0.0672, G_loss: 0.3691\n",
      "  Batch [180/1299] D_loss: -0.0639, G_loss: 0.3933\n",
      "  Batch [190/1299] D_loss: -0.1345, G_loss: 0.3440\n",
      "  Batch [200/1299] D_loss: -0.0688, G_loss: 0.3929\n",
      "  Batch [210/1299] D_loss: -0.6480, G_loss: -0.9810\n",
      "  Batch [220/1299] D_loss: -0.0579, G_loss: 0.1065\n",
      "  Batch [230/1299] D_loss: -1.4159, G_loss: -1.3138\n",
      "  Batch [240/1299] D_loss: -0.2401, G_loss: 0.1598\n",
      "  Batch [250/1299] D_loss: -1.5485, G_loss: -1.4863\n",
      "  Batch [260/1299] D_loss: -0.1386, G_loss: 0.3596\n",
      "  Batch [270/1299] D_loss: -0.3447, G_loss: 0.5607\n",
      "  Batch [280/1299] D_loss: -0.0756, G_loss: 0.7822\n",
      "  Batch [290/1299] D_loss: -0.0724, G_loss: 0.5661\n",
      "  Batch [300/1299] D_loss: -0.0717, G_loss: 0.5582\n",
      "  Batch [310/1299] D_loss: 0.0283, G_loss: 0.2984\n",
      "  Batch [320/1299] D_loss: -0.0298, G_loss: 0.1287\n",
      "  Batch [330/1299] D_loss: -0.2825, G_loss: -0.0143\n",
      "  Batch [340/1299] D_loss: -0.0746, G_loss: 0.1157\n",
      "  Batch [350/1299] D_loss: -0.0521, G_loss: 0.1330\n",
      "  Batch [360/1299] D_loss: -1.0019, G_loss: -2.9359\n",
      "  Batch [370/1299] D_loss: -0.0543, G_loss: 0.2203\n",
      "  Batch [380/1299] D_loss: -0.0766, G_loss: 0.3422\n",
      "  Batch [390/1299] D_loss: -0.0221, G_loss: 0.4941\n",
      "  Batch [400/1299] D_loss: -0.0403, G_loss: 0.6108\n",
      "  Batch [410/1299] D_loss: -0.1392, G_loss: 0.6012\n",
      "  Batch [420/1299] D_loss: -0.1168, G_loss: 0.5231\n",
      "  Batch [430/1299] D_loss: -0.2469, G_loss: -0.0382\n",
      "  Batch [440/1299] D_loss: 0.0048, G_loss: 0.0714\n",
      "  Batch [450/1299] D_loss: -0.1064, G_loss: 0.0705\n",
      "  Batch [460/1299] D_loss: -0.0766, G_loss: 0.2944\n",
      "  Batch [470/1299] D_loss: 0.0030, G_loss: 0.3945\n",
      "  Batch [480/1299] D_loss: -0.0179, G_loss: 0.6089\n",
      "  Batch [490/1299] D_loss: -0.0444, G_loss: 0.4730\n",
      "  Batch [500/1299] D_loss: 0.0763, G_loss: 0.3032\n",
      "  Batch [510/1299] D_loss: -0.0944, G_loss: 0.2640\n",
      "  Batch [520/1299] D_loss: -0.2216, G_loss: -0.0027\n",
      "  Batch [530/1299] D_loss: -0.0097, G_loss: 0.1417\n",
      "  Batch [540/1299] D_loss: -0.1154, G_loss: 0.3218\n",
      "  Batch [550/1299] D_loss: -0.0262, G_loss: 0.4970\n",
      "  Batch [560/1299] D_loss: -0.1600, G_loss: 0.5861\n",
      "  Batch [570/1299] D_loss: -0.0353, G_loss: 0.4965\n",
      "  Batch [580/1299] D_loss: -0.0155, G_loss: 0.2943\n",
      "  Batch [590/1299] D_loss: -0.0677, G_loss: 0.1676\n",
      "  Batch [600/1299] D_loss: -0.4287, G_loss: -0.2917\n",
      "  Batch [610/1299] D_loss: -0.1965, G_loss: 0.1929\n",
      "  Batch [620/1299] D_loss: -0.0074, G_loss: 0.1710\n",
      "  Batch [630/1299] D_loss: -0.0445, G_loss: 0.3192\n",
      "  Batch [640/1299] D_loss: -0.0161, G_loss: 0.3563\n",
      "  Batch [650/1299] D_loss: -0.0214, G_loss: 0.2828\n",
      "  Batch [660/1299] D_loss: -0.0728, G_loss: 0.3042\n",
      "  Batch [670/1299] D_loss: -0.0614, G_loss: 0.2963\n",
      "  Batch [680/1299] D_loss: -0.6797, G_loss: -1.7329\n",
      "  Batch [690/1299] D_loss: 0.0090, G_loss: 0.1548\n",
      "  Batch [700/1299] D_loss: -0.0039, G_loss: 0.1893\n",
      "  Batch [710/1299] D_loss: -0.0453, G_loss: 0.2764\n",
      "  Batch [720/1299] D_loss: -0.0378, G_loss: 0.3232\n",
      "  Batch [730/1299] D_loss: -0.1439, G_loss: 0.2330\n",
      "  Batch [740/1299] D_loss: -0.0525, G_loss: 0.0744\n",
      "  Batch [750/1299] D_loss: -1.2177, G_loss: -1.9986\n",
      "  Batch [760/1299] D_loss: -0.0675, G_loss: 0.0853\n",
      "  Batch [770/1299] D_loss: -0.1775, G_loss: 0.1608\n",
      "  Batch [780/1299] D_loss: -0.3803, G_loss: -0.2741\n",
      "  Batch [790/1299] D_loss: -0.1533, G_loss: 0.3351\n",
      "  Batch [800/1299] D_loss: -0.1698, G_loss: 0.4514\n",
      "  Batch [810/1299] D_loss: -0.0478, G_loss: 0.6099\n",
      "  Batch [820/1299] D_loss: -0.0550, G_loss: 0.5835\n",
      "  Batch [830/1299] D_loss: -0.0623, G_loss: 0.4001\n",
      "  Batch [840/1299] D_loss: -0.0805, G_loss: 0.1786\n",
      "  Batch [850/1299] D_loss: -0.0304, G_loss: 0.1545\n",
      "  Batch [860/1299] D_loss: 0.0029, G_loss: 0.1863\n",
      "  Batch [870/1299] D_loss: -0.0425, G_loss: 0.2189\n",
      "  Batch [880/1299] D_loss: -0.0574, G_loss: 0.2357\n",
      "  Batch [890/1299] D_loss: -1.3841, G_loss: -4.8854\n",
      "  Batch [900/1299] D_loss: -0.2596, G_loss: -0.4442\n",
      "  Batch [910/1299] D_loss: -0.0554, G_loss: 0.2580\n",
      "  Batch [920/1299] D_loss: -0.0317, G_loss: 0.3583\n",
      "  Batch [930/1299] D_loss: 0.0548, G_loss: 0.3107\n",
      "  Batch [940/1299] D_loss: -0.0198, G_loss: 0.4097\n",
      "  Batch [950/1299] D_loss: -1.4754, G_loss: -1.8850\n",
      "  Batch [960/1299] D_loss: -0.0212, G_loss: -0.0075\n",
      "  Batch [970/1299] D_loss: -0.6087, G_loss: -1.4619\n",
      "  Batch [980/1299] D_loss: -0.6123, G_loss: -0.5983\n",
      "  Batch [990/1299] D_loss: -0.9651, G_loss: -1.4529\n",
      "  Batch [1000/1299] D_loss: -0.0234, G_loss: 0.1956\n",
      "  Batch [1010/1299] D_loss: -0.1329, G_loss: 0.3143\n",
      "  Batch [1020/1299] D_loss: -0.0942, G_loss: 0.4813\n",
      "  Batch [1030/1299] D_loss: -0.1601, G_loss: 0.4823\n",
      "  Batch [1040/1299] D_loss: -0.0840, G_loss: 0.3073\n",
      "  Batch [1050/1299] D_loss: -0.9855, G_loss: -0.8979\n",
      "  Batch [1060/1299] D_loss: -0.0581, G_loss: 0.0258\n",
      "  Batch [1070/1299] D_loss: -0.7473, G_loss: -0.6430\n",
      "  Batch [1080/1299] D_loss: -0.0513, G_loss: 0.1826\n",
      "  Batch [1090/1299] D_loss: -0.1257, G_loss: 0.3974\n",
      "  Batch [1100/1299] D_loss: -0.0689, G_loss: 0.4373\n",
      "  Batch [1110/1299] D_loss: -0.0300, G_loss: 0.5158\n",
      "  Batch [1120/1299] D_loss: -0.1196, G_loss: 0.5003\n",
      "  Batch [1130/1299] D_loss: -0.0565, G_loss: 0.2583\n",
      "  Batch [1140/1299] D_loss: -0.5131, G_loss: -2.6580\n",
      "  Batch [1150/1299] D_loss: -0.5590, G_loss: -0.4071\n",
      "  Batch [1160/1299] D_loss: -0.5331, G_loss: -0.0200\n",
      "  Batch [1170/1299] D_loss: -0.3205, G_loss: -0.3899\n",
      "  Batch [1180/1299] D_loss: -0.0894, G_loss: 0.1032\n",
      "  Batch [1190/1299] D_loss: -0.6804, G_loss: -0.5130\n",
      "  Batch [1200/1299] D_loss: -0.0984, G_loss: 0.2581\n",
      "  Batch [1210/1299] D_loss: -0.1497, G_loss: 0.4814\n",
      "  Batch [1220/1299] D_loss: -0.0719, G_loss: 0.7318\n",
      "  Batch [1230/1299] D_loss: -0.2372, G_loss: 0.7420\n",
      "  Batch [1240/1299] D_loss: 0.0130, G_loss: 0.5845\n",
      "  Batch [1250/1299] D_loss: -0.0443, G_loss: 0.4234\n",
      "  Batch [1260/1299] D_loss: -3.7167, G_loss: -2.9743\n",
      "  Batch [1270/1299] D_loss: -0.0110, G_loss: 0.1436\n",
      "  Batch [1280/1299] D_loss: -0.0208, G_loss: 0.1751\n",
      "  Batch [1290/1299] D_loss: -1.5263, G_loss: -2.6027\n",
      "\n",
      "Epoch 41 Summary:\n",
      "  Average D_loss: -0.1417\n",
      "  Average G_loss: -0.0557\n",
      "\n",
      "Epoch [42/100]\n",
      "  Batch [0/1299] D_loss: -0.1350, G_loss: 0.0431\n",
      "  Batch [10/1299] D_loss: -0.0388, G_loss: 0.2068\n",
      "  Batch [20/1299] D_loss: -0.1402, G_loss: 0.3828\n",
      "  Batch [30/1299] D_loss: -0.0440, G_loss: 0.5415\n",
      "  Batch [40/1299] D_loss: 0.0010, G_loss: 0.4917\n",
      "  Batch [50/1299] D_loss: 0.0925, G_loss: 0.3324\n",
      "  Batch [60/1299] D_loss: -0.0464, G_loss: 0.2313\n",
      "  Batch [70/1299] D_loss: -0.0136, G_loss: 0.1085\n",
      "  Batch [80/1299] D_loss: 0.0214, G_loss: 0.1747\n",
      "  Batch [90/1299] D_loss: -0.0021, G_loss: 0.3312\n",
      "  Batch [100/1299] D_loss: -0.0223, G_loss: 0.2936\n",
      "  Batch [110/1299] D_loss: -0.0181, G_loss: 0.2877\n",
      "  Batch [120/1299] D_loss: -0.0412, G_loss: 0.2263\n",
      "  Batch [130/1299] D_loss: -0.7752, G_loss: -0.1148\n",
      "  Batch [140/1299] D_loss: -0.0859, G_loss: -0.0553\n",
      "  Batch [150/1299] D_loss: -0.0596, G_loss: 0.1985\n",
      "  Batch [160/1299] D_loss: -0.0349, G_loss: 0.3426\n",
      "  Batch [170/1299] D_loss: -0.0446, G_loss: 0.4683\n",
      "  Batch [180/1299] D_loss: -0.1095, G_loss: 0.4844\n",
      "  Batch [190/1299] D_loss: -0.1820, G_loss: 0.4626\n",
      "  Batch [200/1299] D_loss: -3.3717, G_loss: -4.8934\n",
      "  Batch [210/1299] D_loss: -0.8456, G_loss: -0.7637\n",
      "  Batch [220/1299] D_loss: -0.2282, G_loss: -0.2221\n",
      "  Batch [230/1299] D_loss: 0.0127, G_loss: 0.1327\n",
      "  Batch [240/1299] D_loss: -0.0474, G_loss: 0.2527\n",
      "  Batch [250/1299] D_loss: -0.0486, G_loss: 0.2983\n",
      "  Batch [260/1299] D_loss: -0.0347, G_loss: 0.2612\n",
      "  Batch [270/1299] D_loss: -0.0663, G_loss: 0.1712\n",
      "  Batch [280/1299] D_loss: -0.9808, G_loss: -1.5616\n",
      "  Batch [290/1299] D_loss: -0.3585, G_loss: -0.0308\n",
      "  Batch [300/1299] D_loss: -1.1582, G_loss: -1.2243\n",
      "  Batch [310/1299] D_loss: -0.4148, G_loss: -0.8871\n",
      "  Batch [320/1299] D_loss: -0.3281, G_loss: -0.5376\n",
      "  Batch [330/1299] D_loss: -0.0966, G_loss: 0.3145\n",
      "  Batch [340/1299] D_loss: -0.0872, G_loss: 0.4361\n",
      "  Batch [350/1299] D_loss: -0.1084, G_loss: 0.5434\n",
      "  Batch [360/1299] D_loss: -0.1535, G_loss: 0.5827\n",
      "  Batch [370/1299] D_loss: 0.0408, G_loss: 0.4304\n",
      "  Batch [380/1299] D_loss: -0.0277, G_loss: 0.2498\n",
      "  Batch [390/1299] D_loss: -0.6500, G_loss: -1.7877\n",
      "  Batch [400/1299] D_loss: -0.4654, G_loss: -0.8527\n",
      "  Batch [410/1299] D_loss: -0.1924, G_loss: 0.0390\n",
      "  Batch [420/1299] D_loss: -0.0433, G_loss: 0.1962\n",
      "  Batch [430/1299] D_loss: -0.0733, G_loss: 0.4554\n",
      "  Batch [440/1299] D_loss: -0.1169, G_loss: 0.4831\n",
      "  Batch [450/1299] D_loss: -0.1869, G_loss: 0.3783\n",
      "  Batch [460/1299] D_loss: -0.0006, G_loss: 0.1872\n",
      "  Batch [470/1299] D_loss: -0.0108, G_loss: 0.0815\n",
      "  Batch [480/1299] D_loss: -0.7351, G_loss: -0.0731\n",
      "  Batch [490/1299] D_loss: -0.1431, G_loss: -0.0467\n",
      "  Batch [500/1299] D_loss: -0.0753, G_loss: 0.1747\n",
      "  Batch [510/1299] D_loss: -0.1376, G_loss: 0.4133\n",
      "  Batch [520/1299] D_loss: -0.0833, G_loss: 0.4838\n",
      "  Batch [530/1299] D_loss: -0.0908, G_loss: 0.5820\n",
      "  Batch [540/1299] D_loss: -0.0447, G_loss: 0.6312\n",
      "  Batch [550/1299] D_loss: 0.0148, G_loss: 0.3538\n",
      "  Batch [560/1299] D_loss: -3.1330, G_loss: -4.8432\n",
      "  Batch [570/1299] D_loss: -0.0481, G_loss: 0.1590\n",
      "  Batch [580/1299] D_loss: -0.0324, G_loss: 0.2057\n",
      "  Batch [590/1299] D_loss: -0.0857, G_loss: 0.2606\n",
      "  Batch [600/1299] D_loss: 0.0237, G_loss: 0.1892\n",
      "  Batch [610/1299] D_loss: -0.0530, G_loss: 0.0817\n",
      "  Batch [620/1299] D_loss: -1.0186, G_loss: -0.2277\n",
      "  Batch [630/1299] D_loss: 0.0082, G_loss: 0.1545\n",
      "  Batch [640/1299] D_loss: -0.0154, G_loss: 0.1090\n",
      "  Batch [650/1299] D_loss: -1.0122, G_loss: -2.1758\n",
      "  Batch [660/1299] D_loss: -0.0082, G_loss: 0.2464\n",
      "  Batch [670/1299] D_loss: -0.1156, G_loss: 0.4145\n",
      "  Batch [680/1299] D_loss: -0.1388, G_loss: 0.5352\n",
      "  Batch [690/1299] D_loss: -0.0447, G_loss: 0.4267\n",
      "  Batch [700/1299] D_loss: -0.1481, G_loss: 0.4871\n",
      "  Batch [710/1299] D_loss: -0.0997, G_loss: 0.3328\n",
      "  Batch [720/1299] D_loss: -0.3501, G_loss: -0.3263\n",
      "  Batch [730/1299] D_loss: -0.4968, G_loss: -1.0627\n",
      "  Batch [740/1299] D_loss: -0.6318, G_loss: -0.9525\n",
      "  Batch [750/1299] D_loss: -0.0285, G_loss: 0.2612\n",
      "  Batch [760/1299] D_loss: -0.0452, G_loss: 0.4279\n",
      "  Batch [770/1299] D_loss: -0.0783, G_loss: 0.5104\n",
      "  Batch [780/1299] D_loss: -0.1746, G_loss: 0.5055\n",
      "  Batch [790/1299] D_loss: -0.1265, G_loss: 0.4332\n",
      "  Batch [800/1299] D_loss: -0.0755, G_loss: 0.2591\n",
      "  Batch [810/1299] D_loss: 0.0550, G_loss: -0.0157\n",
      "  Batch [820/1299] D_loss: -0.3766, G_loss: -0.1748\n",
      "  Batch [830/1299] D_loss: -0.0339, G_loss: 0.1869\n",
      "  Batch [840/1299] D_loss: -0.3199, G_loss: -0.1641\n",
      "  Batch [850/1299] D_loss: -0.0094, G_loss: 0.1919\n",
      "  Batch [860/1299] D_loss: -0.0531, G_loss: 0.2603\n",
      "  Batch [870/1299] D_loss: -0.0716, G_loss: 0.2821\n",
      "  Batch [880/1299] D_loss: -0.0592, G_loss: 0.2675\n",
      "  Batch [890/1299] D_loss: -1.2287, G_loss: -2.5217\n",
      "  Batch [900/1299] D_loss: -0.7418, G_loss: -1.2422\n",
      "  Batch [910/1299] D_loss: -0.0698, G_loss: 0.2082\n",
      "  Batch [920/1299] D_loss: -0.0161, G_loss: 0.1967\n",
      "  Batch [930/1299] D_loss: -0.1637, G_loss: 0.2292\n",
      "  Batch [940/1299] D_loss: -0.0040, G_loss: 0.2669\n",
      "  Batch [950/1299] D_loss: -1.3901, G_loss: -0.5936\n",
      "  Batch [960/1299] D_loss: -0.0166, G_loss: 0.1591\n",
      "  Batch [970/1299] D_loss: -0.0716, G_loss: 0.2224\n",
      "  Batch [980/1299] D_loss: -0.0004, G_loss: 0.2474\n",
      "  Batch [990/1299] D_loss: -0.0745, G_loss: 0.2737\n",
      "  Batch [1000/1299] D_loss: -0.0336, G_loss: 0.2434\n",
      "  Batch [1010/1299] D_loss: -0.0063, G_loss: 0.0927\n",
      "  Batch [1020/1299] D_loss: -0.0227, G_loss: 0.1743\n",
      "  Batch [1030/1299] D_loss: -0.1092, G_loss: 0.3897\n",
      "  Batch [1040/1299] D_loss: -0.0156, G_loss: 0.5039\n",
      "  Batch [1050/1299] D_loss: -0.0428, G_loss: 0.4791\n",
      "  Batch [1060/1299] D_loss: -0.1065, G_loss: 0.4167\n",
      "  Batch [1070/1299] D_loss: -0.0259, G_loss: 0.2253\n",
      "  Batch [1080/1299] D_loss: -1.0787, G_loss: -1.5284\n",
      "  Batch [1090/1299] D_loss: -0.9934, G_loss: -0.8103\n",
      "  Batch [1100/1299] D_loss: -0.1031, G_loss: 0.1600\n",
      "  Batch [1110/1299] D_loss: -0.1566, G_loss: 0.4062\n",
      "  Batch [1120/1299] D_loss: -0.1307, G_loss: 0.5607\n",
      "  Batch [1130/1299] D_loss: -0.0049, G_loss: 0.4959\n",
      "  Batch [1140/1299] D_loss: -0.1141, G_loss: 0.4111\n",
      "  Batch [1150/1299] D_loss: -0.0998, G_loss: 0.3254\n",
      "  Batch [1160/1299] D_loss: -0.3131, G_loss: -0.2206\n",
      "  Batch [1170/1299] D_loss: -1.5812, G_loss: -3.6973\n",
      "  Batch [1180/1299] D_loss: -0.3704, G_loss: -0.7370\n",
      "  Batch [1190/1299] D_loss: -0.1088, G_loss: 0.1781\n",
      "  Batch [1200/1299] D_loss: -0.0999, G_loss: 0.3973\n",
      "  Batch [1210/1299] D_loss: -0.1713, G_loss: 0.5975\n",
      "  Batch [1220/1299] D_loss: -0.1523, G_loss: 0.6417\n",
      "  Batch [1230/1299] D_loss: -0.0888, G_loss: 0.5644\n",
      "  Batch [1240/1299] D_loss: -0.0245, G_loss: 0.4127\n",
      "  Batch [1250/1299] D_loss: -2.5301, G_loss: -2.3443\n",
      "  Batch [1260/1299] D_loss: -0.7885, G_loss: -0.5777\n",
      "  Batch [1270/1299] D_loss: -0.2096, G_loss: 0.0305\n",
      "  Batch [1280/1299] D_loss: -0.2868, G_loss: -0.2407\n",
      "  Batch [1290/1299] D_loss: -0.0852, G_loss: 0.2633\n",
      "\n",
      "Epoch 42 Summary:\n",
      "  Average D_loss: -0.1277\n",
      "  Average G_loss: -0.0547\n",
      "\n",
      "Epoch [43/100]\n",
      "  Batch [0/1299] D_loss: 0.0081, G_loss: 0.2936\n",
      "  Batch [10/1299] D_loss: -0.0649, G_loss: 0.4958\n",
      "  Batch [20/1299] D_loss: -0.0443, G_loss: 0.5019\n",
      "  Batch [30/1299] D_loss: -0.0058, G_loss: 0.4885\n",
      "  Batch [40/1299] D_loss: -0.0463, G_loss: 0.3035\n",
      "  Batch [50/1299] D_loss: -0.2545, G_loss: 0.0845\n",
      "  Batch [60/1299] D_loss: -0.0335, G_loss: 0.1492\n",
      "  Batch [70/1299] D_loss: -2.4053, G_loss: -2.7063\n",
      "  Batch [80/1299] D_loss: -0.0394, G_loss: 0.1277\n",
      "  Batch [90/1299] D_loss: -0.0367, G_loss: 0.2161\n",
      "  Batch [100/1299] D_loss: -0.0139, G_loss: 0.3454\n",
      "  Batch [110/1299] D_loss: -0.0708, G_loss: 0.3486\n",
      "  Batch [120/1299] D_loss: -0.0286, G_loss: 0.2171\n",
      "  Batch [130/1299] D_loss: -0.1196, G_loss: 0.0191\n",
      "  Batch [140/1299] D_loss: -0.4515, G_loss: 0.0821\n",
      "  Batch [150/1299] D_loss: -1.1097, G_loss: -0.6013\n",
      "  Batch [160/1299] D_loss: -1.9479, G_loss: -0.1514\n",
      "  Batch [170/1299] D_loss: -0.1869, G_loss: 0.1300\n",
      "  Batch [180/1299] D_loss: -0.8865, G_loss: -0.0156\n",
      "  Batch [190/1299] D_loss: -0.2260, G_loss: 0.0105\n",
      "  Batch [200/1299] D_loss: -0.1134, G_loss: 0.3390\n",
      "  Batch [210/1299] D_loss: -0.0200, G_loss: 0.4482\n",
      "  Batch [220/1299] D_loss: -0.1591, G_loss: 0.5900\n",
      "  Batch [230/1299] D_loss: -0.0555, G_loss: 0.5919\n",
      "  Batch [240/1299] D_loss: -0.0861, G_loss: 0.4738\n",
      "  Batch [250/1299] D_loss: -0.0341, G_loss: 0.3098\n",
      "  Batch [260/1299] D_loss: -0.9734, G_loss: -0.4397\n",
      "  Batch [270/1299] D_loss: -1.0786, G_loss: -0.5763\n",
      "  Batch [280/1299] D_loss: -0.2051, G_loss: -0.1015\n",
      "  Batch [290/1299] D_loss: -0.1746, G_loss: 0.0245\n",
      "  Batch [300/1299] D_loss: -0.5691, G_loss: -0.0087\n",
      "  Batch [310/1299] D_loss: -0.0538, G_loss: 0.3303\n",
      "  Batch [320/1299] D_loss: -0.1327, G_loss: 0.4631\n",
      "  Batch [330/1299] D_loss: 0.0288, G_loss: 0.5082\n",
      "  Batch [340/1299] D_loss: -0.1090, G_loss: 0.5061\n",
      "  Batch [350/1299] D_loss: -0.0916, G_loss: 0.3662\n",
      "  Batch [360/1299] D_loss: -1.7697, G_loss: -0.4222\n",
      "  Batch [370/1299] D_loss: -0.0863, G_loss: 0.1558\n",
      "  Batch [380/1299] D_loss: 0.0494, G_loss: 0.1922\n",
      "  Batch [390/1299] D_loss: -0.0842, G_loss: 0.2594\n",
      "  Batch [400/1299] D_loss: -0.0088, G_loss: 0.2795\n",
      "  Batch [410/1299] D_loss: -0.0432, G_loss: 0.2356\n",
      "  Batch [420/1299] D_loss: -1.2199, G_loss: -0.0389\n",
      "  Batch [430/1299] D_loss: -0.7446, G_loss: -0.1956\n",
      "  Batch [440/1299] D_loss: -0.5651, G_loss: 0.0080\n",
      "  Batch [450/1299] D_loss: -0.5064, G_loss: -1.0822\n",
      "  Batch [460/1299] D_loss: -0.2192, G_loss: 0.1138\n",
      "  Batch [470/1299] D_loss: -0.0385, G_loss: 0.2040\n",
      "  Batch [480/1299] D_loss: -0.1305, G_loss: 0.3850\n",
      "  Batch [490/1299] D_loss: -0.0977, G_loss: 0.5223\n",
      "  Batch [500/1299] D_loss: -0.0546, G_loss: 0.4603\n",
      "  Batch [510/1299] D_loss: -0.1458, G_loss: 0.3655\n",
      "  Batch [520/1299] D_loss: -1.6179, G_loss: -1.9955\n",
      "  Batch [530/1299] D_loss: -0.0630, G_loss: 0.1843\n",
      "  Batch [540/1299] D_loss: -0.3628, G_loss: -0.2023\n",
      "  Batch [550/1299] D_loss: -0.0557, G_loss: 0.2145\n",
      "  Batch [560/1299] D_loss: -0.1450, G_loss: 0.4864\n",
      "  Batch [570/1299] D_loss: -0.1594, G_loss: 0.5456\n",
      "  Batch [580/1299] D_loss: -0.0483, G_loss: 0.4730\n",
      "  Batch [590/1299] D_loss: -0.0570, G_loss: 0.3009\n",
      "  Batch [600/1299] D_loss: 0.1279, G_loss: -1.6888\n",
      "  Batch [610/1299] D_loss: -1.6417, G_loss: -1.9197\n",
      "  Batch [620/1299] D_loss: -0.6383, G_loss: -1.8610\n",
      "  Batch [630/1299] D_loss: -0.6771, G_loss: -0.7211\n",
      "  Batch [640/1299] D_loss: -0.1636, G_loss: 0.3499\n",
      "  Batch [650/1299] D_loss: -0.0752, G_loss: 0.5306\n",
      "  Batch [660/1299] D_loss: -0.0551, G_loss: 0.6067\n",
      "  Batch [670/1299] D_loss: -0.1167, G_loss: 0.5619\n",
      "  Batch [680/1299] D_loss: 0.0106, G_loss: 0.4187\n",
      "  Batch [690/1299] D_loss: 0.0455, G_loss: 0.4031\n",
      "  Batch [700/1299] D_loss: -0.1042, G_loss: 0.3556\n",
      "  Batch [710/1299] D_loss: -0.0017, G_loss: 0.1335\n",
      "  Batch [720/1299] D_loss: -0.0282, G_loss: 0.1648\n",
      "  Batch [730/1299] D_loss: -0.6041, G_loss: -1.4033\n",
      "  Batch [740/1299] D_loss: -0.1762, G_loss: -0.1994\n",
      "  Batch [750/1299] D_loss: -0.2075, G_loss: -0.2361\n",
      "  Batch [760/1299] D_loss: -1.3484, G_loss: -0.7102\n",
      "  Batch [770/1299] D_loss: -0.0639, G_loss: 0.1526\n",
      "  Batch [780/1299] D_loss: -0.1064, G_loss: 0.2889\n",
      "  Batch [790/1299] D_loss: -0.1280, G_loss: 0.4423\n",
      "  Batch [800/1299] D_loss: -0.0422, G_loss: 0.4585\n",
      "  Batch [810/1299] D_loss: -0.1240, G_loss: 0.5487\n",
      "  Batch [820/1299] D_loss: -0.0816, G_loss: 0.4433\n",
      "  Batch [830/1299] D_loss: -0.1782, G_loss: 0.3489\n",
      "  Batch [840/1299] D_loss: -0.3649, G_loss: -0.8308\n",
      "  Batch [850/1299] D_loss: -0.1013, G_loss: 0.0414\n",
      "  Batch [860/1299] D_loss: -0.0527, G_loss: 0.2323\n",
      "  Batch [870/1299] D_loss: -0.0609, G_loss: 0.2762\n",
      "  Batch [880/1299] D_loss: -0.0531, G_loss: 0.2929\n",
      "  Batch [890/1299] D_loss: -0.0838, G_loss: 0.3242\n",
      "  Batch [900/1299] D_loss: -2.6817, G_loss: -3.7007\n",
      "  Batch [910/1299] D_loss: -0.1891, G_loss: 0.0519\n",
      "  Batch [920/1299] D_loss: -0.3165, G_loss: 0.0204\n",
      "  Batch [930/1299] D_loss: -1.0652, G_loss: -0.9715\n",
      "  Batch [940/1299] D_loss: -0.5626, G_loss: 0.0316\n",
      "  Batch [950/1299] D_loss: -1.2728, G_loss: -0.0551\n",
      "  Batch [960/1299] D_loss: -0.5430, G_loss: 0.0170\n",
      "  Batch [970/1299] D_loss: -0.1944, G_loss: -0.0043\n",
      "  Batch [980/1299] D_loss: -0.2534, G_loss: 0.1280\n",
      "  Batch [990/1299] D_loss: -0.1072, G_loss: 0.2074\n",
      "  Batch [1000/1299] D_loss: -0.1665, G_loss: 0.4828\n",
      "  Batch [1010/1299] D_loss: -0.0424, G_loss: 0.5608\n",
      "  Batch [1020/1299] D_loss: -0.2162, G_loss: 0.5968\n",
      "  Batch [1030/1299] D_loss: -0.0250, G_loss: 0.5541\n",
      "  Batch [1040/1299] D_loss: -1.0849, G_loss: -2.6159\n",
      "  Batch [1050/1299] D_loss: -1.3056, G_loss: -0.4693\n",
      "  Batch [1060/1299] D_loss: -0.2953, G_loss: -0.2167\n",
      "  Batch [1070/1299] D_loss: -0.0269, G_loss: 0.2565\n",
      "  Batch [1080/1299] D_loss: -0.0515, G_loss: 0.3992\n",
      "  Batch [1090/1299] D_loss: -0.0341, G_loss: 0.4832\n",
      "  Batch [1100/1299] D_loss: -0.2019, G_loss: 0.4722\n",
      "  Batch [1110/1299] D_loss: 0.0272, G_loss: 0.3803\n",
      "  Batch [1120/1299] D_loss: -0.0621, G_loss: 0.2277\n",
      "  Batch [1130/1299] D_loss: -0.7530, G_loss: -0.6590\n",
      "  Batch [1140/1299] D_loss: -0.7031, G_loss: -1.3163\n",
      "  Batch [1150/1299] D_loss: 0.0052, G_loss: 0.2251\n",
      "  Batch [1160/1299] D_loss: -0.0486, G_loss: 0.3026\n",
      "  Batch [1170/1299] D_loss: -0.0605, G_loss: 0.4163\n",
      "  Batch [1180/1299] D_loss: -0.0437, G_loss: 0.3221\n",
      "  Batch [1190/1299] D_loss: -0.0280, G_loss: 0.2984\n",
      "  Batch [1200/1299] D_loss: -1.5578, G_loss: -2.3978\n",
      "  Batch [1210/1299] D_loss: 0.0318, G_loss: 0.3099\n",
      "  Batch [1220/1299] D_loss: -0.0197, G_loss: 0.3045\n",
      "  Batch [1230/1299] D_loss: -0.0620, G_loss: 0.4031\n",
      "  Batch [1240/1299] D_loss: -0.0087, G_loss: 0.2336\n",
      "  Batch [1250/1299] D_loss: 0.0030, G_loss: 0.0860\n",
      "  Batch [1260/1299] D_loss: -0.0276, G_loss: 0.1940\n",
      "  Batch [1270/1299] D_loss: -0.0259, G_loss: 0.2806\n",
      "  Batch [1280/1299] D_loss: -0.0018, G_loss: 0.2364\n",
      "  Batch [1290/1299] D_loss: -0.0707, G_loss: 0.2232\n",
      "\n",
      "Epoch 43 Summary:\n",
      "  Average D_loss: -0.1640\n",
      "  Average G_loss: -0.0679\n",
      "\n",
      "Epoch [44/100]\n",
      "  Batch [0/1299] D_loss: -1.5934, G_loss: -1.7677\n",
      "  Batch [10/1299] D_loss: -1.4146, G_loss: -2.5888\n",
      "  Batch [20/1299] D_loss: -0.4579, G_loss: -0.1794\n",
      "  Batch [30/1299] D_loss: -0.0171, G_loss: 0.0802\n",
      "  Batch [40/1299] D_loss: -0.0594, G_loss: 0.1877\n",
      "  Batch [50/1299] D_loss: -0.0971, G_loss: 0.4191\n",
      "  Batch [60/1299] D_loss: -0.0102, G_loss: 0.5343\n",
      "  Batch [70/1299] D_loss: -0.1705, G_loss: 0.6887\n",
      "  Batch [80/1299] D_loss: -0.0795, G_loss: 0.5689\n",
      "  Batch [90/1299] D_loss: -0.0151, G_loss: 0.4475\n",
      "  Batch [100/1299] D_loss: -1.7786, G_loss: -1.7870\n",
      "  Batch [110/1299] D_loss: -1.1394, G_loss: -1.4378\n",
      "  Batch [120/1299] D_loss: 0.0275, G_loss: 0.0887\n",
      "  Batch [130/1299] D_loss: -0.0864, G_loss: 0.2631\n",
      "  Batch [140/1299] D_loss: 0.0269, G_loss: 0.3293\n",
      "  Batch [150/1299] D_loss: -0.2037, G_loss: 0.4781\n",
      "  Batch [160/1299] D_loss: -0.1934, G_loss: 0.4861\n",
      "  Batch [170/1299] D_loss: -0.1040, G_loss: 0.3686\n",
      "  Batch [180/1299] D_loss: -0.3112, G_loss: -0.6895\n",
      "  Batch [190/1299] D_loss: -0.5013, G_loss: -0.2994\n",
      "  Batch [200/1299] D_loss: -0.0154, G_loss: 0.2275\n",
      "  Batch [210/1299] D_loss: -0.0563, G_loss: 0.3220\n",
      "  Batch [220/1299] D_loss: 0.0020, G_loss: 0.4263\n",
      "  Batch [230/1299] D_loss: -0.0406, G_loss: 0.3609\n",
      "  Batch [240/1299] D_loss: -0.0216, G_loss: 0.2792\n",
      "  Batch [250/1299] D_loss: -1.7320, G_loss: -2.8467\n",
      "  Batch [260/1299] D_loss: -0.0165, G_loss: 0.1510\n",
      "  Batch [270/1299] D_loss: -0.0350, G_loss: 0.1759\n",
      "  Batch [280/1299] D_loss: -0.0292, G_loss: 0.2926\n",
      "  Batch [290/1299] D_loss: -0.0096, G_loss: 0.1562\n",
      "  Batch [300/1299] D_loss: -0.0941, G_loss: 0.0098\n",
      "  Batch [310/1299] D_loss: -0.3681, G_loss: -0.0726\n",
      "  Batch [320/1299] D_loss: -0.0649, G_loss: 0.0252\n",
      "  Batch [330/1299] D_loss: -2.1069, G_loss: -4.0051\n",
      "  Batch [340/1299] D_loss: -0.0529, G_loss: 0.1893\n",
      "  Batch [350/1299] D_loss: -0.1624, G_loss: 0.4491\n",
      "  Batch [360/1299] D_loss: -0.1163, G_loss: 0.6092\n",
      "  Batch [370/1299] D_loss: -0.0232, G_loss: 0.6120\n",
      "  Batch [380/1299] D_loss: -0.1990, G_loss: 0.5328\n",
      "  Batch [390/1299] D_loss: -0.0317, G_loss: 0.1817\n",
      "  Batch [400/1299] D_loss: -1.4744, G_loss: -3.6613\n",
      "  Batch [410/1299] D_loss: -1.1431, G_loss: -1.6144\n",
      "  Batch [420/1299] D_loss: -0.1197, G_loss: 0.2259\n",
      "  Batch [430/1299] D_loss: -0.0951, G_loss: 0.3644\n",
      "  Batch [440/1299] D_loss: -0.0344, G_loss: 0.3859\n",
      "  Batch [450/1299] D_loss: -0.1475, G_loss: 0.4766\n",
      "  Batch [460/1299] D_loss: -0.0543, G_loss: 0.4034\n",
      "  Batch [470/1299] D_loss: -1.1218, G_loss: -2.3021\n",
      "  Batch [480/1299] D_loss: -0.3844, G_loss: -0.8346\n",
      "  Batch [490/1299] D_loss: -0.2504, G_loss: -0.4251\n",
      "  Batch [500/1299] D_loss: -0.0186, G_loss: 0.2621\n",
      "  Batch [510/1299] D_loss: -0.0759, G_loss: 0.3406\n",
      "  Batch [520/1299] D_loss: -0.1050, G_loss: 0.4807\n",
      "  Batch [530/1299] D_loss: -0.0295, G_loss: 0.4454\n",
      "  Batch [540/1299] D_loss: 0.0502, G_loss: 0.3417\n",
      "  Batch [550/1299] D_loss: -0.0185, G_loss: 0.3050\n",
      "  Batch [560/1299] D_loss: -0.4674, G_loss: -0.6983\n",
      "  Batch [570/1299] D_loss: 0.0287, G_loss: 0.1512\n",
      "  Batch [580/1299] D_loss: -0.0705, G_loss: 0.1271\n",
      "  Batch [590/1299] D_loss: -0.0470, G_loss: 0.2031\n",
      "  Batch [600/1299] D_loss: -0.0672, G_loss: 0.2815\n",
      "  Batch [610/1299] D_loss: -0.0700, G_loss: 0.3926\n",
      "  Batch [620/1299] D_loss: -0.1162, G_loss: 0.6109\n",
      "  Batch [630/1299] D_loss: -0.0474, G_loss: 0.3530\n",
      "  Batch [640/1299] D_loss: -1.9314, G_loss: -0.4544\n",
      "  Batch [650/1299] D_loss: -0.3150, G_loss: -0.1263\n",
      "  Batch [660/1299] D_loss: -0.0956, G_loss: -0.0068\n",
      "  Batch [670/1299] D_loss: -0.4399, G_loss: -0.2347\n",
      "  Batch [680/1299] D_loss: -0.7644, G_loss: -0.6166\n",
      "  Batch [690/1299] D_loss: -0.0434, G_loss: 0.2104\n",
      "  Batch [700/1299] D_loss: -0.1062, G_loss: 0.4140\n",
      "  Batch [710/1299] D_loss: -0.0632, G_loss: 0.5163\n",
      "  Batch [720/1299] D_loss: -0.0920, G_loss: 0.5861\n",
      "  Batch [730/1299] D_loss: -0.0757, G_loss: 0.4345\n",
      "  Batch [740/1299] D_loss: -0.0253, G_loss: 0.3211\n",
      "  Batch [750/1299] D_loss: -0.1021, G_loss: -0.3622\n",
      "  Batch [760/1299] D_loss: -0.7305, G_loss: -0.3587\n",
      "  Batch [770/1299] D_loss: -0.4128, G_loss: 0.0745\n",
      "  Batch [780/1299] D_loss: 0.0004, G_loss: 0.1242\n",
      "  Batch [790/1299] D_loss: -0.0725, G_loss: 0.2740\n",
      "  Batch [800/1299] D_loss: -0.0804, G_loss: 0.3737\n",
      "  Batch [810/1299] D_loss: -0.0825, G_loss: 0.4919\n",
      "  Batch [820/1299] D_loss: -0.1200, G_loss: 0.3481\n",
      "  Batch [830/1299] D_loss: -0.0493, G_loss: 0.2451\n",
      "  Batch [840/1299] D_loss: -0.7764, G_loss: -0.9260\n",
      "  Batch [850/1299] D_loss: -0.0373, G_loss: 0.2169\n",
      "  Batch [860/1299] D_loss: -0.0036, G_loss: 0.3333\n",
      "  Batch [870/1299] D_loss: -0.0682, G_loss: 0.3604\n",
      "  Batch [880/1299] D_loss: -0.0536, G_loss: 0.3556\n",
      "  Batch [890/1299] D_loss: -0.0294, G_loss: 0.1979\n",
      "  Batch [900/1299] D_loss: -2.2595, G_loss: -2.9516\n",
      "  Batch [910/1299] D_loss: -0.1882, G_loss: -0.4077\n",
      "  Batch [920/1299] D_loss: -0.4554, G_loss: -0.4539\n",
      "  Batch [930/1299] D_loss: -0.5111, G_loss: 0.0083\n",
      "  Batch [940/1299] D_loss: -0.0481, G_loss: 0.1733\n",
      "  Batch [950/1299] D_loss: -0.0337, G_loss: 0.3402\n",
      "  Batch [960/1299] D_loss: -0.1204, G_loss: 0.5474\n",
      "  Batch [970/1299] D_loss: -0.1181, G_loss: 0.5906\n",
      "  Batch [980/1299] D_loss: -0.0601, G_loss: 0.4845\n",
      "  Batch [990/1299] D_loss: 0.0066, G_loss: 0.3676\n",
      "  Batch [1000/1299] D_loss: -0.0392, G_loss: 0.2311\n",
      "  Batch [1010/1299] D_loss: -0.0252, G_loss: 0.1251\n",
      "  Batch [1020/1299] D_loss: -0.0011, G_loss: 0.1586\n",
      "  Batch [1030/1299] D_loss: -0.0128, G_loss: 0.1585\n",
      "  Batch [1040/1299] D_loss: -1.0337, G_loss: -0.1802\n",
      "  Batch [1050/1299] D_loss: -1.2154, G_loss: -1.7391\n",
      "  Batch [1060/1299] D_loss: -0.0373, G_loss: 0.0393\n",
      "  Batch [1070/1299] D_loss: -0.0450, G_loss: 0.2197\n",
      "  Batch [1080/1299] D_loss: -0.0055, G_loss: 0.3278\n",
      "  Batch [1090/1299] D_loss: -0.1012, G_loss: 0.4415\n",
      "  Batch [1100/1299] D_loss: -0.1227, G_loss: 0.3922\n",
      "  Batch [1110/1299] D_loss: -0.0556, G_loss: 0.3342\n",
      "  Batch [1120/1299] D_loss: -0.8268, G_loss: -1.4612\n",
      "  Batch [1130/1299] D_loss: -0.0217, G_loss: 0.1433\n",
      "  Batch [1140/1299] D_loss: 0.0122, G_loss: 0.2148\n",
      "  Batch [1150/1299] D_loss: -0.1186, G_loss: 0.3489\n",
      "  Batch [1160/1299] D_loss: 0.0019, G_loss: 0.3066\n",
      "  Batch [1170/1299] D_loss: -0.1063, G_loss: 0.2843\n",
      "  Batch [1180/1299] D_loss: 0.0028, G_loss: -0.3910\n",
      "  Batch [1190/1299] D_loss: 0.0123, G_loss: 0.1357\n",
      "  Batch [1200/1299] D_loss: -0.0213, G_loss: 0.1951\n",
      "  Batch [1210/1299] D_loss: -0.0265, G_loss: 0.2798\n",
      "  Batch [1220/1299] D_loss: 0.0051, G_loss: 0.2953\n",
      "  Batch [1230/1299] D_loss: -1.0316, G_loss: -1.2051\n",
      "  Batch [1240/1299] D_loss: -0.5415, G_loss: -0.4461\n",
      "  Batch [1250/1299] D_loss: -0.7266, G_loss: -1.1438\n",
      "  Batch [1260/1299] D_loss: -0.0141, G_loss: 0.2248\n",
      "  Batch [1270/1299] D_loss: -0.0810, G_loss: 0.3806\n",
      "  Batch [1280/1299] D_loss: -0.1126, G_loss: 0.4848\n",
      "  Batch [1290/1299] D_loss: -0.0801, G_loss: 0.5262\n",
      "\n",
      "Epoch 44 Summary:\n",
      "  Average D_loss: -0.1230\n",
      "  Average G_loss: -0.0594\n",
      "\n",
      "Epoch [45/100]\n",
      "  Batch [0/1299] D_loss: -0.0072, G_loss: 0.5862\n",
      "  Batch [10/1299] D_loss: -0.1390, G_loss: 0.5802\n",
      "  Batch [20/1299] D_loss: -0.1114, G_loss: 0.2697\n",
      "  Batch [30/1299] D_loss: -0.0254, G_loss: 0.0453\n",
      "  Batch [40/1299] D_loss: -0.6480, G_loss: -0.9561\n",
      "  Batch [50/1299] D_loss: -0.6579, G_loss: -0.1250\n",
      "  Batch [60/1299] D_loss: -0.0043, G_loss: 0.0695\n",
      "  Batch [70/1299] D_loss: -0.0325, G_loss: 0.1843\n",
      "  Batch [80/1299] D_loss: -0.1237, G_loss: 0.4374\n",
      "  Batch [90/1299] D_loss: -0.1901, G_loss: 0.6783\n",
      "  Batch [100/1299] D_loss: -0.1643, G_loss: 0.5984\n",
      "  Batch [110/1299] D_loss: -0.1490, G_loss: 0.4450\n",
      "  Batch [120/1299] D_loss: -0.9377, G_loss: -0.3526\n",
      "  Batch [130/1299] D_loss: 0.0637, G_loss: -0.2289\n",
      "  Batch [140/1299] D_loss: -0.8574, G_loss: -1.6666\n",
      "  Batch [150/1299] D_loss: -0.4788, G_loss: -2.3005\n",
      "  Batch [160/1299] D_loss: -2.5806, G_loss: -3.5324\n",
      "  Batch [170/1299] D_loss: -1.1568, G_loss: 0.0702\n",
      "  Batch [180/1299] D_loss: 0.0035, G_loss: 0.2711\n",
      "  Batch [190/1299] D_loss: -0.0294, G_loss: 0.4846\n",
      "  Batch [200/1299] D_loss: -0.2119, G_loss: 0.6103\n",
      "  Batch [210/1299] D_loss: 0.0723, G_loss: 0.4904\n",
      "  Batch [220/1299] D_loss: -0.1749, G_loss: 0.4918\n",
      "  Batch [230/1299] D_loss: -0.0897, G_loss: 0.4461\n",
      "  Batch [240/1299] D_loss: -0.0667, G_loss: 0.3342\n",
      "  Batch [250/1299] D_loss: -1.2289, G_loss: -0.5941\n",
      "  Batch [260/1299] D_loss: -0.0097, G_loss: 0.1122\n",
      "  Batch [270/1299] D_loss: -0.0626, G_loss: 0.1626\n",
      "  Batch [280/1299] D_loss: -0.0250, G_loss: 0.2993\n",
      "  Batch [290/1299] D_loss: -0.0420, G_loss: 0.3122\n",
      "  Batch [300/1299] D_loss: -0.0232, G_loss: 0.2926\n",
      "  Batch [310/1299] D_loss: -0.0506, G_loss: 0.2239\n",
      "  Batch [320/1299] D_loss: -0.0496, G_loss: 0.2055\n",
      "  Batch [330/1299] D_loss: -0.6599, G_loss: -0.1961\n",
      "  Batch [340/1299] D_loss: -0.9837, G_loss: -1.3180\n",
      "  Batch [350/1299] D_loss: -0.8369, G_loss: -0.7985\n",
      "  Batch [360/1299] D_loss: -0.6289, G_loss: -0.4441\n",
      "  Batch [370/1299] D_loss: -0.0625, G_loss: 0.3707\n",
      "  Batch [380/1299] D_loss: -0.0513, G_loss: 0.4186\n",
      "  Batch [390/1299] D_loss: -0.1136, G_loss: 0.4301\n",
      "  Batch [400/1299] D_loss: -0.1202, G_loss: 0.5153\n",
      "  Batch [410/1299] D_loss: -0.0412, G_loss: 0.3786\n",
      "  Batch [420/1299] D_loss: 0.0344, G_loss: 0.2850\n",
      "  Batch [430/1299] D_loss: -1.1834, G_loss: -1.6269\n",
      "  Batch [440/1299] D_loss: -0.3692, G_loss: -0.5993\n",
      "  Batch [450/1299] D_loss: -1.5442, G_loss: -2.5313\n",
      "  Batch [460/1299] D_loss: -0.2937, G_loss: 0.1364\n",
      "  Batch [470/1299] D_loss: -1.0373, G_loss: -2.1922\n",
      "  Batch [480/1299] D_loss: -0.3329, G_loss: -0.5397\n",
      "  Batch [490/1299] D_loss: -0.8578, G_loss: -0.4372\n",
      "  Batch [500/1299] D_loss: -0.4087, G_loss: 0.1832\n",
      "  Batch [510/1299] D_loss: -0.5039, G_loss: -0.0034\n",
      "  Batch [520/1299] D_loss: -0.0334, G_loss: 0.1818\n",
      "  Batch [530/1299] D_loss: -0.1730, G_loss: 0.6339\n",
      "  Batch [540/1299] D_loss: -0.3820, G_loss: 0.9954\n",
      "  Batch [550/1299] D_loss: -0.0417, G_loss: 0.9851\n",
      "  Batch [560/1299] D_loss: 0.0254, G_loss: 0.5161\n",
      "  Batch [570/1299] D_loss: -0.1340, G_loss: 0.4551\n",
      "  Batch [580/1299] D_loss: -0.0445, G_loss: 0.2646\n",
      "  Batch [590/1299] D_loss: -0.0004, G_loss: 0.1322\n",
      "  Batch [600/1299] D_loss: -0.0493, G_loss: 0.1779\n",
      "  Batch [610/1299] D_loss: -0.0072, G_loss: 0.2674\n",
      "  Batch [620/1299] D_loss: -0.0616, G_loss: 0.3606\n",
      "  Batch [630/1299] D_loss: -0.0815, G_loss: 0.2677\n",
      "  Batch [640/1299] D_loss: -0.0157, G_loss: 0.1344\n",
      "  Batch [650/1299] D_loss: 0.0146, G_loss: 0.2002\n",
      "  Batch [660/1299] D_loss: -0.0328, G_loss: 0.2127\n",
      "  Batch [670/1299] D_loss: -0.0487, G_loss: 0.2464\n",
      "  Batch [680/1299] D_loss: -0.9282, G_loss: -1.7210\n",
      "  Batch [690/1299] D_loss: -1.0274, G_loss: -1.2308\n",
      "  Batch [700/1299] D_loss: -0.0451, G_loss: 0.1736\n",
      "  Batch [710/1299] D_loss: -0.0122, G_loss: 0.2913\n",
      "  Batch [720/1299] D_loss: -0.1312, G_loss: 0.4074\n",
      "  Batch [730/1299] D_loss: -0.1576, G_loss: 0.6180\n",
      "  Batch [740/1299] D_loss: -0.1295, G_loss: 0.5018\n",
      "  Batch [750/1299] D_loss: -0.0934, G_loss: 0.2997\n",
      "  Batch [760/1299] D_loss: -2.2146, G_loss: -3.2929\n",
      "  Batch [770/1299] D_loss: -0.6930, G_loss: -0.3318\n",
      "  Batch [780/1299] D_loss: -0.3084, G_loss: -0.3649\n",
      "  Batch [790/1299] D_loss: -0.2145, G_loss: 0.0512\n",
      "  Batch [800/1299] D_loss: -0.2253, G_loss: -0.1426\n",
      "  Batch [810/1299] D_loss: -0.5224, G_loss: -1.2398\n",
      "  Batch [820/1299] D_loss: -0.7579, G_loss: 0.0812\n",
      "  Batch [830/1299] D_loss: -0.9092, G_loss: -1.0086\n",
      "  Batch [840/1299] D_loss: -0.0790, G_loss: 0.2933\n",
      "  Batch [850/1299] D_loss: -0.0766, G_loss: 0.5950\n",
      "  Batch [860/1299] D_loss: -0.2136, G_loss: 0.7047\n",
      "  Batch [870/1299] D_loss: -0.0645, G_loss: 0.7151\n",
      "  Batch [880/1299] D_loss: -0.1679, G_loss: 0.6799\n",
      "  Batch [890/1299] D_loss: 0.0162, G_loss: 0.5299\n",
      "  Batch [900/1299] D_loss: -0.0534, G_loss: 0.3274\n",
      "  Batch [910/1299] D_loss: -1.2291, G_loss: -1.0318\n",
      "  Batch [920/1299] D_loss: -0.1952, G_loss: -0.5281\n",
      "  Batch [930/1299] D_loss: -0.3353, G_loss: -0.2920\n",
      "  Batch [940/1299] D_loss: -0.0652, G_loss: 0.0666\n",
      "  Batch [950/1299] D_loss: -0.0500, G_loss: 0.2310\n",
      "  Batch [960/1299] D_loss: -0.1043, G_loss: 0.3485\n",
      "  Batch [970/1299] D_loss: -0.0994, G_loss: 0.3959\n",
      "  Batch [980/1299] D_loss: -0.1771, G_loss: 0.2447\n",
      "  Batch [990/1299] D_loss: -0.0326, G_loss: 0.3985\n",
      "  Batch [1000/1299] D_loss: -0.9840, G_loss: -1.7017\n",
      "  Batch [1010/1299] D_loss: -0.8075, G_loss: -0.9152\n",
      "  Batch [1020/1299] D_loss: -0.0857, G_loss: 0.3573\n",
      "  Batch [1030/1299] D_loss: -0.0912, G_loss: 0.4602\n",
      "  Batch [1040/1299] D_loss: -0.0972, G_loss: 0.4320\n",
      "  Batch [1050/1299] D_loss: -0.0487, G_loss: 0.3955\n",
      "  Batch [1060/1299] D_loss: -0.0910, G_loss: 0.3975\n",
      "  Batch [1070/1299] D_loss: -1.3084, G_loss: -4.4675\n",
      "  Batch [1080/1299] D_loss: -0.1511, G_loss: -0.0453\n",
      "  Batch [1090/1299] D_loss: -0.5200, G_loss: -0.2922\n",
      "  Batch [1100/1299] D_loss: -0.3709, G_loss: -0.6264\n",
      "  Batch [1110/1299] D_loss: -0.0528, G_loss: 0.2492\n",
      "  Batch [1120/1299] D_loss: -0.0746, G_loss: 0.4354\n",
      "  Batch [1130/1299] D_loss: -0.1608, G_loss: 0.5427\n",
      "  Batch [1140/1299] D_loss: -0.0730, G_loss: 0.4678\n",
      "  Batch [1150/1299] D_loss: -0.1118, G_loss: 0.5251\n",
      "  Batch [1160/1299] D_loss: -0.0131, G_loss: 0.3813\n",
      "  Batch [1170/1299] D_loss: -0.9167, G_loss: -1.0582\n",
      "  Batch [1180/1299] D_loss: 0.0067, G_loss: 0.0848\n",
      "  Batch [1190/1299] D_loss: -0.0024, G_loss: 0.2156\n",
      "  Batch [1200/1299] D_loss: 0.0073, G_loss: 0.3294\n",
      "  Batch [1210/1299] D_loss: -0.0610, G_loss: 0.3733\n",
      "  Batch [1220/1299] D_loss: -0.1344, G_loss: 0.3386\n",
      "  Batch [1230/1299] D_loss: -1.3523, G_loss: -1.6288\n",
      "  Batch [1240/1299] D_loss: -0.0082, G_loss: 0.1463\n",
      "  Batch [1250/1299] D_loss: -0.0280, G_loss: 0.2206\n",
      "  Batch [1260/1299] D_loss: -0.1050, G_loss: 0.3660\n",
      "  Batch [1270/1299] D_loss: 0.0285, G_loss: 0.4771\n",
      "  Batch [1280/1299] D_loss: -0.0640, G_loss: 0.3733\n",
      "  Batch [1290/1299] D_loss: -0.0393, G_loss: 0.2456\n",
      "\n",
      "Epoch 45 Summary:\n",
      "  Average D_loss: -0.1523\n",
      "  Average G_loss: -0.0747\n",
      "\n",
      "Epoch [46/100]\n",
      "  Batch [0/1299] D_loss: 0.0984, G_loss: 0.1354\n",
      "  Batch [10/1299] D_loss: -0.4096, G_loss: 0.0618\n",
      "  Batch [20/1299] D_loss: -0.5320, G_loss: -2.8115\n",
      "  Batch [30/1299] D_loss: -0.4640, G_loss: -0.5412\n",
      "  Batch [40/1299] D_loss: -0.4547, G_loss: -0.0266\n",
      "  Batch [50/1299] D_loss: -0.3523, G_loss: -0.1348\n",
      "  Batch [60/1299] D_loss: -0.0163, G_loss: 0.2824\n",
      "  Batch [70/1299] D_loss: -0.1357, G_loss: 0.4843\n",
      "  Batch [80/1299] D_loss: -0.1715, G_loss: 0.5862\n",
      "  Batch [90/1299] D_loss: -0.0284, G_loss: 0.7117\n",
      "  Batch [100/1299] D_loss: -0.1278, G_loss: 0.5385\n",
      "  Batch [110/1299] D_loss: -0.0768, G_loss: 0.3412\n",
      "  Batch [120/1299] D_loss: 0.1509, G_loss: 0.1714\n",
      "  Batch [130/1299] D_loss: 0.0228, G_loss: 0.2271\n",
      "  Batch [140/1299] D_loss: -0.0401, G_loss: 0.2109\n",
      "  Batch [150/1299] D_loss: -2.4786, G_loss: -4.4175\n",
      "  Batch [160/1299] D_loss: -0.9499, G_loss: -1.0265\n",
      "  Batch [170/1299] D_loss: -0.3879, G_loss: -0.0482\n",
      "  Batch [180/1299] D_loss: -0.8381, G_loss: -0.2047\n",
      "  Batch [190/1299] D_loss: -0.0776, G_loss: 0.2548\n",
      "  Batch [200/1299] D_loss: -0.1333, G_loss: 0.4584\n",
      "  Batch [210/1299] D_loss: -0.1372, G_loss: 0.6146\n",
      "  Batch [220/1299] D_loss: -0.0599, G_loss: 0.6150\n",
      "  Batch [230/1299] D_loss: -0.0710, G_loss: 0.6186\n",
      "  Batch [240/1299] D_loss: -0.0279, G_loss: 0.4302\n",
      "  Batch [250/1299] D_loss: -1.4663, G_loss: -2.1581\n",
      "  Batch [260/1299] D_loss: -0.2428, G_loss: 0.0518\n",
      "  Batch [270/1299] D_loss: -0.8702, G_loss: -1.3847\n",
      "  Batch [280/1299] D_loss: -0.2480, G_loss: -0.3304\n",
      "  Batch [290/1299] D_loss: -0.4527, G_loss: -1.4444\n",
      "  Batch [300/1299] D_loss: -0.0951, G_loss: 0.1765\n",
      "  Batch [310/1299] D_loss: -0.1475, G_loss: 0.4331\n",
      "  Batch [320/1299] D_loss: -0.0984, G_loss: 0.6371\n",
      "  Batch [330/1299] D_loss: -0.1175, G_loss: 0.5996\n",
      "  Batch [340/1299] D_loss: -0.2059, G_loss: 0.7325\n",
      "  Batch [350/1299] D_loss: -0.0738, G_loss: 0.4196\n",
      "  Batch [360/1299] D_loss: -0.0232, G_loss: 0.3983\n",
      "  Batch [370/1299] D_loss: -0.1247, G_loss: -0.4480\n",
      "  Batch [380/1299] D_loss: -0.5052, G_loss: 0.0135\n",
      "  Batch [390/1299] D_loss: -0.0011, G_loss: 0.1382\n",
      "  Batch [400/1299] D_loss: -0.6639, G_loss: -0.3564\n",
      "  Batch [410/1299] D_loss: -0.6915, G_loss: -0.0254\n",
      "  Batch [420/1299] D_loss: -0.9763, G_loss: -2.8111\n",
      "  Batch [430/1299] D_loss: -0.1220, G_loss: 0.1106\n",
      "  Batch [440/1299] D_loss: -0.0909, G_loss: 0.1363\n",
      "  Batch [450/1299] D_loss: -0.8810, G_loss: -0.7016\n",
      "  Batch [460/1299] D_loss: -0.1221, G_loss: 0.3560\n",
      "  Batch [470/1299] D_loss: -0.0338, G_loss: 0.5275\n",
      "  Batch [480/1299] D_loss: -0.1211, G_loss: 0.4897\n",
      "  Batch [490/1299] D_loss: -0.0214, G_loss: 0.5436\n",
      "  Batch [500/1299] D_loss: 0.0298, G_loss: 0.4124\n",
      "  Batch [510/1299] D_loss: -0.1019, G_loss: 0.4406\n",
      "  Batch [520/1299] D_loss: -1.0467, G_loss: -2.7180\n",
      "  Batch [530/1299] D_loss: -0.3619, G_loss: -0.4639\n",
      "  Batch [540/1299] D_loss: -0.0598, G_loss: 0.2050\n",
      "  Batch [550/1299] D_loss: -0.0576, G_loss: 0.3370\n",
      "  Batch [560/1299] D_loss: -0.0986, G_loss: 0.3353\n",
      "  Batch [570/1299] D_loss: -0.0186, G_loss: 0.3020\n",
      "  Batch [580/1299] D_loss: -1.7497, G_loss: -0.9660\n",
      "  Batch [590/1299] D_loss: -0.0655, G_loss: 0.2573\n",
      "  Batch [600/1299] D_loss: -0.0328, G_loss: 0.3644\n",
      "  Batch [610/1299] D_loss: -0.0109, G_loss: 0.4179\n",
      "  Batch [620/1299] D_loss: -0.0428, G_loss: 0.4368\n",
      "  Batch [630/1299] D_loss: -0.0452, G_loss: 0.4939\n",
      "  Batch [640/1299] D_loss: -0.0410, G_loss: 0.2597\n",
      "  Batch [650/1299] D_loss: -0.0670, G_loss: -0.3303\n",
      "  Batch [660/1299] D_loss: -0.6693, G_loss: -0.0063\n",
      "  Batch [670/1299] D_loss: -0.0717, G_loss: 0.0841\n",
      "  Batch [680/1299] D_loss: -0.0279, G_loss: 0.1982\n",
      "  Batch [690/1299] D_loss: -0.1583, G_loss: 0.4240\n",
      "  Batch [700/1299] D_loss: -0.1284, G_loss: 0.5117\n",
      "  Batch [710/1299] D_loss: -0.0787, G_loss: 0.4928\n",
      "  Batch [720/1299] D_loss: -0.1437, G_loss: 0.4778\n",
      "  Batch [730/1299] D_loss: -0.0464, G_loss: 0.4387\n",
      "  Batch [740/1299] D_loss: -0.0930, G_loss: 0.2260\n",
      "  Batch [750/1299] D_loss: -0.0187, G_loss: -0.1921\n",
      "  Batch [760/1299] D_loss: -0.7844, G_loss: -1.5010\n",
      "  Batch [770/1299] D_loss: -0.0199, G_loss: 0.0817\n",
      "  Batch [780/1299] D_loss: -0.0748, G_loss: 0.0891\n",
      "  Batch [790/1299] D_loss: -0.6619, G_loss: -0.0140\n",
      "  Batch [800/1299] D_loss: -0.2762, G_loss: -0.1694\n",
      "  Batch [810/1299] D_loss: -0.0246, G_loss: 0.1679\n",
      "  Batch [820/1299] D_loss: -0.1091, G_loss: 0.0481\n",
      "  Batch [830/1299] D_loss: -0.1344, G_loss: 0.3683\n",
      "  Batch [840/1299] D_loss: -0.1544, G_loss: 0.4991\n",
      "  Batch [850/1299] D_loss: -0.1275, G_loss: 0.7654\n",
      "  Batch [860/1299] D_loss: -0.2178, G_loss: 0.6327\n",
      "  Batch [870/1299] D_loss: -0.0219, G_loss: 0.3750\n",
      "  Batch [880/1299] D_loss: -3.2722, G_loss: -8.0847\n",
      "  Batch [890/1299] D_loss: -1.2775, G_loss: -0.1451\n",
      "  Batch [900/1299] D_loss: 0.0024, G_loss: 0.1898\n",
      "  Batch [910/1299] D_loss: -0.0523, G_loss: 0.2884\n",
      "  Batch [920/1299] D_loss: -0.0688, G_loss: 0.3713\n",
      "  Batch [930/1299] D_loss: -0.0245, G_loss: 0.4353\n",
      "  Batch [940/1299] D_loss: -2.1901, G_loss: -4.0118\n",
      "  Batch [950/1299] D_loss: -0.3567, G_loss: -0.7364\n",
      "  Batch [960/1299] D_loss: -0.0295, G_loss: 0.3539\n",
      "  Batch [970/1299] D_loss: -0.1600, G_loss: 0.4478\n",
      "  Batch [980/1299] D_loss: -0.1245, G_loss: 0.5131\n",
      "  Batch [990/1299] D_loss: 0.0368, G_loss: 0.5782\n",
      "  Batch [1000/1299] D_loss: -0.0546, G_loss: 0.5135\n",
      "  Batch [1010/1299] D_loss: -0.0345, G_loss: 0.2113\n",
      "  Batch [1020/1299] D_loss: -0.5301, G_loss: -2.7193\n",
      "  Batch [1030/1299] D_loss: -0.1738, G_loss: 0.0372\n",
      "  Batch [1040/1299] D_loss: -0.1668, G_loss: 0.2465\n",
      "  Batch [1050/1299] D_loss: -0.0048, G_loss: 0.4139\n",
      "  Batch [1060/1299] D_loss: -0.0274, G_loss: 0.4041\n",
      "  Batch [1070/1299] D_loss: -0.1243, G_loss: 0.4573\n",
      "  Batch [1080/1299] D_loss: -0.0871, G_loss: 0.3785\n",
      "  Batch [1090/1299] D_loss: -0.0348, G_loss: 0.2970\n",
      "  Batch [1100/1299] D_loss: -0.4482, G_loss: -0.0398\n",
      "  Batch [1110/1299] D_loss: -0.0723, G_loss: 0.3353\n",
      "  Batch [1120/1299] D_loss: -0.0316, G_loss: 0.4078\n",
      "  Batch [1130/1299] D_loss: -0.0544, G_loss: 0.5444\n",
      "  Batch [1140/1299] D_loss: -0.1422, G_loss: 0.4092\n",
      "  Batch [1150/1299] D_loss: -0.0520, G_loss: 0.3735\n",
      "  Batch [1160/1299] D_loss: 0.0012, G_loss: 0.1788\n",
      "  Batch [1170/1299] D_loss: -0.1170, G_loss: 0.0273\n",
      "  Batch [1180/1299] D_loss: -0.7277, G_loss: -0.6236\n",
      "  Batch [1190/1299] D_loss: -0.9818, G_loss: -0.5622\n",
      "  Batch [1200/1299] D_loss: -1.1607, G_loss: -3.0680\n",
      "  Batch [1210/1299] D_loss: -0.5613, G_loss: -0.7463\n",
      "  Batch [1220/1299] D_loss: -0.4651, G_loss: 0.1037\n",
      "  Batch [1230/1299] D_loss: -0.3364, G_loss: 0.0776\n",
      "  Batch [1240/1299] D_loss: -0.7756, G_loss: 0.0993\n",
      "  Batch [1250/1299] D_loss: -0.2379, G_loss: 0.0850\n",
      "  Batch [1260/1299] D_loss: -0.4962, G_loss: -0.0022\n",
      "  Batch [1270/1299] D_loss: -0.2160, G_loss: 0.1600\n",
      "  Batch [1280/1299] D_loss: -0.5234, G_loss: -0.0124\n",
      "  Batch [1290/1299] D_loss: -0.0927, G_loss: 0.1544\n",
      "\n",
      "Epoch 46 Summary:\n",
      "  Average D_loss: -0.1776\n",
      "  Average G_loss: -0.1119\n",
      "\n",
      "Epoch [47/100]\n",
      "  Batch [0/1299] D_loss: -1.6661, G_loss: -0.9485\n",
      "  Batch [10/1299] D_loss: -0.1916, G_loss: -0.0475\n",
      "  Batch [20/1299] D_loss: -0.0871, G_loss: 0.2304\n",
      "  Batch [30/1299] D_loss: -0.1082, G_loss: 0.3384\n",
      "  Batch [40/1299] D_loss: -0.2419, G_loss: 0.5574\n",
      "  Batch [50/1299] D_loss: -0.1315, G_loss: 0.5764\n",
      "  Batch [60/1299] D_loss: -0.1182, G_loss: 0.6105\n",
      "  Batch [70/1299] D_loss: -0.1573, G_loss: 0.3882\n",
      "  Batch [80/1299] D_loss: -0.1240, G_loss: 0.2470\n",
      "  Batch [90/1299] D_loss: -0.8994, G_loss: -0.3112\n",
      "  Batch [100/1299] D_loss: -1.1617, G_loss: -1.3418\n",
      "  Batch [110/1299] D_loss: -1.2298, G_loss: -1.5280\n",
      "  Batch [120/1299] D_loss: -0.3293, G_loss: -0.0595\n",
      "  Batch [130/1299] D_loss: -0.5695, G_loss: 0.1265\n",
      "  Batch [140/1299] D_loss: -0.5793, G_loss: -0.5876\n",
      "  Batch [150/1299] D_loss: -0.1969, G_loss: 0.0098\n",
      "  Batch [160/1299] D_loss: -0.0948, G_loss: 0.4200\n",
      "  Batch [170/1299] D_loss: -0.1296, G_loss: 0.5644\n",
      "  Batch [180/1299] D_loss: -0.1430, G_loss: 0.6845\n",
      "  Batch [190/1299] D_loss: -0.1291, G_loss: 0.6694\n",
      "  Batch [200/1299] D_loss: 0.0234, G_loss: 0.5044\n",
      "  Batch [210/1299] D_loss: -0.0705, G_loss: 0.4414\n",
      "  Batch [220/1299] D_loss: -2.6741, G_loss: -6.5338\n",
      "  Batch [230/1299] D_loss: -0.0770, G_loss: 0.1165\n",
      "  Batch [240/1299] D_loss: -0.1838, G_loss: 0.0611\n",
      "  Batch [250/1299] D_loss: -1.4306, G_loss: -0.4081\n",
      "  Batch [260/1299] D_loss: -0.6645, G_loss: -0.2352\n",
      "  Batch [270/1299] D_loss: -0.6027, G_loss: -0.0567\n",
      "  Batch [280/1299] D_loss: -0.7212, G_loss: -0.3822\n",
      "  Batch [290/1299] D_loss: 0.0180, G_loss: 0.2007\n",
      "  Batch [300/1299] D_loss: -0.0929, G_loss: 0.3784\n",
      "  Batch [310/1299] D_loss: -0.1005, G_loss: 0.5015\n",
      "  Batch [320/1299] D_loss: -0.1807, G_loss: 0.6137\n",
      "  Batch [330/1299] D_loss: -0.1149, G_loss: 0.5868\n",
      "  Batch [340/1299] D_loss: -0.1278, G_loss: 0.4258\n",
      "  Batch [350/1299] D_loss: -2.5872, G_loss: -6.9347\n",
      "  Batch [360/1299] D_loss: -0.0620, G_loss: 0.2864\n",
      "  Batch [370/1299] D_loss: -0.1168, G_loss: 0.4772\n",
      "  Batch [380/1299] D_loss: 0.0280, G_loss: 0.5471\n",
      "  Batch [390/1299] D_loss: -0.1701, G_loss: 0.5545\n",
      "  Batch [400/1299] D_loss: -0.0013, G_loss: 0.4313\n",
      "  Batch [410/1299] D_loss: -4.2611, G_loss: -7.0654\n",
      "  Batch [420/1299] D_loss: -0.0068, G_loss: 0.1982\n",
      "  Batch [430/1299] D_loss: -0.0216, G_loss: 0.2667\n",
      "  Batch [440/1299] D_loss: -0.0416, G_loss: 0.3509\n",
      "  Batch [450/1299] D_loss: -0.1231, G_loss: 0.4595\n",
      "  Batch [460/1299] D_loss: -1.0328, G_loss: -3.9450\n",
      "  Batch [470/1299] D_loss: -0.1071, G_loss: 0.0458\n",
      "  Batch [480/1299] D_loss: -0.0090, G_loss: 0.3307\n",
      "  Batch [490/1299] D_loss: -0.0392, G_loss: 0.3892\n",
      "  Batch [500/1299] D_loss: -0.0616, G_loss: 0.4796\n",
      "  Batch [510/1299] D_loss: 0.0377, G_loss: 0.3318\n",
      "  Batch [520/1299] D_loss: -0.0436, G_loss: 0.2838\n",
      "  Batch [530/1299] D_loss: -0.1146, G_loss: 0.3340\n",
      "  Batch [540/1299] D_loss: -2.2274, G_loss: -3.1948\n",
      "  Batch [550/1299] D_loss: -0.0426, G_loss: 0.1520\n",
      "  Batch [560/1299] D_loss: -0.0148, G_loss: 0.1716\n",
      "  Batch [570/1299] D_loss: -0.0377, G_loss: 0.2408\n",
      "  Batch [580/1299] D_loss: -0.0541, G_loss: 0.2422\n",
      "  Batch [590/1299] D_loss: -0.0623, G_loss: 0.1934\n",
      "  Batch [600/1299] D_loss: -1.5409, G_loss: -2.1228\n",
      "  Batch [610/1299] D_loss: -0.5275, G_loss: -0.2888\n",
      "  Batch [620/1299] D_loss: -1.2219, G_loss: -0.3549\n",
      "  Batch [630/1299] D_loss: -1.3309, G_loss: -0.3952\n",
      "  Batch [640/1299] D_loss: -0.9929, G_loss: 0.0326\n",
      "  Batch [650/1299] D_loss: -0.3036, G_loss: -1.2025\n",
      "  Batch [660/1299] D_loss: -0.0832, G_loss: 0.3264\n",
      "  Batch [670/1299] D_loss: -0.0485, G_loss: 0.4408\n",
      "  Batch [680/1299] D_loss: -0.1365, G_loss: 0.5900\n",
      "  Batch [690/1299] D_loss: -0.0504, G_loss: 0.4690\n",
      "  Batch [700/1299] D_loss: -0.0992, G_loss: 0.2907\n",
      "  Batch [710/1299] D_loss: -0.4808, G_loss: 0.0991\n",
      "  Batch [720/1299] D_loss: -0.0414, G_loss: 0.2383\n",
      "  Batch [730/1299] D_loss: -0.1461, G_loss: 0.4246\n",
      "  Batch [740/1299] D_loss: -0.0879, G_loss: 0.4720\n",
      "  Batch [750/1299] D_loss: -0.0865, G_loss: 0.5260\n",
      "  Batch [760/1299] D_loss: -0.0559, G_loss: 0.3953\n",
      "  Batch [770/1299] D_loss: -0.0349, G_loss: 0.2241\n",
      "  Batch [780/1299] D_loss: -0.7733, G_loss: -0.9784\n",
      "  Batch [790/1299] D_loss: -0.2386, G_loss: -0.0885\n",
      "  Batch [800/1299] D_loss: -0.0536, G_loss: 0.2114\n",
      "  Batch [810/1299] D_loss: -0.0682, G_loss: 0.3240\n",
      "  Batch [820/1299] D_loss: -0.1156, G_loss: 0.3816\n",
      "  Batch [830/1299] D_loss: -0.0182, G_loss: 0.2778\n",
      "  Batch [840/1299] D_loss: -0.0596, G_loss: 0.3040\n",
      "  Batch [850/1299] D_loss: -0.2312, G_loss: 0.0038\n",
      "  Batch [860/1299] D_loss: -1.6506, G_loss: -1.5593\n",
      "  Batch [870/1299] D_loss: -0.5928, G_loss: -0.3559\n",
      "  Batch [880/1299] D_loss: -0.3431, G_loss: 0.0006\n",
      "  Batch [890/1299] D_loss: -0.6791, G_loss: -0.3233\n",
      "  Batch [900/1299] D_loss: -0.1442, G_loss: 0.0756\n",
      "  Batch [910/1299] D_loss: -0.1245, G_loss: 0.1234\n",
      "  Batch [920/1299] D_loss: -0.0863, G_loss: 0.1824\n",
      "  Batch [930/1299] D_loss: -0.0389, G_loss: 0.0987\n",
      "  Batch [940/1299] D_loss: -0.1870, G_loss: 0.2353\n",
      "  Batch [950/1299] D_loss: -0.0761, G_loss: 0.6090\n",
      "  Batch [960/1299] D_loss: -0.0435, G_loss: 0.6394\n",
      "  Batch [970/1299] D_loss: -0.1953, G_loss: 0.6204\n",
      "  Batch [980/1299] D_loss: -0.0438, G_loss: 0.4684\n",
      "  Batch [990/1299] D_loss: -0.0625, G_loss: 0.3673\n",
      "  Batch [1000/1299] D_loss: -0.3292, G_loss: -0.7890\n",
      "  Batch [1010/1299] D_loss: -0.1377, G_loss: 0.0183\n",
      "  Batch [1020/1299] D_loss: -0.4481, G_loss: -0.3037\n",
      "  Batch [1030/1299] D_loss: -0.1252, G_loss: 0.1066\n",
      "  Batch [1040/1299] D_loss: -1.1765, G_loss: -0.4166\n",
      "  Batch [1050/1299] D_loss: -0.1566, G_loss: 0.0412\n",
      "  Batch [1060/1299] D_loss: -0.0941, G_loss: 0.3839\n",
      "  Batch [1070/1299] D_loss: -0.1100, G_loss: 0.6678\n",
      "  Batch [1080/1299] D_loss: -0.1680, G_loss: 0.7379\n",
      "  Batch [1090/1299] D_loss: -0.1217, G_loss: 0.7578\n",
      "  Batch [1100/1299] D_loss: -0.0338, G_loss: 0.5228\n",
      "  Batch [1110/1299] D_loss: -0.0534, G_loss: 0.2688\n",
      "  Batch [1120/1299] D_loss: -0.0194, G_loss: 0.1038\n",
      "  Batch [1130/1299] D_loss: -0.9389, G_loss: -1.2428\n",
      "  Batch [1140/1299] D_loss: -0.2736, G_loss: -0.0140\n",
      "  Batch [1150/1299] D_loss: -0.3281, G_loss: 0.0603\n",
      "  Batch [1160/1299] D_loss: -0.0968, G_loss: 0.2547\n",
      "  Batch [1170/1299] D_loss: -0.2093, G_loss: 0.5182\n",
      "  Batch [1180/1299] D_loss: -0.0892, G_loss: 0.5619\n",
      "  Batch [1190/1299] D_loss: 0.0549, G_loss: 0.5052\n",
      "  Batch [1200/1299] D_loss: -0.0718, G_loss: 0.5079\n",
      "  Batch [1210/1299] D_loss: -0.1169, G_loss: 0.3327\n",
      "  Batch [1220/1299] D_loss: -1.5227, G_loss: -3.4064\n",
      "  Batch [1230/1299] D_loss: 0.0364, G_loss: 0.1230\n",
      "  Batch [1240/1299] D_loss: -0.0314, G_loss: 0.2193\n",
      "  Batch [1250/1299] D_loss: 0.0373, G_loss: 0.3266\n",
      "  Batch [1260/1299] D_loss: -0.0183, G_loss: 0.2587\n",
      "  Batch [1270/1299] D_loss: -0.0912, G_loss: 0.3443\n",
      "  Batch [1280/1299] D_loss: -0.0457, G_loss: 0.2452\n",
      "  Batch [1290/1299] D_loss: -0.0683, G_loss: 0.1053\n",
      "\n",
      "Epoch 47 Summary:\n",
      "  Average D_loss: -0.1732\n",
      "  Average G_loss: -0.0694\n",
      "\n",
      "Epoch [48/100]\n",
      "  Batch [0/1299] D_loss: -0.2882, G_loss: 0.0145\n",
      "  Batch [10/1299] D_loss: -0.1505, G_loss: 0.1676\n",
      "  Batch [20/1299] D_loss: -0.5582, G_loss: 0.1361\n",
      "  Batch [30/1299] D_loss: -0.1291, G_loss: -0.0281\n",
      "  Batch [40/1299] D_loss: -1.3631, G_loss: -0.2933\n",
      "  Batch [50/1299] D_loss: -0.1367, G_loss: 0.2132\n",
      "  Batch [60/1299] D_loss: -0.6716, G_loss: -0.2521\n",
      "  Batch [70/1299] D_loss: -0.0846, G_loss: 0.2475\n",
      "  Batch [80/1299] D_loss: -0.1416, G_loss: 0.5041\n",
      "  Batch [90/1299] D_loss: -0.0186, G_loss: 0.4697\n",
      "  Batch [100/1299] D_loss: -0.0941, G_loss: 0.4911\n",
      "  Batch [110/1299] D_loss: -0.0198, G_loss: 0.4212\n",
      "  Batch [120/1299] D_loss: -0.0250, G_loss: 0.2948\n",
      "  Batch [130/1299] D_loss: -0.8427, G_loss: -0.2427\n",
      "  Batch [140/1299] D_loss: -0.0486, G_loss: 0.1612\n",
      "  Batch [150/1299] D_loss: -0.0393, G_loss: 0.2224\n",
      "  Batch [160/1299] D_loss: -0.0401, G_loss: 0.2964\n",
      "  Batch [170/1299] D_loss: -0.0432, G_loss: 0.3446\n",
      "  Batch [180/1299] D_loss: -0.0930, G_loss: 0.2343\n",
      "  Batch [190/1299] D_loss: -0.0026, G_loss: 0.0532\n",
      "  Batch [200/1299] D_loss: -0.0106, G_loss: 0.1067\n",
      "  Batch [210/1299] D_loss: -0.0057, G_loss: 0.2086\n",
      "  Batch [220/1299] D_loss: -0.0711, G_loss: 0.4089\n",
      "  Batch [230/1299] D_loss: -0.0797, G_loss: 0.3511\n",
      "  Batch [240/1299] D_loss: -0.7106, G_loss: -2.3147\n",
      "  Batch [250/1299] D_loss: -0.0462, G_loss: 0.1977\n",
      "  Batch [260/1299] D_loss: -0.0708, G_loss: 0.3444\n",
      "  Batch [270/1299] D_loss: -0.0542, G_loss: 0.4964\n",
      "  Batch [280/1299] D_loss: -0.0262, G_loss: 0.4728\n",
      "  Batch [290/1299] D_loss: 0.0069, G_loss: 0.3270\n",
      "  Batch [300/1299] D_loss: -0.5507, G_loss: -0.1690\n",
      "  Batch [310/1299] D_loss: -0.0836, G_loss: -0.0044\n",
      "  Batch [320/1299] D_loss: -1.1255, G_loss: -0.9218\n",
      "  Batch [330/1299] D_loss: -0.0843, G_loss: 0.0359\n",
      "  Batch [340/1299] D_loss: -1.6021, G_loss: -0.9242\n",
      "  Batch [350/1299] D_loss: -0.2167, G_loss: 0.0309\n",
      "  Batch [360/1299] D_loss: -0.4797, G_loss: 0.0408\n",
      "  Batch [370/1299] D_loss: -0.1781, G_loss: 0.0660\n",
      "  Batch [380/1299] D_loss: -0.9750, G_loss: -0.7758\n",
      "  Batch [390/1299] D_loss: -0.0993, G_loss: 0.1016\n",
      "  Batch [400/1299] D_loss: -0.0789, G_loss: 0.1508\n",
      "  Batch [410/1299] D_loss: -0.0613, G_loss: 0.4642\n",
      "  Batch [420/1299] D_loss: -0.2253, G_loss: 0.6647\n",
      "  Batch [430/1299] D_loss: -0.1445, G_loss: 0.8353\n",
      "  Batch [440/1299] D_loss: -0.1646, G_loss: 0.7641\n",
      "  Batch [450/1299] D_loss: 0.0259, G_loss: 0.4500\n",
      "  Batch [460/1299] D_loss: 0.0080, G_loss: 0.2688\n",
      "  Batch [470/1299] D_loss: -0.9221, G_loss: -1.2638\n",
      "  Batch [480/1299] D_loss: -0.7203, G_loss: -0.3803\n",
      "  Batch [490/1299] D_loss: -0.1058, G_loss: 0.0871\n",
      "  Batch [500/1299] D_loss: -0.0709, G_loss: 0.2227\n",
      "  Batch [510/1299] D_loss: -0.0158, G_loss: 0.3721\n",
      "  Batch [520/1299] D_loss: -0.0396, G_loss: 0.5337\n",
      "  Batch [530/1299] D_loss: -0.2036, G_loss: 0.5310\n",
      "  Batch [540/1299] D_loss: 0.0194, G_loss: 0.3425\n",
      "  Batch [550/1299] D_loss: -0.0640, G_loss: 0.2746\n",
      "  Batch [560/1299] D_loss: -0.3008, G_loss: -0.0278\n",
      "  Batch [570/1299] D_loss: -0.5986, G_loss: -1.0825\n",
      "  Batch [580/1299] D_loss: -0.0986, G_loss: -0.0372\n",
      "  Batch [590/1299] D_loss: -1.6148, G_loss: -2.8123\n",
      "  Batch [600/1299] D_loss: -0.3669, G_loss: -0.5158\n",
      "  Batch [610/1299] D_loss: -0.1500, G_loss: 0.2588\n",
      "  Batch [620/1299] D_loss: -0.1234, G_loss: 0.5625\n",
      "  Batch [630/1299] D_loss: -0.0950, G_loss: 0.5502\n",
      "  Batch [640/1299] D_loss: -0.1059, G_loss: 0.5644\n",
      "  Batch [650/1299] D_loss: -0.0372, G_loss: 0.4244\n",
      "  Batch [660/1299] D_loss: -0.0243, G_loss: 0.1943\n",
      "  Batch [670/1299] D_loss: -0.1651, G_loss: -0.2243\n",
      "  Batch [680/1299] D_loss: -0.6924, G_loss: -0.1354\n",
      "  Batch [690/1299] D_loss: -0.7831, G_loss: -0.8606\n",
      "  Batch [700/1299] D_loss: -0.4108, G_loss: -0.0937\n",
      "  Batch [710/1299] D_loss: -0.0576, G_loss: 0.3207\n",
      "  Batch [720/1299] D_loss: -0.0559, G_loss: 0.4825\n",
      "  Batch [730/1299] D_loss: -0.0616, G_loss: 0.4857\n",
      "  Batch [740/1299] D_loss: -0.1150, G_loss: 0.5402\n",
      "  Batch [750/1299] D_loss: -0.0295, G_loss: 0.4273\n",
      "  Batch [760/1299] D_loss: -0.1972, G_loss: 0.4418\n",
      "  Batch [770/1299] D_loss: -0.7986, G_loss: -1.2172\n",
      "  Batch [780/1299] D_loss: 0.0025, G_loss: 0.1554\n",
      "  Batch [790/1299] D_loss: 0.0013, G_loss: 0.1442\n",
      "  Batch [800/1299] D_loss: -0.0629, G_loss: 0.2271\n",
      "  Batch [810/1299] D_loss: -0.0311, G_loss: 0.1475\n",
      "  Batch [820/1299] D_loss: -0.0323, G_loss: 0.0659\n",
      "  Batch [830/1299] D_loss: 0.0100, G_loss: 0.1634\n",
      "  Batch [840/1299] D_loss: -0.0224, G_loss: 0.3029\n",
      "  Batch [850/1299] D_loss: -0.0273, G_loss: 0.3324\n",
      "  Batch [860/1299] D_loss: -0.0863, G_loss: 0.3158\n",
      "  Batch [870/1299] D_loss: -0.0841, G_loss: 0.2814\n",
      "  Batch [880/1299] D_loss: -1.7266, G_loss: -2.9274\n",
      "  Batch [890/1299] D_loss: -0.0598, G_loss: 0.1827\n",
      "  Batch [900/1299] D_loss: -0.0634, G_loss: 0.2373\n",
      "  Batch [910/1299] D_loss: -0.1940, G_loss: 0.1504\n",
      "  Batch [920/1299] D_loss: -0.2827, G_loss: -0.5256\n",
      "  Batch [930/1299] D_loss: -0.0329, G_loss: 0.2210\n",
      "  Batch [940/1299] D_loss: -0.1033, G_loss: 0.4774\n",
      "  Batch [950/1299] D_loss: 0.0091, G_loss: 0.4837\n",
      "  Batch [960/1299] D_loss: -0.0099, G_loss: 0.4263\n",
      "  Batch [970/1299] D_loss: -0.0061, G_loss: 0.4152\n",
      "  Batch [980/1299] D_loss: -0.5700, G_loss: -1.3451\n",
      "  Batch [990/1299] D_loss: -1.2088, G_loss: -0.8671\n",
      "  Batch [1000/1299] D_loss: -1.1668, G_loss: -1.9350\n",
      "  Batch [1010/1299] D_loss: -0.0864, G_loss: 0.4099\n",
      "  Batch [1020/1299] D_loss: 0.0156, G_loss: 0.5359\n",
      "  Batch [1030/1299] D_loss: -0.0884, G_loss: 0.4919\n",
      "  Batch [1040/1299] D_loss: -0.1334, G_loss: 0.5690\n",
      "  Batch [1050/1299] D_loss: 0.0158, G_loss: 0.3939\n",
      "  Batch [1060/1299] D_loss: -0.0907, G_loss: 0.2748\n",
      "  Batch [1070/1299] D_loss: -0.1518, G_loss: 0.0152\n",
      "  Batch [1080/1299] D_loss: -1.2565, G_loss: -1.5566\n",
      "  Batch [1090/1299] D_loss: -0.5435, G_loss: -0.0493\n",
      "  Batch [1100/1299] D_loss: -0.7495, G_loss: -0.7444\n",
      "  Batch [1110/1299] D_loss: 0.0136, G_loss: 0.3241\n",
      "  Batch [1120/1299] D_loss: -0.0292, G_loss: 0.4325\n",
      "  Batch [1130/1299] D_loss: -0.1912, G_loss: 0.5383\n",
      "  Batch [1140/1299] D_loss: -0.0300, G_loss: 0.4953\n",
      "  Batch [1150/1299] D_loss: -0.0660, G_loss: 0.6117\n",
      "  Batch [1160/1299] D_loss: -0.0801, G_loss: 0.5284\n",
      "  Batch [1170/1299] D_loss: -0.0637, G_loss: 0.2614\n",
      "  Batch [1180/1299] D_loss: -0.9996, G_loss: -1.7031\n",
      "  Batch [1190/1299] D_loss: -0.0250, G_loss: 0.1085\n",
      "  Batch [1200/1299] D_loss: -0.0222, G_loss: 0.1557\n",
      "  Batch [1210/1299] D_loss: -0.1133, G_loss: 0.3218\n",
      "  Batch [1220/1299] D_loss: -0.0492, G_loss: 0.4858\n",
      "  Batch [1230/1299] D_loss: -0.2043, G_loss: 0.6136\n",
      "  Batch [1240/1299] D_loss: -0.1205, G_loss: 0.5007\n",
      "  Batch [1250/1299] D_loss: -0.0217, G_loss: 0.3668\n",
      "  Batch [1260/1299] D_loss: -0.2255, G_loss: 0.1051\n",
      "  Batch [1270/1299] D_loss: -0.6467, G_loss: -1.7732\n",
      "  Batch [1280/1299] D_loss: 0.0129, G_loss: 0.1157\n",
      "  Batch [1290/1299] D_loss: -1.2879, G_loss: -2.1128\n",
      "\n",
      "Epoch 48 Summary:\n",
      "  Average D_loss: -0.1543\n",
      "  Average G_loss: -0.0785\n",
      "\n",
      "Epoch [49/100]\n",
      "  Batch [0/1299] D_loss: -1.1749, G_loss: -0.4126\n",
      "  Batch [10/1299] D_loss: -0.0362, G_loss: 0.1620\n",
      "  Batch [20/1299] D_loss: -1.6213, G_loss: -3.2776\n",
      "  Batch [30/1299] D_loss: -0.1490, G_loss: 0.2759\n",
      "  Batch [40/1299] D_loss: -0.1746, G_loss: 0.4911\n",
      "  Batch [50/1299] D_loss: -0.1509, G_loss: 0.5580\n",
      "  Batch [60/1299] D_loss: -0.0732, G_loss: 0.6512\n",
      "  Batch [70/1299] D_loss: -0.0019, G_loss: 0.5504\n",
      "  Batch [80/1299] D_loss: -0.0505, G_loss: 0.3155\n",
      "  Batch [90/1299] D_loss: -1.1944, G_loss: -3.3618\n",
      "  Batch [100/1299] D_loss: -0.3435, G_loss: -0.0363\n",
      "  Batch [110/1299] D_loss: -0.0711, G_loss: -0.0155\n",
      "  Batch [120/1299] D_loss: -0.0490, G_loss: 0.2334\n",
      "  Batch [130/1299] D_loss: -0.0192, G_loss: 0.3090\n",
      "  Batch [140/1299] D_loss: -0.0145, G_loss: 0.2676\n",
      "  Batch [150/1299] D_loss: -0.0294, G_loss: 0.3013\n",
      "  Batch [160/1299] D_loss: -2.1433, G_loss: -3.7437\n",
      "  Batch [170/1299] D_loss: -0.7778, G_loss: -0.4199\n",
      "  Batch [180/1299] D_loss: -0.0439, G_loss: 0.2596\n",
      "  Batch [190/1299] D_loss: -0.0034, G_loss: 0.3533\n",
      "  Batch [200/1299] D_loss: -0.0353, G_loss: 0.5144\n",
      "  Batch [210/1299] D_loss: -0.0203, G_loss: 0.5342\n",
      "  Batch [220/1299] D_loss: -0.0762, G_loss: 0.4675\n",
      "  Batch [230/1299] D_loss: -0.0913, G_loss: 0.3614\n",
      "  Batch [240/1299] D_loss: -0.4045, G_loss: -0.5931\n",
      "  Batch [250/1299] D_loss: -1.6270, G_loss: -1.7733\n",
      "  Batch [260/1299] D_loss: -0.0253, G_loss: 0.1601\n",
      "  Batch [270/1299] D_loss: -0.0585, G_loss: 0.3286\n",
      "  Batch [280/1299] D_loss: -0.1276, G_loss: 0.3828\n",
      "  Batch [290/1299] D_loss: -0.0672, G_loss: 0.3695\n",
      "  Batch [300/1299] D_loss: -0.1098, G_loss: 0.3288\n",
      "  Batch [310/1299] D_loss: -1.9266, G_loss: -2.7705\n",
      "  Batch [320/1299] D_loss: -1.3115, G_loss: -2.7126\n",
      "  Batch [330/1299] D_loss: -0.7502, G_loss: -0.6159\n",
      "  Batch [340/1299] D_loss: -0.0596, G_loss: 0.1558\n",
      "  Batch [350/1299] D_loss: -0.3233, G_loss: 0.0910\n",
      "  Batch [360/1299] D_loss: -0.9684, G_loss: -0.9265\n",
      "  Batch [370/1299] D_loss: -0.0376, G_loss: 0.2128\n",
      "  Batch [380/1299] D_loss: -0.1040, G_loss: 0.4819\n",
      "  Batch [390/1299] D_loss: -0.2390, G_loss: 0.6919\n",
      "  Batch [400/1299] D_loss: -0.0086, G_loss: 0.7450\n",
      "  Batch [410/1299] D_loss: -0.1755, G_loss: 0.6695\n",
      "  Batch [420/1299] D_loss: -0.1397, G_loss: 0.5947\n",
      "  Batch [430/1299] D_loss: -0.0601, G_loss: 0.3221\n",
      "  Batch [440/1299] D_loss: -0.3542, G_loss: -0.5821\n",
      "  Batch [450/1299] D_loss: -1.2316, G_loss: -1.8739\n",
      "  Batch [460/1299] D_loss: -0.3694, G_loss: -0.0447\n",
      "  Batch [470/1299] D_loss: -0.5198, G_loss: -0.2450\n",
      "  Batch [480/1299] D_loss: -1.2460, G_loss: -0.2336\n",
      "  Batch [490/1299] D_loss: -0.0439, G_loss: 0.2439\n",
      "  Batch [500/1299] D_loss: 0.0048, G_loss: 0.3102\n",
      "  Batch [510/1299] D_loss: -0.1555, G_loss: 0.4642\n",
      "  Batch [520/1299] D_loss: -0.1014, G_loss: 0.3980\n",
      "  Batch [530/1299] D_loss: -0.0491, G_loss: 0.3683\n",
      "  Batch [540/1299] D_loss: -0.0285, G_loss: 0.3038\n",
      "  Batch [550/1299] D_loss: -0.8329, G_loss: -0.5602\n",
      "  Batch [560/1299] D_loss: -0.0249, G_loss: 0.1882\n",
      "  Batch [570/1299] D_loss: -0.0990, G_loss: 0.2836\n",
      "  Batch [580/1299] D_loss: -0.0307, G_loss: 0.3166\n",
      "  Batch [590/1299] D_loss: -0.0544, G_loss: 0.4556\n",
      "  Batch [600/1299] D_loss: -0.0447, G_loss: 0.3523\n",
      "  Batch [610/1299] D_loss: -2.8455, G_loss: -2.5231\n",
      "  Batch [620/1299] D_loss: -0.0162, G_loss: 0.1471\n",
      "  Batch [630/1299] D_loss: 0.0022, G_loss: 0.1083\n",
      "  Batch [640/1299] D_loss: 0.0060, G_loss: 0.0901\n",
      "  Batch [650/1299] D_loss: -0.2298, G_loss: -0.3302\n",
      "  Batch [660/1299] D_loss: -0.0540, G_loss: 0.0881\n",
      "  Batch [670/1299] D_loss: -0.5891, G_loss: -1.0851\n",
      "  Batch [680/1299] D_loss: -0.1226, G_loss: 0.1337\n",
      "  Batch [690/1299] D_loss: -0.4831, G_loss: -0.1419\n",
      "  Batch [700/1299] D_loss: -0.1168, G_loss: 0.0534\n",
      "  Batch [710/1299] D_loss: -0.0628, G_loss: 0.3880\n",
      "  Batch [720/1299] D_loss: -0.1328, G_loss: 0.5873\n",
      "  Batch [730/1299] D_loss: -0.0934, G_loss: 0.5056\n",
      "  Batch [740/1299] D_loss: -0.0445, G_loss: 0.4261\n",
      "  Batch [750/1299] D_loss: -0.0233, G_loss: 0.2242\n",
      "  Batch [760/1299] D_loss: -0.1815, G_loss: 0.1420\n",
      "  Batch [770/1299] D_loss: -1.6226, G_loss: -1.3490\n",
      "  Batch [780/1299] D_loss: -0.5383, G_loss: 0.0452\n",
      "  Batch [790/1299] D_loss: -0.3042, G_loss: -0.0210\n",
      "  Batch [800/1299] D_loss: -0.0056, G_loss: 0.2082\n",
      "  Batch [810/1299] D_loss: -0.0351, G_loss: 0.3133\n",
      "  Batch [820/1299] D_loss: 0.0059, G_loss: 0.2793\n",
      "  Batch [830/1299] D_loss: -0.0648, G_loss: 0.3275\n",
      "  Batch [840/1299] D_loss: -2.3780, G_loss: -2.9247\n",
      "  Batch [850/1299] D_loss: -0.2704, G_loss: 0.0220\n",
      "  Batch [860/1299] D_loss: -0.0023, G_loss: 0.2260\n",
      "  Batch [870/1299] D_loss: -0.0244, G_loss: 0.3309\n",
      "  Batch [880/1299] D_loss: -0.0436, G_loss: 0.3945\n",
      "  Batch [890/1299] D_loss: -0.0046, G_loss: 0.4520\n",
      "  Batch [900/1299] D_loss: -0.1165, G_loss: 0.3328\n",
      "  Batch [910/1299] D_loss: -0.0846, G_loss: 0.2480\n",
      "  Batch [920/1299] D_loss: -0.4049, G_loss: -0.2656\n",
      "  Batch [930/1299] D_loss: -0.5743, G_loss: -0.0438\n",
      "  Batch [940/1299] D_loss: -0.3348, G_loss: -0.4453\n",
      "  Batch [950/1299] D_loss: -0.9981, G_loss: -0.9684\n",
      "  Batch [960/1299] D_loss: -0.2948, G_loss: -0.0702\n",
      "  Batch [970/1299] D_loss: -0.0016, G_loss: 0.3261\n",
      "  Batch [980/1299] D_loss: -0.1365, G_loss: 0.4932\n",
      "  Batch [990/1299] D_loss: 0.0265, G_loss: 0.5299\n",
      "  Batch [1000/1299] D_loss: 0.0327, G_loss: 0.4265\n",
      "  Batch [1010/1299] D_loss: -0.0102, G_loss: 0.2859\n",
      "  Batch [1020/1299] D_loss: -1.3262, G_loss: -1.2578\n",
      "  Batch [1030/1299] D_loss: -0.8172, G_loss: -2.2436\n",
      "  Batch [1040/1299] D_loss: -0.1998, G_loss: 0.0608\n",
      "  Batch [1050/1299] D_loss: -0.6964, G_loss: -0.9453\n",
      "  Batch [1060/1299] D_loss: -0.0469, G_loss: 0.1946\n",
      "  Batch [1070/1299] D_loss: -0.0021, G_loss: 0.4621\n",
      "  Batch [1080/1299] D_loss: -0.1026, G_loss: 0.5828\n",
      "  Batch [1090/1299] D_loss: -0.1422, G_loss: 0.6545\n",
      "  Batch [1100/1299] D_loss: 0.0052, G_loss: 0.4278\n",
      "  Batch [1110/1299] D_loss: -0.0255, G_loss: 0.2313\n",
      "  Batch [1120/1299] D_loss: -0.2250, G_loss: 0.0342\n",
      "  Batch [1130/1299] D_loss: -0.0041, G_loss: 0.1874\n",
      "  Batch [1140/1299] D_loss: -0.0185, G_loss: 0.2675\n",
      "  Batch [1150/1299] D_loss: -0.0937, G_loss: 0.3366\n",
      "  Batch [1160/1299] D_loss: -0.0536, G_loss: 0.3173\n",
      "  Batch [1170/1299] D_loss: -0.0628, G_loss: 0.2478\n",
      "  Batch [1180/1299] D_loss: -2.1367, G_loss: -0.5665\n",
      "  Batch [1190/1299] D_loss: -0.0675, G_loss: 0.0477\n",
      "  Batch [1200/1299] D_loss: -0.5372, G_loss: 0.1333\n",
      "  Batch [1210/1299] D_loss: -0.5455, G_loss: -1.1292\n",
      "  Batch [1220/1299] D_loss: -0.0403, G_loss: 0.1638\n",
      "  Batch [1230/1299] D_loss: -0.1416, G_loss: 0.4431\n",
      "  Batch [1240/1299] D_loss: -0.1139, G_loss: 0.5936\n",
      "  Batch [1250/1299] D_loss: -0.0417, G_loss: 0.5819\n",
      "  Batch [1260/1299] D_loss: -0.1818, G_loss: 0.5331\n",
      "  Batch [1270/1299] D_loss: 0.0105, G_loss: 0.3054\n",
      "  Batch [1280/1299] D_loss: -2.7469, G_loss: -5.0776\n",
      "  Batch [1290/1299] D_loss: -0.0445, G_loss: 0.1793\n",
      "\n",
      "Epoch 49 Summary:\n",
      "  Average D_loss: -0.1513\n",
      "  Average G_loss: -0.0556\n",
      "\n",
      "Epoch [50/100]\n",
      "  Batch [0/1299] D_loss: -0.0102, G_loss: 0.2615\n",
      "  Batch [10/1299] D_loss: -0.0306, G_loss: 0.2659\n",
      "  Batch [20/1299] D_loss: -0.1184, G_loss: 0.3073\n",
      "  Batch [30/1299] D_loss: -0.0497, G_loss: 0.3233\n",
      "  Batch [40/1299] D_loss: -0.0381, G_loss: -0.0631\n",
      "  Batch [50/1299] D_loss: -0.0011, G_loss: 0.1621\n",
      "  Batch [60/1299] D_loss: -0.0197, G_loss: 0.1395\n",
      "  Batch [70/1299] D_loss: -0.0239, G_loss: 0.1733\n",
      "  Batch [80/1299] D_loss: -1.7606, G_loss: -3.3680\n",
      "  Batch [90/1299] D_loss: -0.7643, G_loss: -1.4547\n",
      "  Batch [100/1299] D_loss: -0.0133, G_loss: 0.1515\n",
      "  Batch [110/1299] D_loss: -0.0901, G_loss: 0.4225\n",
      "  Batch [120/1299] D_loss: -0.1659, G_loss: 0.5533\n",
      "  Batch [130/1299] D_loss: -0.0413, G_loss: 0.4485\n",
      "  Batch [140/1299] D_loss: -0.0828, G_loss: 0.4226\n",
      "  Batch [150/1299] D_loss: -0.0229, G_loss: 0.2649\n",
      "  Batch [160/1299] D_loss: -0.8113, G_loss: -0.3976\n",
      "  Batch [170/1299] D_loss: -0.0823, G_loss: 0.2058\n",
      "  Batch [180/1299] D_loss: -0.0133, G_loss: 0.2720\n",
      "  Batch [190/1299] D_loss: -0.0323, G_loss: 0.4490\n",
      "  Batch [200/1299] D_loss: 0.0450, G_loss: 0.3454\n",
      "  Batch [210/1299] D_loss: -2.0346, G_loss: -1.3656\n",
      "  Batch [220/1299] D_loss: -1.2457, G_loss: -1.3838\n",
      "  Batch [230/1299] D_loss: -0.0460, G_loss: 0.2121\n",
      "  Batch [240/1299] D_loss: -0.0424, G_loss: 0.2900\n",
      "  Batch [250/1299] D_loss: -0.0853, G_loss: 0.4046\n",
      "  Batch [260/1299] D_loss: -0.0296, G_loss: 0.3029\n",
      "  Batch [270/1299] D_loss: -0.1374, G_loss: 0.3965\n",
      "  Batch [280/1299] D_loss: -1.3287, G_loss: -2.2346\n",
      "  Batch [290/1299] D_loss: -0.0459, G_loss: 0.0545\n",
      "  Batch [300/1299] D_loss: -0.1155, G_loss: 0.1013\n",
      "  Batch [310/1299] D_loss: -0.0914, G_loss: 0.3125\n",
      "  Batch [320/1299] D_loss: -0.1482, G_loss: 0.3739\n",
      "  Batch [330/1299] D_loss: -0.0064, G_loss: 0.4018\n",
      "  Batch [340/1299] D_loss: -0.0783, G_loss: 0.2281\n",
      "  Batch [350/1299] D_loss: -0.5739, G_loss: 0.0455\n",
      "  Batch [360/1299] D_loss: -0.6123, G_loss: -0.2011\n",
      "  Batch [370/1299] D_loss: -0.4748, G_loss: -0.2523\n",
      "  Batch [380/1299] D_loss: -0.8632, G_loss: -0.8211\n",
      "  Batch [390/1299] D_loss: -0.0937, G_loss: 0.1858\n",
      "  Batch [400/1299] D_loss: -0.1030, G_loss: 0.3351\n",
      "  Batch [410/1299] D_loss: -0.0416, G_loss: 0.4580\n",
      "  Batch [420/1299] D_loss: -0.0916, G_loss: 0.3814\n",
      "  Batch [430/1299] D_loss: -0.0859, G_loss: 0.3490\n",
      "  Batch [440/1299] D_loss: -0.4440, G_loss: -1.8711\n",
      "  Batch [450/1299] D_loss: -0.2610, G_loss: 0.0072\n",
      "  Batch [460/1299] D_loss: -0.0407, G_loss: 0.2680\n",
      "  Batch [470/1299] D_loss: -0.0870, G_loss: 0.4582\n",
      "  Batch [480/1299] D_loss: -0.0599, G_loss: 0.5762\n",
      "  Batch [490/1299] D_loss: -0.0173, G_loss: 0.5428\n",
      "  Batch [500/1299] D_loss: -0.1033, G_loss: 0.4316\n",
      "  Batch [510/1299] D_loss: -0.1891, G_loss: 0.3430\n",
      "  Batch [520/1299] D_loss: -3.3422, G_loss: -3.5074\n",
      "  Batch [530/1299] D_loss: -0.6436, G_loss: -1.1895\n",
      "  Batch [540/1299] D_loss: -0.0398, G_loss: 0.1627\n",
      "  Batch [550/1299] D_loss: -0.0610, G_loss: 0.2634\n",
      "  Batch [560/1299] D_loss: -0.0159, G_loss: 0.4089\n",
      "  Batch [570/1299] D_loss: -0.0088, G_loss: 0.3678\n",
      "  Batch [580/1299] D_loss: -2.2024, G_loss: -3.4272\n",
      "  Batch [590/1299] D_loss: -1.7908, G_loss: -3.0753\n",
      "  Batch [600/1299] D_loss: -0.3632, G_loss: -0.0807\n",
      "  Batch [610/1299] D_loss: -0.0186, G_loss: 0.1335\n",
      "  Batch [620/1299] D_loss: -0.6834, G_loss: -0.1222\n",
      "  Batch [630/1299] D_loss: -0.1204, G_loss: 0.0338\n",
      "  Batch [640/1299] D_loss: -0.1749, G_loss: 0.1826\n",
      "  Batch [650/1299] D_loss: -0.0854, G_loss: 0.2831\n",
      "  Batch [660/1299] D_loss: -0.0992, G_loss: 0.3950\n",
      "  Batch [670/1299] D_loss: -0.1724, G_loss: 0.7274\n",
      "  Batch [680/1299] D_loss: -0.0240, G_loss: 0.6694\n",
      "  Batch [690/1299] D_loss: -0.0988, G_loss: 0.4233\n",
      "  Batch [700/1299] D_loss: -1.4143, G_loss: -5.3299\n",
      "  Batch [710/1299] D_loss: -0.7724, G_loss: -1.0160\n",
      "  Batch [720/1299] D_loss: -0.0291, G_loss: 0.1378\n",
      "  Batch [730/1299] D_loss: -0.9739, G_loss: -0.3223\n",
      "  Batch [740/1299] D_loss: -0.1770, G_loss: -0.2315\n",
      "  Batch [750/1299] D_loss: -0.0442, G_loss: 0.1047\n",
      "  Batch [760/1299] D_loss: -1.2505, G_loss: -1.2021\n",
      "  Batch [770/1299] D_loss: -0.2965, G_loss: 0.0988\n",
      "  Batch [780/1299] D_loss: -0.7653, G_loss: -0.8465\n",
      "  Batch [790/1299] D_loss: -0.9545, G_loss: -0.1788\n",
      "  Batch [800/1299] D_loss: -0.4862, G_loss: -0.0294\n",
      "  Batch [810/1299] D_loss: 0.0036, G_loss: 0.1324\n",
      "  Batch [820/1299] D_loss: -0.0755, G_loss: 0.2894\n",
      "  Batch [830/1299] D_loss: -0.0945, G_loss: 0.4254\n",
      "  Batch [840/1299] D_loss: -0.0233, G_loss: 0.4725\n",
      "  Batch [850/1299] D_loss: -0.0881, G_loss: 0.4951\n",
      "  Batch [860/1299] D_loss: -0.0318, G_loss: 0.3681\n",
      "  Batch [870/1299] D_loss: -1.4785, G_loss: -1.5664\n",
      "  Batch [880/1299] D_loss: -0.2017, G_loss: 0.1043\n",
      "  Batch [890/1299] D_loss: -0.8580, G_loss: -0.7576\n",
      "  Batch [900/1299] D_loss: -0.3939, G_loss: 0.0304\n",
      "  Batch [910/1299] D_loss: -1.4235, G_loss: 0.0723\n",
      "  Batch [920/1299] D_loss: -0.9488, G_loss: -0.7323\n",
      "  Batch [930/1299] D_loss: -0.7098, G_loss: -0.0478\n",
      "  Batch [940/1299] D_loss: -1.0093, G_loss: -0.7431\n",
      "  Batch [950/1299] D_loss: -0.1616, G_loss: 0.1966\n",
      "  Batch [960/1299] D_loss: -1.2820, G_loss: -0.9953\n",
      "  Batch [970/1299] D_loss: -0.0571, G_loss: 0.2284\n",
      "  Batch [980/1299] D_loss: -0.9834, G_loss: -1.6056\n",
      "  Batch [990/1299] D_loss: -0.1978, G_loss: 0.1399\n",
      "  Batch [1000/1299] D_loss: -0.4642, G_loss: 0.2046\n",
      "  Batch [1010/1299] D_loss: -0.4434, G_loss: 0.2376\n",
      "  Batch [1020/1299] D_loss: -0.1794, G_loss: 0.5427\n",
      "  Batch [1030/1299] D_loss: -0.2026, G_loss: 0.6487\n",
      "  Batch [1040/1299] D_loss: -0.0942, G_loss: 0.4943\n",
      "  Batch [1050/1299] D_loss: -0.7196, G_loss: -5.3504\n",
      "  Batch [1060/1299] D_loss: -0.3824, G_loss: -0.2560\n",
      "  Batch [1070/1299] D_loss: -0.0487, G_loss: 0.3527\n",
      "  Batch [1080/1299] D_loss: -0.1028, G_loss: 0.4213\n",
      "  Batch [1090/1299] D_loss: -0.1045, G_loss: 0.4349\n",
      "  Batch [1100/1299] D_loss: -0.0598, G_loss: 0.4518\n",
      "  Batch [1110/1299] D_loss: -0.0253, G_loss: 0.3322\n",
      "  Batch [1120/1299] D_loss: -0.0452, G_loss: 0.1602\n",
      "  Batch [1130/1299] D_loss: -0.0279, G_loss: 0.1391\n",
      "  Batch [1140/1299] D_loss: -0.7126, G_loss: -1.7552\n",
      "  Batch [1150/1299] D_loss: -0.0356, G_loss: 0.1943\n",
      "  Batch [1160/1299] D_loss: -0.0661, G_loss: 0.2888\n",
      "  Batch [1170/1299] D_loss: -0.0496, G_loss: 0.3094\n",
      "  Batch [1180/1299] D_loss: -0.1018, G_loss: 0.2393\n",
      "  Batch [1190/1299] D_loss: -1.2171, G_loss: -1.1229\n",
      "  Batch [1200/1299] D_loss: -0.0579, G_loss: 0.1368\n",
      "  Batch [1210/1299] D_loss: -0.0597, G_loss: 0.4090\n",
      "  Batch [1220/1299] D_loss: -0.0365, G_loss: 0.3489\n",
      "  Batch [1230/1299] D_loss: -0.0604, G_loss: 0.5668\n",
      "  Batch [1240/1299] D_loss: -0.0158, G_loss: 0.3440\n",
      "  Batch [1250/1299] D_loss: -0.0921, G_loss: 0.4090\n",
      "  Batch [1260/1299] D_loss: -3.1092, G_loss: -6.7236\n",
      "  Batch [1270/1299] D_loss: -0.2018, G_loss: -0.4180\n",
      "  Batch [1280/1299] D_loss: -0.8264, G_loss: -0.2979\n",
      "  Batch [1290/1299] D_loss: -0.0568, G_loss: 0.2234\n",
      "\n",
      "Epoch 50 Summary:\n",
      "  Average D_loss: -0.1617\n",
      "  Average G_loss: -0.0801\n",
      "\n",
      "Epoch [51/100]\n",
      "  Batch [0/1299] D_loss: -0.0632, G_loss: 0.3888\n",
      "  Batch [10/1299] D_loss: -0.0713, G_loss: 0.5376\n",
      "  Batch [20/1299] D_loss: 0.0068, G_loss: 0.5394\n",
      "  Batch [30/1299] D_loss: -0.0869, G_loss: 0.4387\n",
      "  Batch [40/1299] D_loss: -0.0980, G_loss: 0.3328\n",
      "  Batch [50/1299] D_loss: -2.3773, G_loss: -4.7557\n",
      "  Batch [60/1299] D_loss: -0.9221, G_loss: -0.2771\n",
      "  Batch [70/1299] D_loss: -0.2168, G_loss: -0.1400\n",
      "  Batch [80/1299] D_loss: -0.1497, G_loss: 0.1132\n",
      "  Batch [90/1299] D_loss: -0.2641, G_loss: -0.1530\n",
      "  Batch [100/1299] D_loss: -0.5492, G_loss: -1.1046\n",
      "  Batch [110/1299] D_loss: -0.0539, G_loss: 0.1883\n",
      "  Batch [120/1299] D_loss: -0.0816, G_loss: 0.3530\n",
      "  Batch [130/1299] D_loss: -0.0701, G_loss: 0.4293\n",
      "  Batch [140/1299] D_loss: -0.0291, G_loss: 0.5093\n",
      "  Batch [150/1299] D_loss: -0.0670, G_loss: 0.3547\n",
      "  Batch [160/1299] D_loss: -0.0888, G_loss: 0.3455\n",
      "  Batch [170/1299] D_loss: -0.6865, G_loss: -2.5816\n",
      "  Batch [180/1299] D_loss: -1.1801, G_loss: -5.2193\n",
      "  Batch [190/1299] D_loss: -0.0763, G_loss: 0.2020\n",
      "  Batch [200/1299] D_loss: -0.1165, G_loss: 0.4990\n",
      "  Batch [210/1299] D_loss: -0.2296, G_loss: 0.6758\n",
      "  Batch [220/1299] D_loss: -0.0703, G_loss: 0.6734\n",
      "  Batch [230/1299] D_loss: -0.0710, G_loss: 0.6410\n",
      "  Batch [240/1299] D_loss: -0.1816, G_loss: 0.6128\n",
      "  Batch [250/1299] D_loss: -2.3346, G_loss: -3.0653\n",
      "  Batch [260/1299] D_loss: 0.0051, G_loss: 0.0630\n",
      "  Batch [270/1299] D_loss: -1.1236, G_loss: -2.8680\n",
      "  Batch [280/1299] D_loss: -0.0488, G_loss: 0.1726\n",
      "  Batch [290/1299] D_loss: -0.0380, G_loss: 0.3250\n",
      "  Batch [300/1299] D_loss: -0.0412, G_loss: 0.3882\n",
      "  Batch [310/1299] D_loss: -0.0582, G_loss: 0.4393\n",
      "  Batch [320/1299] D_loss: -0.0738, G_loss: 0.3583\n",
      "  Batch [330/1299] D_loss: -0.0869, G_loss: 0.2765\n",
      "  Batch [340/1299] D_loss: -1.0064, G_loss: -2.3162\n",
      "  Batch [350/1299] D_loss: -0.2177, G_loss: -0.4536\n",
      "  Batch [360/1299] D_loss: -0.2170, G_loss: -0.0362\n",
      "  Batch [370/1299] D_loss: -0.0630, G_loss: 0.2301\n",
      "  Batch [380/1299] D_loss: -0.0237, G_loss: 0.4433\n",
      "  Batch [390/1299] D_loss: -0.0975, G_loss: 0.5683\n",
      "  Batch [400/1299] D_loss: -0.0389, G_loss: 0.5717\n",
      "  Batch [410/1299] D_loss: -0.0867, G_loss: 0.4893\n",
      "  Batch [420/1299] D_loss: -0.0403, G_loss: 0.3759\n",
      "  Batch [430/1299] D_loss: -2.1261, G_loss: -4.5431\n",
      "  Batch [440/1299] D_loss: -0.3482, G_loss: -0.4976\n",
      "  Batch [450/1299] D_loss: -0.0099, G_loss: 0.2003\n",
      "  Batch [460/1299] D_loss: -0.0184, G_loss: 0.2105\n",
      "  Batch [470/1299] D_loss: -0.0356, G_loss: 0.2418\n",
      "  Batch [480/1299] D_loss: -0.0090, G_loss: 0.2035\n",
      "  Batch [490/1299] D_loss: -1.0785, G_loss: -0.0899\n",
      "  Batch [500/1299] D_loss: -0.0036, G_loss: -0.0733\n",
      "  Batch [510/1299] D_loss: -0.2868, G_loss: -0.8324\n",
      "  Batch [520/1299] D_loss: -0.2065, G_loss: 0.0664\n",
      "  Batch [530/1299] D_loss: -0.1835, G_loss: -0.0028\n",
      "  Batch [540/1299] D_loss: -0.4876, G_loss: -0.6075\n",
      "  Batch [550/1299] D_loss: -0.5829, G_loss: -0.6378\n",
      "  Batch [560/1299] D_loss: -0.4857, G_loss: -0.4531\n",
      "  Batch [570/1299] D_loss: -0.3460, G_loss: -0.5276\n",
      "  Batch [580/1299] D_loss: -0.1333, G_loss: 0.3809\n",
      "  Batch [590/1299] D_loss: -0.1291, G_loss: 0.6170\n",
      "  Batch [600/1299] D_loss: -0.2301, G_loss: 0.7570\n",
      "  Batch [610/1299] D_loss: -0.3262, G_loss: 0.7013\n",
      "  Batch [620/1299] D_loss: -0.0511, G_loss: 0.4173\n",
      "  Batch [630/1299] D_loss: -0.0669, G_loss: 0.3566\n",
      "  Batch [640/1299] D_loss: -0.0015, G_loss: 0.1226\n",
      "  Batch [650/1299] D_loss: 0.0026, G_loss: 0.2271\n",
      "  Batch [660/1299] D_loss: 0.0471, G_loss: 0.2994\n",
      "  Batch [670/1299] D_loss: -0.0744, G_loss: 0.3082\n",
      "  Batch [680/1299] D_loss: -2.4924, G_loss: -6.2816\n",
      "  Batch [690/1299] D_loss: -0.0315, G_loss: 0.0609\n",
      "  Batch [700/1299] D_loss: -0.0145, G_loss: 0.1681\n",
      "  Batch [710/1299] D_loss: -0.1216, G_loss: 0.3786\n",
      "  Batch [720/1299] D_loss: -0.1313, G_loss: 0.4879\n",
      "  Batch [730/1299] D_loss: -0.0576, G_loss: 0.4390\n",
      "  Batch [740/1299] D_loss: 0.0815, G_loss: 0.2848\n",
      "  Batch [750/1299] D_loss: -1.4961, G_loss: -3.4909\n",
      "  Batch [760/1299] D_loss: 0.0576, G_loss: 0.1569\n",
      "  Batch [770/1299] D_loss: -0.0880, G_loss: 0.3285\n",
      "  Batch [780/1299] D_loss: -0.0904, G_loss: 0.5221\n",
      "  Batch [790/1299] D_loss: -0.1241, G_loss: 0.5364\n",
      "  Batch [800/1299] D_loss: -0.0539, G_loss: 0.5203\n",
      "  Batch [810/1299] D_loss: -0.1168, G_loss: 0.3579\n",
      "  Batch [820/1299] D_loss: -0.4162, G_loss: -0.9668\n",
      "  Batch [830/1299] D_loss: -0.4522, G_loss: -0.6774\n",
      "  Batch [840/1299] D_loss: -2.2696, G_loss: -1.9157\n",
      "  Batch [850/1299] D_loss: -0.5197, G_loss: -1.0631\n",
      "  Batch [860/1299] D_loss: -0.0528, G_loss: 0.1686\n",
      "  Batch [870/1299] D_loss: -0.1243, G_loss: 0.3034\n",
      "  Batch [880/1299] D_loss: -0.1410, G_loss: 0.5791\n",
      "  Batch [890/1299] D_loss: -0.1070, G_loss: 0.6399\n",
      "  Batch [900/1299] D_loss: -0.0319, G_loss: 0.5613\n",
      "  Batch [910/1299] D_loss: 0.0235, G_loss: 0.4006\n",
      "  Batch [920/1299] D_loss: -0.0293, G_loss: 0.0995\n",
      "  Batch [930/1299] D_loss: -0.3115, G_loss: -0.7421\n",
      "  Batch [940/1299] D_loss: -0.7049, G_loss: -1.0250\n",
      "  Batch [950/1299] D_loss: -0.0435, G_loss: 0.2460\n",
      "  Batch [960/1299] D_loss: -0.0037, G_loss: 0.4747\n",
      "  Batch [970/1299] D_loss: -0.1505, G_loss: 0.5554\n",
      "  Batch [980/1299] D_loss: -0.1245, G_loss: 0.5410\n",
      "  Batch [990/1299] D_loss: -0.0165, G_loss: 0.3782\n",
      "  Batch [1000/1299] D_loss: -2.6959, G_loss: -6.6196\n",
      "  Batch [1010/1299] D_loss: -0.5678, G_loss: -0.1619\n",
      "  Batch [1020/1299] D_loss: -0.6510, G_loss: -0.8073\n",
      "  Batch [1030/1299] D_loss: 0.0183, G_loss: 0.2346\n",
      "  Batch [1040/1299] D_loss: -0.0603, G_loss: 0.3353\n",
      "  Batch [1050/1299] D_loss: -0.0828, G_loss: 0.3526\n",
      "  Batch [1060/1299] D_loss: -0.3864, G_loss: 0.2316\n",
      "  Batch [1070/1299] D_loss: -0.1034, G_loss: 0.0456\n",
      "  Batch [1080/1299] D_loss: -0.2767, G_loss: -0.4041\n",
      "  Batch [1090/1299] D_loss: -0.0760, G_loss: 0.3817\n",
      "  Batch [1100/1299] D_loss: -0.1512, G_loss: 0.4709\n",
      "  Batch [1110/1299] D_loss: -0.1461, G_loss: 0.5568\n",
      "  Batch [1120/1299] D_loss: -0.1071, G_loss: 0.4993\n",
      "  Batch [1130/1299] D_loss: -0.1240, G_loss: 0.4434\n",
      "  Batch [1140/1299] D_loss: -2.1778, G_loss: -4.3510\n",
      "  Batch [1150/1299] D_loss: -0.0179, G_loss: 0.1412\n",
      "  Batch [1160/1299] D_loss: -0.1614, G_loss: 0.0403\n",
      "  Batch [1170/1299] D_loss: 0.4686, G_loss: 0.1601\n",
      "  Batch [1180/1299] D_loss: -0.3745, G_loss: 0.0496\n",
      "  Batch [1190/1299] D_loss: -0.0174, G_loss: 0.0742\n",
      "  Batch [1200/1299] D_loss: -0.0294, G_loss: 0.2292\n",
      "  Batch [1210/1299] D_loss: -0.0775, G_loss: 0.3521\n",
      "  Batch [1220/1299] D_loss: 0.0039, G_loss: 0.3284\n",
      "  Batch [1230/1299] D_loss: -0.0686, G_loss: 0.4104\n",
      "  Batch [1240/1299] D_loss: -0.0544, G_loss: 0.2464\n",
      "  Batch [1250/1299] D_loss: -0.4743, G_loss: -0.7297\n",
      "  Batch [1260/1299] D_loss: -0.0286, G_loss: 0.2543\n",
      "  Batch [1270/1299] D_loss: -0.0213, G_loss: 0.2934\n",
      "  Batch [1280/1299] D_loss: 0.0106, G_loss: 0.3891\n",
      "  Batch [1290/1299] D_loss: -0.0923, G_loss: 0.3776\n",
      "\n",
      "Epoch 51 Summary:\n",
      "  Average D_loss: -0.1370\n",
      "  Average G_loss: -0.0420\n",
      "\n",
      "Models saved at epoch 51:\n",
      "Generator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/saved_models/run_20250113_135205_dataset/generator_20250113_135205_dataset_epoch_51.pt\n",
      "Discriminator: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/saved_models/run_20250113_135205_dataset/discriminator_20250113_135205_dataset_epoch_51.pt\n",
      "\n",
      "Epoch [52/100]\n",
      "  Batch [0/1299] D_loss: -0.0516, G_loss: 0.3346\n",
      "  Batch [10/1299] D_loss: -0.3012, G_loss: -0.5446\n",
      "  Batch [20/1299] D_loss: -0.9206, G_loss: -0.4135\n",
      "  Batch [30/1299] D_loss: -0.0279, G_loss: 0.0656\n",
      "  Batch [40/1299] D_loss: -0.0353, G_loss: 0.2415\n",
      "  Batch [50/1299] D_loss: -0.0390, G_loss: 0.3944\n",
      "  Batch [60/1299] D_loss: -0.1311, G_loss: 0.5012\n",
      "  Batch [70/1299] D_loss: -0.1378, G_loss: 0.4077\n",
      "  Batch [80/1299] D_loss: -0.0485, G_loss: 0.3731\n",
      "  Batch [90/1299] D_loss: -3.6947, G_loss: -3.9771\n",
      "  Batch [100/1299] D_loss: -0.0139, G_loss: 0.0696\n",
      "  Batch [110/1299] D_loss: -1.1111, G_loss: -1.2099\n",
      "  Batch [120/1299] D_loss: -0.8671, G_loss: -1.6999\n",
      "  Batch [130/1299] D_loss: -0.3168, G_loss: -0.2168\n",
      "  Batch [140/1299] D_loss: -0.0842, G_loss: 0.2974\n",
      "  Batch [150/1299] D_loss: -0.0566, G_loss: 0.4136\n",
      "  Batch [160/1299] D_loss: -0.0151, G_loss: 0.5151\n",
      "  Batch [170/1299] D_loss: -0.1303, G_loss: 0.5075\n",
      "  Batch [180/1299] D_loss: -0.1183, G_loss: 0.5424\n",
      "  Batch [190/1299] D_loss: -0.0493, G_loss: 0.3214\n",
      "  Batch [200/1299] D_loss: -0.1105, G_loss: -0.1016\n",
      "  Batch [210/1299] D_loss: -0.1202, G_loss: 0.0710\n",
      "  Batch [220/1299] D_loss: -0.0113, G_loss: 0.1722\n",
      "  Batch [230/1299] D_loss: -0.0815, G_loss: 0.3089\n",
      "  Batch [240/1299] D_loss: -0.0586, G_loss: 0.4061\n",
      "  Batch [250/1299] D_loss: -0.0461, G_loss: 0.4224\n",
      "  Batch [260/1299] D_loss: -0.0191, G_loss: 0.3284\n",
      "  Batch [270/1299] D_loss: -1.3051, G_loss: -3.5983\n",
      "  Batch [280/1299] D_loss: -0.0217, G_loss: 0.1053\n",
      "  Batch [290/1299] D_loss: -0.0343, G_loss: 0.2213\n",
      "  Batch [300/1299] D_loss: -0.0265, G_loss: 0.2201\n",
      "  Batch [310/1299] D_loss: -0.1273, G_loss: 0.4999\n",
      "  Batch [320/1299] D_loss: -0.1054, G_loss: 0.4674\n",
      "  Batch [330/1299] D_loss: -0.0678, G_loss: 0.3366\n",
      "  Batch [340/1299] D_loss: -0.0659, G_loss: 0.1972\n",
      "  Batch [350/1299] D_loss: -0.1747, G_loss: 0.0279\n",
      "  Batch [360/1299] D_loss: -0.0824, G_loss: 0.0715\n",
      "  Batch [370/1299] D_loss: -0.0319, G_loss: 0.1403\n",
      "  Batch [380/1299] D_loss: -0.0959, G_loss: -0.0245\n",
      "  Batch [390/1299] D_loss: -0.1062, G_loss: 0.3633\n",
      "  Batch [400/1299] D_loss: 0.0109, G_loss: 0.4461\n",
      "  Batch [410/1299] D_loss: -0.1074, G_loss: 0.4890\n",
      "  Batch [420/1299] D_loss: 0.0407, G_loss: 0.4213\n",
      "  Batch [430/1299] D_loss: -0.1267, G_loss: 0.4737\n",
      "  Batch [440/1299] D_loss: -2.0271, G_loss: -1.8362\n",
      "  Batch [450/1299] D_loss: -0.0137, G_loss: 0.0381\n",
      "  Batch [460/1299] D_loss: -1.2584, G_loss: -2.5859\n",
      "  Batch [470/1299] D_loss: -0.0944, G_loss: 0.3020\n",
      "  Batch [480/1299] D_loss: -0.0583, G_loss: 0.4592\n",
      "  Batch [490/1299] D_loss: -0.0224, G_loss: 0.4395\n",
      "  Batch [500/1299] D_loss: 0.0163, G_loss: 0.4463\n",
      "  Batch [510/1299] D_loss: -0.0209, G_loss: 0.3732\n",
      "  Batch [520/1299] D_loss: -0.1142, G_loss: 0.2339\n",
      "  Batch [530/1299] D_loss: -0.9550, G_loss: -3.5542\n",
      "  Batch [540/1299] D_loss: -0.1804, G_loss: 0.0939\n",
      "  Batch [550/1299] D_loss: -0.5379, G_loss: -1.3506\n",
      "  Batch [560/1299] D_loss: -0.0310, G_loss: 0.2938\n",
      "  Batch [570/1299] D_loss: 0.0082, G_loss: 0.3891\n",
      "  Batch [580/1299] D_loss: -0.0321, G_loss: 0.4394\n",
      "  Batch [590/1299] D_loss: -0.1850, G_loss: 0.5594\n",
      "  Batch [600/1299] D_loss: -0.0628, G_loss: 0.4895\n",
      "  Batch [610/1299] D_loss: -0.0200, G_loss: 0.3212\n",
      "  Batch [620/1299] D_loss: -0.6542, G_loss: -1.6415\n",
      "  Batch [630/1299] D_loss: -0.0530, G_loss: 0.0367\n",
      "  Batch [640/1299] D_loss: -1.1531, G_loss: -0.7210\n",
      "  Batch [650/1299] D_loss: -0.1170, G_loss: -0.0629\n",
      "  Batch [660/1299] D_loss: -0.0555, G_loss: 0.2731\n",
      "  Batch [670/1299] D_loss: -0.0718, G_loss: 0.4309\n",
      "  Batch [680/1299] D_loss: -0.0355, G_loss: 0.4590\n",
      "  Batch [690/1299] D_loss: -0.0127, G_loss: 0.4061\n",
      "  Batch [700/1299] D_loss: -0.0588, G_loss: 0.4175\n",
      "  Batch [710/1299] D_loss: 0.0116, G_loss: 0.2472\n",
      "  Batch [720/1299] D_loss: -4.0885, G_loss: -5.9423\n",
      "  Batch [730/1299] D_loss: 0.0277, G_loss: 0.1991\n",
      "  Batch [740/1299] D_loss: -0.0227, G_loss: 0.3215\n",
      "  Batch [750/1299] D_loss: -0.0788, G_loss: 0.3742\n",
      "  Batch [760/1299] D_loss: -0.2548, G_loss: 0.4872\n",
      "  Batch [770/1299] D_loss: -0.0585, G_loss: 0.2515\n",
      "  Batch [780/1299] D_loss: -1.9909, G_loss: -2.4230\n",
      "  Batch [790/1299] D_loss: -0.3114, G_loss: -0.0987\n",
      "  Batch [800/1299] D_loss: -0.0935, G_loss: -0.0983\n",
      "  Batch [810/1299] D_loss: -0.7275, G_loss: -0.7680\n",
      "  Batch [820/1299] D_loss: -0.7301, G_loss: -0.1966\n",
      "  Batch [830/1299] D_loss: -0.3444, G_loss: -0.0369\n",
      "  Batch [840/1299] D_loss: -1.0244, G_loss: -1.7412\n",
      "  Batch [850/1299] D_loss: -0.0950, G_loss: 0.1313\n",
      "  Batch [860/1299] D_loss: -0.2173, G_loss: 0.3927\n",
      "  Batch [870/1299] D_loss: -0.0802, G_loss: 0.4875\n",
      "  Batch [880/1299] D_loss: -0.1110, G_loss: 0.5910\n",
      "  Batch [890/1299] D_loss: -0.0833, G_loss: 0.5298\n",
      "  Batch [900/1299] D_loss: -1.7105, G_loss: -1.5804\n",
      "  Batch [910/1299] D_loss: -0.5574, G_loss: 0.1059\n",
      "  Batch [920/1299] D_loss: -1.0581, G_loss: -1.6686\n",
      "  Batch [930/1299] D_loss: -0.0349, G_loss: 0.2008\n",
      "  Batch [940/1299] D_loss: -0.0233, G_loss: 0.3359\n",
      "  Batch [950/1299] D_loss: -0.1460, G_loss: 0.5286\n",
      "  Batch [960/1299] D_loss: -0.0230, G_loss: 0.4096\n",
      "  Batch [970/1299] D_loss: 0.0432, G_loss: 0.3827\n",
      "  Batch [980/1299] D_loss: -0.0532, G_loss: 0.3308\n",
      "  Batch [990/1299] D_loss: -0.3762, G_loss: -0.2270\n",
      "  Batch [1000/1299] D_loss: -0.2134, G_loss: 0.0107\n",
      "  Batch [1010/1299] D_loss: -0.0534, G_loss: 0.1476\n",
      "  Batch [1020/1299] D_loss: -0.2166, G_loss: 0.1580\n",
      "  Batch [1030/1299] D_loss: -1.0304, G_loss: -0.9100\n",
      "  Batch [1040/1299] D_loss: -0.0298, G_loss: 0.2146\n",
      "  Batch [1050/1299] D_loss: -1.3517, G_loss: 0.1205\n",
      "  Batch [1060/1299] D_loss: -0.0959, G_loss: 0.0679\n",
      "  Batch [1070/1299] D_loss: -1.1588, G_loss: -2.6277\n",
      "  Batch [1080/1299] D_loss: -0.2101, G_loss: 0.1309\n",
      "  Batch [1090/1299] D_loss: -0.6002, G_loss: -0.8646\n",
      "  Batch [1100/1299] D_loss: -0.0237, G_loss: 0.1545\n",
      "  Batch [1110/1299] D_loss: -0.0368, G_loss: 0.1381\n",
      "  Batch [1120/1299] D_loss: -0.4728, G_loss: 0.0170\n",
      "  Batch [1130/1299] D_loss: -1.6080, G_loss: -0.3367\n",
      "  Batch [1140/1299] D_loss: -0.1528, G_loss: 0.0752\n",
      "  Batch [1150/1299] D_loss: -0.2635, G_loss: 0.1603\n",
      "  Batch [1160/1299] D_loss: -0.6791, G_loss: -0.3833\n",
      "  Batch [1170/1299] D_loss: -0.2947, G_loss: -0.0436\n",
      "  Batch [1180/1299] D_loss: -0.7541, G_loss: 0.0013\n",
      "  Batch [1190/1299] D_loss: -0.9427, G_loss: -2.1775\n",
      "  Batch [1200/1299] D_loss: -0.2113, G_loss: 0.1394\n",
      "  Batch [1210/1299] D_loss: -0.2099, G_loss: 0.5463\n",
      "  Batch [1220/1299] D_loss: -0.3107, G_loss: 0.9238\n",
      "  Batch [1230/1299] D_loss: -0.1019, G_loss: 0.8379\n",
      "  Batch [1240/1299] D_loss: 0.0903, G_loss: 0.6993\n",
      "  Batch [1250/1299] D_loss: -0.1870, G_loss: 0.1463\n",
      "  Batch [1260/1299] D_loss: -0.5639, G_loss: -0.4536\n",
      "  Batch [1270/1299] D_loss: -0.3750, G_loss: -0.2749\n",
      "  Batch [1280/1299] D_loss: -0.0729, G_loss: 0.2215\n",
      "  Batch [1290/1299] D_loss: -0.0805, G_loss: 0.3670\n",
      "\n",
      "Epoch 52 Summary:\n",
      "  Average D_loss: -0.1779\n",
      "  Average G_loss: -0.1059\n",
      "\n",
      "Epoch [53/100]\n",
      "  Batch [0/1299] D_loss: -0.0723, G_loss: 0.4888\n",
      "  Batch [10/1299] D_loss: -0.0533, G_loss: 0.5061\n",
      "  Batch [20/1299] D_loss: -0.0466, G_loss: 0.5566\n",
      "  Batch [30/1299] D_loss: -0.1243, G_loss: 0.4201\n",
      "  Batch [40/1299] D_loss: -0.1315, G_loss: 0.4187\n",
      "  Batch [50/1299] D_loss: -0.1813, G_loss: -0.0206\n",
      "  Batch [60/1299] D_loss: -0.5555, G_loss: -0.3582\n",
      "  Batch [70/1299] D_loss: -0.8231, G_loss: -3.2777\n",
      "  Batch [80/1299] D_loss: -0.6839, G_loss: -1.9745\n",
      "  Batch [90/1299] D_loss: -0.6348, G_loss: -0.0919\n",
      "  Batch [100/1299] D_loss: -0.5113, G_loss: -0.5435\n",
      "  Batch [110/1299] D_loss: -0.2006, G_loss: 0.4890\n",
      "  Batch [120/1299] D_loss: -0.1471, G_loss: 0.6568\n",
      "  Batch [130/1299] D_loss: -0.1735, G_loss: 0.7020\n",
      "  Batch [140/1299] D_loss: -0.1465, G_loss: 0.7156\n",
      "  Batch [150/1299] D_loss: -0.0836, G_loss: 0.7199\n",
      "  Batch [160/1299] D_loss: -0.0733, G_loss: 0.3184\n",
      "  Batch [170/1299] D_loss: -1.5571, G_loss: -1.7183\n",
      "  Batch [180/1299] D_loss: -0.1725, G_loss: -0.0611\n",
      "  Batch [190/1299] D_loss: -0.1005, G_loss: 0.1279\n",
      "  Batch [200/1299] D_loss: -0.0694, G_loss: 0.2579\n",
      "  Batch [210/1299] D_loss: -0.1073, G_loss: 0.4138\n",
      "  Batch [220/1299] D_loss: -0.1114, G_loss: 0.5190\n",
      "  Batch [230/1299] D_loss: -0.0926, G_loss: 0.4993\n",
      "  Batch [240/1299] D_loss: -0.0852, G_loss: 0.4870\n",
      "  Batch [250/1299] D_loss: -0.0957, G_loss: 0.2804\n",
      "  Batch [260/1299] D_loss: -2.5714, G_loss: -5.5011\n",
      "  Batch [270/1299] D_loss: -0.0080, G_loss: 0.2145\n",
      "  Batch [280/1299] D_loss: -0.0107, G_loss: 0.2557\n",
      "  Batch [290/1299] D_loss: -0.0262, G_loss: 0.3147\n",
      "  Batch [300/1299] D_loss: -0.0713, G_loss: 0.3113\n",
      "  Batch [310/1299] D_loss: -0.0830, G_loss: 0.3335\n",
      "  Batch [320/1299] D_loss: -0.1048, G_loss: 0.3521\n",
      "  Batch [330/1299] D_loss: -2.0795, G_loss: -1.7543\n",
      "  Batch [340/1299] D_loss: -0.0355, G_loss: 0.1237\n",
      "  Batch [350/1299] D_loss: -0.5385, G_loss: -1.0302\n",
      "  Batch [360/1299] D_loss: -1.3617, G_loss: -0.8925\n",
      "  Batch [370/1299] D_loss: -0.0259, G_loss: 0.2055\n",
      "  Batch [380/1299] D_loss: -0.0662, G_loss: 0.4555\n",
      "  Batch [390/1299] D_loss: -0.0831, G_loss: 0.5281\n",
      "  Batch [400/1299] D_loss: -0.0678, G_loss: 0.4853\n",
      "  Batch [410/1299] D_loss: -0.0775, G_loss: 0.5863\n",
      "  Batch [420/1299] D_loss: -0.1226, G_loss: 0.4700\n",
      "  Batch [430/1299] D_loss: -0.1065, G_loss: 0.2827\n",
      "  Batch [440/1299] D_loss: -0.7623, G_loss: -1.7985\n",
      "  Batch [450/1299] D_loss: -0.7418, G_loss: -1.2672\n",
      "  Batch [460/1299] D_loss: -0.9792, G_loss: 0.0054\n",
      "  Batch [470/1299] D_loss: -0.1201, G_loss: 0.0992\n",
      "  Batch [480/1299] D_loss: -0.0774, G_loss: 0.1652\n",
      "  Batch [490/1299] D_loss: -0.0622, G_loss: 0.3171\n",
      "  Batch [500/1299] D_loss: -0.1076, G_loss: 0.4573\n",
      "  Batch [510/1299] D_loss: -0.0828, G_loss: 0.5981\n",
      "  Batch [520/1299] D_loss: -0.1033, G_loss: 0.5753\n",
      "  Batch [530/1299] D_loss: -0.0051, G_loss: 0.4026\n",
      "  Batch [540/1299] D_loss: -2.5448, G_loss: -2.9745\n",
      "  Batch [550/1299] D_loss: -0.8987, G_loss: -1.8647\n",
      "  Batch [560/1299] D_loss: -0.5288, G_loss: -1.2936\n",
      "  Batch [570/1299] D_loss: -0.9976, G_loss: -0.3796\n",
      "  Batch [580/1299] D_loss: -0.0620, G_loss: 0.1786\n",
      "  Batch [590/1299] D_loss: -0.0808, G_loss: 0.4895\n",
      "  Batch [600/1299] D_loss: -0.1710, G_loss: 0.6996\n",
      "  Batch [610/1299] D_loss: -0.0978, G_loss: 0.6318\n",
      "  Batch [620/1299] D_loss: -0.1515, G_loss: 0.7762\n",
      "  Batch [630/1299] D_loss: -0.1202, G_loss: 0.5836\n",
      "  Batch [640/1299] D_loss: -0.0814, G_loss: 0.4431\n",
      "  Batch [650/1299] D_loss: -3.1555, G_loss: -4.0014\n",
      "  Batch [660/1299] D_loss: -0.0827, G_loss: 0.1480\n",
      "  Batch [670/1299] D_loss: -0.8854, G_loss: -0.4400\n",
      "  Batch [680/1299] D_loss: -0.1554, G_loss: 0.1055\n",
      "  Batch [690/1299] D_loss: -0.6899, G_loss: -0.0487\n",
      "  Batch [700/1299] D_loss: -1.2176, G_loss: -0.5102\n",
      "  Batch [710/1299] D_loss: -0.3840, G_loss: 0.0670\n",
      "  Batch [720/1299] D_loss: -0.7414, G_loss: -3.2697\n",
      "  Batch [730/1299] D_loss: -0.1864, G_loss: 0.1578\n",
      "  Batch [740/1299] D_loss: -0.1106, G_loss: 0.3821\n",
      "  Batch [750/1299] D_loss: -0.0805, G_loss: 0.5803\n",
      "  Batch [760/1299] D_loss: -0.1350, G_loss: 0.6966\n",
      "  Batch [770/1299] D_loss: -0.2566, G_loss: 0.7719\n",
      "  Batch [780/1299] D_loss: -0.1066, G_loss: 0.7164\n",
      "  Batch [790/1299] D_loss: -0.1164, G_loss: 0.5131\n",
      "  Batch [800/1299] D_loss: -0.0917, G_loss: 0.3407\n",
      "  Batch [810/1299] D_loss: -1.3066, G_loss: -2.8087\n",
      "  Batch [820/1299] D_loss: -0.3794, G_loss: -0.6651\n",
      "  Batch [830/1299] D_loss: 0.0037, G_loss: 0.2065\n",
      "  Batch [840/1299] D_loss: -0.0634, G_loss: 0.4047\n",
      "  Batch [850/1299] D_loss: -0.1147, G_loss: 0.5372\n",
      "  Batch [860/1299] D_loss: -0.1768, G_loss: 0.4450\n",
      "  Batch [870/1299] D_loss: -0.0996, G_loss: 0.4103\n",
      "  Batch [880/1299] D_loss: -1.4972, G_loss: -3.0772\n",
      "  Batch [890/1299] D_loss: 0.0284, G_loss: 0.0470\n",
      "  Batch [900/1299] D_loss: -0.0194, G_loss: 0.1125\n",
      "  Batch [910/1299] D_loss: -0.0346, G_loss: 0.1607\n",
      "  Batch [920/1299] D_loss: -0.0808, G_loss: 0.3085\n",
      "  Batch [930/1299] D_loss: -0.1387, G_loss: 0.2682\n",
      "  Batch [940/1299] D_loss: -2.2712, G_loss: -2.8662\n",
      "  Batch [950/1299] D_loss: 0.0003, G_loss: 0.1957\n",
      "  Batch [960/1299] D_loss: 0.0007, G_loss: 0.3025\n",
      "  Batch [970/1299] D_loss: -0.0835, G_loss: 0.3652\n",
      "  Batch [980/1299] D_loss: -0.0721, G_loss: 0.3182\n",
      "  Batch [990/1299] D_loss: -0.0750, G_loss: -0.0522\n",
      "  Batch [1000/1299] D_loss: -0.5683, G_loss: -0.2318\n",
      "  Batch [1010/1299] D_loss: -0.0652, G_loss: 0.1003\n",
      "  Batch [1020/1299] D_loss: -0.0358, G_loss: 0.0608\n",
      "  Batch [1030/1299] D_loss: -0.0203, G_loss: 0.1148\n",
      "  Batch [1040/1299] D_loss: -0.2213, G_loss: 0.0905\n",
      "  Batch [1050/1299] D_loss: -0.0329, G_loss: 0.1745\n",
      "  Batch [1060/1299] D_loss: -0.3299, G_loss: 0.0475\n",
      "  Batch [1070/1299] D_loss: -0.2397, G_loss: -0.2307\n",
      "  Batch [1080/1299] D_loss: -0.1147, G_loss: 0.1569\n",
      "  Batch [1090/1299] D_loss: -0.5220, G_loss: -0.2220\n",
      "  Batch [1100/1299] D_loss: -1.1457, G_loss: -0.9074\n",
      "  Batch [1110/1299] D_loss: -0.0455, G_loss: 0.3331\n",
      "  Batch [1120/1299] D_loss: -0.2006, G_loss: 0.5584\n",
      "  Batch [1130/1299] D_loss: -0.0881, G_loss: 0.6185\n",
      "  Batch [1140/1299] D_loss: -0.1838, G_loss: 0.6356\n",
      "  Batch [1150/1299] D_loss: -0.0030, G_loss: 0.4902\n",
      "  Batch [1160/1299] D_loss: -0.0295, G_loss: 0.3052\n",
      "  Batch [1170/1299] D_loss: -1.3721, G_loss: -2.0613\n",
      "  Batch [1180/1299] D_loss: -0.1257, G_loss: -0.1423\n",
      "  Batch [1190/1299] D_loss: -0.2055, G_loss: 0.0557\n",
      "  Batch [1200/1299] D_loss: -0.0428, G_loss: 0.3150\n",
      "  Batch [1210/1299] D_loss: -0.0569, G_loss: 0.4545\n",
      "  Batch [1220/1299] D_loss: -0.1322, G_loss: 0.5742\n",
      "  Batch [1230/1299] D_loss: -0.1118, G_loss: 0.5193\n",
      "  Batch [1240/1299] D_loss: -0.0781, G_loss: 0.4057\n",
      "  Batch [1250/1299] D_loss: -0.9973, G_loss: -4.5261\n",
      "  Batch [1260/1299] D_loss: 0.0133, G_loss: 0.1714\n",
      "  Batch [1270/1299] D_loss: -0.0192, G_loss: 0.2538\n",
      "  Batch [1280/1299] D_loss: -0.0027, G_loss: 0.4403\n",
      "  Batch [1290/1299] D_loss: -0.1214, G_loss: 0.4492\n",
      "\n",
      "Epoch 53 Summary:\n",
      "  Average D_loss: -0.1698\n",
      "  Average G_loss: -0.0556\n",
      "\n",
      "Epoch [54/100]\n",
      "  Batch [0/1299] D_loss: -0.1691, G_loss: 0.4495\n",
      "  Batch [10/1299] D_loss: -0.0668, G_loss: 0.3340\n",
      "  Batch [20/1299] D_loss: -0.9609, G_loss: -0.2301\n",
      "  Batch [30/1299] D_loss: -0.8422, G_loss: -0.5215\n",
      "  Batch [40/1299] D_loss: -1.6293, G_loss: -0.4789\n",
      "  Batch [50/1299] D_loss: -0.1021, G_loss: 0.2337\n",
      "  Batch [60/1299] D_loss: -0.0834, G_loss: 0.3579\n",
      "  Batch [70/1299] D_loss: -0.0466, G_loss: 0.4150\n",
      "  Batch [80/1299] D_loss: -0.1176, G_loss: 0.3571\n",
      "  Batch [90/1299] D_loss: -0.0126, G_loss: 0.3183\n",
      "  Batch [100/1299] D_loss: -0.7810, G_loss: -1.2615\n",
      "  Batch [110/1299] D_loss: -0.0276, G_loss: 0.1038\n",
      "  Batch [120/1299] D_loss: -0.0297, G_loss: 0.2767\n",
      "  Batch [130/1299] D_loss: -0.0546, G_loss: 0.4979\n",
      "  Batch [140/1299] D_loss: -0.1904, G_loss: 0.7901\n",
      "  Batch [150/1299] D_loss: -0.2070, G_loss: 0.6814\n",
      "  Batch [160/1299] D_loss: -0.1246, G_loss: 0.7003\n",
      "  Batch [170/1299] D_loss: -0.0089, G_loss: 0.4657\n",
      "  Batch [180/1299] D_loss: -1.7929, G_loss: -5.2397\n",
      "  Batch [190/1299] D_loss: -0.3377, G_loss: -0.5948\n",
      "  Batch [200/1299] D_loss: -0.0423, G_loss: 0.1346\n",
      "  Batch [210/1299] D_loss: -0.0678, G_loss: 0.3000\n",
      "  Batch [220/1299] D_loss: -0.0517, G_loss: 0.4141\n",
      "  Batch [230/1299] D_loss: -0.0197, G_loss: 0.4491\n",
      "  Batch [240/1299] D_loss: -0.1136, G_loss: 0.2973\n",
      "  Batch [250/1299] D_loss: -1.1442, G_loss: -2.5011\n",
      "  Batch [260/1299] D_loss: -1.1195, G_loss: -2.4948\n",
      "  Batch [270/1299] D_loss: -0.0416, G_loss: 0.2039\n",
      "  Batch [280/1299] D_loss: -0.8979, G_loss: 0.0068\n",
      "  Batch [290/1299] D_loss: -0.0846, G_loss: 0.3032\n",
      "  Batch [300/1299] D_loss: -0.0326, G_loss: 0.4080\n",
      "  Batch [310/1299] D_loss: 0.0158, G_loss: 0.3592\n",
      "  Batch [320/1299] D_loss: -0.0580, G_loss: 0.4304\n",
      "  Batch [330/1299] D_loss: -0.0474, G_loss: 0.3329\n",
      "  Batch [340/1299] D_loss: -1.3850, G_loss: -5.0263\n",
      "  Batch [350/1299] D_loss: -0.9419, G_loss: -0.8926\n",
      "  Batch [360/1299] D_loss: -0.0199, G_loss: 0.0068\n",
      "  Batch [370/1299] D_loss: -0.0840, G_loss: 0.2787\n",
      "  Batch [380/1299] D_loss: -0.0577, G_loss: 0.3733\n",
      "  Batch [390/1299] D_loss: -0.0265, G_loss: 0.4249\n",
      "  Batch [400/1299] D_loss: -0.1153, G_loss: 0.4397\n",
      "  Batch [410/1299] D_loss: -0.0563, G_loss: 0.2635\n",
      "  Batch [420/1299] D_loss: -1.8058, G_loss: -1.9550\n",
      "  Batch [430/1299] D_loss: 0.0324, G_loss: 0.0321\n",
      "  Batch [440/1299] D_loss: -0.9374, G_loss: -0.7971\n",
      "  Batch [450/1299] D_loss: 0.0081, G_loss: 0.1723\n",
      "  Batch [460/1299] D_loss: -0.2574, G_loss: -0.7404\n",
      "  Batch [470/1299] D_loss: -0.0308, G_loss: 0.1897\n",
      "  Batch [480/1299] D_loss: -0.2330, G_loss: 0.5098\n",
      "  Batch [490/1299] D_loss: -0.2196, G_loss: 0.5647\n",
      "  Batch [500/1299] D_loss: -0.0658, G_loss: 0.5491\n",
      "  Batch [510/1299] D_loss: -0.0563, G_loss: 0.4550\n",
      "  Batch [520/1299] D_loss: -1.7783, G_loss: -1.4356\n",
      "  Batch [530/1299] D_loss: -1.4050, G_loss: -1.7103\n",
      "  Batch [540/1299] D_loss: -0.0421, G_loss: 0.1374\n",
      "  Batch [550/1299] D_loss: -0.1027, G_loss: 0.3441\n",
      "  Batch [560/1299] D_loss: -0.1420, G_loss: 0.4577\n",
      "  Batch [570/1299] D_loss: -0.0775, G_loss: 0.4505\n",
      "  Batch [580/1299] D_loss: -0.0643, G_loss: 0.3777\n",
      "  Batch [590/1299] D_loss: -0.7635, G_loss: -1.0004\n",
      "  Batch [600/1299] D_loss: -1.1653, G_loss: -0.2823\n",
      "  Batch [610/1299] D_loss: -0.0302, G_loss: 0.2234\n",
      "  Batch [620/1299] D_loss: -0.1470, G_loss: 0.2509\n",
      "  Batch [630/1299] D_loss: -0.0778, G_loss: 0.2800\n",
      "  Batch [640/1299] D_loss: -0.5617, G_loss: -0.8078\n",
      "  Batch [650/1299] D_loss: -0.2010, G_loss: -0.0584\n",
      "  Batch [660/1299] D_loss: -0.4469, G_loss: -1.1688\n",
      "  Batch [670/1299] D_loss: -0.0766, G_loss: 0.2753\n",
      "  Batch [680/1299] D_loss: -0.1896, G_loss: 0.4602\n",
      "  Batch [690/1299] D_loss: -0.1511, G_loss: 0.5331\n",
      "  Batch [700/1299] D_loss: 0.0201, G_loss: 0.2864\n",
      "  Batch [710/1299] D_loss: -0.1605, G_loss: 0.4036\n",
      "  Batch [720/1299] D_loss: -1.3629, G_loss: -0.0369\n",
      "  Batch [730/1299] D_loss: -0.0638, G_loss: 0.1855\n",
      "  Batch [740/1299] D_loss: -1.0511, G_loss: -0.5351\n",
      "  Batch [750/1299] D_loss: -0.0178, G_loss: 0.2202\n",
      "  Batch [760/1299] D_loss: -0.0384, G_loss: 0.3816\n",
      "  Batch [770/1299] D_loss: -0.0133, G_loss: 0.3867\n",
      "  Batch [780/1299] D_loss: -0.0882, G_loss: 0.5387\n",
      "  Batch [790/1299] D_loss: -0.1070, G_loss: 0.3831\n",
      "  Batch [800/1299] D_loss: -1.7972, G_loss: -1.2194\n",
      "  Batch [810/1299] D_loss: 0.2448, G_loss: 0.0841\n",
      "  Batch [820/1299] D_loss: -0.5236, G_loss: -0.0934\n",
      "  Batch [830/1299] D_loss: -0.4438, G_loss: -0.0350\n",
      "  Batch [840/1299] D_loss: -0.0844, G_loss: 0.3188\n",
      "  Batch [850/1299] D_loss: -0.0535, G_loss: 0.3901\n",
      "  Batch [860/1299] D_loss: -0.0896, G_loss: 0.4283\n",
      "  Batch [870/1299] D_loss: -0.0469, G_loss: 0.4899\n",
      "  Batch [880/1299] D_loss: -0.0401, G_loss: 0.4097\n",
      "  Batch [890/1299] D_loss: -0.0573, G_loss: 0.3232\n",
      "  Batch [900/1299] D_loss: -0.7805, G_loss: -1.1271\n",
      "  Batch [910/1299] D_loss: -1.8559, G_loss: -2.9251\n",
      "  Batch [920/1299] D_loss: -0.0376, G_loss: 0.0517\n",
      "  Batch [930/1299] D_loss: 0.0577, G_loss: 0.0518\n",
      "  Batch [940/1299] D_loss: -0.0426, G_loss: 0.1928\n",
      "  Batch [950/1299] D_loss: -0.1907, G_loss: 0.4459\n",
      "  Batch [960/1299] D_loss: 0.0078, G_loss: 0.5631\n",
      "  Batch [970/1299] D_loss: -0.1701, G_loss: 0.7283\n",
      "  Batch [980/1299] D_loss: -0.0382, G_loss: 0.4200\n",
      "  Batch [990/1299] D_loss: -0.0827, G_loss: 0.4031\n",
      "  Batch [1000/1299] D_loss: -0.0297, G_loss: 0.3084\n",
      "  Batch [1010/1299] D_loss: -2.0281, G_loss: -6.0164\n",
      "  Batch [1020/1299] D_loss: -0.0157, G_loss: 0.1276\n",
      "  Batch [1030/1299] D_loss: -0.0141, G_loss: 0.1480\n",
      "  Batch [1040/1299] D_loss: -0.0360, G_loss: 0.2965\n",
      "  Batch [1050/1299] D_loss: -0.0138, G_loss: 0.3339\n",
      "  Batch [1060/1299] D_loss: -0.0462, G_loss: 0.2701\n",
      "  Batch [1070/1299] D_loss: -0.0537, G_loss: 0.1844\n",
      "  Batch [1080/1299] D_loss: -0.1524, G_loss: -0.0112\n",
      "  Batch [1090/1299] D_loss: -0.4029, G_loss: -0.1579\n",
      "  Batch [1100/1299] D_loss: -0.7444, G_loss: -0.2657\n",
      "  Batch [1110/1299] D_loss: -0.5039, G_loss: -0.0456\n",
      "  Batch [1120/1299] D_loss: -0.0203, G_loss: 0.2151\n",
      "  Batch [1130/1299] D_loss: 0.0163, G_loss: 0.3622\n",
      "  Batch [1140/1299] D_loss: -0.1196, G_loss: 0.5292\n",
      "  Batch [1150/1299] D_loss: -0.0343, G_loss: 0.5277\n",
      "  Batch [1160/1299] D_loss: -0.0826, G_loss: 0.3784\n",
      "  Batch [1170/1299] D_loss: -0.1078, G_loss: 0.2556\n",
      "  Batch [1180/1299] D_loss: -0.8369, G_loss: -1.5378\n",
      "  Batch [1190/1299] D_loss: -0.5191, G_loss: -0.7478\n",
      "  Batch [1200/1299] D_loss: -1.0885, G_loss: -1.5050\n",
      "  Batch [1210/1299] D_loss: -0.7515, G_loss: -0.4655\n",
      "  Batch [1220/1299] D_loss: -0.1981, G_loss: 0.0556\n",
      "  Batch [1230/1299] D_loss: -0.5919, G_loss: -0.5853\n",
      "  Batch [1240/1299] D_loss: -0.1369, G_loss: 0.2174\n",
      "  Batch [1250/1299] D_loss: -0.0594, G_loss: 0.4491\n",
      "  Batch [1260/1299] D_loss: -0.1998, G_loss: 0.6890\n",
      "  Batch [1270/1299] D_loss: -0.1351, G_loss: 0.5512\n",
      "  Batch [1280/1299] D_loss: -0.0370, G_loss: 0.4993\n",
      "  Batch [1290/1299] D_loss: -0.0506, G_loss: 0.2873\n",
      "\n",
      "Epoch 54 Summary:\n",
      "  Average D_loss: -0.1474\n",
      "  Average G_loss: -0.0662\n",
      "\n",
      "Epoch [55/100]\n",
      "  Batch [0/1299] D_loss: -1.5401, G_loss: -2.5516\n",
      "  Batch [10/1299] D_loss: 0.0099, G_loss: 0.1920\n",
      "  Batch [20/1299] D_loss: -0.0208, G_loss: 0.2176\n",
      "  Batch [30/1299] D_loss: 0.0064, G_loss: 0.2788\n",
      "  Batch [40/1299] D_loss: -0.0714, G_loss: 0.2709\n",
      "  Batch [50/1299] D_loss: -5.1687, G_loss: -7.0481\n",
      "  Batch [60/1299] D_loss: 0.0080, G_loss: 0.1310\n",
      "  Batch [70/1299] D_loss: -0.1072, G_loss: 0.3257\n",
      "  Batch [80/1299] D_loss: -0.0116, G_loss: 0.2633\n",
      "  Batch [90/1299] D_loss: -0.0306, G_loss: 0.2753\n",
      "  Batch [100/1299] D_loss: -0.0214, G_loss: 0.2055\n",
      "  Batch [110/1299] D_loss: -0.4967, G_loss: -0.2640\n",
      "  Batch [120/1299] D_loss: -0.2014, G_loss: 0.0997\n",
      "  Batch [130/1299] D_loss: -0.6879, G_loss: -0.8350\n",
      "  Batch [140/1299] D_loss: -0.6931, G_loss: -0.4130\n",
      "  Batch [150/1299] D_loss: -0.9033, G_loss: -0.0039\n",
      "  Batch [160/1299] D_loss: -0.7431, G_loss: 0.1047\n",
      "  Batch [170/1299] D_loss: -0.1142, G_loss: -0.0089\n",
      "  Batch [180/1299] D_loss: -0.0709, G_loss: 0.2277\n",
      "  Batch [190/1299] D_loss: -0.0682, G_loss: 0.6431\n",
      "  Batch [200/1299] D_loss: 0.0587, G_loss: 0.6430\n",
      "  Batch [210/1299] D_loss: -0.0255, G_loss: 0.6092\n",
      "  Batch [220/1299] D_loss: -0.0575, G_loss: 0.3726\n",
      "  Batch [230/1299] D_loss: 0.0339, G_loss: 0.1924\n",
      "  Batch [240/1299] D_loss: -0.2327, G_loss: -0.9199\n",
      "  Batch [250/1299] D_loss: -0.4483, G_loss: 0.0187\n",
      "  Batch [260/1299] D_loss: -0.5139, G_loss: -0.7482\n",
      "  Batch [270/1299] D_loss: -0.4486, G_loss: -0.7556\n",
      "  Batch [280/1299] D_loss: -1.1433, G_loss: -1.3163\n",
      "  Batch [290/1299] D_loss: -0.7240, G_loss: -0.7911\n",
      "  Batch [300/1299] D_loss: -0.0498, G_loss: 0.2763\n",
      "  Batch [310/1299] D_loss: -0.2030, G_loss: 0.5984\n",
      "  Batch [320/1299] D_loss: -0.1018, G_loss: 0.7942\n",
      "  Batch [330/1299] D_loss: -0.1138, G_loss: 0.6913\n",
      "  Batch [340/1299] D_loss: -0.1648, G_loss: 0.4697\n",
      "  Batch [350/1299] D_loss: -0.5714, G_loss: 0.0851\n",
      "  Batch [360/1299] D_loss: 0.0364, G_loss: 0.0925\n",
      "  Batch [370/1299] D_loss: -0.7229, G_loss: -1.6599\n",
      "  Batch [380/1299] D_loss: -0.0123, G_loss: 0.2367\n",
      "  Batch [390/1299] D_loss: -0.0313, G_loss: 0.3868\n",
      "  Batch [400/1299] D_loss: -0.1475, G_loss: 0.4996\n",
      "  Batch [410/1299] D_loss: -0.0208, G_loss: 0.4128\n",
      "  Batch [420/1299] D_loss: -0.0579, G_loss: 0.2677\n",
      "  Batch [430/1299] D_loss: -0.5099, G_loss: -4.5056\n",
      "  Batch [440/1299] D_loss: -0.3082, G_loss: -0.0848\n",
      "  Batch [450/1299] D_loss: -0.5724, G_loss: -0.1753\n",
      "  Batch [460/1299] D_loss: -1.4640, G_loss: -1.3662\n",
      "  Batch [470/1299] D_loss: -0.1047, G_loss: 0.1199\n",
      "  Batch [480/1299] D_loss: -0.5991, G_loss: 0.0607\n",
      "  Batch [490/1299] D_loss: -0.0997, G_loss: 0.2064\n",
      "  Batch [500/1299] D_loss: -0.5042, G_loss: -0.0793\n",
      "  Batch [510/1299] D_loss: -0.1881, G_loss: 0.1001\n",
      "  Batch [520/1299] D_loss: -0.6564, G_loss: -0.0142\n",
      "  Batch [530/1299] D_loss: -0.5004, G_loss: 0.0699\n",
      "  Batch [540/1299] D_loss: -0.0969, G_loss: 0.3468\n",
      "  Batch [550/1299] D_loss: -0.1433, G_loss: 0.6756\n",
      "  Batch [560/1299] D_loss: -0.1366, G_loss: 0.8809\n",
      "  Batch [570/1299] D_loss: -0.1624, G_loss: 0.6221\n",
      "  Batch [580/1299] D_loss: -0.0976, G_loss: 0.4335\n",
      "  Batch [590/1299] D_loss: -0.0920, G_loss: 0.3891\n",
      "  Batch [600/1299] D_loss: -0.5691, G_loss: -1.0277\n",
      "  Batch [610/1299] D_loss: -0.7665, G_loss: -1.7820\n",
      "  Batch [620/1299] D_loss: -0.0391, G_loss: 0.1500\n",
      "  Batch [630/1299] D_loss: -0.0741, G_loss: 0.2769\n",
      "  Batch [640/1299] D_loss: -0.0637, G_loss: 0.1911\n",
      "  Batch [650/1299] D_loss: -0.1046, G_loss: 0.3385\n",
      "  Batch [660/1299] D_loss: -1.4710, G_loss: -1.9999\n",
      "  Batch [670/1299] D_loss: -0.0885, G_loss: 0.1579\n",
      "  Batch [680/1299] D_loss: -0.4018, G_loss: -0.4904\n",
      "  Batch [690/1299] D_loss: -0.0669, G_loss: 0.3325\n",
      "  Batch [700/1299] D_loss: -0.1353, G_loss: 0.4610\n",
      "  Batch [710/1299] D_loss: -0.1648, G_loss: 0.5246\n",
      "  Batch [720/1299] D_loss: -0.0797, G_loss: 0.5398\n",
      "  Batch [730/1299] D_loss: -0.0313, G_loss: 0.4260\n",
      "  Batch [740/1299] D_loss: -0.0874, G_loss: 0.2787\n",
      "  Batch [750/1299] D_loss: -2.6879, G_loss: -3.8725\n",
      "  Batch [760/1299] D_loss: -0.2079, G_loss: 0.0875\n",
      "  Batch [770/1299] D_loss: -0.1594, G_loss: -0.0666\n",
      "  Batch [780/1299] D_loss: -0.7806, G_loss: -3.8644\n",
      "  Batch [790/1299] D_loss: -0.3757, G_loss: -0.5361\n",
      "  Batch [800/1299] D_loss: -1.1692, G_loss: -0.0180\n",
      "  Batch [810/1299] D_loss: -1.0505, G_loss: -0.1046\n",
      "  Batch [820/1299] D_loss: -0.0810, G_loss: 0.1580\n",
      "  Batch [830/1299] D_loss: -0.0350, G_loss: 0.1916\n",
      "  Batch [840/1299] D_loss: 0.0004, G_loss: 0.4570\n",
      "  Batch [850/1299] D_loss: -0.1010, G_loss: 0.5748\n",
      "  Batch [860/1299] D_loss: -0.1156, G_loss: 0.6833\n",
      "  Batch [870/1299] D_loss: -0.0716, G_loss: 0.4697\n",
      "  Batch [880/1299] D_loss: -0.0296, G_loss: 0.3443\n",
      "  Batch [890/1299] D_loss: -0.9496, G_loss: -1.8764\n",
      "  Batch [900/1299] D_loss: -0.6412, G_loss: -0.3817\n",
      "  Batch [910/1299] D_loss: -0.0923, G_loss: -0.2079\n",
      "  Batch [920/1299] D_loss: -0.0745, G_loss: 0.2317\n",
      "  Batch [930/1299] D_loss: -0.0650, G_loss: 0.4358\n",
      "  Batch [940/1299] D_loss: -0.0897, G_loss: 0.4592\n",
      "  Batch [950/1299] D_loss: -0.1367, G_loss: 0.6160\n",
      "  Batch [960/1299] D_loss: 0.0075, G_loss: 0.3474\n",
      "  Batch [970/1299] D_loss: -4.3355, G_loss: -6.4707\n",
      "  Batch [980/1299] D_loss: -0.0568, G_loss: 0.1866\n",
      "  Batch [990/1299] D_loss: -0.0379, G_loss: 0.2783\n",
      "  Batch [1000/1299] D_loss: -0.0256, G_loss: 0.3183\n",
      "  Batch [1010/1299] D_loss: -0.0793, G_loss: 0.3921\n",
      "  Batch [1020/1299] D_loss: -0.1047, G_loss: 0.4231\n",
      "  Batch [1030/1299] D_loss: -2.0555, G_loss: -4.3230\n",
      "  Batch [1040/1299] D_loss: -0.0855, G_loss: 0.0294\n",
      "  Batch [1050/1299] D_loss: -0.9500, G_loss: -0.5160\n",
      "  Batch [1060/1299] D_loss: -0.1319, G_loss: 0.0324\n",
      "  Batch [1070/1299] D_loss: -0.9237, G_loss: -1.4352\n",
      "  Batch [1080/1299] D_loss: -0.0641, G_loss: 0.0605\n",
      "  Batch [1090/1299] D_loss: -0.1258, G_loss: 0.0461\n",
      "  Batch [1100/1299] D_loss: -0.3094, G_loss: 0.0291\n",
      "  Batch [1110/1299] D_loss: -0.0561, G_loss: 0.1386\n",
      "  Batch [1120/1299] D_loss: -0.0483, G_loss: 0.4753\n",
      "  Batch [1130/1299] D_loss: -0.2524, G_loss: 0.5554\n",
      "  Batch [1140/1299] D_loss: -0.1556, G_loss: 0.6244\n",
      "  Batch [1150/1299] D_loss: 0.0449, G_loss: 0.4594\n",
      "  Batch [1160/1299] D_loss: -0.0123, G_loss: 0.3183\n",
      "  Batch [1170/1299] D_loss: -1.3084, G_loss: -2.5543\n",
      "  Batch [1180/1299] D_loss: 0.0091, G_loss: 0.1166\n",
      "  Batch [1190/1299] D_loss: -0.0193, G_loss: 0.1218\n",
      "  Batch [1200/1299] D_loss: -0.0416, G_loss: 0.1292\n",
      "  Batch [1210/1299] D_loss: -0.2276, G_loss: -0.7463\n",
      "  Batch [1220/1299] D_loss: -0.5662, G_loss: -1.8437\n",
      "  Batch [1230/1299] D_loss: -0.0304, G_loss: 0.1758\n",
      "  Batch [1240/1299] D_loss: -0.0070, G_loss: 0.3714\n",
      "  Batch [1250/1299] D_loss: -0.0646, G_loss: 0.3776\n",
      "  Batch [1260/1299] D_loss: -0.0501, G_loss: 0.3269\n",
      "  Batch [1270/1299] D_loss: -0.5678, G_loss: -0.7145\n",
      "  Batch [1280/1299] D_loss: -2.3709, G_loss: -4.5189\n",
      "  Batch [1290/1299] D_loss: -0.0022, G_loss: 0.1792\n",
      "\n",
      "Epoch 55 Summary:\n",
      "  Average D_loss: -0.1481\n",
      "  Average G_loss: -0.0816\n",
      "\n",
      "Epoch [56/100]\n",
      "  Batch [0/1299] D_loss: 0.0082, G_loss: 0.3329\n",
      "  Batch [10/1299] D_loss: -0.1350, G_loss: 0.4924\n",
      "  Batch [20/1299] D_loss: -0.0896, G_loss: 0.5699\n",
      "  Batch [30/1299] D_loss: -0.1449, G_loss: 0.5450\n",
      "  Batch [40/1299] D_loss: -2.3273, G_loss: -1.8968\n",
      "  Batch [50/1299] D_loss: -0.6580, G_loss: -1.4381\n",
      "  Batch [60/1299] D_loss: -0.8337, G_loss: -0.6835\n",
      "  Batch [70/1299] D_loss: -0.4452, G_loss: -0.0114\n",
      "  Batch [80/1299] D_loss: -0.0624, G_loss: 0.0388\n",
      "  Batch [90/1299] D_loss: -0.8457, G_loss: -0.1307\n",
      "  Batch [100/1299] D_loss: -0.5112, G_loss: -0.7343\n",
      "  Batch [110/1299] D_loss: -0.0173, G_loss: 0.2079\n",
      "  Batch [120/1299] D_loss: -0.1268, G_loss: 0.4701\n",
      "  Batch [130/1299] D_loss: -0.0653, G_loss: 0.5066\n",
      "  Batch [140/1299] D_loss: -0.1394, G_loss: 0.6652\n",
      "  Batch [150/1299] D_loss: -0.1203, G_loss: 0.4097\n",
      "  Batch [160/1299] D_loss: -4.2862, G_loss: -6.1151\n",
      "  Batch [170/1299] D_loss: -1.3528, G_loss: -1.0868\n",
      "  Batch [180/1299] D_loss: -0.3001, G_loss: 0.0900\n",
      "  Batch [190/1299] D_loss: -0.5623, G_loss: -0.2153\n",
      "  Batch [200/1299] D_loss: -0.8959, G_loss: 0.1380\n",
      "  Batch [210/1299] D_loss: -0.0605, G_loss: 0.2204\n",
      "  Batch [220/1299] D_loss: -0.0739, G_loss: 0.4757\n",
      "  Batch [230/1299] D_loss: -0.1949, G_loss: 0.5201\n",
      "  Batch [240/1299] D_loss: -0.0885, G_loss: 0.5273\n",
      "  Batch [250/1299] D_loss: -0.1495, G_loss: 0.5365\n",
      "  Batch [260/1299] D_loss: -0.0677, G_loss: 0.3924\n",
      "  Batch [270/1299] D_loss: -1.3369, G_loss: -0.1236\n",
      "  Batch [280/1299] D_loss: -0.2630, G_loss: -0.9377\n",
      "  Batch [290/1299] D_loss: -1.3023, G_loss: -3.1425\n",
      "  Batch [300/1299] D_loss: -0.2381, G_loss: -0.1745\n",
      "  Batch [310/1299] D_loss: -0.3384, G_loss: 0.0346\n",
      "  Batch [320/1299] D_loss: -0.0224, G_loss: 0.3270\n",
      "  Batch [330/1299] D_loss: -0.1227, G_loss: 0.5261\n",
      "  Batch [340/1299] D_loss: 0.0210, G_loss: 0.6311\n",
      "  Batch [350/1299] D_loss: -0.0808, G_loss: 0.6683\n",
      "  Batch [360/1299] D_loss: -0.1030, G_loss: 0.4149\n",
      "  Batch [370/1299] D_loss: -0.7394, G_loss: -1.2575\n",
      "  Batch [380/1299] D_loss: -0.5723, G_loss: -0.8315\n",
      "  Batch [390/1299] D_loss: 0.0395, G_loss: 0.1826\n",
      "  Batch [400/1299] D_loss: -0.0724, G_loss: 0.3149\n",
      "  Batch [410/1299] D_loss: -0.2464, G_loss: 0.6533\n",
      "  Batch [420/1299] D_loss: -0.3148, G_loss: 0.7134\n",
      "  Batch [430/1299] D_loss: -0.0852, G_loss: 0.7001\n",
      "  Batch [440/1299] D_loss: -0.1626, G_loss: 0.6235\n",
      "  Batch [450/1299] D_loss: -0.4468, G_loss: -1.1680\n",
      "  Batch [460/1299] D_loss: -0.1283, G_loss: -0.0108\n",
      "  Batch [470/1299] D_loss: -0.6754, G_loss: -0.5540\n",
      "  Batch [480/1299] D_loss: -1.0357, G_loss: -1.5647\n",
      "  Batch [490/1299] D_loss: -0.3725, G_loss: -0.5720\n",
      "  Batch [500/1299] D_loss: -0.0207, G_loss: 0.1471\n",
      "  Batch [510/1299] D_loss: -0.1601, G_loss: 0.4388\n",
      "  Batch [520/1299] D_loss: -0.2208, G_loss: 0.7504\n",
      "  Batch [530/1299] D_loss: -0.1637, G_loss: 0.6502\n",
      "  Batch [540/1299] D_loss: 0.0012, G_loss: 0.5799\n",
      "  Batch [550/1299] D_loss: -0.0538, G_loss: 0.4928\n",
      "  Batch [560/1299] D_loss: -0.1901, G_loss: 0.5644\n",
      "  Batch [570/1299] D_loss: -2.1165, G_loss: -2.9434\n",
      "  Batch [580/1299] D_loss: -0.0046, G_loss: 0.0563\n",
      "  Batch [590/1299] D_loss: -0.0328, G_loss: 0.1447\n",
      "  Batch [600/1299] D_loss: -0.0350, G_loss: 0.1994\n",
      "  Batch [610/1299] D_loss: -0.1039, G_loss: 0.4009\n",
      "  Batch [620/1299] D_loss: -0.1055, G_loss: 0.3990\n",
      "  Batch [630/1299] D_loss: -0.0900, G_loss: 0.3494\n",
      "  Batch [640/1299] D_loss: -0.0617, G_loss: 0.3272\n",
      "  Batch [650/1299] D_loss: -0.0191, G_loss: 0.1429\n",
      "  Batch [660/1299] D_loss: -0.0084, G_loss: 0.2486\n",
      "  Batch [670/1299] D_loss: -0.0583, G_loss: 0.2466\n",
      "  Batch [680/1299] D_loss: -0.0596, G_loss: 0.1789\n",
      "  Batch [690/1299] D_loss: -0.1951, G_loss: 0.1078\n",
      "  Batch [700/1299] D_loss: -0.0257, G_loss: 0.1097\n",
      "  Batch [710/1299] D_loss: -0.0420, G_loss: 0.2338\n",
      "  Batch [720/1299] D_loss: -0.0819, G_loss: 0.3815\n",
      "  Batch [730/1299] D_loss: 0.0101, G_loss: 0.2777\n",
      "  Batch [740/1299] D_loss: -0.0973, G_loss: 0.4619\n",
      "  Batch [750/1299] D_loss: -0.0608, G_loss: 0.1079\n",
      "  Batch [760/1299] D_loss: 0.0202, G_loss: 0.2035\n",
      "  Batch [770/1299] D_loss: -0.0392, G_loss: 0.2464\n",
      "  Batch [780/1299] D_loss: -0.0156, G_loss: 0.3207\n",
      "  Batch [790/1299] D_loss: 0.0044, G_loss: 0.2957\n",
      "  Batch [800/1299] D_loss: -1.1406, G_loss: -4.6697\n",
      "  Batch [810/1299] D_loss: -0.3778, G_loss: -0.9850\n",
      "  Batch [820/1299] D_loss: -0.1486, G_loss: -0.0050\n",
      "  Batch [830/1299] D_loss: -0.9785, G_loss: -2.3061\n",
      "  Batch [840/1299] D_loss: -0.0370, G_loss: 0.2186\n",
      "  Batch [850/1299] D_loss: -0.0471, G_loss: 0.4119\n",
      "  Batch [860/1299] D_loss: -0.0675, G_loss: 0.5974\n",
      "  Batch [870/1299] D_loss: -0.1576, G_loss: 0.6194\n",
      "  Batch [880/1299] D_loss: -0.1531, G_loss: 0.4926\n",
      "  Batch [890/1299] D_loss: -0.0285, G_loss: 0.2637\n",
      "  Batch [900/1299] D_loss: -0.0438, G_loss: 0.0719\n",
      "  Batch [910/1299] D_loss: -0.9829, G_loss: -1.0218\n",
      "  Batch [920/1299] D_loss: -0.5044, G_loss: -0.3162\n",
      "  Batch [930/1299] D_loss: -0.8866, G_loss: -0.4442\n",
      "  Batch [940/1299] D_loss: -0.2536, G_loss: -0.3788\n",
      "  Batch [950/1299] D_loss: -1.3962, G_loss: -2.2961\n",
      "  Batch [960/1299] D_loss: -0.1215, G_loss: 0.2921\n",
      "  Batch [970/1299] D_loss: -0.1000, G_loss: 0.4600\n",
      "  Batch [980/1299] D_loss: -0.2602, G_loss: 0.7192\n",
      "  Batch [990/1299] D_loss: -0.0863, G_loss: 0.5942\n",
      "  Batch [1000/1299] D_loss: -0.0022, G_loss: 0.3818\n",
      "  Batch [1010/1299] D_loss: -0.0244, G_loss: 0.2157\n",
      "  Batch [1020/1299] D_loss: -0.5855, G_loss: -2.4031\n",
      "  Batch [1030/1299] D_loss: 0.0120, G_loss: 0.2534\n",
      "  Batch [1040/1299] D_loss: -0.0790, G_loss: 0.3725\n",
      "  Batch [1050/1299] D_loss: -0.1075, G_loss: 0.3859\n",
      "  Batch [1060/1299] D_loss: 0.0083, G_loss: 0.3229\n",
      "  Batch [1070/1299] D_loss: -0.0834, G_loss: 0.2783\n",
      "  Batch [1080/1299] D_loss: -0.1182, G_loss: 0.0081\n",
      "  Batch [1090/1299] D_loss: -0.0251, G_loss: 0.2575\n",
      "  Batch [1100/1299] D_loss: -0.0441, G_loss: 0.3435\n",
      "  Batch [1110/1299] D_loss: -0.0308, G_loss: 0.3943\n",
      "  Batch [1120/1299] D_loss: -0.0929, G_loss: 0.4683\n",
      "  Batch [1130/1299] D_loss: -0.1199, G_loss: 0.2801\n",
      "  Batch [1140/1299] D_loss: -1.2585, G_loss: -0.2793\n",
      "  Batch [1150/1299] D_loss: -1.1078, G_loss: -1.0109\n",
      "  Batch [1160/1299] D_loss: -0.5211, G_loss: -0.9089\n",
      "  Batch [1170/1299] D_loss: -0.0150, G_loss: 0.1862\n",
      "  Batch [1180/1299] D_loss: 0.0034, G_loss: 0.1763\n",
      "  Batch [1190/1299] D_loss: -0.0170, G_loss: 0.2587\n",
      "  Batch [1200/1299] D_loss: -0.0356, G_loss: 0.3728\n",
      "  Batch [1210/1299] D_loss: 0.0043, G_loss: 0.4565\n",
      "  Batch [1220/1299] D_loss: -0.0629, G_loss: 0.3058\n",
      "  Batch [1230/1299] D_loss: -0.0125, G_loss: 0.2315\n",
      "  Batch [1240/1299] D_loss: -0.4109, G_loss: -0.2719\n",
      "  Batch [1250/1299] D_loss: -2.2715, G_loss: -1.4593\n",
      "  Batch [1260/1299] D_loss: -1.2982, G_loss: -0.9806\n",
      "  Batch [1270/1299] D_loss: -0.1224, G_loss: 0.3178\n",
      "  Batch [1280/1299] D_loss: -0.1392, G_loss: 0.4775\n",
      "  Batch [1290/1299] D_loss: -0.2416, G_loss: 0.6814\n",
      "\n",
      "Epoch 56 Summary:\n",
      "  Average D_loss: -0.1634\n",
      "  Average G_loss: -0.0555\n",
      "\n",
      "Epoch [57/100]\n",
      "  Batch [0/1299] D_loss: -0.1255, G_loss: 0.6831\n",
      "  Batch [10/1299] D_loss: -0.0647, G_loss: 0.3869\n",
      "  Batch [20/1299] D_loss: -0.0020, G_loss: 0.2957\n",
      "  Batch [30/1299] D_loss: -1.3859, G_loss: -2.8684\n",
      "  Batch [40/1299] D_loss: -0.3288, G_loss: -0.0823\n",
      "  Batch [50/1299] D_loss: -0.4715, G_loss: -0.3985\n",
      "  Batch [60/1299] D_loss: -0.4111, G_loss: -0.2572\n",
      "  Batch [70/1299] D_loss: -0.0274, G_loss: 0.2262\n",
      "  Batch [80/1299] D_loss: -0.0709, G_loss: 0.3811\n",
      "  Batch [90/1299] D_loss: -0.0579, G_loss: 0.4398\n",
      "  Batch [100/1299] D_loss: -0.0306, G_loss: 0.4845\n",
      "  Batch [110/1299] D_loss: -0.0314, G_loss: 0.3455\n",
      "  Batch [120/1299] D_loss: -0.9825, G_loss: -1.8965\n",
      "  Batch [130/1299] D_loss: -0.0337, G_loss: 0.1961\n",
      "  Batch [140/1299] D_loss: -0.0797, G_loss: 0.3412\n",
      "  Batch [150/1299] D_loss: -0.1246, G_loss: 0.6591\n",
      "  Batch [160/1299] D_loss: -0.1050, G_loss: 0.7275\n",
      "  Batch [170/1299] D_loss: 0.0238, G_loss: 0.5870\n",
      "  Batch [180/1299] D_loss: 0.0240, G_loss: 0.3751\n",
      "  Batch [190/1299] D_loss: -0.9237, G_loss: -2.0176\n",
      "  Batch [200/1299] D_loss: 0.0239, G_loss: 0.1254\n",
      "  Batch [210/1299] D_loss: -0.7269, G_loss: -0.6166\n",
      "  Batch [220/1299] D_loss: -0.3290, G_loss: -1.0933\n",
      "  Batch [230/1299] D_loss: -0.0368, G_loss: 0.2422\n",
      "  Batch [240/1299] D_loss: -0.1267, G_loss: 0.3777\n",
      "  Batch [250/1299] D_loss: -0.1631, G_loss: 0.5937\n",
      "  Batch [260/1299] D_loss: -0.1492, G_loss: 0.5719\n",
      "  Batch [270/1299] D_loss: -0.1789, G_loss: 0.5001\n",
      "  Batch [280/1299] D_loss: -3.6828, G_loss: -6.2666\n",
      "  Batch [290/1299] D_loss: -0.6081, G_loss: -0.8014\n",
      "  Batch [300/1299] D_loss: -0.3029, G_loss: 0.0578\n",
      "  Batch [310/1299] D_loss: -0.0134, G_loss: 0.2471\n",
      "  Batch [320/1299] D_loss: -0.1050, G_loss: 0.5127\n",
      "  Batch [330/1299] D_loss: -0.1713, G_loss: 0.5657\n",
      "  Batch [340/1299] D_loss: -0.0136, G_loss: 0.5742\n",
      "  Batch [350/1299] D_loss: -0.0659, G_loss: 0.4860\n",
      "  Batch [360/1299] D_loss: -0.0570, G_loss: 0.4067\n",
      "  Batch [370/1299] D_loss: -1.9202, G_loss: -1.5741\n",
      "  Batch [380/1299] D_loss: -0.1046, G_loss: 0.0643\n",
      "  Batch [390/1299] D_loss: -0.9120, G_loss: -1.6025\n",
      "  Batch [400/1299] D_loss: -0.0335, G_loss: 0.0706\n",
      "  Batch [410/1299] D_loss: -0.0701, G_loss: 0.1229\n",
      "  Batch [420/1299] D_loss: -0.0800, G_loss: 0.3325\n",
      "  Batch [430/1299] D_loss: -0.0606, G_loss: 0.4639\n",
      "  Batch [440/1299] D_loss: -0.0498, G_loss: 0.5213\n",
      "  Batch [450/1299] D_loss: -0.0175, G_loss: 0.3630\n",
      "  Batch [460/1299] D_loss: -0.2264, G_loss: 0.2328\n",
      "  Batch [470/1299] D_loss: -0.4548, G_loss: 0.0765\n",
      "  Batch [480/1299] D_loss: -1.1573, G_loss: -1.4188\n",
      "  Batch [490/1299] D_loss: -0.4745, G_loss: -0.0633\n",
      "  Batch [500/1299] D_loss: -0.0278, G_loss: 0.1665\n",
      "  Batch [510/1299] D_loss: -0.0484, G_loss: 0.4339\n",
      "  Batch [520/1299] D_loss: -0.1188, G_loss: 0.5473\n",
      "  Batch [530/1299] D_loss: -0.1772, G_loss: 0.4975\n",
      "  Batch [540/1299] D_loss: -0.1396, G_loss: 0.5447\n",
      "  Batch [550/1299] D_loss: -0.0316, G_loss: 0.4209\n",
      "  Batch [560/1299] D_loss: -1.9366, G_loss: -4.5558\n",
      "  Batch [570/1299] D_loss: -0.0129, G_loss: 0.1645\n",
      "  Batch [580/1299] D_loss: -0.0976, G_loss: 0.3315\n",
      "  Batch [590/1299] D_loss: -0.0370, G_loss: 0.3694\n",
      "  Batch [600/1299] D_loss: -0.0863, G_loss: 0.3843\n",
      "  Batch [610/1299] D_loss: -0.0570, G_loss: 0.3038\n",
      "  Batch [620/1299] D_loss: -0.0380, G_loss: 0.1603\n",
      "  Batch [630/1299] D_loss: -0.0284, G_loss: 0.0236\n",
      "  Batch [640/1299] D_loss: -0.2877, G_loss: 0.1231\n",
      "  Batch [650/1299] D_loss: -0.3083, G_loss: -0.0219\n",
      "  Batch [660/1299] D_loss: -0.3039, G_loss: -0.1441\n",
      "  Batch [670/1299] D_loss: -0.1261, G_loss: 0.3950\n",
      "  Batch [680/1299] D_loss: -0.0548, G_loss: 0.5751\n",
      "  Batch [690/1299] D_loss: -0.0647, G_loss: 0.4060\n",
      "  Batch [700/1299] D_loss: -0.0749, G_loss: 0.5128\n",
      "  Batch [710/1299] D_loss: -0.0541, G_loss: 0.2675\n",
      "  Batch [720/1299] D_loss: -2.3927, G_loss: -3.2744\n",
      "  Batch [730/1299] D_loss: -0.6314, G_loss: -0.1050\n",
      "  Batch [740/1299] D_loss: -0.0316, G_loss: 0.0358\n",
      "  Batch [750/1299] D_loss: -1.7956, G_loss: -1.8657\n",
      "  Batch [760/1299] D_loss: -0.3960, G_loss: 0.0162\n",
      "  Batch [770/1299] D_loss: -0.0042, G_loss: 0.1960\n",
      "  Batch [780/1299] D_loss: -0.0635, G_loss: 0.4343\n",
      "  Batch [790/1299] D_loss: -0.1379, G_loss: 0.6117\n",
      "  Batch [800/1299] D_loss: -0.0496, G_loss: 0.5806\n",
      "  Batch [810/1299] D_loss: -0.0016, G_loss: 0.3804\n",
      "  Batch [820/1299] D_loss: -0.0786, G_loss: 0.2168\n",
      "  Batch [830/1299] D_loss: 0.0844, G_loss: -0.9579\n",
      "  Batch [840/1299] D_loss: -0.7006, G_loss: -1.4655\n",
      "  Batch [850/1299] D_loss: -0.7839, G_loss: -0.3288\n",
      "  Batch [860/1299] D_loss: -0.0587, G_loss: 0.1951\n",
      "  Batch [870/1299] D_loss: -0.8695, G_loss: -0.0343\n",
      "  Batch [880/1299] D_loss: -0.0108, G_loss: 0.2290\n",
      "  Batch [890/1299] D_loss: -0.0696, G_loss: 0.3653\n",
      "  Batch [900/1299] D_loss: -0.0216, G_loss: 0.4425\n",
      "  Batch [910/1299] D_loss: -0.0403, G_loss: 0.4014\n",
      "  Batch [920/1299] D_loss: 0.0121, G_loss: 0.4194\n",
      "  Batch [930/1299] D_loss: -0.0475, G_loss: 0.2348\n",
      "  Batch [940/1299] D_loss: -1.1555, G_loss: -3.5069\n",
      "  Batch [950/1299] D_loss: -0.4444, G_loss: 0.0808\n",
      "  Batch [960/1299] D_loss: -0.3362, G_loss: -0.1754\n",
      "  Batch [970/1299] D_loss: -0.5215, G_loss: -0.0582\n",
      "  Batch [980/1299] D_loss: -0.0367, G_loss: 0.2438\n",
      "  Batch [990/1299] D_loss: -0.0267, G_loss: 0.3234\n",
      "  Batch [1000/1299] D_loss: -0.1235, G_loss: 0.5159\n",
      "  Batch [1010/1299] D_loss: -0.1283, G_loss: 0.7074\n",
      "  Batch [1020/1299] D_loss: -0.0165, G_loss: 0.4804\n",
      "  Batch [1030/1299] D_loss: -0.0983, G_loss: 0.3558\n",
      "  Batch [1040/1299] D_loss: -0.4833, G_loss: -1.2997\n",
      "  Batch [1050/1299] D_loss: -0.3400, G_loss: -0.2988\n",
      "  Batch [1060/1299] D_loss: -0.7458, G_loss: -1.2851\n",
      "  Batch [1070/1299] D_loss: -0.0179, G_loss: 0.2966\n",
      "  Batch [1080/1299] D_loss: -0.1220, G_loss: 0.3648\n",
      "  Batch [1090/1299] D_loss: -0.0306, G_loss: 0.6230\n",
      "  Batch [1100/1299] D_loss: -0.0168, G_loss: 0.5161\n",
      "  Batch [1110/1299] D_loss: -0.0318, G_loss: 0.5181\n",
      "  Batch [1120/1299] D_loss: -0.0325, G_loss: 0.3333\n",
      "  Batch [1130/1299] D_loss: -2.3049, G_loss: -2.5202\n",
      "  Batch [1140/1299] D_loss: 0.0016, G_loss: 0.1666\n",
      "  Batch [1150/1299] D_loss: -0.0476, G_loss: 0.2692\n",
      "  Batch [1160/1299] D_loss: 0.0049, G_loss: 0.2815\n",
      "  Batch [1170/1299] D_loss: 0.0126, G_loss: 0.2859\n",
      "  Batch [1180/1299] D_loss: -0.0214, G_loss: 0.1858\n",
      "  Batch [1190/1299] D_loss: 0.0651, G_loss: 0.1138\n",
      "  Batch [1200/1299] D_loss: -0.0310, G_loss: 0.1760\n",
      "  Batch [1210/1299] D_loss: -0.1101, G_loss: 0.3210\n",
      "  Batch [1220/1299] D_loss: -0.0187, G_loss: 0.2295\n",
      "  Batch [1230/1299] D_loss: -0.0219, G_loss: 0.1579\n",
      "  Batch [1240/1299] D_loss: -0.0650, G_loss: 0.2444\n",
      "  Batch [1250/1299] D_loss: -0.0375, G_loss: 0.2405\n",
      "  Batch [1260/1299] D_loss: -0.0545, G_loss: 0.3384\n",
      "  Batch [1270/1299] D_loss: 0.0085, G_loss: 0.2890\n",
      "  Batch [1280/1299] D_loss: -0.0401, G_loss: 0.3253\n",
      "  Batch [1290/1299] D_loss: -0.0052, G_loss: 0.0988\n",
      "\n",
      "Epoch 57 Summary:\n",
      "  Average D_loss: -0.1497\n",
      "  Average G_loss: -0.0663\n",
      "\n",
      "Epoch [58/100]\n",
      "  Batch [0/1299] D_loss: -0.0086, G_loss: 0.1346\n",
      "  Batch [10/1299] D_loss: -1.3013, G_loss: -0.8994\n",
      "  Batch [20/1299] D_loss: -0.0259, G_loss: 0.1430\n",
      "  Batch [30/1299] D_loss: -0.0571, G_loss: 0.2732\n",
      "  Batch [40/1299] D_loss: -0.1071, G_loss: 0.3239\n",
      "  Batch [50/1299] D_loss: -0.0155, G_loss: 0.1862\n",
      "  Batch [60/1299] D_loss: -0.0136, G_loss: 0.0969\n",
      "  Batch [70/1299] D_loss: -0.0315, G_loss: 0.1523\n",
      "  Batch [80/1299] D_loss: -0.0623, G_loss: 0.3138\n",
      "  Batch [90/1299] D_loss: -0.0419, G_loss: 0.3276\n",
      "  Batch [100/1299] D_loss: -0.0202, G_loss: 0.1964\n",
      "  Batch [110/1299] D_loss: -1.9714, G_loss: -1.7526\n",
      "  Batch [120/1299] D_loss: -0.1291, G_loss: -0.6534\n",
      "  Batch [130/1299] D_loss: -0.0682, G_loss: 0.0811\n",
      "  Batch [140/1299] D_loss: -0.0182, G_loss: 0.1938\n",
      "  Batch [150/1299] D_loss: -0.0361, G_loss: 0.2476\n",
      "  Batch [160/1299] D_loss: -0.0940, G_loss: 0.3625\n",
      "  Batch [170/1299] D_loss: -0.0443, G_loss: 0.2418\n",
      "  Batch [180/1299] D_loss: -0.0728, G_loss: 0.2071\n",
      "  Batch [190/1299] D_loss: -0.0115, G_loss: 0.0976\n",
      "  Batch [200/1299] D_loss: -0.1039, G_loss: 0.2674\n",
      "  Batch [210/1299] D_loss: -0.1033, G_loss: 0.4563\n",
      "  Batch [220/1299] D_loss: -0.0776, G_loss: 0.3254\n",
      "  Batch [230/1299] D_loss: -0.0956, G_loss: 0.3797\n",
      "  Batch [240/1299] D_loss: -0.0411, G_loss: 0.1578\n",
      "  Batch [250/1299] D_loss: 0.0091, G_loss: 0.2177\n",
      "  Batch [260/1299] D_loss: -0.0675, G_loss: 0.2314\n",
      "  Batch [270/1299] D_loss: -0.0289, G_loss: 0.3176\n",
      "  Batch [280/1299] D_loss: -0.0083, G_loss: 0.2682\n",
      "  Batch [290/1299] D_loss: -0.9717, G_loss: -0.7042\n",
      "  Batch [300/1299] D_loss: -0.2657, G_loss: -0.1102\n",
      "  Batch [310/1299] D_loss: -0.9024, G_loss: -1.4330\n",
      "  Batch [320/1299] D_loss: 0.0075, G_loss: 0.0264\n",
      "  Batch [330/1299] D_loss: -0.6565, G_loss: -0.1550\n",
      "  Batch [340/1299] D_loss: -0.0625, G_loss: 0.2484\n",
      "  Batch [350/1299] D_loss: -0.1282, G_loss: 0.4613\n",
      "  Batch [360/1299] D_loss: -0.1070, G_loss: 0.4895\n",
      "  Batch [370/1299] D_loss: -0.0633, G_loss: 0.4792\n",
      "  Batch [380/1299] D_loss: -0.0892, G_loss: 0.4014\n",
      "  Batch [390/1299] D_loss: -0.0548, G_loss: 0.2183\n",
      "  Batch [400/1299] D_loss: -0.7115, G_loss: -0.8127\n",
      "  Batch [410/1299] D_loss: -0.3467, G_loss: -0.0093\n",
      "  Batch [420/1299] D_loss: -0.0410, G_loss: 0.2248\n",
      "  Batch [430/1299] D_loss: -0.0691, G_loss: 0.2537\n",
      "  Batch [440/1299] D_loss: -0.0471, G_loss: 0.4127\n",
      "  Batch [450/1299] D_loss: -0.0599, G_loss: 0.3367\n",
      "  Batch [460/1299] D_loss: -0.0705, G_loss: 0.4401\n",
      "  Batch [470/1299] D_loss: -1.2393, G_loss: -2.1851\n",
      "  Batch [480/1299] D_loss: -0.3339, G_loss: 0.0259\n",
      "  Batch [490/1299] D_loss: -0.0387, G_loss: 0.1906\n",
      "  Batch [500/1299] D_loss: -0.0199, G_loss: 0.3026\n",
      "  Batch [510/1299] D_loss: -0.1418, G_loss: 0.5151\n",
      "  Batch [520/1299] D_loss: -0.0470, G_loss: 0.4375\n",
      "  Batch [530/1299] D_loss: 0.0030, G_loss: 0.3607\n",
      "  Batch [540/1299] D_loss: -2.0337, G_loss: -0.8062\n",
      "  Batch [550/1299] D_loss: -1.3278, G_loss: -2.2134\n",
      "  Batch [560/1299] D_loss: -0.0101, G_loss: 0.1028\n",
      "  Batch [570/1299] D_loss: -0.0434, G_loss: 0.0735\n",
      "  Batch [580/1299] D_loss: -0.3952, G_loss: -0.3468\n",
      "  Batch [590/1299] D_loss: -0.3604, G_loss: 0.0671\n",
      "  Batch [600/1299] D_loss: -0.0619, G_loss: 0.2025\n",
      "  Batch [610/1299] D_loss: -0.1376, G_loss: 0.3541\n",
      "  Batch [620/1299] D_loss: -0.0788, G_loss: 0.4601\n",
      "  Batch [630/1299] D_loss: -0.0138, G_loss: 0.3847\n",
      "  Batch [640/1299] D_loss: -0.1378, G_loss: 0.2127\n",
      "  Batch [650/1299] D_loss: -0.4635, G_loss: -0.3506\n",
      "  Batch [660/1299] D_loss: -0.3683, G_loss: -1.1675\n",
      "  Batch [670/1299] D_loss: -0.0880, G_loss: 0.1139\n",
      "  Batch [680/1299] D_loss: -0.0984, G_loss: 0.2655\n",
      "  Batch [690/1299] D_loss: -0.0762, G_loss: 0.5929\n",
      "  Batch [700/1299] D_loss: -0.2032, G_loss: 0.6742\n",
      "  Batch [710/1299] D_loss: -0.0039, G_loss: 0.6146\n",
      "  Batch [720/1299] D_loss: -0.1785, G_loss: 0.6246\n",
      "  Batch [730/1299] D_loss: -0.0708, G_loss: 0.3240\n",
      "  Batch [740/1299] D_loss: -0.0251, G_loss: 0.0890\n",
      "  Batch [750/1299] D_loss: -1.4740, G_loss: -3.7379\n",
      "  Batch [760/1299] D_loss: -0.0540, G_loss: 0.1239\n",
      "  Batch [770/1299] D_loss: -0.0425, G_loss: 0.1911\n",
      "  Batch [780/1299] D_loss: -0.0965, G_loss: 0.3529\n",
      "  Batch [790/1299] D_loss: -0.1097, G_loss: 0.3889\n",
      "  Batch [800/1299] D_loss: -0.0701, G_loss: 0.4122\n",
      "  Batch [810/1299] D_loss: -0.0847, G_loss: 0.4205\n",
      "  Batch [820/1299] D_loss: -0.0005, G_loss: 0.2988\n",
      "  Batch [830/1299] D_loss: -1.2302, G_loss: -0.4541\n",
      "  Batch [840/1299] D_loss: -0.2634, G_loss: 0.0454\n",
      "  Batch [850/1299] D_loss: 0.0356, G_loss: 0.1902\n",
      "  Batch [860/1299] D_loss: -0.8062, G_loss: -0.4326\n",
      "  Batch [870/1299] D_loss: -0.3961, G_loss: -0.2516\n",
      "  Batch [880/1299] D_loss: -0.4120, G_loss: -0.3436\n",
      "  Batch [890/1299] D_loss: -1.1047, G_loss: -1.5793\n",
      "  Batch [900/1299] D_loss: -0.6039, G_loss: -1.1567\n",
      "  Batch [910/1299] D_loss: -0.1910, G_loss: 0.0126\n",
      "  Batch [920/1299] D_loss: -0.0982, G_loss: 0.1079\n",
      "  Batch [930/1299] D_loss: -0.2329, G_loss: 0.0471\n",
      "  Batch [940/1299] D_loss: -0.0987, G_loss: 0.2872\n",
      "  Batch [950/1299] D_loss: -0.1398, G_loss: 0.6373\n",
      "  Batch [960/1299] D_loss: -0.1885, G_loss: 0.8138\n",
      "  Batch [970/1299] D_loss: 0.1004, G_loss: 0.6788\n",
      "  Batch [980/1299] D_loss: -0.1794, G_loss: 0.6285\n",
      "  Batch [990/1299] D_loss: -0.0512, G_loss: 0.4640\n",
      "  Batch [1000/1299] D_loss: -2.2743, G_loss: -3.4687\n",
      "  Batch [1010/1299] D_loss: 0.0344, G_loss: 0.1418\n",
      "  Batch [1020/1299] D_loss: -0.0582, G_loss: 0.1840\n",
      "  Batch [1030/1299] D_loss: -0.0418, G_loss: 0.2678\n",
      "  Batch [1040/1299] D_loss: -0.1025, G_loss: 0.4021\n",
      "  Batch [1050/1299] D_loss: -0.2019, G_loss: 0.4211\n",
      "  Batch [1060/1299] D_loss: -1.2927, G_loss: -0.9412\n",
      "  Batch [1070/1299] D_loss: -0.0136, G_loss: 0.1892\n",
      "  Batch [1080/1299] D_loss: -0.0270, G_loss: 0.1665\n",
      "  Batch [1090/1299] D_loss: -0.0493, G_loss: 0.3188\n",
      "  Batch [1100/1299] D_loss: -0.0603, G_loss: 0.2887\n",
      "  Batch [1110/1299] D_loss: -3.2184, G_loss: -8.5494\n",
      "  Batch [1120/1299] D_loss: -0.0227, G_loss: 0.1286\n",
      "  Batch [1130/1299] D_loss: -0.0955, G_loss: 0.2698\n",
      "  Batch [1140/1299] D_loss: -0.2400, G_loss: 0.4065\n",
      "  Batch [1150/1299] D_loss: -0.0198, G_loss: 0.3442\n",
      "  Batch [1160/1299] D_loss: -0.0109, G_loss: 0.3513\n",
      "  Batch [1170/1299] D_loss: -1.0488, G_loss: -2.8883\n",
      "  Batch [1180/1299] D_loss: -0.0087, G_loss: 0.2411\n",
      "  Batch [1190/1299] D_loss: -0.0648, G_loss: 0.3156\n",
      "  Batch [1200/1299] D_loss: -0.1238, G_loss: 0.3449\n",
      "  Batch [1210/1299] D_loss: -0.1968, G_loss: 0.0730\n",
      "  Batch [1220/1299] D_loss: -0.4225, G_loss: -0.0653\n",
      "  Batch [1230/1299] D_loss: -0.1906, G_loss: -0.0606\n",
      "  Batch [1240/1299] D_loss: -0.0300, G_loss: 0.2494\n",
      "  Batch [1250/1299] D_loss: 0.0549, G_loss: 0.3803\n",
      "  Batch [1260/1299] D_loss: -0.1254, G_loss: 0.4722\n",
      "  Batch [1270/1299] D_loss: -0.1879, G_loss: 0.4990\n",
      "  Batch [1280/1299] D_loss: -0.0170, G_loss: 0.2860\n",
      "  Batch [1290/1299] D_loss: -0.7797, G_loss: -2.5169\n",
      "\n",
      "Epoch 58 Summary:\n",
      "  Average D_loss: -0.1322\n",
      "  Average G_loss: -0.0658\n",
      "\n",
      "Epoch [59/100]\n",
      "  Batch [0/1299] D_loss: -0.7895, G_loss: 0.0246\n",
      "  Batch [10/1299] D_loss: -0.1049, G_loss: 0.1880\n",
      "  Batch [20/1299] D_loss: -0.0452, G_loss: 0.2905\n",
      "  Batch [30/1299] D_loss: -0.0690, G_loss: 0.3856\n",
      "  Batch [40/1299] D_loss: -0.1304, G_loss: 0.3709\n",
      "  Batch [50/1299] D_loss: -0.0654, G_loss: 0.2362\n",
      "  Batch [60/1299] D_loss: -1.6858, G_loss: -1.4141\n",
      "  Batch [70/1299] D_loss: -0.0099, G_loss: 0.2587\n",
      "  Batch [80/1299] D_loss: -0.0614, G_loss: 0.2820\n",
      "  Batch [90/1299] D_loss: -0.0656, G_loss: 0.3206\n",
      "  Batch [100/1299] D_loss: -0.0440, G_loss: 0.3055\n",
      "  Batch [110/1299] D_loss: -0.4107, G_loss: -0.4534\n",
      "  Batch [120/1299] D_loss: -0.2697, G_loss: 0.0963\n",
      "  Batch [130/1299] D_loss: -0.1428, G_loss: 0.4376\n",
      "  Batch [140/1299] D_loss: 0.0136, G_loss: 0.4118\n",
      "  Batch [150/1299] D_loss: -0.0122, G_loss: 0.4190\n",
      "  Batch [160/1299] D_loss: -0.3721, G_loss: -2.6313\n",
      "  Batch [170/1299] D_loss: -0.6318, G_loss: -0.8239\n",
      "  Batch [180/1299] D_loss: -0.2949, G_loss: -0.2235\n",
      "  Batch [190/1299] D_loss: -0.0387, G_loss: 0.3224\n",
      "  Batch [200/1299] D_loss: -0.1046, G_loss: 0.4996\n",
      "  Batch [210/1299] D_loss: -0.0066, G_loss: 0.3748\n",
      "  Batch [220/1299] D_loss: -0.0196, G_loss: 0.3402\n",
      "  Batch [230/1299] D_loss: -2.7029, G_loss: -3.6701\n",
      "  Batch [240/1299] D_loss: 0.0060, G_loss: 0.1468\n",
      "  Batch [250/1299] D_loss: -0.0351, G_loss: 0.1964\n",
      "  Batch [260/1299] D_loss: -0.1012, G_loss: 0.3654\n",
      "  Batch [270/1299] D_loss: -0.0504, G_loss: 0.3606\n",
      "  Batch [280/1299] D_loss: 0.0023, G_loss: 0.2544\n",
      "  Batch [290/1299] D_loss: -1.1857, G_loss: -2.9407\n",
      "  Batch [300/1299] D_loss: -0.0552, G_loss: 0.1714\n",
      "  Batch [310/1299] D_loss: -0.0593, G_loss: 0.2817\n",
      "  Batch [320/1299] D_loss: -0.0644, G_loss: 0.3671\n",
      "  Batch [330/1299] D_loss: -0.0191, G_loss: 0.4087\n",
      "  Batch [340/1299] D_loss: -0.0788, G_loss: 0.3248\n",
      "  Batch [350/1299] D_loss: -0.8457, G_loss: -0.8278\n",
      "  Batch [360/1299] D_loss: -0.5530, G_loss: -0.0025\n",
      "  Batch [370/1299] D_loss: -0.0284, G_loss: 0.2332\n",
      "  Batch [380/1299] D_loss: -0.0237, G_loss: 0.3341\n",
      "  Batch [390/1299] D_loss: -0.0544, G_loss: 0.3975\n",
      "  Batch [400/1299] D_loss: 0.0113, G_loss: 0.3363\n",
      "  Batch [410/1299] D_loss: -0.0499, G_loss: 0.3139\n",
      "  Batch [420/1299] D_loss: -2.2829, G_loss: -2.8381\n",
      "  Batch [430/1299] D_loss: -0.4098, G_loss: 0.1065\n",
      "  Batch [440/1299] D_loss: -0.0146, G_loss: 0.2437\n",
      "  Batch [450/1299] D_loss: -0.0106, G_loss: 0.2377\n",
      "  Batch [460/1299] D_loss: -0.0432, G_loss: 0.3568\n",
      "  Batch [470/1299] D_loss: -0.1180, G_loss: 0.0798\n",
      "  Batch [480/1299] D_loss: -0.4826, G_loss: -0.6454\n",
      "  Batch [490/1299] D_loss: -0.2509, G_loss: -0.1176\n",
      "  Batch [500/1299] D_loss: -0.0401, G_loss: 0.2251\n",
      "  Batch [510/1299] D_loss: -0.0141, G_loss: 0.3318\n",
      "  Batch [520/1299] D_loss: -0.0660, G_loss: 0.3281\n",
      "  Batch [530/1299] D_loss: -0.0524, G_loss: 0.3424\n",
      "  Batch [540/1299] D_loss: -0.1305, G_loss: 0.2557\n",
      "  Batch [550/1299] D_loss: -0.7848, G_loss: -0.2249\n",
      "  Batch [560/1299] D_loss: -0.0260, G_loss: 0.1035\n",
      "  Batch [570/1299] D_loss: -0.0710, G_loss: 0.1536\n",
      "  Batch [580/1299] D_loss: -0.0940, G_loss: 0.3841\n",
      "  Batch [590/1299] D_loss: -0.1249, G_loss: 0.6183\n",
      "  Batch [600/1299] D_loss: -0.0969, G_loss: 0.5295\n",
      "  Batch [610/1299] D_loss: -0.0221, G_loss: 0.3903\n",
      "  Batch [620/1299] D_loss: 0.0158, G_loss: 0.2423\n",
      "  Batch [630/1299] D_loss: -0.7985, G_loss: -0.3234\n",
      "  Batch [640/1299] D_loss: -0.8838, G_loss: -0.5726\n",
      "  Batch [650/1299] D_loss: -0.0316, G_loss: 0.1293\n",
      "  Batch [660/1299] D_loss: -0.0730, G_loss: 0.3738\n",
      "  Batch [670/1299] D_loss: -0.0634, G_loss: 0.4132\n",
      "  Batch [680/1299] D_loss: -0.0705, G_loss: 0.4896\n",
      "  Batch [690/1299] D_loss: -0.1066, G_loss: 0.4705\n",
      "  Batch [700/1299] D_loss: -1.2620, G_loss: -0.1077\n",
      "  Batch [710/1299] D_loss: -0.0234, G_loss: 0.1786\n",
      "  Batch [720/1299] D_loss: -0.0079, G_loss: 0.1946\n",
      "  Batch [730/1299] D_loss: -0.0993, G_loss: 0.3206\n",
      "  Batch [740/1299] D_loss: -0.0668, G_loss: 0.2942\n",
      "  Batch [750/1299] D_loss: -0.0182, G_loss: 0.2772\n",
      "  Batch [760/1299] D_loss: -0.5498, G_loss: -0.9806\n",
      "  Batch [770/1299] D_loss: -0.1471, G_loss: 0.1559\n",
      "  Batch [780/1299] D_loss: -0.0508, G_loss: 0.2442\n",
      "  Batch [790/1299] D_loss: -0.0084, G_loss: 0.2330\n",
      "  Batch [800/1299] D_loss: -0.0641, G_loss: 0.2903\n",
      "  Batch [810/1299] D_loss: -0.2595, G_loss: -0.5047\n",
      "  Batch [820/1299] D_loss: -0.7708, G_loss: -0.6469\n",
      "  Batch [830/1299] D_loss: -0.2578, G_loss: -1.4659\n",
      "  Batch [840/1299] D_loss: -0.0422, G_loss: 0.1783\n",
      "  Batch [850/1299] D_loss: 0.0086, G_loss: 0.3614\n",
      "  Batch [860/1299] D_loss: -0.1502, G_loss: 0.4920\n",
      "  Batch [870/1299] D_loss: -0.0415, G_loss: 0.5048\n",
      "  Batch [880/1299] D_loss: -0.0706, G_loss: 0.3513\n",
      "  Batch [890/1299] D_loss: 0.0005, G_loss: 0.1900\n",
      "  Batch [900/1299] D_loss: -0.3401, G_loss: -1.1842\n",
      "  Batch [910/1299] D_loss: -1.3525, G_loss: -0.8087\n",
      "  Batch [920/1299] D_loss: -0.0521, G_loss: 0.0600\n",
      "  Batch [930/1299] D_loss: -0.0237, G_loss: 0.2987\n",
      "  Batch [940/1299] D_loss: -0.0160, G_loss: 0.3958\n",
      "  Batch [950/1299] D_loss: -0.0129, G_loss: 0.6313\n",
      "  Batch [960/1299] D_loss: -0.2636, G_loss: 0.4388\n",
      "  Batch [970/1299] D_loss: -0.1141, G_loss: 0.3904\n",
      "  Batch [980/1299] D_loss: -1.1776, G_loss: -1.3238\n",
      "  Batch [990/1299] D_loss: -0.0181, G_loss: 0.1358\n",
      "  Batch [1000/1299] D_loss: -0.5583, G_loss: -3.1483\n",
      "  Batch [1010/1299] D_loss: -0.0287, G_loss: 0.1238\n",
      "  Batch [1020/1299] D_loss: -0.0693, G_loss: 0.2892\n",
      "  Batch [1030/1299] D_loss: -0.1384, G_loss: 0.4928\n",
      "  Batch [1040/1299] D_loss: -0.1326, G_loss: 0.6162\n",
      "  Batch [1050/1299] D_loss: -0.0903, G_loss: 0.5186\n",
      "  Batch [1060/1299] D_loss: -0.1245, G_loss: 0.3913\n",
      "  Batch [1070/1299] D_loss: -0.5419, G_loss: -0.9704\n",
      "  Batch [1080/1299] D_loss: -1.1736, G_loss: -0.5418\n",
      "  Batch [1090/1299] D_loss: -0.0235, G_loss: 0.2235\n",
      "  Batch [1100/1299] D_loss: -0.0926, G_loss: 0.4684\n",
      "  Batch [1110/1299] D_loss: -0.1121, G_loss: 0.5288\n",
      "  Batch [1120/1299] D_loss: -0.0774, G_loss: 0.5144\n",
      "  Batch [1130/1299] D_loss: -0.0943, G_loss: 0.3867\n",
      "  Batch [1140/1299] D_loss: -0.0667, G_loss: 0.2232\n",
      "  Batch [1150/1299] D_loss: -0.6923, G_loss: -0.0549\n",
      "  Batch [1160/1299] D_loss: -0.0160, G_loss: 0.0558\n",
      "  Batch [1170/1299] D_loss: -0.1440, G_loss: 0.0348\n",
      "  Batch [1180/1299] D_loss: -0.2545, G_loss: -0.4651\n",
      "  Batch [1190/1299] D_loss: -0.0795, G_loss: 0.0969\n",
      "  Batch [1200/1299] D_loss: -1.2333, G_loss: -1.1044\n",
      "  Batch [1210/1299] D_loss: -1.2839, G_loss: -0.2259\n",
      "  Batch [1220/1299] D_loss: -0.2379, G_loss: 0.2261\n",
      "  Batch [1230/1299] D_loss: -0.1259, G_loss: 0.5101\n",
      "  Batch [1240/1299] D_loss: -0.0050, G_loss: 0.5930\n",
      "  Batch [1250/1299] D_loss: 0.0133, G_loss: 0.6228\n",
      "  Batch [1260/1299] D_loss: -0.0335, G_loss: 0.4679\n",
      "  Batch [1270/1299] D_loss: 0.0659, G_loss: 0.2489\n",
      "  Batch [1280/1299] D_loss: -0.0413, G_loss: -0.0295\n",
      "  Batch [1290/1299] D_loss: -0.0545, G_loss: 0.0599\n",
      "\n",
      "Epoch 59 Summary:\n",
      "  Average D_loss: -0.1287\n",
      "  Average G_loss: -0.0613\n",
      "\n",
      "Epoch [60/100]\n",
      "  Batch [0/1299] D_loss: -0.0120, G_loss: 0.1852\n",
      "  Batch [10/1299] D_loss: -0.1212, G_loss: 0.3781\n",
      "  Batch [20/1299] D_loss: 0.0226, G_loss: 0.4107\n",
      "  Batch [30/1299] D_loss: -0.0083, G_loss: 0.4939\n",
      "  Batch [40/1299] D_loss: -0.0708, G_loss: 0.4101\n",
      "  Batch [50/1299] D_loss: -0.0909, G_loss: 0.2692\n",
      "  Batch [60/1299] D_loss: -2.1806, G_loss: -6.6162\n",
      "  Batch [70/1299] D_loss: -0.0127, G_loss: 0.0727\n",
      "  Batch [80/1299] D_loss: -0.6283, G_loss: -0.0995\n",
      "  Batch [90/1299] D_loss: -0.6189, G_loss: -0.3097\n",
      "  Batch [100/1299] D_loss: -0.0733, G_loss: 0.2499\n",
      "  Batch [110/1299] D_loss: -0.0756, G_loss: 0.3886\n",
      "  Batch [120/1299] D_loss: -0.1213, G_loss: 0.3991\n",
      "  Batch [130/1299] D_loss: -0.1196, G_loss: 0.5081\n",
      "  Batch [140/1299] D_loss: -0.0666, G_loss: 0.4369\n",
      "  Batch [150/1299] D_loss: -2.1107, G_loss: -2.5234\n",
      "  Batch [160/1299] D_loss: -0.2359, G_loss: 0.0096\n",
      "  Batch [170/1299] D_loss: -0.1803, G_loss: 0.1781\n",
      "  Batch [180/1299] D_loss: -0.0117, G_loss: 0.1253\n",
      "  Batch [190/1299] D_loss: -0.0511, G_loss: 0.3978\n",
      "  Batch [200/1299] D_loss: -0.1440, G_loss: 0.5948\n",
      "  Batch [210/1299] D_loss: -0.0880, G_loss: 0.5540\n",
      "  Batch [220/1299] D_loss: -0.0683, G_loss: 0.5092\n",
      "  Batch [230/1299] D_loss: -0.1035, G_loss: 0.2824\n",
      "  Batch [240/1299] D_loss: -1.3379, G_loss: -2.2864\n",
      "  Batch [250/1299] D_loss: -0.5383, G_loss: -0.6960\n",
      "  Batch [260/1299] D_loss: -1.2643, G_loss: -0.7042\n",
      "  Batch [270/1299] D_loss: -0.0118, G_loss: 0.1078\n",
      "  Batch [280/1299] D_loss: -0.5077, G_loss: -0.8640\n",
      "  Batch [290/1299] D_loss: -0.0594, G_loss: 0.2875\n",
      "  Batch [300/1299] D_loss: -0.0955, G_loss: 0.4471\n",
      "  Batch [310/1299] D_loss: 0.0254, G_loss: 0.4623\n",
      "  Batch [320/1299] D_loss: -0.2138, G_loss: 0.5716\n",
      "  Batch [330/1299] D_loss: -0.0220, G_loss: 0.4022\n",
      "  Batch [340/1299] D_loss: -2.9827, G_loss: -4.1008\n",
      "  Batch [350/1299] D_loss: -0.0947, G_loss: 0.2039\n",
      "  Batch [360/1299] D_loss: 0.0086, G_loss: 0.1752\n",
      "  Batch [370/1299] D_loss: -0.5979, G_loss: -2.8988\n",
      "  Batch [380/1299] D_loss: -0.1897, G_loss: 0.1080\n",
      "  Batch [390/1299] D_loss: -0.0973, G_loss: 0.3470\n",
      "  Batch [400/1299] D_loss: -0.0909, G_loss: 0.4468\n",
      "  Batch [410/1299] D_loss: -0.1277, G_loss: 0.3904\n",
      "  Batch [420/1299] D_loss: -0.0362, G_loss: 0.3651\n",
      "  Batch [430/1299] D_loss: -0.0430, G_loss: 0.3783\n",
      "  Batch [440/1299] D_loss: -0.0529, G_loss: 0.2466\n",
      "  Batch [450/1299] D_loss: -0.0327, G_loss: 0.0879\n",
      "  Batch [460/1299] D_loss: -0.3721, G_loss: -0.1299\n",
      "  Batch [470/1299] D_loss: -0.0255, G_loss: 0.1261\n",
      "  Batch [480/1299] D_loss: -0.0466, G_loss: 0.2829\n",
      "  Batch [490/1299] D_loss: -0.1100, G_loss: 0.3776\n",
      "  Batch [500/1299] D_loss: -0.0006, G_loss: 0.4806\n",
      "  Batch [510/1299] D_loss: -0.0088, G_loss: 0.4697\n",
      "  Batch [520/1299] D_loss: 0.0014, G_loss: 0.3123\n",
      "  Batch [530/1299] D_loss: -1.8566, G_loss: -1.2949\n",
      "  Batch [540/1299] D_loss: -0.6163, G_loss: -0.8044\n",
      "  Batch [550/1299] D_loss: -0.0156, G_loss: 0.1473\n",
      "  Batch [560/1299] D_loss: -0.0780, G_loss: 0.2899\n",
      "  Batch [570/1299] D_loss: -0.0357, G_loss: 0.3431\n",
      "  Batch [580/1299] D_loss: -0.0805, G_loss: 0.4547\n",
      "  Batch [590/1299] D_loss: -0.0765, G_loss: 0.4016\n",
      "  Batch [600/1299] D_loss: -0.0477, G_loss: 0.0416\n",
      "  Batch [610/1299] D_loss: -0.0129, G_loss: -0.5977\n",
      "  Batch [620/1299] D_loss: -0.0500, G_loss: 0.1080\n",
      "  Batch [630/1299] D_loss: -0.0327, G_loss: 0.2554\n",
      "  Batch [640/1299] D_loss: -0.0393, G_loss: 0.2180\n",
      "  Batch [650/1299] D_loss: -0.0318, G_loss: 0.2260\n",
      "  Batch [660/1299] D_loss: -0.3809, G_loss: -0.1538\n",
      "  Batch [670/1299] D_loss: -0.0180, G_loss: 0.1048\n",
      "  Batch [680/1299] D_loss: -0.0775, G_loss: 0.2162\n",
      "  Batch [690/1299] D_loss: -0.0071, G_loss: 0.2860\n",
      "  Batch [700/1299] D_loss: -0.0793, G_loss: 0.3489\n",
      "  Batch [710/1299] D_loss: -0.1585, G_loss: 0.3101\n",
      "  Batch [720/1299] D_loss: -0.0457, G_loss: 0.1606\n",
      "  Batch [730/1299] D_loss: -0.0076, G_loss: 0.1336\n",
      "  Batch [740/1299] D_loss: -0.0699, G_loss: 0.2717\n",
      "  Batch [750/1299] D_loss: -0.0434, G_loss: 0.2743\n",
      "  Batch [760/1299] D_loss: -0.0809, G_loss: 0.3054\n",
      "  Batch [770/1299] D_loss: -1.2149, G_loss: -3.2236\n",
      "  Batch [780/1299] D_loss: -0.7593, G_loss: -0.4033\n",
      "  Batch [790/1299] D_loss: -0.6947, G_loss: -0.8438\n",
      "  Batch [800/1299] D_loss: -0.0540, G_loss: 0.2560\n",
      "  Batch [810/1299] D_loss: -0.0606, G_loss: 0.4923\n",
      "  Batch [820/1299] D_loss: -0.1739, G_loss: 0.5618\n",
      "  Batch [830/1299] D_loss: -0.0163, G_loss: 0.4692\n",
      "  Batch [840/1299] D_loss: -0.0144, G_loss: 0.2769\n",
      "  Batch [850/1299] D_loss: -0.8147, G_loss: -2.6148\n",
      "  Batch [860/1299] D_loss: -0.4365, G_loss: -0.0174\n",
      "  Batch [870/1299] D_loss: -1.4946, G_loss: -1.3554\n",
      "  Batch [880/1299] D_loss: -0.5194, G_loss: -0.0634\n",
      "  Batch [890/1299] D_loss: -0.6466, G_loss: -2.7666\n",
      "  Batch [900/1299] D_loss: -0.8417, G_loss: 0.0288\n",
      "  Batch [910/1299] D_loss: -0.0657, G_loss: 0.1546\n",
      "  Batch [920/1299] D_loss: -0.2432, G_loss: 0.0708\n",
      "  Batch [930/1299] D_loss: -0.0665, G_loss: 0.1471\n",
      "  Batch [940/1299] D_loss: -0.1537, G_loss: 0.4061\n",
      "  Batch [950/1299] D_loss: -0.1376, G_loss: 0.7142\n",
      "  Batch [960/1299] D_loss: -0.1530, G_loss: 0.7985\n",
      "  Batch [970/1299] D_loss: -0.1718, G_loss: 0.4535\n",
      "  Batch [980/1299] D_loss: -0.0590, G_loss: 0.4805\n",
      "  Batch [990/1299] D_loss: -2.1633, G_loss: -3.3206\n",
      "  Batch [1000/1299] D_loss: -1.0707, G_loss: -1.5013\n",
      "  Batch [1010/1299] D_loss: -0.6297, G_loss: -0.1996\n",
      "  Batch [1020/1299] D_loss: -0.4705, G_loss: -0.6149\n",
      "  Batch [1030/1299] D_loss: -0.1786, G_loss: 0.0453\n",
      "  Batch [1040/1299] D_loss: -0.7181, G_loss: -1.0414\n",
      "  Batch [1050/1299] D_loss: -0.0946, G_loss: 0.3124\n",
      "  Batch [1060/1299] D_loss: -0.1443, G_loss: 0.6009\n",
      "  Batch [1070/1299] D_loss: -0.0109, G_loss: 0.5771\n",
      "  Batch [1080/1299] D_loss: 0.0233, G_loss: 0.6055\n",
      "  Batch [1090/1299] D_loss: -0.0429, G_loss: 0.4441\n",
      "  Batch [1100/1299] D_loss: 0.0098, G_loss: 0.2712\n",
      "  Batch [1110/1299] D_loss: -0.8205, G_loss: -1.2247\n",
      "  Batch [1120/1299] D_loss: -0.2022, G_loss: 0.0249\n",
      "  Batch [1130/1299] D_loss: -0.9503, G_loss: -0.5964\n",
      "  Batch [1140/1299] D_loss: -0.1165, G_loss: 0.2348\n",
      "  Batch [1150/1299] D_loss: -0.0998, G_loss: 0.4654\n",
      "  Batch [1160/1299] D_loss: -0.1122, G_loss: 0.4451\n",
      "  Batch [1170/1299] D_loss: -0.2380, G_loss: 0.4686\n",
      "  Batch [1180/1299] D_loss: -0.0812, G_loss: 0.3238\n",
      "  Batch [1190/1299] D_loss: -1.0058, G_loss: -2.0301\n",
      "  Batch [1200/1299] D_loss: -0.1511, G_loss: 0.1602\n",
      "  Batch [1210/1299] D_loss: -1.1949, G_loss: -2.1597\n",
      "  Batch [1220/1299] D_loss: -0.0452, G_loss: 0.2613\n",
      "  Batch [1230/1299] D_loss: -0.0694, G_loss: 0.3785\n",
      "  Batch [1240/1299] D_loss: -0.0915, G_loss: 0.4365\n",
      "  Batch [1250/1299] D_loss: -0.1291, G_loss: 0.4363\n",
      "  Batch [1260/1299] D_loss: -0.0294, G_loss: 0.3555\n",
      "  Batch [1270/1299] D_loss: -0.0478, G_loss: 0.2924\n",
      "  Batch [1280/1299] D_loss: -0.6750, G_loss: -0.5192\n",
      "  Batch [1290/1299] D_loss: -0.0238, G_loss: 0.1975\n",
      "\n",
      "Epoch 60 Summary:\n",
      "  Average D_loss: -0.1431\n",
      "  Average G_loss: -0.0516\n",
      "\n",
      "Epoch [61/100]\n",
      "  Batch [0/1299] D_loss: -1.2928, G_loss: -2.5279\n",
      "  Batch [10/1299] D_loss: -0.0787, G_loss: 0.0814\n",
      "  Batch [20/1299] D_loss: -1.4543, G_loss: -1.8125\n",
      "  Batch [30/1299] D_loss: -0.2217, G_loss: -0.0701\n",
      "  Batch [40/1299] D_loss: -0.0908, G_loss: 0.2759\n",
      "  Batch [50/1299] D_loss: -0.1173, G_loss: 0.4939\n",
      "  Batch [60/1299] D_loss: -0.1061, G_loss: 0.6073\n",
      "  Batch [70/1299] D_loss: -0.2098, G_loss: 0.6875\n",
      "  Batch [80/1299] D_loss: -0.0895, G_loss: 0.5099\n",
      "  Batch [90/1299] D_loss: -0.0458, G_loss: 0.3616\n",
      "  Batch [100/1299] D_loss: -0.0647, G_loss: 0.2798\n",
      "  Batch [110/1299] D_loss: -1.6214, G_loss: -3.4192\n",
      "  Batch [120/1299] D_loss: -0.7994, G_loss: -1.7883\n",
      "  Batch [130/1299] D_loss: -0.3988, G_loss: -0.8765\n",
      "  Batch [140/1299] D_loss: -0.0128, G_loss: 0.3263\n",
      "  Batch [150/1299] D_loss: -0.0080, G_loss: 0.3916\n",
      "  Batch [160/1299] D_loss: -0.0851, G_loss: 0.5062\n",
      "  Batch [170/1299] D_loss: -0.0300, G_loss: 0.4527\n",
      "  Batch [180/1299] D_loss: -0.1317, G_loss: 0.4957\n",
      "  Batch [190/1299] D_loss: -1.2262, G_loss: -0.8266\n",
      "  Batch [200/1299] D_loss: -0.0955, G_loss: -0.1244\n",
      "  Batch [210/1299] D_loss: -0.0266, G_loss: 0.1202\n",
      "  Batch [220/1299] D_loss: -1.0911, G_loss: -0.6741\n",
      "  Batch [230/1299] D_loss: -0.1731, G_loss: -0.0082\n",
      "  Batch [240/1299] D_loss: -0.0623, G_loss: 0.2581\n",
      "  Batch [250/1299] D_loss: -0.1175, G_loss: 0.4473\n",
      "  Batch [260/1299] D_loss: -0.1171, G_loss: 0.4350\n",
      "  Batch [270/1299] D_loss: -0.0827, G_loss: 0.4173\n",
      "  Batch [280/1299] D_loss: -0.0507, G_loss: 0.4180\n",
      "  Batch [290/1299] D_loss: -0.4712, G_loss: -1.0463\n",
      "  Batch [300/1299] D_loss: -0.7531, G_loss: -1.3692\n",
      "  Batch [310/1299] D_loss: -0.1641, G_loss: -0.0253\n",
      "  Batch [320/1299] D_loss: -0.0655, G_loss: 0.2570\n",
      "  Batch [330/1299] D_loss: -0.1414, G_loss: 0.4376\n",
      "  Batch [340/1299] D_loss: -0.1038, G_loss: 0.5556\n",
      "  Batch [350/1299] D_loss: -0.1153, G_loss: 0.5635\n",
      "  Batch [360/1299] D_loss: -0.0613, G_loss: 0.4422\n",
      "  Batch [370/1299] D_loss: -0.0891, G_loss: 0.3059\n",
      "  Batch [380/1299] D_loss: -0.2505, G_loss: -0.1534\n",
      "  Batch [390/1299] D_loss: -0.0230, G_loss: 0.1496\n",
      "  Batch [400/1299] D_loss: -0.0188, G_loss: 0.1478\n",
      "  Batch [410/1299] D_loss: -0.0486, G_loss: 0.1788\n",
      "  Batch [420/1299] D_loss: -0.3972, G_loss: -0.1295\n",
      "  Batch [430/1299] D_loss: -0.1971, G_loss: 0.0407\n",
      "  Batch [440/1299] D_loss: -0.3428, G_loss: -0.0404\n",
      "  Batch [450/1299] D_loss: -0.4606, G_loss: -0.9146\n",
      "  Batch [460/1299] D_loss: -0.5046, G_loss: -0.0990\n",
      "  Batch [470/1299] D_loss: -0.7149, G_loss: 0.0390\n",
      "  Batch [480/1299] D_loss: -1.2658, G_loss: -1.1013\n",
      "  Batch [490/1299] D_loss: -0.0697, G_loss: 0.1874\n",
      "  Batch [500/1299] D_loss: -0.4187, G_loss: 0.0745\n",
      "  Batch [510/1299] D_loss: -0.0405, G_loss: 0.2922\n",
      "  Batch [520/1299] D_loss: -0.0764, G_loss: 0.5608\n",
      "  Batch [530/1299] D_loss: -0.0826, G_loss: 0.5445\n",
      "  Batch [540/1299] D_loss: -0.1289, G_loss: 0.5850\n",
      "  Batch [550/1299] D_loss: -0.8742, G_loss: -1.9733\n",
      "  Batch [560/1299] D_loss: -0.5761, G_loss: 0.1144\n",
      "  Batch [570/1299] D_loss: -0.2705, G_loss: 0.0766\n",
      "  Batch [580/1299] D_loss: -0.5542, G_loss: -0.3280\n",
      "  Batch [590/1299] D_loss: -0.0859, G_loss: 0.1831\n",
      "  Batch [600/1299] D_loss: -0.0366, G_loss: 0.4446\n",
      "  Batch [610/1299] D_loss: -0.1060, G_loss: 0.5121\n",
      "  Batch [620/1299] D_loss: -0.0248, G_loss: 0.5813\n",
      "  Batch [630/1299] D_loss: 0.0208, G_loss: 0.4460\n",
      "  Batch [640/1299] D_loss: -0.0475, G_loss: 0.4261\n",
      "  Batch [650/1299] D_loss: -0.1142, G_loss: 0.2803\n",
      "  Batch [660/1299] D_loss: -1.0698, G_loss: -2.8916\n",
      "  Batch [670/1299] D_loss: -0.5416, G_loss: -0.1106\n",
      "  Batch [680/1299] D_loss: -0.0676, G_loss: 0.1946\n",
      "  Batch [690/1299] D_loss: -0.0579, G_loss: 0.3080\n",
      "  Batch [700/1299] D_loss: -0.0635, G_loss: 0.4062\n",
      "  Batch [710/1299] D_loss: -0.1272, G_loss: 0.5378\n",
      "  Batch [720/1299] D_loss: -0.1278, G_loss: 0.4965\n",
      "  Batch [730/1299] D_loss: -0.1611, G_loss: 0.1514\n",
      "  Batch [740/1299] D_loss: -0.0469, G_loss: 0.1332\n",
      "  Batch [750/1299] D_loss: -0.0429, G_loss: 0.3025\n",
      "  Batch [760/1299] D_loss: -0.1209, G_loss: 0.5339\n",
      "  Batch [770/1299] D_loss: -0.1720, G_loss: 0.6862\n",
      "  Batch [780/1299] D_loss: -0.0814, G_loss: 0.4548\n",
      "  Batch [790/1299] D_loss: -0.0328, G_loss: 0.1805\n",
      "  Batch [800/1299] D_loss: -0.0125, G_loss: 0.0930\n",
      "  Batch [810/1299] D_loss: -1.3665, G_loss: -0.0944\n",
      "  Batch [820/1299] D_loss: -1.0852, G_loss: -1.9689\n",
      "  Batch [830/1299] D_loss: -0.0148, G_loss: 0.0590\n",
      "  Batch [840/1299] D_loss: -0.4584, G_loss: -0.1361\n",
      "  Batch [850/1299] D_loss: -0.7631, G_loss: -0.6700\n",
      "  Batch [860/1299] D_loss: -0.5474, G_loss: -0.2332\n",
      "  Batch [870/1299] D_loss: -0.1402, G_loss: 0.3332\n",
      "  Batch [880/1299] D_loss: -0.0190, G_loss: 0.3729\n",
      "  Batch [890/1299] D_loss: -0.0413, G_loss: 0.5234\n",
      "  Batch [900/1299] D_loss: -0.0299, G_loss: 0.5241\n",
      "  Batch [910/1299] D_loss: -0.0924, G_loss: 0.4518\n",
      "  Batch [920/1299] D_loss: -0.0356, G_loss: 0.3312\n",
      "  Batch [930/1299] D_loss: -1.7184, G_loss: -2.1582\n",
      "  Batch [940/1299] D_loss: -0.0021, G_loss: 0.2111\n",
      "  Batch [950/1299] D_loss: -0.0636, G_loss: 0.3295\n",
      "  Batch [960/1299] D_loss: -0.0738, G_loss: 0.3275\n",
      "  Batch [970/1299] D_loss: -0.0321, G_loss: 0.2282\n",
      "  Batch [980/1299] D_loss: -0.2214, G_loss: -0.3637\n",
      "  Batch [990/1299] D_loss: -0.0450, G_loss: 0.0924\n",
      "  Batch [1000/1299] D_loss: -0.0465, G_loss: 0.3209\n",
      "  Batch [1010/1299] D_loss: -0.1255, G_loss: 0.4606\n",
      "  Batch [1020/1299] D_loss: -0.1827, G_loss: 0.6337\n",
      "  Batch [1030/1299] D_loss: -0.1203, G_loss: 0.7192\n",
      "  Batch [1040/1299] D_loss: -0.0471, G_loss: 0.5137\n",
      "  Batch [1050/1299] D_loss: -0.0275, G_loss: 0.2549\n",
      "  Batch [1060/1299] D_loss: -0.2999, G_loss: -0.3745\n",
      "  Batch [1070/1299] D_loss: -0.0377, G_loss: 0.1897\n",
      "  Batch [1080/1299] D_loss: -0.0374, G_loss: 0.3263\n",
      "  Batch [1090/1299] D_loss: -0.1220, G_loss: 0.4904\n",
      "  Batch [1100/1299] D_loss: -0.0656, G_loss: 0.5678\n",
      "  Batch [1110/1299] D_loss: -0.1151, G_loss: 0.4981\n",
      "  Batch [1120/1299] D_loss: -0.0740, G_loss: 0.3680\n",
      "  Batch [1130/1299] D_loss: -0.0548, G_loss: 0.2036\n",
      "  Batch [1140/1299] D_loss: -0.0180, G_loss: 0.1035\n",
      "  Batch [1150/1299] D_loss: -0.0585, G_loss: 0.1883\n",
      "  Batch [1160/1299] D_loss: 0.0103, G_loss: 0.1647\n",
      "  Batch [1170/1299] D_loss: -1.7152, G_loss: -2.1469\n",
      "  Batch [1180/1299] D_loss: -0.6214, G_loss: -1.3531\n",
      "  Batch [1190/1299] D_loss: -0.0219, G_loss: 0.1961\n",
      "  Batch [1200/1299] D_loss: -0.0532, G_loss: 0.3130\n",
      "  Batch [1210/1299] D_loss: -0.0859, G_loss: 0.3619\n",
      "  Batch [1220/1299] D_loss: -0.1626, G_loss: 0.4649\n",
      "  Batch [1230/1299] D_loss: -0.0372, G_loss: 0.5438\n",
      "  Batch [1240/1299] D_loss: -0.0275, G_loss: 0.1647\n",
      "  Batch [1250/1299] D_loss: -0.8197, G_loss: -0.2879\n",
      "  Batch [1260/1299] D_loss: -0.0193, G_loss: 0.1911\n",
      "  Batch [1270/1299] D_loss: -0.0197, G_loss: 0.1665\n",
      "  Batch [1280/1299] D_loss: -0.0593, G_loss: 0.2361\n",
      "  Batch [1290/1299] D_loss: -1.5039, G_loss: -1.4220\n",
      "\n",
      "Epoch 61 Summary:\n",
      "  Average D_loss: -0.1532\n",
      "  Average G_loss: -0.0915\n",
      "\n",
      "Epoch [62/100]\n",
      "  Batch [0/1299] D_loss: -0.1779, G_loss: 0.0760\n",
      "  Batch [10/1299] D_loss: -0.2830, G_loss: 0.0083\n",
      "  Batch [20/1299] D_loss: -0.0195, G_loss: 0.2419\n",
      "  Batch [30/1299] D_loss: -0.0308, G_loss: 0.4940\n",
      "  Batch [40/1299] D_loss: -0.2502, G_loss: 0.6902\n",
      "  Batch [50/1299] D_loss: -0.0819, G_loss: 0.5761\n",
      "  Batch [60/1299] D_loss: -0.0486, G_loss: 0.2703\n",
      "  Batch [70/1299] D_loss: -1.6482, G_loss: -2.1799\n",
      "  Batch [80/1299] D_loss: -0.0271, G_loss: 0.1542\n",
      "  Batch [90/1299] D_loss: -0.0197, G_loss: 0.2558\n",
      "  Batch [100/1299] D_loss: -0.0283, G_loss: 0.2998\n",
      "  Batch [110/1299] D_loss: -0.0786, G_loss: 0.3420\n",
      "  Batch [120/1299] D_loss: -2.4737, G_loss: -6.2980\n",
      "  Batch [130/1299] D_loss: -0.0307, G_loss: 0.0519\n",
      "  Batch [140/1299] D_loss: -0.9220, G_loss: -0.7464\n",
      "  Batch [150/1299] D_loss: -0.3192, G_loss: -0.4124\n",
      "  Batch [160/1299] D_loss: -0.1091, G_loss: 0.4093\n",
      "  Batch [170/1299] D_loss: -0.1301, G_loss: 0.6038\n",
      "  Batch [180/1299] D_loss: 0.0054, G_loss: 0.3775\n",
      "  Batch [190/1299] D_loss: -0.1261, G_loss: 0.5734\n",
      "  Batch [200/1299] D_loss: -0.0487, G_loss: 0.3629\n",
      "  Batch [210/1299] D_loss: -1.9355, G_loss: -3.1052\n",
      "  Batch [220/1299] D_loss: -1.1597, G_loss: -0.2253\n",
      "  Batch [230/1299] D_loss: -0.0469, G_loss: 0.1638\n",
      "  Batch [240/1299] D_loss: 0.0106, G_loss: 0.2921\n",
      "  Batch [250/1299] D_loss: -0.0652, G_loss: 0.4185\n",
      "  Batch [260/1299] D_loss: -0.1556, G_loss: 0.4648\n",
      "  Batch [270/1299] D_loss: -0.0768, G_loss: 0.4418\n",
      "  Batch [280/1299] D_loss: -2.2920, G_loss: -2.1816\n",
      "  Batch [290/1299] D_loss: -0.9548, G_loss: -0.8478\n",
      "  Batch [300/1299] D_loss: -0.3887, G_loss: -0.1607\n",
      "  Batch [310/1299] D_loss: -0.4416, G_loss: -0.6296\n",
      "  Batch [320/1299] D_loss: -0.3736, G_loss: 0.0106\n",
      "  Batch [330/1299] D_loss: -0.0433, G_loss: 0.1586\n",
      "  Batch [340/1299] D_loss: -0.1474, G_loss: 0.3748\n",
      "  Batch [350/1299] D_loss: -0.1103, G_loss: 0.5481\n",
      "  Batch [360/1299] D_loss: -0.1353, G_loss: 0.5513\n",
      "  Batch [370/1299] D_loss: -0.1961, G_loss: 0.4621\n",
      "  Batch [380/1299] D_loss: -0.0537, G_loss: 0.3255\n",
      "  Batch [390/1299] D_loss: -1.5384, G_loss: -2.8202\n",
      "  Batch [400/1299] D_loss: -0.1783, G_loss: 0.0957\n",
      "  Batch [410/1299] D_loss: -0.0507, G_loss: 0.2770\n",
      "  Batch [420/1299] D_loss: -0.0707, G_loss: 0.3557\n",
      "  Batch [430/1299] D_loss: -0.0159, G_loss: 0.3145\n",
      "  Batch [440/1299] D_loss: -0.0294, G_loss: 0.2170\n",
      "  Batch [450/1299] D_loss: -0.6169, G_loss: -0.1484\n",
      "  Batch [460/1299] D_loss: -1.3257, G_loss: -0.8357\n",
      "  Batch [470/1299] D_loss: 0.0307, G_loss: 0.2517\n",
      "  Batch [480/1299] D_loss: -0.1414, G_loss: 0.3984\n",
      "  Batch [490/1299] D_loss: -0.1431, G_loss: 0.4199\n",
      "  Batch [500/1299] D_loss: -0.0756, G_loss: 0.4544\n",
      "  Batch [510/1299] D_loss: -0.1014, G_loss: 0.3102\n",
      "  Batch [520/1299] D_loss: -0.0783, G_loss: 0.2020\n",
      "  Batch [530/1299] D_loss: -0.0081, G_loss: 0.1060\n",
      "  Batch [540/1299] D_loss: 0.0152, G_loss: 0.2237\n",
      "  Batch [550/1299] D_loss: -0.0415, G_loss: 0.2756\n",
      "  Batch [560/1299] D_loss: 0.0245, G_loss: 0.3560\n",
      "  Batch [570/1299] D_loss: -0.0166, G_loss: 0.1958\n",
      "  Batch [580/1299] D_loss: -0.5051, G_loss: -0.9302\n",
      "  Batch [590/1299] D_loss: -0.1654, G_loss: 0.0280\n",
      "  Batch [600/1299] D_loss: -0.6163, G_loss: -0.3743\n",
      "  Batch [610/1299] D_loss: -0.4252, G_loss: -1.0867\n",
      "  Batch [620/1299] D_loss: -0.3066, G_loss: -0.7161\n",
      "  Batch [630/1299] D_loss: -0.0329, G_loss: 0.2785\n",
      "  Batch [640/1299] D_loss: -0.0620, G_loss: 0.4307\n",
      "  Batch [650/1299] D_loss: -0.1077, G_loss: 0.5537\n",
      "  Batch [660/1299] D_loss: -0.0035, G_loss: 0.4885\n",
      "  Batch [670/1299] D_loss: -0.0538, G_loss: 0.3727\n",
      "  Batch [680/1299] D_loss: -0.0800, G_loss: 0.1396\n",
      "  Batch [690/1299] D_loss: -0.0825, G_loss: 0.0467\n",
      "  Batch [700/1299] D_loss: 0.0116, G_loss: 0.2402\n",
      "  Batch [710/1299] D_loss: -0.0344, G_loss: 0.3258\n",
      "  Batch [720/1299] D_loss: -0.0737, G_loss: 0.2918\n",
      "  Batch [730/1299] D_loss: -0.0642, G_loss: 0.2858\n",
      "  Batch [740/1299] D_loss: -2.4949, G_loss: -4.0454\n",
      "  Batch [750/1299] D_loss: 0.0523, G_loss: 0.1075\n",
      "  Batch [760/1299] D_loss: -0.0118, G_loss: 0.2332\n",
      "  Batch [770/1299] D_loss: -0.0232, G_loss: 0.3023\n",
      "  Batch [780/1299] D_loss: -0.7521, G_loss: -0.1798\n",
      "  Batch [790/1299] D_loss: -0.0698, G_loss: 0.0438\n",
      "  Batch [800/1299] D_loss: -0.9180, G_loss: -2.2138\n",
      "  Batch [810/1299] D_loss: -0.2593, G_loss: -0.0315\n",
      "  Batch [820/1299] D_loss: -0.0927, G_loss: 0.3140\n",
      "  Batch [830/1299] D_loss: -0.1351, G_loss: 0.3571\n",
      "  Batch [840/1299] D_loss: -0.0263, G_loss: 0.3934\n",
      "  Batch [850/1299] D_loss: -0.0594, G_loss: 0.2339\n",
      "  Batch [860/1299] D_loss: 0.0293, G_loss: 0.0694\n",
      "  Batch [870/1299] D_loss: -0.4259, G_loss: -0.1920\n",
      "  Batch [880/1299] D_loss: -0.3558, G_loss: 0.0596\n",
      "  Batch [890/1299] D_loss: -0.4997, G_loss: -0.4054\n",
      "  Batch [900/1299] D_loss: 0.0028, G_loss: 0.2797\n",
      "  Batch [910/1299] D_loss: -0.1354, G_loss: 0.4587\n",
      "  Batch [920/1299] D_loss: -0.0560, G_loss: 0.4903\n",
      "  Batch [930/1299] D_loss: -0.1446, G_loss: 0.5872\n",
      "  Batch [940/1299] D_loss: -0.0939, G_loss: 0.5169\n",
      "  Batch [950/1299] D_loss: -0.0006, G_loss: 0.2568\n",
      "  Batch [960/1299] D_loss: -1.0655, G_loss: -1.5002\n",
      "  Batch [970/1299] D_loss: -0.6491, G_loss: -0.4662\n",
      "  Batch [980/1299] D_loss: 0.0229, G_loss: 0.1701\n",
      "  Batch [990/1299] D_loss: -0.0001, G_loss: 0.2462\n",
      "  Batch [1000/1299] D_loss: -0.0001, G_loss: 0.1493\n",
      "  Batch [1010/1299] D_loss: -1.2597, G_loss: -0.6530\n",
      "  Batch [1020/1299] D_loss: -0.1197, G_loss: 0.0800\n",
      "  Batch [1030/1299] D_loss: -0.1564, G_loss: 0.0134\n",
      "  Batch [1040/1299] D_loss: -0.1146, G_loss: 0.1486\n",
      "  Batch [1050/1299] D_loss: -0.1326, G_loss: 0.4348\n",
      "  Batch [1060/1299] D_loss: -0.0328, G_loss: 0.4227\n",
      "  Batch [1070/1299] D_loss: -0.0888, G_loss: 0.4495\n",
      "  Batch [1080/1299] D_loss: -0.0805, G_loss: 0.4864\n",
      "  Batch [1090/1299] D_loss: -0.0782, G_loss: 0.3911\n",
      "  Batch [1100/1299] D_loss: -1.2048, G_loss: -1.0517\n",
      "  Batch [1110/1299] D_loss: -0.0263, G_loss: 0.0897\n",
      "  Batch [1120/1299] D_loss: -0.0336, G_loss: 0.1611\n",
      "  Batch [1130/1299] D_loss: -0.0561, G_loss: 0.3200\n",
      "  Batch [1140/1299] D_loss: -0.0217, G_loss: 0.5334\n",
      "  Batch [1150/1299] D_loss: -0.1108, G_loss: 0.5885\n",
      "  Batch [1160/1299] D_loss: -0.0488, G_loss: 0.4989\n",
      "  Batch [1170/1299] D_loss: -0.0592, G_loss: 0.3134\n",
      "  Batch [1180/1299] D_loss: -2.3533, G_loss: -1.5369\n",
      "  Batch [1190/1299] D_loss: -1.2220, G_loss: -0.6198\n",
      "  Batch [1200/1299] D_loss: -0.0059, G_loss: 0.0780\n",
      "  Batch [1210/1299] D_loss: -0.8888, G_loss: -0.0694\n",
      "  Batch [1220/1299] D_loss: -0.2473, G_loss: -0.0765\n",
      "  Batch [1230/1299] D_loss: -1.1235, G_loss: -2.4322\n",
      "  Batch [1240/1299] D_loss: -0.6437, G_loss: 0.0333\n",
      "  Batch [1250/1299] D_loss: -0.3962, G_loss: -0.2460\n",
      "  Batch [1260/1299] D_loss: -0.0797, G_loss: 0.3152\n",
      "  Batch [1270/1299] D_loss: -0.0529, G_loss: 0.5227\n",
      "  Batch [1280/1299] D_loss: -0.0316, G_loss: 0.6652\n",
      "  Batch [1290/1299] D_loss: -0.0530, G_loss: 0.5551\n",
      "\n",
      "Epoch 62 Summary:\n",
      "  Average D_loss: -0.1471\n",
      "  Average G_loss: -0.0634\n",
      "\n",
      "Epoch [63/100]\n",
      "  Batch [0/1299] D_loss: -0.0555, G_loss: 0.5408\n",
      "  Batch [10/1299] D_loss: -0.0580, G_loss: 0.4925\n",
      "  Batch [20/1299] D_loss: -2.7158, G_loss: -4.5006\n",
      "  Batch [30/1299] D_loss: -1.1151, G_loss: -2.8242\n",
      "  Batch [40/1299] D_loss: -0.0840, G_loss: 0.1061\n",
      "  Batch [50/1299] D_loss: -0.0689, G_loss: 0.0694\n",
      "  Batch [60/1299] D_loss: -1.0246, G_loss: -2.9858\n",
      "  Batch [70/1299] D_loss: -1.0862, G_loss: -0.8447\n",
      "  Batch [80/1299] D_loss: -0.7487, G_loss: -0.4176\n",
      "  Batch [90/1299] D_loss: -1.3038, G_loss: -3.8797\n",
      "  Batch [100/1299] D_loss: -0.0882, G_loss: 0.1495\n",
      "  Batch [110/1299] D_loss: -0.1926, G_loss: 0.4408\n",
      "  Batch [120/1299] D_loss: -0.1574, G_loss: 0.5655\n",
      "  Batch [130/1299] D_loss: -0.0051, G_loss: 0.6097\n",
      "  Batch [140/1299] D_loss: -0.2594, G_loss: 0.6713\n",
      "  Batch [150/1299] D_loss: 0.0420, G_loss: 0.4140\n",
      "  Batch [160/1299] D_loss: -0.0847, G_loss: 0.2532\n",
      "  Batch [170/1299] D_loss: -0.9963, G_loss: -1.6167\n",
      "  Batch [180/1299] D_loss: 0.0043, G_loss: 0.1066\n",
      "  Batch [190/1299] D_loss: -0.6996, G_loss: -0.2746\n",
      "  Batch [200/1299] D_loss: -0.3164, G_loss: -0.0001\n",
      "  Batch [210/1299] D_loss: -0.3783, G_loss: 0.1722\n",
      "  Batch [220/1299] D_loss: -0.0827, G_loss: 0.3206\n",
      "  Batch [230/1299] D_loss: -0.0642, G_loss: 0.4684\n",
      "  Batch [240/1299] D_loss: -0.0851, G_loss: 0.4483\n",
      "  Batch [250/1299] D_loss: -0.1535, G_loss: 0.5585\n",
      "  Batch [260/1299] D_loss: -0.1418, G_loss: 0.5255\n",
      "  Batch [270/1299] D_loss: -0.8908, G_loss: -2.0659\n",
      "  Batch [280/1299] D_loss: -0.8989, G_loss: -0.3330\n",
      "  Batch [290/1299] D_loss: -0.9311, G_loss: -2.4058\n",
      "  Batch [300/1299] D_loss: -0.0701, G_loss: 0.0340\n",
      "  Batch [310/1299] D_loss: -0.0263, G_loss: 0.1799\n",
      "  Batch [320/1299] D_loss: -0.1212, G_loss: 0.2985\n",
      "  Batch [330/1299] D_loss: -0.1131, G_loss: 0.4235\n",
      "  Batch [340/1299] D_loss: -0.0766, G_loss: 0.3714\n",
      "  Batch [350/1299] D_loss: -0.1933, G_loss: 0.3647\n",
      "  Batch [360/1299] D_loss: -0.0782, G_loss: 0.2894\n",
      "  Batch [370/1299] D_loss: -0.5216, G_loss: -0.0487\n",
      "  Batch [380/1299] D_loss: -0.7091, G_loss: -0.7316\n",
      "  Batch [390/1299] D_loss: -0.2937, G_loss: -0.3355\n",
      "  Batch [400/1299] D_loss: -0.9223, G_loss: -0.4762\n",
      "  Batch [410/1299] D_loss: -0.7741, G_loss: -2.2657\n",
      "  Batch [420/1299] D_loss: -0.4091, G_loss: -0.1531\n",
      "  Batch [430/1299] D_loss: -0.0856, G_loss: 0.4904\n",
      "  Batch [440/1299] D_loss: -0.1377, G_loss: 0.6072\n",
      "  Batch [450/1299] D_loss: -0.2105, G_loss: 0.8617\n",
      "  Batch [460/1299] D_loss: -0.1754, G_loss: 0.6876\n",
      "  Batch [470/1299] D_loss: -0.0782, G_loss: 0.5502\n",
      "  Batch [480/1299] D_loss: -0.0378, G_loss: 0.2725\n",
      "  Batch [490/1299] D_loss: -1.3207, G_loss: 0.1196\n",
      "  Batch [500/1299] D_loss: 0.0167, G_loss: 0.1407\n",
      "  Batch [510/1299] D_loss: -0.4373, G_loss: -0.6494\n",
      "  Batch [520/1299] D_loss: -0.0486, G_loss: 0.1089\n",
      "  Batch [530/1299] D_loss: -0.0322, G_loss: 0.2795\n",
      "  Batch [540/1299] D_loss: -0.1398, G_loss: 0.4602\n",
      "  Batch [550/1299] D_loss: -0.1520, G_loss: 0.5708\n",
      "  Batch [560/1299] D_loss: -0.1099, G_loss: 0.5304\n",
      "  Batch [570/1299] D_loss: -0.0788, G_loss: 0.4074\n",
      "  Batch [580/1299] D_loss: -3.3419, G_loss: -3.1804\n",
      "  Batch [590/1299] D_loss: -0.3031, G_loss: -0.3243\n",
      "  Batch [600/1299] D_loss: -0.1796, G_loss: 0.0666\n",
      "  Batch [610/1299] D_loss: -1.1405, G_loss: -0.6715\n",
      "  Batch [620/1299] D_loss: -0.3617, G_loss: -0.5586\n",
      "  Batch [630/1299] D_loss: -0.3172, G_loss: -1.2183\n",
      "  Batch [640/1299] D_loss: -0.6628, G_loss: -1.0728\n",
      "  Batch [650/1299] D_loss: -0.8207, G_loss: -1.3437\n",
      "  Batch [660/1299] D_loss: -0.7591, G_loss: -0.4121\n",
      "  Batch [670/1299] D_loss: -0.1678, G_loss: 0.0358\n",
      "  Batch [680/1299] D_loss: -0.7056, G_loss: -0.3371\n",
      "  Batch [690/1299] D_loss: -0.2407, G_loss: 0.4235\n",
      "  Batch [700/1299] D_loss: -0.3199, G_loss: 0.7223\n",
      "  Batch [710/1299] D_loss: -0.3100, G_loss: 0.8703\n",
      "  Batch [720/1299] D_loss: -0.2111, G_loss: 1.0170\n",
      "  Batch [730/1299] D_loss: -0.1594, G_loss: 0.7428\n",
      "  Batch [740/1299] D_loss: -0.0292, G_loss: 0.4724\n",
      "  Batch [750/1299] D_loss: -2.1860, G_loss: -1.6915\n",
      "  Batch [760/1299] D_loss: -1.4702, G_loss: -2.7767\n",
      "  Batch [770/1299] D_loss: -0.5711, G_loss: -1.0959\n",
      "  Batch [780/1299] D_loss: -0.1203, G_loss: -0.0456\n",
      "  Batch [790/1299] D_loss: 0.0420, G_loss: 0.1265\n",
      "  Batch [800/1299] D_loss: -0.6066, G_loss: -0.0702\n",
      "  Batch [810/1299] D_loss: -0.2834, G_loss: 0.0140\n",
      "  Batch [820/1299] D_loss: -0.1549, G_loss: 0.3219\n",
      "  Batch [830/1299] D_loss: -0.1519, G_loss: 0.4636\n",
      "  Batch [840/1299] D_loss: -0.1769, G_loss: 0.7245\n",
      "  Batch [850/1299] D_loss: -0.1607, G_loss: 0.5765\n",
      "  Batch [860/1299] D_loss: -0.1399, G_loss: 0.4913\n",
      "  Batch [870/1299] D_loss: -0.1344, G_loss: 0.4179\n",
      "  Batch [880/1299] D_loss: -0.1230, G_loss: -0.2467\n",
      "  Batch [890/1299] D_loss: -0.0804, G_loss: 0.1516\n",
      "  Batch [900/1299] D_loss: -0.0015, G_loss: 0.2863\n",
      "  Batch [910/1299] D_loss: -0.1049, G_loss: 0.3430\n",
      "  Batch [920/1299] D_loss: -0.0862, G_loss: 0.4611\n",
      "  Batch [930/1299] D_loss: -0.0842, G_loss: 0.2979\n",
      "  Batch [940/1299] D_loss: -0.2536, G_loss: 0.0476\n",
      "  Batch [950/1299] D_loss: -0.0227, G_loss: 0.1991\n",
      "  Batch [960/1299] D_loss: -0.0331, G_loss: 0.2636\n",
      "  Batch [970/1299] D_loss: -0.1409, G_loss: 0.4937\n",
      "  Batch [980/1299] D_loss: 0.0550, G_loss: 0.3593\n",
      "  Batch [990/1299] D_loss: -0.0725, G_loss: 0.4952\n",
      "  Batch [1000/1299] D_loss: -0.0279, G_loss: 0.1835\n",
      "  Batch [1010/1299] D_loss: -0.1126, G_loss: 0.0093\n",
      "  Batch [1020/1299] D_loss: 0.0112, G_loss: 0.1910\n",
      "  Batch [1030/1299] D_loss: -0.0581, G_loss: 0.3056\n",
      "  Batch [1040/1299] D_loss: -0.1165, G_loss: 0.5540\n",
      "  Batch [1050/1299] D_loss: -0.0637, G_loss: 0.4673\n",
      "  Batch [1060/1299] D_loss: -0.0221, G_loss: 0.3051\n",
      "  Batch [1070/1299] D_loss: -0.0350, G_loss: 0.2710\n",
      "  Batch [1080/1299] D_loss: -1.0310, G_loss: -1.2048\n",
      "  Batch [1090/1299] D_loss: -0.4815, G_loss: -0.3993\n",
      "  Batch [1100/1299] D_loss: -0.0595, G_loss: 0.3380\n",
      "  Batch [1110/1299] D_loss: -0.0080, G_loss: 0.3177\n",
      "  Batch [1120/1299] D_loss: -0.0310, G_loss: 0.4694\n",
      "  Batch [1130/1299] D_loss: -0.0143, G_loss: 0.3837\n",
      "  Batch [1140/1299] D_loss: -0.0219, G_loss: 0.3787\n",
      "  Batch [1150/1299] D_loss: -0.0312, G_loss: 0.2365\n",
      "  Batch [1160/1299] D_loss: -0.3217, G_loss: -0.1533\n",
      "  Batch [1170/1299] D_loss: -0.1782, G_loss: -0.0971\n",
      "  Batch [1180/1299] D_loss: -0.6925, G_loss: -0.4109\n",
      "  Batch [1190/1299] D_loss: 0.0463, G_loss: 0.1244\n",
      "  Batch [1200/1299] D_loss: -0.0209, G_loss: 0.1898\n",
      "  Batch [1210/1299] D_loss: -0.0809, G_loss: 0.2287\n",
      "  Batch [1220/1299] D_loss: -0.1263, G_loss: 0.2669\n",
      "  Batch [1230/1299] D_loss: -1.7458, G_loss: -2.1996\n",
      "  Batch [1240/1299] D_loss: -0.1755, G_loss: -0.0430\n",
      "  Batch [1250/1299] D_loss: -0.0427, G_loss: 0.1972\n",
      "  Batch [1260/1299] D_loss: -0.0967, G_loss: 0.2937\n",
      "  Batch [1270/1299] D_loss: -0.1357, G_loss: 0.3603\n",
      "  Batch [1280/1299] D_loss: -0.0561, G_loss: 0.3523\n",
      "  Batch [1290/1299] D_loss: -0.0102, G_loss: 0.2078\n",
      "\n",
      "Epoch 63 Summary:\n",
      "  Average D_loss: -0.1676\n",
      "  Average G_loss: -0.0931\n",
      "\n",
      "Epoch [64/100]\n",
      "  Batch [0/1299] D_loss: -0.0891, G_loss: 0.0289\n",
      "  Batch [10/1299] D_loss: -0.0357, G_loss: 0.1120\n",
      "  Batch [20/1299] D_loss: -0.0488, G_loss: 0.2968\n",
      "  Batch [30/1299] D_loss: -0.0091, G_loss: 0.3969\n",
      "  Batch [40/1299] D_loss: -0.1194, G_loss: 0.5666\n",
      "  Batch [50/1299] D_loss: -0.0124, G_loss: 0.3788\n",
      "  Batch [60/1299] D_loss: -0.0705, G_loss: 0.2900\n",
      "  Batch [70/1299] D_loss: -0.2382, G_loss: -0.6806\n",
      "  Batch [80/1299] D_loss: -0.5813, G_loss: -0.4665\n",
      "  Batch [90/1299] D_loss: -0.3234, G_loss: -0.9391\n",
      "  Batch [100/1299] D_loss: -0.0347, G_loss: 0.1016\n",
      "  Batch [110/1299] D_loss: -0.0393, G_loss: 0.2572\n",
      "  Batch [120/1299] D_loss: -0.0711, G_loss: 0.3668\n",
      "  Batch [130/1299] D_loss: -0.0632, G_loss: 0.5976\n",
      "  Batch [140/1299] D_loss: -0.0439, G_loss: 0.5157\n",
      "  Batch [150/1299] D_loss: -0.0941, G_loss: 0.3939\n",
      "  Batch [160/1299] D_loss: -0.0378, G_loss: 0.2065\n",
      "  Batch [170/1299] D_loss: -0.0694, G_loss: 0.0377\n",
      "  Batch [180/1299] D_loss: -0.4646, G_loss: -1.5164\n",
      "  Batch [190/1299] D_loss: -0.2396, G_loss: -0.0380\n",
      "  Batch [200/1299] D_loss: -0.1686, G_loss: -0.1059\n",
      "  Batch [210/1299] D_loss: -0.0730, G_loss: 0.1032\n",
      "  Batch [220/1299] D_loss: -0.1049, G_loss: 0.2740\n",
      "  Batch [230/1299] D_loss: -0.0749, G_loss: 0.3936\n",
      "  Batch [240/1299] D_loss: -0.1180, G_loss: 0.5733\n",
      "  Batch [250/1299] D_loss: -0.0576, G_loss: 0.5043\n",
      "  Batch [260/1299] D_loss: -0.0514, G_loss: 0.4502\n",
      "  Batch [270/1299] D_loss: -0.0007, G_loss: 0.3107\n",
      "  Batch [280/1299] D_loss: 0.0141, G_loss: 0.1574\n",
      "  Batch [290/1299] D_loss: -0.0303, G_loss: 0.3048\n",
      "  Batch [300/1299] D_loss: -0.0740, G_loss: 0.3596\n",
      "  Batch [310/1299] D_loss: -0.0557, G_loss: 0.3072\n",
      "  Batch [320/1299] D_loss: -0.0940, G_loss: 0.1761\n",
      "  Batch [330/1299] D_loss: 0.0262, G_loss: -0.0745\n",
      "  Batch [340/1299] D_loss: -0.0316, G_loss: 0.2306\n",
      "  Batch [350/1299] D_loss: -0.0315, G_loss: 0.2901\n",
      "  Batch [360/1299] D_loss: -0.0369, G_loss: 0.2957\n",
      "  Batch [370/1299] D_loss: -0.0463, G_loss: 0.2708\n",
      "  Batch [380/1299] D_loss: -0.0376, G_loss: 0.1480\n",
      "  Batch [390/1299] D_loss: -0.2802, G_loss: -0.0844\n",
      "  Batch [400/1299] D_loss: -0.0625, G_loss: 0.1581\n",
      "  Batch [410/1299] D_loss: -0.0856, G_loss: 0.2540\n",
      "  Batch [420/1299] D_loss: -0.0422, G_loss: 0.1931\n",
      "  Batch [430/1299] D_loss: -0.0380, G_loss: 0.3154\n",
      "  Batch [440/1299] D_loss: -0.0099, G_loss: 0.2486\n",
      "  Batch [450/1299] D_loss: -0.6373, G_loss: -0.5298\n",
      "  Batch [460/1299] D_loss: -0.0232, G_loss: 0.1032\n",
      "  Batch [470/1299] D_loss: -1.0696, G_loss: -0.3913\n",
      "  Batch [480/1299] D_loss: -0.0336, G_loss: 0.1314\n",
      "  Batch [490/1299] D_loss: -0.0304, G_loss: 0.2357\n",
      "  Batch [500/1299] D_loss: -0.0947, G_loss: 0.2582\n",
      "  Batch [510/1299] D_loss: -0.0435, G_loss: 0.3013\n",
      "  Batch [520/1299] D_loss: -0.0546, G_loss: 0.2720\n",
      "  Batch [530/1299] D_loss: -0.0738, G_loss: 0.2371\n",
      "  Batch [540/1299] D_loss: -0.0246, G_loss: 0.1054\n",
      "  Batch [550/1299] D_loss: -0.0408, G_loss: 0.2418\n",
      "  Batch [560/1299] D_loss: -0.0458, G_loss: 0.3715\n",
      "  Batch [570/1299] D_loss: -0.0599, G_loss: 0.4121\n",
      "  Batch [580/1299] D_loss: 0.0229, G_loss: 0.2592\n",
      "  Batch [590/1299] D_loss: -0.3162, G_loss: -1.2321\n",
      "  Batch [600/1299] D_loss: -0.0497, G_loss: 0.0793\n",
      "  Batch [610/1299] D_loss: 0.0114, G_loss: 0.1029\n",
      "  Batch [620/1299] D_loss: 0.0199, G_loss: 0.2389\n",
      "  Batch [630/1299] D_loss: -0.0432, G_loss: 0.3127\n",
      "  Batch [640/1299] D_loss: -0.0756, G_loss: 0.3254\n",
      "  Batch [650/1299] D_loss: -0.0402, G_loss: 0.2868\n",
      "  Batch [660/1299] D_loss: -0.0327, G_loss: 0.2277\n",
      "  Batch [670/1299] D_loss: -0.4630, G_loss: -0.5034\n",
      "  Batch [680/1299] D_loss: -0.0426, G_loss: 0.1042\n",
      "  Batch [690/1299] D_loss: -0.0345, G_loss: 0.2087\n",
      "  Batch [700/1299] D_loss: -0.1266, G_loss: 0.3086\n",
      "  Batch [710/1299] D_loss: 0.0514, G_loss: 0.3074\n",
      "  Batch [720/1299] D_loss: -0.0213, G_loss: 0.2799\n",
      "  Batch [730/1299] D_loss: -0.0034, G_loss: 0.0973\n",
      "  Batch [740/1299] D_loss: 0.0001, G_loss: 0.1484\n",
      "  Batch [750/1299] D_loss: -0.0331, G_loss: 0.1958\n",
      "  Batch [760/1299] D_loss: -0.0702, G_loss: 0.2394\n",
      "  Batch [770/1299] D_loss: -0.1022, G_loss: 0.2908\n",
      "  Batch [780/1299] D_loss: -0.7714, G_loss: -0.1152\n",
      "  Batch [790/1299] D_loss: -0.1613, G_loss: -0.2438\n",
      "  Batch [800/1299] D_loss: -0.7999, G_loss: -1.3793\n",
      "  Batch [810/1299] D_loss: -0.0140, G_loss: 0.1243\n",
      "  Batch [820/1299] D_loss: -0.0318, G_loss: 0.2394\n",
      "  Batch [830/1299] D_loss: -0.1827, G_loss: 0.4410\n",
      "  Batch [840/1299] D_loss: -0.0964, G_loss: 0.5525\n",
      "  Batch [850/1299] D_loss: 0.0739, G_loss: 0.3778\n",
      "  Batch [860/1299] D_loss: -0.1306, G_loss: 0.3517\n",
      "  Batch [870/1299] D_loss: 0.0068, G_loss: 0.2057\n",
      "  Batch [880/1299] D_loss: 0.0189, G_loss: 0.0994\n",
      "  Batch [890/1299] D_loss: -0.0264, G_loss: 0.1376\n",
      "  Batch [900/1299] D_loss: -0.0092, G_loss: 0.1697\n",
      "  Batch [910/1299] D_loss: -0.6636, G_loss: -3.2027\n",
      "  Batch [920/1299] D_loss: -1.4933, G_loss: -2.0039\n",
      "  Batch [930/1299] D_loss: -0.0780, G_loss: -0.1321\n",
      "  Batch [940/1299] D_loss: -0.0218, G_loss: 0.2501\n",
      "  Batch [950/1299] D_loss: -0.0279, G_loss: 0.3941\n",
      "  Batch [960/1299] D_loss: -0.1104, G_loss: 0.6036\n",
      "  Batch [970/1299] D_loss: -0.1402, G_loss: 0.5492\n",
      "  Batch [980/1299] D_loss: -0.0030, G_loss: 0.3301\n",
      "  Batch [990/1299] D_loss: -0.0899, G_loss: 0.3200\n",
      "  Batch [1000/1299] D_loss: -0.4055, G_loss: -1.1111\n",
      "  Batch [1010/1299] D_loss: -0.0213, G_loss: 0.0735\n",
      "  Batch [1020/1299] D_loss: -0.7813, G_loss: -0.1451\n",
      "  Batch [1030/1299] D_loss: -0.0457, G_loss: 0.1385\n",
      "  Batch [1040/1299] D_loss: -0.0844, G_loss: 0.3052\n",
      "  Batch [1050/1299] D_loss: -0.1241, G_loss: 0.3549\n",
      "  Batch [1060/1299] D_loss: -0.1894, G_loss: 0.5864\n",
      "  Batch [1070/1299] D_loss: -0.1066, G_loss: 0.4361\n",
      "  Batch [1080/1299] D_loss: -0.0022, G_loss: 0.2971\n",
      "  Batch [1090/1299] D_loss: -0.0409, G_loss: 0.2416\n",
      "  Batch [1100/1299] D_loss: -0.2514, G_loss: -0.1801\n",
      "  Batch [1110/1299] D_loss: -0.0260, G_loss: 0.1777\n",
      "  Batch [1120/1299] D_loss: -1.1839, G_loss: -0.9919\n",
      "  Batch [1130/1299] D_loss: -0.0607, G_loss: 0.1181\n",
      "  Batch [1140/1299] D_loss: -0.0092, G_loss: 0.2199\n",
      "  Batch [1150/1299] D_loss: -0.0129, G_loss: 0.3386\n",
      "  Batch [1160/1299] D_loss: -0.0024, G_loss: 0.3826\n",
      "  Batch [1170/1299] D_loss: -0.0476, G_loss: 0.2384\n",
      "  Batch [1180/1299] D_loss: -0.0371, G_loss: 0.1616\n",
      "  Batch [1190/1299] D_loss: -0.5690, G_loss: -0.4917\n",
      "  Batch [1200/1299] D_loss: 0.0114, G_loss: 0.2135\n",
      "  Batch [1210/1299] D_loss: -0.0811, G_loss: 0.3722\n",
      "  Batch [1220/1299] D_loss: 0.0334, G_loss: 0.2690\n",
      "  Batch [1230/1299] D_loss: -0.0628, G_loss: 0.2993\n",
      "  Batch [1240/1299] D_loss: -2.1563, G_loss: -3.9680\n",
      "  Batch [1250/1299] D_loss: -0.0240, G_loss: 0.1292\n",
      "  Batch [1260/1299] D_loss: -0.0537, G_loss: 0.2235\n",
      "  Batch [1270/1299] D_loss: -0.0531, G_loss: 0.2791\n",
      "  Batch [1280/1299] D_loss: -0.0260, G_loss: 0.2855\n",
      "  Batch [1290/1299] D_loss: -0.0824, G_loss: 0.1207\n",
      "\n",
      "Epoch 64 Summary:\n",
      "  Average D_loss: -0.1212\n",
      "  Average G_loss: -0.0496\n",
      "\n",
      "Epoch [65/100]\n",
      "  Batch [0/1299] D_loss: -1.1059, G_loss: -3.3119\n",
      "  Batch [10/1299] D_loss: -0.0017, G_loss: 0.1084\n",
      "  Batch [20/1299] D_loss: -0.0306, G_loss: 0.1756\n",
      "  Batch [30/1299] D_loss: -0.0581, G_loss: 0.2552\n",
      "  Batch [40/1299] D_loss: -0.0529, G_loss: 0.3217\n",
      "  Batch [50/1299] D_loss: -0.0233, G_loss: 0.3030\n",
      "  Batch [60/1299] D_loss: -1.6115, G_loss: -3.7251\n",
      "  Batch [70/1299] D_loss: -0.3450, G_loss: 0.0026\n",
      "  Batch [80/1299] D_loss: -0.7014, G_loss: -0.8657\n",
      "  Batch [90/1299] D_loss: -1.0569, G_loss: -0.4230\n",
      "  Batch [100/1299] D_loss: -0.7261, G_loss: -0.2904\n",
      "  Batch [110/1299] D_loss: -0.0215, G_loss: 0.1968\n",
      "  Batch [120/1299] D_loss: 0.0556, G_loss: 0.3142\n",
      "  Batch [130/1299] D_loss: 0.0552, G_loss: 0.3485\n",
      "  Batch [140/1299] D_loss: -0.0639, G_loss: 0.5002\n",
      "  Batch [150/1299] D_loss: -0.0799, G_loss: 0.2972\n",
      "  Batch [160/1299] D_loss: -2.2765, G_loss: -3.1827\n",
      "  Batch [170/1299] D_loss: -0.4958, G_loss: -0.0156\n",
      "  Batch [180/1299] D_loss: -0.9042, G_loss: -1.9701\n",
      "  Batch [190/1299] D_loss: -0.5593, G_loss: -0.4645\n",
      "  Batch [200/1299] D_loss: -0.5515, G_loss: -0.2580\n",
      "  Batch [210/1299] D_loss: -0.0694, G_loss: 0.1393\n",
      "  Batch [220/1299] D_loss: -0.5596, G_loss: -0.7325\n",
      "  Batch [230/1299] D_loss: -0.0768, G_loss: 0.2136\n",
      "  Batch [240/1299] D_loss: -0.1331, G_loss: 0.5612\n",
      "  Batch [250/1299] D_loss: 0.0097, G_loss: 0.5861\n",
      "  Batch [260/1299] D_loss: -0.0137, G_loss: 0.5924\n",
      "  Batch [270/1299] D_loss: -0.0704, G_loss: 0.5650\n",
      "  Batch [280/1299] D_loss: -0.0385, G_loss: 0.4371\n",
      "  Batch [290/1299] D_loss: -2.7259, G_loss: -2.3419\n",
      "  Batch [300/1299] D_loss: -0.0888, G_loss: -0.2087\n",
      "  Batch [310/1299] D_loss: -0.9540, G_loss: -1.1039\n",
      "  Batch [320/1299] D_loss: -0.5662, G_loss: -0.2568\n",
      "  Batch [330/1299] D_loss: -0.6154, G_loss: -1.2147\n",
      "  Batch [340/1299] D_loss: -0.9395, G_loss: -1.1349\n",
      "  Batch [350/1299] D_loss: -0.1332, G_loss: 0.0453\n",
      "  Batch [360/1299] D_loss: -1.2347, G_loss: 0.0590\n",
      "  Batch [370/1299] D_loss: -0.0567, G_loss: 0.2278\n",
      "  Batch [380/1299] D_loss: -0.0828, G_loss: 0.3535\n",
      "  Batch [390/1299] D_loss: -0.1697, G_loss: 0.4423\n",
      "  Batch [400/1299] D_loss: -0.1342, G_loss: 0.5799\n",
      "  Batch [410/1299] D_loss: -0.0057, G_loss: 0.4925\n",
      "  Batch [420/1299] D_loss: -0.0094, G_loss: 0.4060\n",
      "  Batch [430/1299] D_loss: -3.8765, G_loss: -5.7834\n",
      "  Batch [440/1299] D_loss: -0.9679, G_loss: -0.5850\n",
      "  Batch [450/1299] D_loss: -0.5863, G_loss: -1.7967\n",
      "  Batch [460/1299] D_loss: -0.8167, G_loss: -0.9839\n",
      "  Batch [470/1299] D_loss: -0.2677, G_loss: 0.1498\n",
      "  Batch [480/1299] D_loss: -0.0374, G_loss: 0.2448\n",
      "  Batch [490/1299] D_loss: -0.0371, G_loss: 0.4675\n",
      "  Batch [500/1299] D_loss: -0.0598, G_loss: 0.4840\n",
      "  Batch [510/1299] D_loss: -0.0571, G_loss: 0.6163\n",
      "  Batch [520/1299] D_loss: -0.0298, G_loss: 0.5318\n",
      "  Batch [530/1299] D_loss: -0.0086, G_loss: 0.3378\n",
      "  Batch [540/1299] D_loss: -0.7766, G_loss: -0.6695\n",
      "  Batch [550/1299] D_loss: -0.4726, G_loss: -0.2420\n",
      "  Batch [560/1299] D_loss: -1.3242, G_loss: -1.7227\n",
      "  Batch [570/1299] D_loss: -0.7247, G_loss: -0.2135\n",
      "  Batch [580/1299] D_loss: -0.6808, G_loss: -1.2434\n",
      "  Batch [590/1299] D_loss: -0.0658, G_loss: 0.0156\n",
      "  Batch [600/1299] D_loss: -0.4559, G_loss: -0.5436\n",
      "  Batch [610/1299] D_loss: -0.2317, G_loss: 0.1240\n",
      "  Batch [620/1299] D_loss: -0.4393, G_loss: -0.1530\n",
      "  Batch [630/1299] D_loss: -0.3331, G_loss: -0.4491\n",
      "  Batch [640/1299] D_loss: -0.5971, G_loss: 0.0501\n",
      "  Batch [650/1299] D_loss: -0.0623, G_loss: 0.1442\n",
      "  Batch [660/1299] D_loss: -0.2047, G_loss: 0.4175\n",
      "  Batch [670/1299] D_loss: -0.2382, G_loss: 0.7844\n",
      "  Batch [680/1299] D_loss: -0.3199, G_loss: 0.9888\n",
      "  Batch [690/1299] D_loss: 0.0494, G_loss: 0.8400\n",
      "  Batch [700/1299] D_loss: 0.0467, G_loss: 0.4270\n",
      "  Batch [710/1299] D_loss: -1.6090, G_loss: -3.5098\n",
      "  Batch [720/1299] D_loss: -1.4297, G_loss: -0.3577\n",
      "  Batch [730/1299] D_loss: -0.5122, G_loss: -0.9330\n",
      "  Batch [740/1299] D_loss: -0.6119, G_loss: -0.0073\n",
      "  Batch [750/1299] D_loss: -0.9172, G_loss: -0.1551\n",
      "  Batch [760/1299] D_loss: -0.1548, G_loss: 0.1503\n",
      "  Batch [770/1299] D_loss: -0.1357, G_loss: 0.3343\n",
      "  Batch [780/1299] D_loss: -0.5740, G_loss: 0.2795\n",
      "  Batch [790/1299] D_loss: -0.1320, G_loss: 0.3904\n",
      "  Batch [800/1299] D_loss: -0.1239, G_loss: 0.6633\n",
      "  Batch [810/1299] D_loss: -0.0659, G_loss: 0.6446\n",
      "  Batch [820/1299] D_loss: -0.0554, G_loss: 0.4118\n",
      "  Batch [830/1299] D_loss: -0.0121, G_loss: 0.4267\n",
      "  Batch [840/1299] D_loss: -0.0092, G_loss: 0.3272\n",
      "  Batch [850/1299] D_loss: -2.1447, G_loss: -0.9467\n",
      "  Batch [860/1299] D_loss: 0.0251, G_loss: 0.1370\n",
      "  Batch [870/1299] D_loss: -0.0903, G_loss: 0.1363\n",
      "  Batch [880/1299] D_loss: -1.4207, G_loss: -4.6078\n",
      "  Batch [890/1299] D_loss: -0.7843, G_loss: -1.2273\n",
      "  Batch [900/1299] D_loss: -0.1983, G_loss: 0.1317\n",
      "  Batch [910/1299] D_loss: -2.3336, G_loss: -2.4413\n",
      "  Batch [920/1299] D_loss: -0.1174, G_loss: 0.1780\n",
      "  Batch [930/1299] D_loss: -0.0587, G_loss: 0.5477\n",
      "  Batch [940/1299] D_loss: -0.0504, G_loss: 0.7257\n",
      "  Batch [950/1299] D_loss: -0.2567, G_loss: 0.8094\n",
      "  Batch [960/1299] D_loss: 0.0122, G_loss: 0.6610\n",
      "  Batch [970/1299] D_loss: -0.0874, G_loss: 0.5466\n",
      "  Batch [980/1299] D_loss: -0.1120, G_loss: 0.4415\n",
      "  Batch [990/1299] D_loss: -1.1259, G_loss: -3.2603\n",
      "  Batch [1000/1299] D_loss: -0.4256, G_loss: -0.5424\n",
      "  Batch [1010/1299] D_loss: -0.3105, G_loss: 0.0747\n",
      "  Batch [1020/1299] D_loss: -0.4153, G_loss: -0.9504\n",
      "  Batch [1030/1299] D_loss: -0.5287, G_loss: -0.3492\n",
      "  Batch [1040/1299] D_loss: -0.1356, G_loss: 0.2491\n",
      "  Batch [1050/1299] D_loss: -0.0130, G_loss: 0.3929\n",
      "  Batch [1060/1299] D_loss: -0.1840, G_loss: 0.6164\n",
      "  Batch [1070/1299] D_loss: -0.1356, G_loss: 0.7629\n",
      "  Batch [1080/1299] D_loss: -0.1400, G_loss: 0.6640\n",
      "  Batch [1090/1299] D_loss: -0.0749, G_loss: 0.5955\n",
      "  Batch [1100/1299] D_loss: -0.0560, G_loss: 0.4596\n",
      "  Batch [1110/1299] D_loss: -0.8234, G_loss: -2.0547\n",
      "  Batch [1120/1299] D_loss: -0.0527, G_loss: 0.1249\n",
      "  Batch [1130/1299] D_loss: -0.5319, G_loss: -0.9337\n",
      "  Batch [1140/1299] D_loss: 0.0125, G_loss: 0.2792\n",
      "  Batch [1150/1299] D_loss: -0.1455, G_loss: 0.5193\n",
      "  Batch [1160/1299] D_loss: -0.0024, G_loss: 0.3989\n",
      "  Batch [1170/1299] D_loss: -0.0630, G_loss: 0.2862\n",
      "  Batch [1180/1299] D_loss: -1.5105, G_loss: -2.4390\n",
      "  Batch [1190/1299] D_loss: -0.0203, G_loss: 0.1417\n",
      "  Batch [1200/1299] D_loss: -0.5804, G_loss: -1.0746\n",
      "  Batch [1210/1299] D_loss: -0.2316, G_loss: -0.0155\n",
      "  Batch [1220/1299] D_loss: -0.3484, G_loss: -0.0891\n",
      "  Batch [1230/1299] D_loss: -0.0397, G_loss: 0.1806\n",
      "  Batch [1240/1299] D_loss: -0.0706, G_loss: 0.3635\n",
      "  Batch [1250/1299] D_loss: -0.1021, G_loss: 0.4833\n",
      "  Batch [1260/1299] D_loss: -0.1218, G_loss: 0.6030\n",
      "  Batch [1270/1299] D_loss: -0.1079, G_loss: 0.5303\n",
      "  Batch [1280/1299] D_loss: -0.0425, G_loss: 0.4526\n",
      "  Batch [1290/1299] D_loss: -0.9447, G_loss: -0.2320\n",
      "\n",
      "Epoch 65 Summary:\n",
      "  Average D_loss: -0.1882\n",
      "  Average G_loss: -0.0937\n",
      "\n",
      "Epoch [66/100]\n",
      "  Batch [0/1299] D_loss: 0.0138, G_loss: 0.1169\n",
      "  Batch [10/1299] D_loss: -0.0372, G_loss: 0.1828\n",
      "  Batch [20/1299] D_loss: -0.0819, G_loss: 0.2740\n",
      "  Batch [30/1299] D_loss: -0.0321, G_loss: 0.4023\n",
      "  Batch [40/1299] D_loss: -0.1295, G_loss: 0.3068\n",
      "  Batch [50/1299] D_loss: -0.0843, G_loss: 0.2320\n",
      "  Batch [60/1299] D_loss: -0.6426, G_loss: 0.0812\n",
      "  Batch [70/1299] D_loss: -0.0716, G_loss: 0.2221\n",
      "  Batch [80/1299] D_loss: -0.0184, G_loss: 0.2464\n",
      "  Batch [90/1299] D_loss: -0.0409, G_loss: 0.2004\n",
      "  Batch [100/1299] D_loss: -1.0062, G_loss: -0.8168\n",
      "  Batch [110/1299] D_loss: -0.2375, G_loss: 0.0161\n",
      "  Batch [120/1299] D_loss: -0.0986, G_loss: 0.0230\n",
      "  Batch [130/1299] D_loss: -0.1960, G_loss: -0.0239\n",
      "  Batch [140/1299] D_loss: -0.3290, G_loss: -0.1001\n",
      "  Batch [150/1299] D_loss: -1.1392, G_loss: -1.3305\n",
      "  Batch [160/1299] D_loss: -0.0158, G_loss: 0.2034\n",
      "  Batch [170/1299] D_loss: -0.0866, G_loss: 0.4050\n",
      "  Batch [180/1299] D_loss: -0.1313, G_loss: 0.5023\n",
      "  Batch [190/1299] D_loss: -0.0596, G_loss: 0.3642\n",
      "  Batch [200/1299] D_loss: -0.1147, G_loss: 0.5570\n",
      "  Batch [210/1299] D_loss: -0.0824, G_loss: 0.3101\n",
      "  Batch [220/1299] D_loss: -0.3922, G_loss: -0.0919\n",
      "  Batch [230/1299] D_loss: -0.1049, G_loss: 0.1767\n",
      "  Batch [240/1299] D_loss: -0.0191, G_loss: 0.2985\n",
      "  Batch [250/1299] D_loss: -0.1053, G_loss: 0.4642\n",
      "  Batch [260/1299] D_loss: -0.1173, G_loss: 0.2938\n",
      "  Batch [270/1299] D_loss: -1.0730, G_loss: -2.0557\n",
      "  Batch [280/1299] D_loss: -0.0882, G_loss: 0.1630\n",
      "  Batch [290/1299] D_loss: -0.1304, G_loss: 0.1465\n",
      "  Batch [300/1299] D_loss: -0.0622, G_loss: 0.3116\n",
      "  Batch [310/1299] D_loss: -0.0661, G_loss: 0.3880\n",
      "  Batch [320/1299] D_loss: -0.0183, G_loss: 0.4155\n",
      "  Batch [330/1299] D_loss: -0.1028, G_loss: 0.3562\n",
      "  Batch [340/1299] D_loss: -1.9629, G_loss: -4.4468\n",
      "  Batch [350/1299] D_loss: 0.0209, G_loss: 0.2229\n",
      "  Batch [360/1299] D_loss: -0.0613, G_loss: 0.2406\n",
      "  Batch [370/1299] D_loss: -0.0470, G_loss: 0.2471\n",
      "  Batch [380/1299] D_loss: -0.0581, G_loss: 0.1581\n",
      "  Batch [390/1299] D_loss: -1.3356, G_loss: -1.0507\n",
      "  Batch [400/1299] D_loss: -0.2049, G_loss: -0.1874\n",
      "  Batch [410/1299] D_loss: -0.0458, G_loss: 0.2025\n",
      "  Batch [420/1299] D_loss: -0.1039, G_loss: 0.2669\n",
      "  Batch [430/1299] D_loss: -0.1069, G_loss: 0.4533\n",
      "  Batch [440/1299] D_loss: -0.0753, G_loss: 0.2670\n",
      "  Batch [450/1299] D_loss: -0.0370, G_loss: 0.1975\n",
      "  Batch [460/1299] D_loss: -1.0874, G_loss: -2.7887\n",
      "  Batch [470/1299] D_loss: -1.5362, G_loss: -3.0724\n",
      "  Batch [480/1299] D_loss: -0.1460, G_loss: 0.0714\n",
      "  Batch [490/1299] D_loss: -0.0320, G_loss: 0.2578\n",
      "  Batch [500/1299] D_loss: -0.1282, G_loss: 0.4166\n",
      "  Batch [510/1299] D_loss: -0.0712, G_loss: 0.4735\n",
      "  Batch [520/1299] D_loss: -0.0724, G_loss: 0.4457\n",
      "  Batch [530/1299] D_loss: -0.0658, G_loss: 0.2924\n",
      "  Batch [540/1299] D_loss: -0.0604, G_loss: 0.1970\n",
      "  Batch [550/1299] D_loss: -0.6800, G_loss: -1.3498\n",
      "  Batch [560/1299] D_loss: -0.3583, G_loss: -0.6428\n",
      "  Batch [570/1299] D_loss: -0.6131, G_loss: -0.6030\n",
      "  Batch [580/1299] D_loss: -0.0504, G_loss: 0.0907\n",
      "  Batch [590/1299] D_loss: -0.1101, G_loss: 0.0217\n",
      "  Batch [600/1299] D_loss: -1.8512, G_loss: -2.1284\n",
      "  Batch [610/1299] D_loss: -0.0476, G_loss: 0.2768\n",
      "  Batch [620/1299] D_loss: -0.0417, G_loss: 0.5438\n",
      "  Batch [630/1299] D_loss: -0.0845, G_loss: 0.6707\n",
      "  Batch [640/1299] D_loss: -0.0478, G_loss: 0.6447\n",
      "  Batch [650/1299] D_loss: 0.0510, G_loss: 0.3724\n",
      "  Batch [660/1299] D_loss: -0.1204, G_loss: 0.4165\n",
      "  Batch [670/1299] D_loss: -1.3987, G_loss: -1.5176\n",
      "  Batch [680/1299] D_loss: -0.0139, G_loss: 0.0954\n",
      "  Batch [690/1299] D_loss: -0.1106, G_loss: 0.1444\n",
      "  Batch [700/1299] D_loss: -1.0747, G_loss: -0.0125\n",
      "  Batch [710/1299] D_loss: -0.1449, G_loss: 0.0093\n",
      "  Batch [720/1299] D_loss: -0.1052, G_loss: 0.3698\n",
      "  Batch [730/1299] D_loss: -0.0232, G_loss: 0.3067\n",
      "  Batch [740/1299] D_loss: -0.0960, G_loss: 0.3368\n",
      "  Batch [750/1299] D_loss: -2.1287, G_loss: -1.9040\n",
      "  Batch [760/1299] D_loss: -0.3214, G_loss: -0.6822\n",
      "  Batch [770/1299] D_loss: -0.0043, G_loss: 0.1767\n",
      "  Batch [780/1299] D_loss: -0.0509, G_loss: 0.3748\n",
      "  Batch [790/1299] D_loss: 0.0275, G_loss: 0.4091\n",
      "  Batch [800/1299] D_loss: -0.0125, G_loss: 0.5601\n",
      "  Batch [810/1299] D_loss: -0.0466, G_loss: 0.3991\n",
      "  Batch [820/1299] D_loss: -0.0573, G_loss: 0.3192\n",
      "  Batch [830/1299] D_loss: -2.1310, G_loss: -2.7539\n",
      "  Batch [840/1299] D_loss: -0.3093, G_loss: -0.3535\n",
      "  Batch [850/1299] D_loss: -0.6305, G_loss: -0.9809\n",
      "  Batch [860/1299] D_loss: -0.8847, G_loss: -1.2676\n",
      "  Batch [870/1299] D_loss: -0.0234, G_loss: 0.2373\n",
      "  Batch [880/1299] D_loss: -0.0221, G_loss: 0.2876\n",
      "  Batch [890/1299] D_loss: -0.1483, G_loss: 0.4735\n",
      "  Batch [900/1299] D_loss: -0.1980, G_loss: 0.5544\n",
      "  Batch [910/1299] D_loss: -0.0825, G_loss: 0.4656\n",
      "  Batch [920/1299] D_loss: -0.0944, G_loss: 0.3403\n",
      "  Batch [930/1299] D_loss: -0.0610, G_loss: 0.1903\n",
      "  Batch [940/1299] D_loss: -0.0502, G_loss: 0.1308\n",
      "  Batch [950/1299] D_loss: -0.4832, G_loss: -1.1616\n",
      "  Batch [960/1299] D_loss: -0.0003, G_loss: 0.2289\n",
      "  Batch [970/1299] D_loss: -0.0475, G_loss: 0.2907\n",
      "  Batch [980/1299] D_loss: -0.0156, G_loss: 0.2887\n",
      "  Batch [990/1299] D_loss: -0.0463, G_loss: 0.3323\n",
      "  Batch [1000/1299] D_loss: -0.0685, G_loss: 0.2747\n",
      "  Batch [1010/1299] D_loss: -1.3104, G_loss: -2.1638\n",
      "  Batch [1020/1299] D_loss: -0.2596, G_loss: -0.0710\n",
      "  Batch [1030/1299] D_loss: -0.2229, G_loss: -0.2283\n",
      "  Batch [1040/1299] D_loss: -0.1861, G_loss: -0.1197\n",
      "  Batch [1050/1299] D_loss: -0.1612, G_loss: -0.0252\n",
      "  Batch [1060/1299] D_loss: -0.6990, G_loss: -0.2781\n",
      "  Batch [1070/1299] D_loss: -1.2397, G_loss: -0.1056\n",
      "  Batch [1080/1299] D_loss: -0.0737, G_loss: 0.1171\n",
      "  Batch [1090/1299] D_loss: -0.1160, G_loss: 0.4157\n",
      "  Batch [1100/1299] D_loss: -0.1687, G_loss: 0.6219\n",
      "  Batch [1110/1299] D_loss: -0.0968, G_loss: 0.6981\n",
      "  Batch [1120/1299] D_loss: -0.0482, G_loss: 0.3780\n",
      "  Batch [1130/1299] D_loss: -0.0774, G_loss: 0.3164\n",
      "  Batch [1140/1299] D_loss: -0.7349, G_loss: -0.3832\n",
      "  Batch [1150/1299] D_loss: -0.0490, G_loss: 0.1163\n",
      "  Batch [1160/1299] D_loss: -0.0633, G_loss: 0.2740\n",
      "  Batch [1170/1299] D_loss: -0.0422, G_loss: 0.4569\n",
      "  Batch [1180/1299] D_loss: -0.0325, G_loss: 0.4261\n",
      "  Batch [1190/1299] D_loss: -0.0641, G_loss: 0.4208\n",
      "  Batch [1200/1299] D_loss: -0.0446, G_loss: 0.3299\n",
      "  Batch [1210/1299] D_loss: -2.9015, G_loss: -3.5010\n",
      "  Batch [1220/1299] D_loss: -0.5503, G_loss: -0.0441\n",
      "  Batch [1230/1299] D_loss: -0.7098, G_loss: -0.5950\n",
      "  Batch [1240/1299] D_loss: -0.0893, G_loss: 0.2904\n",
      "  Batch [1250/1299] D_loss: -0.0375, G_loss: 0.3366\n",
      "  Batch [1260/1299] D_loss: -0.1568, G_loss: 0.5376\n",
      "  Batch [1270/1299] D_loss: -0.0692, G_loss: 0.3900\n",
      "  Batch [1280/1299] D_loss: -0.1447, G_loss: 0.3617\n",
      "  Batch [1290/1299] D_loss: -0.9877, G_loss: -0.5573\n",
      "\n",
      "Epoch 66 Summary:\n",
      "  Average D_loss: -0.1428\n",
      "  Average G_loss: -0.0562\n",
      "\n",
      "Epoch [67/100]\n",
      "  Batch [0/1299] D_loss: -1.7545, G_loss: -1.5331\n",
      "  Batch [10/1299] D_loss: -0.0943, G_loss: 0.0730\n",
      "  Batch [20/1299] D_loss: -0.1021, G_loss: 0.2918\n",
      "  Batch [30/1299] D_loss: -0.0272, G_loss: 0.3425\n",
      "  Batch [40/1299] D_loss: -0.0455, G_loss: 0.3481\n",
      "  Batch [50/1299] D_loss: -0.1244, G_loss: 0.2738\n",
      "  Batch [60/1299] D_loss: -0.3326, G_loss: -0.1922\n",
      "  Batch [70/1299] D_loss: -0.6618, G_loss: -0.9258\n",
      "  Batch [80/1299] D_loss: -0.1925, G_loss: 0.0056\n",
      "  Batch [90/1299] D_loss: -0.0184, G_loss: 0.1987\n",
      "  Batch [100/1299] D_loss: -0.1087, G_loss: 0.3574\n",
      "  Batch [110/1299] D_loss: -0.0823, G_loss: 0.3388\n",
      "  Batch [120/1299] D_loss: -0.1260, G_loss: 0.3544\n",
      "  Batch [130/1299] D_loss: -0.0860, G_loss: 0.3783\n",
      "  Batch [140/1299] D_loss: 0.0328, G_loss: 0.2307\n",
      "  Batch [150/1299] D_loss: -2.8771, G_loss: -4.6801\n",
      "  Batch [160/1299] D_loss: -0.0074, G_loss: 0.1254\n",
      "  Batch [170/1299] D_loss: -0.0240, G_loss: 0.2859\n",
      "  Batch [180/1299] D_loss: -0.1143, G_loss: 0.4018\n",
      "  Batch [190/1299] D_loss: -0.1002, G_loss: 0.5596\n",
      "  Batch [200/1299] D_loss: -0.1477, G_loss: 0.4649\n",
      "  Batch [210/1299] D_loss: -0.0181, G_loss: 0.3584\n",
      "  Batch [220/1299] D_loss: -0.4437, G_loss: -0.2407\n",
      "  Batch [230/1299] D_loss: -0.7009, G_loss: -1.7673\n",
      "  Batch [240/1299] D_loss: -1.0200, G_loss: -0.1014\n",
      "  Batch [250/1299] D_loss: -0.3242, G_loss: 0.0660\n",
      "  Batch [260/1299] D_loss: -0.5593, G_loss: -0.7243\n",
      "  Batch [270/1299] D_loss: -0.0691, G_loss: 0.1810\n",
      "  Batch [280/1299] D_loss: -0.2888, G_loss: 0.1078\n",
      "  Batch [290/1299] D_loss: -0.6153, G_loss: -1.4682\n",
      "  Batch [300/1299] D_loss: -0.1837, G_loss: 0.0117\n",
      "  Batch [310/1299] D_loss: -0.0690, G_loss: -0.0268\n",
      "  Batch [320/1299] D_loss: -0.4860, G_loss: -1.5725\n",
      "  Batch [330/1299] D_loss: -0.2189, G_loss: -0.0197\n",
      "  Batch [340/1299] D_loss: -0.0688, G_loss: 0.2169\n",
      "  Batch [350/1299] D_loss: -0.1293, G_loss: 0.6218\n",
      "  Batch [360/1299] D_loss: -0.1965, G_loss: 0.6808\n",
      "  Batch [370/1299] D_loss: -0.1742, G_loss: 0.8184\n",
      "  Batch [380/1299] D_loss: -0.3256, G_loss: 0.6358\n",
      "  Batch [390/1299] D_loss: -0.0799, G_loss: 0.2394\n",
      "  Batch [400/1299] D_loss: -2.6705, G_loss: -4.1277\n",
      "  Batch [410/1299] D_loss: -0.0055, G_loss: 0.1589\n",
      "  Batch [420/1299] D_loss: -0.0450, G_loss: 0.1769\n",
      "  Batch [430/1299] D_loss: -0.0122, G_loss: 0.2252\n",
      "  Batch [440/1299] D_loss: -0.0667, G_loss: 0.2622\n",
      "  Batch [450/1299] D_loss: -1.2441, G_loss: -2.1423\n",
      "  Batch [460/1299] D_loss: -0.0351, G_loss: 0.1991\n",
      "  Batch [470/1299] D_loss: -0.0171, G_loss: 0.1962\n",
      "  Batch [480/1299] D_loss: -0.0074, G_loss: 0.2323\n",
      "  Batch [490/1299] D_loss: -0.0231, G_loss: 0.2513\n",
      "  Batch [500/1299] D_loss: -1.3921, G_loss: -2.9851\n",
      "  Batch [510/1299] D_loss: -0.0158, G_loss: 0.1873\n",
      "  Batch [520/1299] D_loss: -0.0321, G_loss: 0.2406\n",
      "  Batch [530/1299] D_loss: -0.0548, G_loss: 0.3298\n",
      "  Batch [540/1299] D_loss: -0.0808, G_loss: 0.3733\n",
      "  Batch [550/1299] D_loss: -0.0687, G_loss: 0.3088\n",
      "  Batch [560/1299] D_loss: -1.4900, G_loss: -1.9830\n",
      "  Batch [570/1299] D_loss: -0.7871, G_loss: -1.1008\n",
      "  Batch [580/1299] D_loss: -0.6803, G_loss: -0.2014\n",
      "  Batch [590/1299] D_loss: -0.0498, G_loss: 0.2504\n",
      "  Batch [600/1299] D_loss: -0.0669, G_loss: 0.2754\n",
      "  Batch [610/1299] D_loss: -0.0533, G_loss: 0.3441\n",
      "  Batch [620/1299] D_loss: -0.0782, G_loss: 0.1867\n",
      "  Batch [630/1299] D_loss: -1.2340, G_loss: -2.2967\n",
      "  Batch [640/1299] D_loss: -0.1377, G_loss: -0.3060\n",
      "  Batch [650/1299] D_loss: -0.0574, G_loss: 0.1994\n",
      "  Batch [660/1299] D_loss: -0.1175, G_loss: 0.4129\n",
      "  Batch [670/1299] D_loss: -0.1389, G_loss: 0.4036\n",
      "  Batch [680/1299] D_loss: -0.0998, G_loss: 0.4254\n",
      "  Batch [690/1299] D_loss: 0.0060, G_loss: 0.2675\n",
      "  Batch [700/1299] D_loss: -0.1262, G_loss: 0.1500\n",
      "  Batch [710/1299] D_loss: -1.3531, G_loss: -0.3914\n",
      "  Batch [720/1299] D_loss: -0.0503, G_loss: 0.2490\n",
      "  Batch [730/1299] D_loss: -0.0747, G_loss: 0.4133\n",
      "  Batch [740/1299] D_loss: -0.0174, G_loss: 0.4813\n",
      "  Batch [750/1299] D_loss: -0.0263, G_loss: 0.4236\n",
      "  Batch [760/1299] D_loss: -0.0331, G_loss: 0.5083\n",
      "  Batch [770/1299] D_loss: -1.0604, G_loss: -3.0014\n",
      "  Batch [780/1299] D_loss: -0.8077, G_loss: -0.2164\n",
      "  Batch [790/1299] D_loss: -0.3725, G_loss: -0.2704\n",
      "  Batch [800/1299] D_loss: -0.0672, G_loss: 0.2390\n",
      "  Batch [810/1299] D_loss: -0.0417, G_loss: 0.3853\n",
      "  Batch [820/1299] D_loss: 0.0556, G_loss: 0.4099\n",
      "  Batch [830/1299] D_loss: -0.0241, G_loss: 0.4670\n",
      "  Batch [840/1299] D_loss: -0.1024, G_loss: 0.4042\n",
      "  Batch [850/1299] D_loss: -2.8013, G_loss: -4.7956\n",
      "  Batch [860/1299] D_loss: -1.3386, G_loss: -0.3164\n",
      "  Batch [870/1299] D_loss: -0.4571, G_loss: -2.5154\n",
      "  Batch [880/1299] D_loss: -0.1315, G_loss: 0.2557\n",
      "  Batch [890/1299] D_loss: -0.1493, G_loss: 0.4776\n",
      "  Batch [900/1299] D_loss: -0.1284, G_loss: 0.4482\n",
      "  Batch [910/1299] D_loss: -0.0940, G_loss: 0.5083\n",
      "  Batch [920/1299] D_loss: -0.1681, G_loss: 0.3432\n",
      "  Batch [930/1299] D_loss: -0.2286, G_loss: -0.1219\n",
      "  Batch [940/1299] D_loss: -0.0724, G_loss: 0.1272\n",
      "  Batch [950/1299] D_loss: -0.0355, G_loss: 0.2318\n",
      "  Batch [960/1299] D_loss: -0.0538, G_loss: 0.2448\n",
      "  Batch [970/1299] D_loss: -0.0494, G_loss: 0.3163\n",
      "  Batch [980/1299] D_loss: -0.0754, G_loss: 0.2098\n",
      "  Batch [990/1299] D_loss: -0.7522, G_loss: -1.7428\n",
      "  Batch [1000/1299] D_loss: -0.1393, G_loss: 0.1056\n",
      "  Batch [1010/1299] D_loss: -0.8915, G_loss: -0.0408\n",
      "  Batch [1020/1299] D_loss: -0.0552, G_loss: 0.1850\n",
      "  Batch [1030/1299] D_loss: -0.0985, G_loss: 0.3457\n",
      "  Batch [1040/1299] D_loss: -0.1841, G_loss: 0.5060\n",
      "  Batch [1050/1299] D_loss: -0.1351, G_loss: 0.4845\n",
      "  Batch [1060/1299] D_loss: -0.1220, G_loss: 0.3691\n",
      "  Batch [1070/1299] D_loss: -1.9661, G_loss: -2.4650\n",
      "  Batch [1080/1299] D_loss: -0.6985, G_loss: 0.0166\n",
      "  Batch [1090/1299] D_loss: -0.8219, G_loss: -0.9060\n",
      "  Batch [1100/1299] D_loss: -0.0179, G_loss: 0.1684\n",
      "  Batch [1110/1299] D_loss: -0.0311, G_loss: 0.2074\n",
      "  Batch [1120/1299] D_loss: -0.0282, G_loss: 0.1673\n",
      "  Batch [1130/1299] D_loss: -3.2669, G_loss: -4.4405\n",
      "  Batch [1140/1299] D_loss: -0.1669, G_loss: 0.1991\n",
      "  Batch [1150/1299] D_loss: -0.0095, G_loss: 0.3040\n",
      "  Batch [1160/1299] D_loss: 0.0238, G_loss: 0.2219\n",
      "  Batch [1170/1299] D_loss: -0.2588, G_loss: -0.1482\n",
      "  Batch [1180/1299] D_loss: -0.7913, G_loss: -1.0662\n",
      "  Batch [1190/1299] D_loss: -0.0389, G_loss: 0.2122\n",
      "  Batch [1200/1299] D_loss: -0.0293, G_loss: 0.3454\n",
      "  Batch [1210/1299] D_loss: -0.0565, G_loss: 0.3609\n",
      "  Batch [1220/1299] D_loss: -0.0115, G_loss: 0.2867\n",
      "  Batch [1230/1299] D_loss: -3.2441, G_loss: -6.9150\n",
      "  Batch [1240/1299] D_loss: -1.4681, G_loss: -1.7829\n",
      "  Batch [1250/1299] D_loss: -0.1136, G_loss: 0.2642\n",
      "  Batch [1260/1299] D_loss: -0.0735, G_loss: 0.2459\n",
      "  Batch [1270/1299] D_loss: -0.1541, G_loss: 0.4490\n",
      "  Batch [1280/1299] D_loss: -0.0720, G_loss: 0.4559\n",
      "  Batch [1290/1299] D_loss: -0.0402, G_loss: 0.2645\n",
      "\n",
      "Epoch 67 Summary:\n",
      "  Average D_loss: -0.1462\n",
      "  Average G_loss: -0.0644\n",
      "\n",
      "Epoch [68/100]\n",
      "  Batch [0/1299] D_loss: -0.5888, G_loss: -0.0087\n",
      "  Batch [10/1299] D_loss: -0.0071, G_loss: 0.1483\n",
      "  Batch [20/1299] D_loss: -0.0330, G_loss: 0.2752\n",
      "  Batch [30/1299] D_loss: -0.0141, G_loss: 0.3819\n",
      "  Batch [40/1299] D_loss: -0.0324, G_loss: 0.4626\n",
      "  Batch [50/1299] D_loss: -0.0784, G_loss: 0.3839\n",
      "  Batch [60/1299] D_loss: -0.0619, G_loss: 0.3064\n",
      "  Batch [70/1299] D_loss: -3.4714, G_loss: -5.4793\n",
      "  Batch [80/1299] D_loss: -0.2643, G_loss: -0.4103\n",
      "  Batch [90/1299] D_loss: -0.1571, G_loss: 0.0499\n",
      "  Batch [100/1299] D_loss: -0.0386, G_loss: 0.2759\n",
      "  Batch [110/1299] D_loss: -0.0287, G_loss: 0.3733\n",
      "  Batch [120/1299] D_loss: -0.0143, G_loss: 0.4330\n",
      "  Batch [130/1299] D_loss: 0.0008, G_loss: 0.3795\n",
      "  Batch [140/1299] D_loss: -0.0974, G_loss: 0.3731\n",
      "  Batch [150/1299] D_loss: -1.2182, G_loss: -1.4034\n",
      "  Batch [160/1299] D_loss: -0.0151, G_loss: 0.1143\n",
      "  Batch [170/1299] D_loss: -0.0242, G_loss: 0.2088\n",
      "  Batch [180/1299] D_loss: 0.0028, G_loss: 0.2377\n",
      "  Batch [190/1299] D_loss: -0.1128, G_loss: 0.3629\n",
      "  Batch [200/1299] D_loss: -0.0590, G_loss: 0.2832\n",
      "  Batch [210/1299] D_loss: -0.0335, G_loss: 0.2475\n",
      "  Batch [220/1299] D_loss: -0.0388, G_loss: 0.1305\n",
      "  Batch [230/1299] D_loss: -0.4723, G_loss: -0.1875\n",
      "  Batch [240/1299] D_loss: -1.0629, G_loss: -1.3222\n",
      "  Batch [250/1299] D_loss: -0.6028, G_loss: -0.6642\n",
      "  Batch [260/1299] D_loss: -0.0278, G_loss: 0.2028\n",
      "  Batch [270/1299] D_loss: -0.1084, G_loss: 0.4124\n",
      "  Batch [280/1299] D_loss: 0.0267, G_loss: 0.6075\n",
      "  Batch [290/1299] D_loss: -0.2027, G_loss: 0.4994\n",
      "  Batch [300/1299] D_loss: -0.0222, G_loss: 0.3255\n",
      "  Batch [310/1299] D_loss: -0.0507, G_loss: 0.2271\n",
      "  Batch [320/1299] D_loss: -0.7653, G_loss: -0.9375\n",
      "  Batch [330/1299] D_loss: -0.0053, G_loss: 0.2035\n",
      "  Batch [340/1299] D_loss: -0.0333, G_loss: 0.2165\n",
      "  Batch [350/1299] D_loss: -1.1100, G_loss: -0.3805\n",
      "  Batch [360/1299] D_loss: -0.0234, G_loss: 0.1389\n",
      "  Batch [370/1299] D_loss: -0.4943, G_loss: -0.8457\n",
      "  Batch [380/1299] D_loss: -0.1761, G_loss: -0.3653\n",
      "  Batch [390/1299] D_loss: -0.0240, G_loss: 0.2768\n",
      "  Batch [400/1299] D_loss: -0.0710, G_loss: 0.4648\n",
      "  Batch [410/1299] D_loss: -0.0896, G_loss: 0.5864\n",
      "  Batch [420/1299] D_loss: -0.0599, G_loss: 0.5505\n",
      "  Batch [430/1299] D_loss: -0.0700, G_loss: 0.2448\n",
      "  Batch [440/1299] D_loss: -3.8093, G_loss: -3.4292\n",
      "  Batch [450/1299] D_loss: -0.5094, G_loss: -0.8862\n",
      "  Batch [460/1299] D_loss: -0.0218, G_loss: 0.1948\n",
      "  Batch [470/1299] D_loss: 0.0253, G_loss: 0.3444\n",
      "  Batch [480/1299] D_loss: -0.0720, G_loss: 0.3856\n",
      "  Batch [490/1299] D_loss: -0.0144, G_loss: 0.3308\n",
      "  Batch [500/1299] D_loss: -0.0587, G_loss: 0.2656\n",
      "  Batch [510/1299] D_loss: -0.1215, G_loss: 0.1283\n",
      "  Batch [520/1299] D_loss: -1.0899, G_loss: -1.3600\n",
      "  Batch [530/1299] D_loss: -0.1223, G_loss: 0.1877\n",
      "  Batch [540/1299] D_loss: -0.0939, G_loss: 0.2338\n",
      "  Batch [550/1299] D_loss: -0.0943, G_loss: 0.3644\n",
      "  Batch [560/1299] D_loss: -0.0917, G_loss: 0.6087\n",
      "  Batch [570/1299] D_loss: -0.0061, G_loss: 0.5447\n",
      "  Batch [580/1299] D_loss: -0.0564, G_loss: 0.4745\n",
      "  Batch [590/1299] D_loss: -0.0228, G_loss: 0.3246\n",
      "  Batch [600/1299] D_loss: -0.7095, G_loss: -3.8156\n",
      "  Batch [610/1299] D_loss: 0.0022, G_loss: 0.1580\n",
      "  Batch [620/1299] D_loss: 0.0096, G_loss: 0.2148\n",
      "  Batch [630/1299] D_loss: -0.0133, G_loss: 0.1554\n",
      "  Batch [640/1299] D_loss: -2.7736, G_loss: -2.2995\n",
      "  Batch [650/1299] D_loss: -0.1157, G_loss: 0.0316\n",
      "  Batch [660/1299] D_loss: -0.2981, G_loss: -0.2674\n",
      "  Batch [670/1299] D_loss: -0.0558, G_loss: 0.2072\n",
      "  Batch [680/1299] D_loss: -0.0750, G_loss: 0.3127\n",
      "  Batch [690/1299] D_loss: -0.0202, G_loss: 0.4716\n",
      "  Batch [700/1299] D_loss: -0.1026, G_loss: 0.5642\n",
      "  Batch [710/1299] D_loss: -0.0411, G_loss: 0.2925\n",
      "  Batch [720/1299] D_loss: -1.4057, G_loss: -2.6212\n",
      "  Batch [730/1299] D_loss: -0.3567, G_loss: -0.1106\n",
      "  Batch [740/1299] D_loss: -0.6112, G_loss: -0.1848\n",
      "  Batch [750/1299] D_loss: 0.0104, G_loss: 0.1696\n",
      "  Batch [760/1299] D_loss: -0.0412, G_loss: 0.2248\n",
      "  Batch [770/1299] D_loss: -0.0273, G_loss: 0.2254\n",
      "  Batch [780/1299] D_loss: -1.5100, G_loss: -1.2783\n",
      "  Batch [790/1299] D_loss: -0.0331, G_loss: 0.0739\n",
      "  Batch [800/1299] D_loss: -0.0149, G_loss: 0.1555\n",
      "  Batch [810/1299] D_loss: 0.0278, G_loss: 0.2491\n",
      "  Batch [820/1299] D_loss: -0.0548, G_loss: 0.3893\n",
      "  Batch [830/1299] D_loss: -0.1058, G_loss: 0.3686\n",
      "  Batch [840/1299] D_loss: -0.0722, G_loss: 0.2866\n",
      "  Batch [850/1299] D_loss: -0.0382, G_loss: 0.1371\n",
      "  Batch [860/1299] D_loss: -0.0942, G_loss: -0.2055\n",
      "  Batch [870/1299] D_loss: -0.0351, G_loss: 0.1785\n",
      "  Batch [880/1299] D_loss: -0.0286, G_loss: 0.2187\n",
      "  Batch [890/1299] D_loss: -0.0124, G_loss: 0.2841\n",
      "  Batch [900/1299] D_loss: -1.8950, G_loss: -2.0609\n",
      "  Batch [910/1299] D_loss: -0.3777, G_loss: -0.0866\n",
      "  Batch [920/1299] D_loss: -0.3141, G_loss: 0.1464\n",
      "  Batch [930/1299] D_loss: -0.0362, G_loss: 0.2445\n",
      "  Batch [940/1299] D_loss: -0.1510, G_loss: 0.4561\n",
      "  Batch [950/1299] D_loss: -0.0365, G_loss: 0.4967\n",
      "  Batch [960/1299] D_loss: 0.0222, G_loss: 0.5261\n",
      "  Batch [970/1299] D_loss: 0.0216, G_loss: 0.3570\n",
      "  Batch [980/1299] D_loss: -2.4370, G_loss: -4.6669\n",
      "  Batch [990/1299] D_loss: -0.0214, G_loss: 0.1625\n",
      "  Batch [1000/1299] D_loss: 0.0040, G_loss: 0.3349\n",
      "  Batch [1010/1299] D_loss: 0.0303, G_loss: 0.2994\n",
      "  Batch [1020/1299] D_loss: -0.0221, G_loss: 0.1977\n",
      "  Batch [1030/1299] D_loss: -2.9535, G_loss: -4.2648\n",
      "  Batch [1040/1299] D_loss: -0.0540, G_loss: -0.1400\n",
      "  Batch [1050/1299] D_loss: -0.0968, G_loss: -0.2027\n",
      "  Batch [1060/1299] D_loss: -0.3579, G_loss: -0.0455\n",
      "  Batch [1070/1299] D_loss: -0.1231, G_loss: 0.1257\n",
      "  Batch [1080/1299] D_loss: -0.2742, G_loss: 0.1132\n",
      "  Batch [1090/1299] D_loss: -0.1459, G_loss: 0.1363\n",
      "  Batch [1100/1299] D_loss: -0.0606, G_loss: 0.3495\n",
      "  Batch [1110/1299] D_loss: -0.1205, G_loss: 0.5310\n",
      "  Batch [1120/1299] D_loss: -0.0394, G_loss: 0.5854\n",
      "  Batch [1130/1299] D_loss: -0.0803, G_loss: 0.5608\n",
      "  Batch [1140/1299] D_loss: -0.0655, G_loss: 0.4279\n",
      "  Batch [1150/1299] D_loss: -0.0333, G_loss: 0.2140\n",
      "  Batch [1160/1299] D_loss: -0.4880, G_loss: -0.1909\n",
      "  Batch [1170/1299] D_loss: -0.2579, G_loss: 0.0708\n",
      "  Batch [1180/1299] D_loss: -0.0532, G_loss: 0.2332\n",
      "  Batch [1190/1299] D_loss: -0.0743, G_loss: 0.2458\n",
      "  Batch [1200/1299] D_loss: -0.0844, G_loss: 0.3876\n",
      "  Batch [1210/1299] D_loss: -0.0579, G_loss: 0.4107\n",
      "  Batch [1220/1299] D_loss: -0.0364, G_loss: 0.2799\n",
      "  Batch [1230/1299] D_loss: -0.3082, G_loss: -0.1886\n",
      "  Batch [1240/1299] D_loss: -0.0613, G_loss: 0.2097\n",
      "  Batch [1250/1299] D_loss: -0.1357, G_loss: 0.4199\n",
      "  Batch [1260/1299] D_loss: -0.0409, G_loss: 0.2967\n",
      "  Batch [1270/1299] D_loss: -0.0075, G_loss: 0.3795\n",
      "  Batch [1280/1299] D_loss: -0.0905, G_loss: 0.2392\n",
      "  Batch [1290/1299] D_loss: -0.3840, G_loss: -0.3823\n",
      "\n",
      "Epoch 68 Summary:\n",
      "  Average D_loss: -0.1251\n",
      "  Average G_loss: -0.0559\n",
      "\n",
      "Epoch [69/100]\n",
      "  Batch [0/1299] D_loss: -0.0236, G_loss: 0.1576\n",
      "  Batch [10/1299] D_loss: -0.0527, G_loss: 0.3064\n",
      "  Batch [20/1299] D_loss: -0.0466, G_loss: 0.3967\n",
      "  Batch [30/1299] D_loss: -0.0933, G_loss: 0.4603\n",
      "  Batch [40/1299] D_loss: -0.0480, G_loss: 0.2782\n",
      "  Batch [50/1299] D_loss: -2.8542, G_loss: -1.4541\n",
      "  Batch [60/1299] D_loss: -0.1402, G_loss: -0.5430\n",
      "  Batch [70/1299] D_loss: -0.2380, G_loss: -0.0169\n",
      "  Batch [80/1299] D_loss: -0.0122, G_loss: 0.2843\n",
      "  Batch [90/1299] D_loss: -0.1069, G_loss: 0.4609\n",
      "  Batch [100/1299] D_loss: -0.1101, G_loss: 0.5237\n",
      "  Batch [110/1299] D_loss: -0.0324, G_loss: 0.4222\n",
      "  Batch [120/1299] D_loss: -0.1020, G_loss: 0.4617\n",
      "  Batch [130/1299] D_loss: -0.0644, G_loss: 0.2378\n",
      "  Batch [140/1299] D_loss: -0.8118, G_loss: -1.4267\n",
      "  Batch [150/1299] D_loss: -0.7044, G_loss: -0.2314\n",
      "  Batch [160/1299] D_loss: -0.0533, G_loss: 0.3137\n",
      "  Batch [170/1299] D_loss: -0.0737, G_loss: 0.3213\n",
      "  Batch [180/1299] D_loss: -0.0378, G_loss: 0.4188\n",
      "  Batch [190/1299] D_loss: -0.0928, G_loss: 0.4274\n",
      "  Batch [200/1299] D_loss: -0.0791, G_loss: 0.3831\n",
      "  Batch [210/1299] D_loss: -1.8103, G_loss: -3.0607\n",
      "  Batch [220/1299] D_loss: -0.0364, G_loss: 0.0166\n",
      "  Batch [230/1299] D_loss: -0.6360, G_loss: -1.7075\n",
      "  Batch [240/1299] D_loss: -0.0709, G_loss: 0.2525\n",
      "  Batch [250/1299] D_loss: -0.0376, G_loss: 0.3227\n",
      "  Batch [260/1299] D_loss: -0.1041, G_loss: 0.4725\n",
      "  Batch [270/1299] D_loss: -0.0673, G_loss: 0.4363\n",
      "  Batch [280/1299] D_loss: -0.0950, G_loss: 0.4035\n",
      "  Batch [290/1299] D_loss: -2.9789, G_loss: -3.9784\n",
      "  Batch [300/1299] D_loss: -0.3758, G_loss: -0.7742\n",
      "  Batch [310/1299] D_loss: -0.0462, G_loss: 0.1936\n",
      "  Batch [320/1299] D_loss: -0.0342, G_loss: 0.3168\n",
      "  Batch [330/1299] D_loss: -0.0342, G_loss: 0.3324\n",
      "  Batch [340/1299] D_loss: -0.0965, G_loss: 0.3247\n",
      "  Batch [350/1299] D_loss: -0.0942, G_loss: 0.3167\n",
      "  Batch [360/1299] D_loss: -0.0212, G_loss: 0.2102\n",
      "  Batch [370/1299] D_loss: -0.2156, G_loss: -0.0461\n",
      "  Batch [380/1299] D_loss: -0.9876, G_loss: -1.9043\n",
      "  Batch [390/1299] D_loss: -0.0016, G_loss: 0.1755\n",
      "  Batch [400/1299] D_loss: -0.0679, G_loss: 0.2797\n",
      "  Batch [410/1299] D_loss: -0.0056, G_loss: 0.2237\n",
      "  Batch [420/1299] D_loss: -0.0820, G_loss: 0.2857\n",
      "  Batch [430/1299] D_loss: -1.6725, G_loss: -2.8149\n",
      "  Batch [440/1299] D_loss: 0.0968, G_loss: 0.1515\n",
      "  Batch [450/1299] D_loss: -0.0271, G_loss: 0.2326\n",
      "  Batch [460/1299] D_loss: 0.0085, G_loss: 0.2302\n",
      "  Batch [470/1299] D_loss: -0.0759, G_loss: 0.2247\n",
      "  Batch [480/1299] D_loss: -0.8703, G_loss: -0.6844\n",
      "  Batch [490/1299] D_loss: -0.0301, G_loss: 0.2025\n",
      "  Batch [500/1299] D_loss: 0.0156, G_loss: 0.2245\n",
      "  Batch [510/1299] D_loss: -0.0060, G_loss: 0.2980\n",
      "  Batch [520/1299] D_loss: -0.0338, G_loss: 0.2965\n",
      "  Batch [530/1299] D_loss: -1.5700, G_loss: -1.0254\n",
      "  Batch [540/1299] D_loss: -0.0079, G_loss: 0.0901\n",
      "  Batch [550/1299] D_loss: 0.1631, G_loss: -0.1939\n",
      "  Batch [560/1299] D_loss: -1.1968, G_loss: -0.2955\n",
      "  Batch [570/1299] D_loss: 0.0013, G_loss: 0.0331\n",
      "  Batch [580/1299] D_loss: -0.1660, G_loss: 0.2544\n",
      "  Batch [590/1299] D_loss: -0.0846, G_loss: 0.3159\n",
      "  Batch [600/1299] D_loss: -0.0684, G_loss: 0.3750\n",
      "  Batch [610/1299] D_loss: -0.0272, G_loss: 0.2975\n",
      "  Batch [620/1299] D_loss: -0.1230, G_loss: 0.2332\n",
      "  Batch [630/1299] D_loss: -0.0512, G_loss: -0.0168\n",
      "  Batch [640/1299] D_loss: -0.0409, G_loss: 0.1518\n",
      "  Batch [650/1299] D_loss: 0.0302, G_loss: 0.2536\n",
      "  Batch [660/1299] D_loss: 0.0078, G_loss: 0.4223\n",
      "  Batch [670/1299] D_loss: -0.0245, G_loss: 0.3230\n",
      "  Batch [680/1299] D_loss: -0.0514, G_loss: 0.3459\n",
      "  Batch [690/1299] D_loss: -0.0504, G_loss: 0.1472\n",
      "  Batch [700/1299] D_loss: -0.0177, G_loss: 0.1325\n",
      "  Batch [710/1299] D_loss: -0.0288, G_loss: 0.1770\n",
      "  Batch [720/1299] D_loss: -0.0448, G_loss: 0.1585\n",
      "  Batch [730/1299] D_loss: -0.1676, G_loss: -0.3540\n",
      "  Batch [740/1299] D_loss: -0.9301, G_loss: -1.3545\n",
      "  Batch [750/1299] D_loss: -0.2659, G_loss: -0.0463\n",
      "  Batch [760/1299] D_loss: -0.0153, G_loss: 0.2958\n",
      "  Batch [770/1299] D_loss: -0.0424, G_loss: 0.4563\n",
      "  Batch [780/1299] D_loss: -0.0739, G_loss: 0.5221\n",
      "  Batch [790/1299] D_loss: -0.0849, G_loss: 0.4571\n",
      "  Batch [800/1299] D_loss: -0.0476, G_loss: 0.2957\n",
      "  Batch [810/1299] D_loss: -0.8933, G_loss: -3.8616\n",
      "  Batch [820/1299] D_loss: -0.2508, G_loss: -0.1136\n",
      "  Batch [830/1299] D_loss: -0.2453, G_loss: -0.2899\n",
      "  Batch [840/1299] D_loss: -0.2591, G_loss: 0.0141\n",
      "  Batch [850/1299] D_loss: -0.0397, G_loss: 0.1611\n",
      "  Batch [860/1299] D_loss: -0.0535, G_loss: 0.3089\n",
      "  Batch [870/1299] D_loss: -0.0298, G_loss: 0.4782\n",
      "  Batch [880/1299] D_loss: -0.0851, G_loss: 0.4916\n",
      "  Batch [890/1299] D_loss: -0.1263, G_loss: 0.3412\n",
      "  Batch [900/1299] D_loss: -0.0545, G_loss: 0.3242\n",
      "  Batch [910/1299] D_loss: -1.1101, G_loss: -3.3119\n",
      "  Batch [920/1299] D_loss: -0.0037, G_loss: 0.2842\n",
      "  Batch [930/1299] D_loss: 0.0043, G_loss: 0.2332\n",
      "  Batch [940/1299] D_loss: -0.1455, G_loss: 0.3428\n",
      "  Batch [950/1299] D_loss: -0.0734, G_loss: 0.2388\n",
      "  Batch [960/1299] D_loss: 0.0094, G_loss: 0.1851\n",
      "  Batch [970/1299] D_loss: -0.5198, G_loss: 0.1161\n",
      "  Batch [980/1299] D_loss: -0.2030, G_loss: 0.0179\n",
      "  Batch [990/1299] D_loss: -0.0390, G_loss: 0.2459\n",
      "  Batch [1000/1299] D_loss: -0.0599, G_loss: 0.4160\n",
      "  Batch [1010/1299] D_loss: -0.0225, G_loss: 0.3361\n",
      "  Batch [1020/1299] D_loss: -0.0515, G_loss: 0.3861\n",
      "  Batch [1030/1299] D_loss: -0.1078, G_loss: 0.3333\n",
      "  Batch [1040/1299] D_loss: -0.0767, G_loss: 0.1009\n",
      "  Batch [1050/1299] D_loss: 0.0279, G_loss: 0.0702\n",
      "  Batch [1060/1299] D_loss: -1.1010, G_loss: -0.5042\n",
      "  Batch [1070/1299] D_loss: 0.0156, G_loss: 0.1566\n",
      "  Batch [1080/1299] D_loss: -0.0421, G_loss: 0.3342\n",
      "  Batch [1090/1299] D_loss: -0.0310, G_loss: 0.4163\n",
      "  Batch [1100/1299] D_loss: -0.1081, G_loss: 0.4512\n",
      "  Batch [1110/1299] D_loss: -0.1110, G_loss: 0.3373\n",
      "  Batch [1120/1299] D_loss: -0.1054, G_loss: 0.2389\n",
      "  Batch [1130/1299] D_loss: -0.6689, G_loss: -1.1978\n",
      "  Batch [1140/1299] D_loss: -0.3196, G_loss: -0.4929\n",
      "  Batch [1150/1299] D_loss: -0.5171, G_loss: -1.4070\n",
      "  Batch [1160/1299] D_loss: -0.6966, G_loss: -0.1161\n",
      "  Batch [1170/1299] D_loss: -0.3760, G_loss: -0.3501\n",
      "  Batch [1180/1299] D_loss: 0.0305, G_loss: 0.1722\n",
      "  Batch [1190/1299] D_loss: -0.0629, G_loss: 0.3987\n",
      "  Batch [1200/1299] D_loss: -0.0916, G_loss: 0.4384\n",
      "  Batch [1210/1299] D_loss: 0.0769, G_loss: 0.4138\n",
      "  Batch [1220/1299] D_loss: -0.0215, G_loss: 0.5557\n",
      "  Batch [1230/1299] D_loss: -0.0155, G_loss: 0.2734\n",
      "  Batch [1240/1299] D_loss: -0.2090, G_loss: 0.1095\n",
      "  Batch [1250/1299] D_loss: -0.0067, G_loss: 0.1225\n",
      "  Batch [1260/1299] D_loss: -0.0234, G_loss: 0.2322\n",
      "  Batch [1270/1299] D_loss: -0.0450, G_loss: 0.3262\n",
      "  Batch [1280/1299] D_loss: -0.0730, G_loss: 0.3628\n",
      "  Batch [1290/1299] D_loss: -0.0321, G_loss: 0.3756\n",
      "\n",
      "Epoch 69 Summary:\n",
      "  Average D_loss: -0.1279\n",
      "  Average G_loss: -0.0487\n",
      "\n",
      "Epoch [70/100]\n",
      "  Batch [0/1299] D_loss: -0.0579, G_loss: 0.1978\n",
      "  Batch [10/1299] D_loss: -0.1001, G_loss: 0.1265\n",
      "  Batch [20/1299] D_loss: -0.0083, G_loss: 0.1801\n",
      "  Batch [30/1299] D_loss: -0.0464, G_loss: 0.1693\n",
      "  Batch [40/1299] D_loss: -0.1471, G_loss: 0.0374\n",
      "  Batch [50/1299] D_loss: -0.0186, G_loss: 0.1793\n",
      "  Batch [60/1299] D_loss: -0.1358, G_loss: 0.2998\n",
      "  Batch [70/1299] D_loss: -0.0216, G_loss: 0.3076\n",
      "  Batch [80/1299] D_loss: -0.0164, G_loss: 0.3426\n",
      "  Batch [90/1299] D_loss: -0.1287, G_loss: 0.3984\n",
      "  Batch [100/1299] D_loss: -0.1108, G_loss: 0.0798\n",
      "  Batch [110/1299] D_loss: -0.4728, G_loss: 0.0489\n",
      "  Batch [120/1299] D_loss: -0.4019, G_loss: -0.9124\n",
      "  Batch [130/1299] D_loss: -0.0157, G_loss: 0.1644\n",
      "  Batch [140/1299] D_loss: -0.0404, G_loss: 0.2210\n",
      "  Batch [150/1299] D_loss: -0.0455, G_loss: 0.3309\n",
      "  Batch [160/1299] D_loss: -0.0994, G_loss: 0.3603\n",
      "  Batch [170/1299] D_loss: -0.2898, G_loss: -0.5281\n",
      "  Batch [180/1299] D_loss: -0.6003, G_loss: -2.2737\n",
      "  Batch [190/1299] D_loss: -0.0429, G_loss: 0.2373\n",
      "  Batch [200/1299] D_loss: -0.0234, G_loss: 0.4040\n",
      "  Batch [210/1299] D_loss: -0.1296, G_loss: 0.4908\n",
      "  Batch [220/1299] D_loss: -0.1146, G_loss: 0.3639\n",
      "  Batch [230/1299] D_loss: -0.0362, G_loss: 0.3186\n",
      "  Batch [240/1299] D_loss: -0.0580, G_loss: 0.2034\n",
      "  Batch [250/1299] D_loss: -0.4745, G_loss: -0.0305\n",
      "  Batch [260/1299] D_loss: -0.2884, G_loss: -0.8972\n",
      "  Batch [270/1299] D_loss: -0.2769, G_loss: -0.1961\n",
      "  Batch [280/1299] D_loss: -0.2209, G_loss: -0.0343\n",
      "  Batch [290/1299] D_loss: -0.1214, G_loss: 0.1377\n",
      "  Batch [300/1299] D_loss: -0.0696, G_loss: 0.2679\n",
      "  Batch [310/1299] D_loss: -0.0862, G_loss: 0.4342\n",
      "  Batch [320/1299] D_loss: -0.1035, G_loss: 0.5037\n",
      "  Batch [330/1299] D_loss: -0.0952, G_loss: 0.5138\n",
      "  Batch [340/1299] D_loss: -0.0374, G_loss: 0.2997\n",
      "  Batch [350/1299] D_loss: -0.0664, G_loss: 0.2691\n",
      "  Batch [360/1299] D_loss: 0.0313, G_loss: 0.1823\n",
      "  Batch [370/1299] D_loss: -0.0384, G_loss: 0.2513\n",
      "  Batch [380/1299] D_loss: 0.0018, G_loss: 0.3073\n",
      "  Batch [390/1299] D_loss: -0.0651, G_loss: 0.2505\n",
      "  Batch [400/1299] D_loss: -0.0299, G_loss: 0.2286\n",
      "  Batch [410/1299] D_loss: -0.0378, G_loss: 0.1315\n",
      "  Batch [420/1299] D_loss: -0.2249, G_loss: 0.0142\n",
      "  Batch [430/1299] D_loss: -0.2761, G_loss: 0.0073\n",
      "  Batch [440/1299] D_loss: -0.0359, G_loss: 0.2337\n",
      "  Batch [450/1299] D_loss: -0.0931, G_loss: 0.3317\n",
      "  Batch [460/1299] D_loss: -0.0687, G_loss: 0.2904\n",
      "  Batch [470/1299] D_loss: -0.0345, G_loss: 0.2438\n",
      "  Batch [480/1299] D_loss: -0.7830, G_loss: -1.0177\n",
      "  Batch [490/1299] D_loss: -0.4284, G_loss: -1.4965\n",
      "  Batch [500/1299] D_loss: -0.3963, G_loss: 0.0917\n",
      "  Batch [510/1299] D_loss: 0.0208, G_loss: 0.2830\n",
      "  Batch [520/1299] D_loss: -0.0521, G_loss: 0.4741\n",
      "  Batch [530/1299] D_loss: -0.1523, G_loss: 0.6114\n",
      "  Batch [540/1299] D_loss: -0.0379, G_loss: 0.5427\n",
      "  Batch [550/1299] D_loss: 0.1072, G_loss: 0.4074\n",
      "  Batch [560/1299] D_loss: -0.0460, G_loss: 0.3253\n",
      "  Batch [570/1299] D_loss: -0.0157, G_loss: 0.1583\n",
      "  Batch [580/1299] D_loss: -0.0312, G_loss: 0.1934\n",
      "  Batch [590/1299] D_loss: -0.0088, G_loss: 0.2069\n",
      "  Batch [600/1299] D_loss: -2.8487, G_loss: -5.9087\n",
      "  Batch [610/1299] D_loss: -0.7068, G_loss: -0.8627\n",
      "  Batch [620/1299] D_loss: -0.0085, G_loss: 0.2294\n",
      "  Batch [630/1299] D_loss: -0.0765, G_loss: 0.3666\n",
      "  Batch [640/1299] D_loss: -0.1047, G_loss: 0.4871\n",
      "  Batch [650/1299] D_loss: -0.0610, G_loss: 0.5119\n",
      "  Batch [660/1299] D_loss: 0.0086, G_loss: 0.4046\n",
      "  Batch [670/1299] D_loss: -0.2335, G_loss: -0.0435\n",
      "  Batch [680/1299] D_loss: -0.1031, G_loss: 0.0813\n",
      "  Batch [690/1299] D_loss: -0.1234, G_loss: 0.0386\n",
      "  Batch [700/1299] D_loss: -0.0525, G_loss: 0.1264\n",
      "  Batch [710/1299] D_loss: -0.0481, G_loss: 0.2322\n",
      "  Batch [720/1299] D_loss: 0.0086, G_loss: 0.3096\n",
      "  Batch [730/1299] D_loss: -0.1296, G_loss: 0.3410\n",
      "  Batch [740/1299] D_loss: -0.0630, G_loss: 0.3089\n",
      "  Batch [750/1299] D_loss: 0.0031, G_loss: 0.1707\n",
      "  Batch [760/1299] D_loss: 0.0064, G_loss: 0.1540\n",
      "  Batch [770/1299] D_loss: -0.0155, G_loss: 0.2216\n",
      "  Batch [780/1299] D_loss: -0.0143, G_loss: 0.2386\n",
      "  Batch [790/1299] D_loss: -0.0610, G_loss: 0.2215\n",
      "  Batch [800/1299] D_loss: -2.3865, G_loss: -3.4528\n",
      "  Batch [810/1299] D_loss: -0.0014, G_loss: 0.1206\n",
      "  Batch [820/1299] D_loss: -0.0330, G_loss: 0.1722\n",
      "  Batch [830/1299] D_loss: -0.0333, G_loss: 0.2714\n",
      "  Batch [840/1299] D_loss: -0.1083, G_loss: 0.3730\n",
      "  Batch [850/1299] D_loss: 0.0692, G_loss: 0.3105\n",
      "  Batch [860/1299] D_loss: -0.0720, G_loss: 0.3932\n",
      "  Batch [870/1299] D_loss: -3.1258, G_loss: -5.3449\n",
      "  Batch [880/1299] D_loss: -0.0218, G_loss: 0.1288\n",
      "  Batch [890/1299] D_loss: -0.0137, G_loss: 0.1624\n",
      "  Batch [900/1299] D_loss: -0.0330, G_loss: 0.2654\n",
      "  Batch [910/1299] D_loss: -0.0369, G_loss: 0.1706\n",
      "  Batch [920/1299] D_loss: -0.0771, G_loss: 0.2222\n",
      "  Batch [930/1299] D_loss: -0.3455, G_loss: -0.1773\n",
      "  Batch [940/1299] D_loss: 0.0122, G_loss: 0.1267\n",
      "  Batch [950/1299] D_loss: -0.0057, G_loss: 0.0910\n",
      "  Batch [960/1299] D_loss: -1.0581, G_loss: -0.3855\n",
      "  Batch [970/1299] D_loss: -0.0038, G_loss: 0.1475\n",
      "  Batch [980/1299] D_loss: -1.4331, G_loss: -3.0732\n",
      "  Batch [990/1299] D_loss: -0.0646, G_loss: 0.1937\n",
      "  Batch [1000/1299] D_loss: -0.0683, G_loss: 0.3426\n",
      "  Batch [1010/1299] D_loss: -0.0544, G_loss: 0.3908\n",
      "  Batch [1020/1299] D_loss: -0.0595, G_loss: 0.4083\n",
      "  Batch [1030/1299] D_loss: -0.0452, G_loss: 0.2530\n",
      "  Batch [1040/1299] D_loss: -1.0323, G_loss: -1.0290\n",
      "  Batch [1050/1299] D_loss: 0.0020, G_loss: 0.0816\n",
      "  Batch [1060/1299] D_loss: -0.5686, G_loss: -0.8699\n",
      "  Batch [1070/1299] D_loss: -0.0269, G_loss: 0.1700\n",
      "  Batch [1080/1299] D_loss: -0.1098, G_loss: 0.3441\n",
      "  Batch [1090/1299] D_loss: -0.1443, G_loss: 0.4586\n",
      "  Batch [1100/1299] D_loss: -0.1242, G_loss: 0.4768\n",
      "  Batch [1110/1299] D_loss: -0.0725, G_loss: 0.3965\n",
      "  Batch [1120/1299] D_loss: -2.2816, G_loss: -3.5126\n",
      "  Batch [1130/1299] D_loss: -0.0189, G_loss: 0.1022\n",
      "  Batch [1140/1299] D_loss: -0.8579, G_loss: 0.0189\n",
      "  Batch [1150/1299] D_loss: 0.0000, G_loss: 0.1738\n",
      "  Batch [1160/1299] D_loss: 0.0010, G_loss: 0.2516\n",
      "  Batch [1170/1299] D_loss: 0.0063, G_loss: 0.3438\n",
      "  Batch [1180/1299] D_loss: -0.0612, G_loss: 0.2725\n",
      "  Batch [1190/1299] D_loss: -0.0364, G_loss: 0.3177\n",
      "  Batch [1200/1299] D_loss: -0.0762, G_loss: 0.2199\n",
      "  Batch [1210/1299] D_loss: -0.4938, G_loss: -0.4738\n",
      "  Batch [1220/1299] D_loss: -0.8546, G_loss: -0.6657\n",
      "  Batch [1230/1299] D_loss: -0.2324, G_loss: 0.1525\n",
      "  Batch [1240/1299] D_loss: -0.0279, G_loss: 0.3526\n",
      "  Batch [1250/1299] D_loss: -0.1783, G_loss: 0.5812\n",
      "  Batch [1260/1299] D_loss: -0.0910, G_loss: 0.5563\n",
      "  Batch [1270/1299] D_loss: -0.0530, G_loss: 0.3950\n",
      "  Batch [1280/1299] D_loss: -0.0253, G_loss: 0.3073\n",
      "  Batch [1290/1299] D_loss: -0.6150, G_loss: -2.3504\n",
      "\n",
      "Epoch 70 Summary:\n",
      "  Average D_loss: -0.1276\n",
      "  Average G_loss: -0.0697\n",
      "\n",
      "Epoch [71/100]\n",
      "  Batch [0/1299] D_loss: -0.0498, G_loss: 0.1124\n",
      "  Batch [10/1299] D_loss: -0.0631, G_loss: 0.2119\n",
      "  Batch [20/1299] D_loss: -0.0491, G_loss: 0.2646\n",
      "  Batch [30/1299] D_loss: -0.0734, G_loss: 0.2621\n",
      "  Batch [40/1299] D_loss: -0.0611, G_loss: 0.1920\n",
      "  Batch [50/1299] D_loss: -1.0354, G_loss: -0.8234\n",
      "  Batch [60/1299] D_loss: -0.0061, G_loss: 0.1269\n",
      "  Batch [70/1299] D_loss: -0.0791, G_loss: 0.2231\n",
      "  Batch [80/1299] D_loss: -0.0410, G_loss: 0.3739\n",
      "  Batch [90/1299] D_loss: -0.0827, G_loss: 0.4004\n",
      "  Batch [100/1299] D_loss: -0.1072, G_loss: 0.4700\n",
      "  Batch [110/1299] D_loss: -0.0171, G_loss: 0.3441\n",
      "  Batch [120/1299] D_loss: -0.1284, G_loss: 0.3747\n",
      "  Batch [130/1299] D_loss: -0.5223, G_loss: -0.7548\n",
      "  Batch [140/1299] D_loss: -0.2559, G_loss: -1.2206\n",
      "  Batch [150/1299] D_loss: 0.0097, G_loss: 0.2585\n",
      "  Batch [160/1299] D_loss: 0.0268, G_loss: 0.3224\n",
      "  Batch [170/1299] D_loss: 0.0281, G_loss: 0.3093\n",
      "  Batch [180/1299] D_loss: -0.0197, G_loss: 0.2715\n",
      "  Batch [190/1299] D_loss: -0.0032, G_loss: 0.2813\n",
      "  Batch [200/1299] D_loss: -1.9398, G_loss: -1.7019\n",
      "  Batch [210/1299] D_loss: -0.4573, G_loss: -0.3703\n",
      "  Batch [220/1299] D_loss: -0.2167, G_loss: 0.0442\n",
      "  Batch [230/1299] D_loss: -0.0525, G_loss: 0.2122\n",
      "  Batch [240/1299] D_loss: 0.0060, G_loss: 0.1228\n",
      "  Batch [250/1299] D_loss: -0.3281, G_loss: -0.1728\n",
      "  Batch [260/1299] D_loss: -0.1285, G_loss: 0.1412\n",
      "  Batch [270/1299] D_loss: -0.0061, G_loss: 0.2281\n",
      "  Batch [280/1299] D_loss: -0.0681, G_loss: 0.4253\n",
      "  Batch [290/1299] D_loss: -0.0727, G_loss: 0.4084\n",
      "  Batch [300/1299] D_loss: -0.0721, G_loss: 0.4864\n",
      "  Batch [310/1299] D_loss: -0.0586, G_loss: 0.4961\n",
      "  Batch [320/1299] D_loss: -0.0885, G_loss: 0.2894\n",
      "  Batch [330/1299] D_loss: 0.1654, G_loss: -2.1670\n",
      "  Batch [340/1299] D_loss: -0.4142, G_loss: -1.2047\n",
      "  Batch [350/1299] D_loss: -0.2045, G_loss: -0.0125\n",
      "  Batch [360/1299] D_loss: -0.0692, G_loss: 0.4181\n",
      "  Batch [370/1299] D_loss: -0.0509, G_loss: 0.5223\n",
      "  Batch [380/1299] D_loss: -0.0961, G_loss: 0.4824\n",
      "  Batch [390/1299] D_loss: 0.0444, G_loss: 0.4235\n",
      "  Batch [400/1299] D_loss: -0.0872, G_loss: 0.2013\n",
      "  Batch [410/1299] D_loss: -0.0140, G_loss: 0.1792\n",
      "  Batch [420/1299] D_loss: -0.3126, G_loss: -1.0834\n",
      "  Batch [430/1299] D_loss: -0.2323, G_loss: -1.1958\n",
      "  Batch [440/1299] D_loss: -0.2454, G_loss: 0.1058\n",
      "  Batch [450/1299] D_loss: -0.0631, G_loss: 0.1835\n",
      "  Batch [460/1299] D_loss: -0.0548, G_loss: 0.2902\n",
      "  Batch [470/1299] D_loss: -0.0924, G_loss: 0.4802\n",
      "  Batch [480/1299] D_loss: -0.0971, G_loss: 0.5265\n",
      "  Batch [490/1299] D_loss: -0.1317, G_loss: 0.6335\n",
      "  Batch [500/1299] D_loss: -0.0926, G_loss: 0.4998\n",
      "  Batch [510/1299] D_loss: -0.0545, G_loss: 0.1963\n",
      "  Batch [520/1299] D_loss: -0.1169, G_loss: 0.0252\n",
      "  Batch [530/1299] D_loss: -0.6518, G_loss: 0.0291\n",
      "  Batch [540/1299] D_loss: -0.0078, G_loss: 0.1616\n",
      "  Batch [550/1299] D_loss: -0.0841, G_loss: 0.2211\n",
      "  Batch [560/1299] D_loss: 0.0157, G_loss: 0.3793\n",
      "  Batch [570/1299] D_loss: -0.0533, G_loss: 0.3621\n",
      "  Batch [580/1299] D_loss: -0.0714, G_loss: 0.2629\n",
      "  Batch [590/1299] D_loss: -1.3270, G_loss: -1.6398\n",
      "  Batch [600/1299] D_loss: -0.8070, G_loss: -0.8018\n",
      "  Batch [610/1299] D_loss: -1.4425, G_loss: -0.1169\n",
      "  Batch [620/1299] D_loss: 0.0234, G_loss: 0.0894\n",
      "  Batch [630/1299] D_loss: -0.8438, G_loss: -2.2021\n",
      "  Batch [640/1299] D_loss: -0.1590, G_loss: 0.0719\n",
      "  Batch [650/1299] D_loss: -0.0834, G_loss: 0.2352\n",
      "  Batch [660/1299] D_loss: -0.1916, G_loss: 0.5598\n",
      "  Batch [670/1299] D_loss: -0.2220, G_loss: 0.7791\n",
      "  Batch [680/1299] D_loss: -0.2584, G_loss: 0.8088\n",
      "  Batch [690/1299] D_loss: -0.0473, G_loss: 0.5802\n",
      "  Batch [700/1299] D_loss: -0.0026, G_loss: 0.4561\n",
      "  Batch [710/1299] D_loss: -0.0875, G_loss: 0.3320\n",
      "  Batch [720/1299] D_loss: -0.5881, G_loss: -1.0278\n",
      "  Batch [730/1299] D_loss: -1.3728, G_loss: -1.6900\n",
      "  Batch [740/1299] D_loss: -0.5101, G_loss: 0.0516\n",
      "  Batch [750/1299] D_loss: -0.2991, G_loss: -0.3535\n",
      "  Batch [760/1299] D_loss: -1.1393, G_loss: -1.0563\n",
      "  Batch [770/1299] D_loss: -0.3949, G_loss: -0.1004\n",
      "  Batch [780/1299] D_loss: -0.5324, G_loss: 0.0999\n",
      "  Batch [790/1299] D_loss: -0.3914, G_loss: -0.0622\n",
      "  Batch [800/1299] D_loss: -0.0790, G_loss: 0.2989\n",
      "  Batch [810/1299] D_loss: -0.1096, G_loss: 0.5421\n",
      "  Batch [820/1299] D_loss: -0.2213, G_loss: 0.5377\n",
      "  Batch [830/1299] D_loss: -0.1691, G_loss: 0.6384\n",
      "  Batch [840/1299] D_loss: -0.1350, G_loss: 0.5142\n",
      "  Batch [850/1299] D_loss: -0.0632, G_loss: 0.2901\n",
      "  Batch [860/1299] D_loss: -2.2712, G_loss: -5.4816\n",
      "  Batch [870/1299] D_loss: -0.0399, G_loss: 0.0429\n",
      "  Batch [880/1299] D_loss: -0.0143, G_loss: 0.1775\n",
      "  Batch [890/1299] D_loss: -0.1159, G_loss: 0.3593\n",
      "  Batch [900/1299] D_loss: -0.0722, G_loss: 0.3851\n",
      "  Batch [910/1299] D_loss: -0.0124, G_loss: 0.3613\n",
      "  Batch [920/1299] D_loss: -0.0319, G_loss: 0.4045\n",
      "  Batch [930/1299] D_loss: -0.0203, G_loss: 0.2149\n",
      "  Batch [940/1299] D_loss: -0.5733, G_loss: -2.3619\n",
      "  Batch [950/1299] D_loss: -0.1162, G_loss: -0.0498\n",
      "  Batch [960/1299] D_loss: -0.4698, G_loss: -0.1805\n",
      "  Batch [970/1299] D_loss: -0.0606, G_loss: 0.2239\n",
      "  Batch [980/1299] D_loss: -0.1379, G_loss: 0.5292\n",
      "  Batch [990/1299] D_loss: -0.2144, G_loss: 0.5345\n",
      "  Batch [1000/1299] D_loss: -0.1158, G_loss: 0.5937\n",
      "  Batch [1010/1299] D_loss: 0.0207, G_loss: 0.4164\n",
      "  Batch [1020/1299] D_loss: -0.0635, G_loss: 0.2876\n",
      "  Batch [1030/1299] D_loss: -1.0347, G_loss: -0.6788\n",
      "  Batch [1040/1299] D_loss: -0.9233, G_loss: -0.0903\n",
      "  Batch [1050/1299] D_loss: -0.5768, G_loss: 0.0539\n",
      "  Batch [1060/1299] D_loss: -0.3072, G_loss: -0.1318\n",
      "  Batch [1070/1299] D_loss: -0.2769, G_loss: -0.1159\n",
      "  Batch [1080/1299] D_loss: -0.1245, G_loss: 0.3890\n",
      "  Batch [1090/1299] D_loss: -0.2001, G_loss: 0.5258\n",
      "  Batch [1100/1299] D_loss: -0.1376, G_loss: 0.5724\n",
      "  Batch [1110/1299] D_loss: -0.0892, G_loss: 0.5688\n",
      "  Batch [1120/1299] D_loss: 0.0590, G_loss: 0.3999\n",
      "  Batch [1130/1299] D_loss: 0.0283, G_loss: 0.4001\n",
      "  Batch [1140/1299] D_loss: -0.3629, G_loss: -0.3976\n",
      "  Batch [1150/1299] D_loss: -0.0293, G_loss: 0.2065\n",
      "  Batch [1160/1299] D_loss: -0.0626, G_loss: 0.2355\n",
      "  Batch [1170/1299] D_loss: -0.0826, G_loss: 0.2845\n",
      "  Batch [1180/1299] D_loss: -0.0206, G_loss: 0.2428\n",
      "  Batch [1190/1299] D_loss: -0.9459, G_loss: -2.2755\n",
      "  Batch [1200/1299] D_loss: -0.3679, G_loss: -0.0420\n",
      "  Batch [1210/1299] D_loss: -0.3868, G_loss: -0.9997\n",
      "  Batch [1220/1299] D_loss: -0.0438, G_loss: 0.1334\n",
      "  Batch [1230/1299] D_loss: -0.0653, G_loss: 0.2603\n",
      "  Batch [1240/1299] D_loss: -0.0746, G_loss: 0.2954\n",
      "  Batch [1250/1299] D_loss: -0.0429, G_loss: 0.2879\n",
      "  Batch [1260/1299] D_loss: -1.4159, G_loss: -1.9128\n",
      "  Batch [1270/1299] D_loss: -0.1037, G_loss: -0.0616\n",
      "  Batch [1280/1299] D_loss: -0.0359, G_loss: 0.2244\n",
      "  Batch [1290/1299] D_loss: -0.0089, G_loss: 0.4640\n",
      "\n",
      "Epoch 71 Summary:\n",
      "  Average D_loss: -0.1476\n",
      "  Average G_loss: -0.0729\n",
      "\n",
      "Epoch [72/100]\n",
      "  Batch [0/1299] D_loss: -0.0365, G_loss: 0.6378\n",
      "  Batch [10/1299] D_loss: -0.0966, G_loss: 0.5729\n",
      "  Batch [20/1299] D_loss: -0.0668, G_loss: 0.5121\n",
      "  Batch [30/1299] D_loss: -0.0288, G_loss: 0.3165\n",
      "  Batch [40/1299] D_loss: -0.1690, G_loss: -0.6557\n",
      "  Batch [50/1299] D_loss: -0.1594, G_loss: -0.0884\n",
      "  Batch [60/1299] D_loss: -0.2107, G_loss: -0.0996\n",
      "  Batch [70/1299] D_loss: -0.7732, G_loss: -0.0028\n",
      "  Batch [80/1299] D_loss: -0.0268, G_loss: 0.1069\n",
      "  Batch [90/1299] D_loss: -0.0456, G_loss: 0.0724\n",
      "  Batch [100/1299] D_loss: -0.7568, G_loss: -0.5770\n",
      "  Batch [110/1299] D_loss: -0.3256, G_loss: 0.0965\n",
      "  Batch [120/1299] D_loss: -0.3538, G_loss: -0.5750\n",
      "  Batch [130/1299] D_loss: -0.4297, G_loss: -0.7193\n",
      "  Batch [140/1299] D_loss: -0.0660, G_loss: 0.4412\n",
      "  Batch [150/1299] D_loss: -0.1622, G_loss: 0.6507\n",
      "  Batch [160/1299] D_loss: -0.1274, G_loss: 0.6609\n",
      "  Batch [170/1299] D_loss: 0.0080, G_loss: 0.4453\n",
      "  Batch [180/1299] D_loss: -0.0679, G_loss: 0.3580\n",
      "  Batch [190/1299] D_loss: -0.5952, G_loss: -0.1625\n",
      "  Batch [200/1299] D_loss: -0.3969, G_loss: -0.4220\n",
      "  Batch [210/1299] D_loss: -1.2885, G_loss: -1.8247\n",
      "  Batch [220/1299] D_loss: -0.1491, G_loss: 0.0283\n",
      "  Batch [230/1299] D_loss: -0.4735, G_loss: -0.5622\n",
      "  Batch [240/1299] D_loss: -0.0223, G_loss: 0.2227\n",
      "  Batch [250/1299] D_loss: -0.1217, G_loss: 0.3658\n",
      "  Batch [260/1299] D_loss: -0.0276, G_loss: 0.3324\n",
      "  Batch [270/1299] D_loss: -0.0004, G_loss: 0.4101\n",
      "  Batch [280/1299] D_loss: 0.0095, G_loss: 0.3968\n",
      "  Batch [290/1299] D_loss: -0.1016, G_loss: 0.3870\n",
      "  Batch [300/1299] D_loss: -1.7302, G_loss: -2.2286\n",
      "  Batch [310/1299] D_loss: -1.1237, G_loss: -0.5420\n",
      "  Batch [320/1299] D_loss: -0.9415, G_loss: -2.2332\n",
      "  Batch [330/1299] D_loss: -0.0272, G_loss: 0.3574\n",
      "  Batch [340/1299] D_loss: -0.0842, G_loss: 0.4645\n",
      "  Batch [350/1299] D_loss: -0.0422, G_loss: 0.4726\n",
      "  Batch [360/1299] D_loss: -0.0363, G_loss: 0.5317\n",
      "  Batch [370/1299] D_loss: -0.1889, G_loss: 0.3324\n",
      "  Batch [380/1299] D_loss: -0.5419, G_loss: -1.7950\n",
      "  Batch [390/1299] D_loss: -0.2592, G_loss: -0.2743\n",
      "  Batch [400/1299] D_loss: -0.5940, G_loss: -0.6332\n",
      "  Batch [410/1299] D_loss: -0.0790, G_loss: 0.2855\n",
      "  Batch [420/1299] D_loss: -0.1352, G_loss: 0.5719\n",
      "  Batch [430/1299] D_loss: -0.0097, G_loss: 0.4606\n",
      "  Batch [440/1299] D_loss: -0.0711, G_loss: 0.5975\n",
      "  Batch [450/1299] D_loss: -0.0720, G_loss: 0.4123\n",
      "  Batch [460/1299] D_loss: -0.0572, G_loss: 0.3090\n",
      "  Batch [470/1299] D_loss: -1.0604, G_loss: -2.0566\n",
      "  Batch [480/1299] D_loss: 0.0125, G_loss: 0.2181\n",
      "  Batch [490/1299] D_loss: -0.0317, G_loss: 0.2494\n",
      "  Batch [500/1299] D_loss: -0.0155, G_loss: 0.2249\n",
      "  Batch [510/1299] D_loss: -3.0257, G_loss: -4.4108\n",
      "  Batch [520/1299] D_loss: -0.0279, G_loss: 0.2244\n",
      "  Batch [530/1299] D_loss: -0.0791, G_loss: 0.3319\n",
      "  Batch [540/1299] D_loss: -0.0270, G_loss: 0.3890\n",
      "  Batch [550/1299] D_loss: -0.1377, G_loss: 0.4364\n",
      "  Batch [560/1299] D_loss: -0.0526, G_loss: 0.3146\n",
      "  Batch [570/1299] D_loss: 0.0201, G_loss: 0.1212\n",
      "  Batch [580/1299] D_loss: -0.6789, G_loss: -2.0844\n",
      "  Batch [590/1299] D_loss: -0.0051, G_loss: 0.0374\n",
      "  Batch [600/1299] D_loss: -0.0184, G_loss: 0.0986\n",
      "  Batch [610/1299] D_loss: -0.0339, G_loss: 0.2232\n",
      "  Batch [620/1299] D_loss: 0.0416, G_loss: 0.3788\n",
      "  Batch [630/1299] D_loss: -0.0504, G_loss: 0.4066\n",
      "  Batch [640/1299] D_loss: 0.0114, G_loss: 0.3026\n",
      "  Batch [650/1299] D_loss: -0.0274, G_loss: 0.2722\n",
      "  Batch [660/1299] D_loss: -0.0467, G_loss: 0.0379\n",
      "  Batch [670/1299] D_loss: -0.7566, G_loss: -0.9975\n",
      "  Batch [680/1299] D_loss: -0.4462, G_loss: -0.0161\n",
      "  Batch [690/1299] D_loss: -0.5450, G_loss: -2.1717\n",
      "  Batch [700/1299] D_loss: -0.0638, G_loss: 0.2885\n",
      "  Batch [710/1299] D_loss: -0.1449, G_loss: 0.4211\n",
      "  Batch [720/1299] D_loss: -0.0903, G_loss: 0.4449\n",
      "  Batch [730/1299] D_loss: 0.0022, G_loss: 0.4931\n",
      "  Batch [740/1299] D_loss: -0.0700, G_loss: 0.3675\n",
      "  Batch [750/1299] D_loss: -0.0342, G_loss: 0.3837\n",
      "  Batch [760/1299] D_loss: -0.9487, G_loss: -2.6617\n",
      "  Batch [770/1299] D_loss: -0.3779, G_loss: -0.2439\n",
      "  Batch [780/1299] D_loss: -0.6496, G_loss: -0.8525\n",
      "  Batch [790/1299] D_loss: -0.0494, G_loss: 0.0836\n",
      "  Batch [800/1299] D_loss: -0.2930, G_loss: -0.3895\n",
      "  Batch [810/1299] D_loss: -0.4492, G_loss: -0.0208\n",
      "  Batch [820/1299] D_loss: -0.3622, G_loss: 0.0656\n",
      "  Batch [830/1299] D_loss: -0.1385, G_loss: 0.1550\n",
      "  Batch [840/1299] D_loss: -1.4303, G_loss: -0.4223\n",
      "  Batch [850/1299] D_loss: -0.2083, G_loss: 0.2334\n",
      "  Batch [860/1299] D_loss: -0.0763, G_loss: 0.4347\n",
      "  Batch [870/1299] D_loss: -0.0606, G_loss: 0.5742\n",
      "  Batch [880/1299] D_loss: -0.0764, G_loss: 0.6188\n",
      "  Batch [890/1299] D_loss: -0.0271, G_loss: 0.4988\n",
      "  Batch [900/1299] D_loss: -0.0432, G_loss: 0.4395\n",
      "  Batch [910/1299] D_loss: -0.0623, G_loss: 0.2979\n",
      "  Batch [920/1299] D_loss: -1.8009, G_loss: -3.7582\n",
      "  Batch [930/1299] D_loss: -0.3308, G_loss: -0.0428\n",
      "  Batch [940/1299] D_loss: 0.0046, G_loss: 0.1225\n",
      "  Batch [950/1299] D_loss: -0.1054, G_loss: 0.2303\n",
      "  Batch [960/1299] D_loss: -0.0543, G_loss: 0.3776\n",
      "  Batch [970/1299] D_loss: -0.1128, G_loss: 0.4146\n",
      "  Batch [980/1299] D_loss: -0.0623, G_loss: 0.4338\n",
      "  Batch [990/1299] D_loss: -0.1160, G_loss: 0.3300\n",
      "  Batch [1000/1299] D_loss: -0.4839, G_loss: -1.3790\n",
      "  Batch [1010/1299] D_loss: -0.2954, G_loss: 0.1368\n",
      "  Batch [1020/1299] D_loss: -1.2703, G_loss: -0.1305\n",
      "  Batch [1030/1299] D_loss: -0.0501, G_loss: 0.0219\n",
      "  Batch [1040/1299] D_loss: -0.3887, G_loss: -0.1436\n",
      "  Batch [1050/1299] D_loss: -0.0981, G_loss: -0.1789\n",
      "  Batch [1060/1299] D_loss: -1.3393, G_loss: -1.5108\n",
      "  Batch [1070/1299] D_loss: -0.4276, G_loss: -0.5405\n",
      "  Batch [1080/1299] D_loss: -0.0669, G_loss: 0.2238\n",
      "  Batch [1090/1299] D_loss: -0.0498, G_loss: 0.5049\n",
      "  Batch [1100/1299] D_loss: -0.0506, G_loss: 0.5760\n",
      "  Batch [1110/1299] D_loss: -0.1142, G_loss: 0.7142\n",
      "  Batch [1120/1299] D_loss: -0.1345, G_loss: 0.6680\n",
      "  Batch [1130/1299] D_loss: -0.1141, G_loss: 0.4248\n",
      "  Batch [1140/1299] D_loss: -0.0818, G_loss: 0.2313\n",
      "  Batch [1150/1299] D_loss: -0.0024, G_loss: 0.1440\n",
      "  Batch [1160/1299] D_loss: -0.0477, G_loss: 0.2919\n",
      "  Batch [1170/1299] D_loss: -0.0521, G_loss: 0.2616\n",
      "  Batch [1180/1299] D_loss: 0.0194, G_loss: 0.2957\n",
      "  Batch [1190/1299] D_loss: -0.0964, G_loss: 0.2487\n",
      "  Batch [1200/1299] D_loss: -1.1825, G_loss: -2.1037\n",
      "  Batch [1210/1299] D_loss: -0.5656, G_loss: -0.6732\n",
      "  Batch [1220/1299] D_loss: -0.3500, G_loss: -0.0574\n",
      "  Batch [1230/1299] D_loss: -0.0711, G_loss: 0.3054\n",
      "  Batch [1240/1299] D_loss: -0.1017, G_loss: 0.3459\n",
      "  Batch [1250/1299] D_loss: -0.0309, G_loss: 0.4056\n",
      "  Batch [1260/1299] D_loss: -0.0912, G_loss: 0.4085\n",
      "  Batch [1270/1299] D_loss: -0.0348, G_loss: 0.2471\n",
      "  Batch [1280/1299] D_loss: -0.9576, G_loss: 0.0258\n",
      "  Batch [1290/1299] D_loss: -0.0781, G_loss: 0.2801\n",
      "\n",
      "Epoch 72 Summary:\n",
      "  Average D_loss: -0.1709\n",
      "  Average G_loss: -0.0688\n",
      "\n",
      "Epoch [73/100]\n",
      "  Batch [0/1299] D_loss: -0.0460, G_loss: 0.3451\n",
      "  Batch [10/1299] D_loss: -0.1163, G_loss: 0.3635\n",
      "  Batch [20/1299] D_loss: -0.1150, G_loss: 0.4830\n",
      "  Batch [30/1299] D_loss: -0.0589, G_loss: 0.2765\n",
      "  Batch [40/1299] D_loss: -2.4353, G_loss: -4.4115\n",
      "  Batch [50/1299] D_loss: 0.0101, G_loss: 0.1988\n",
      "  Batch [60/1299] D_loss: -0.0148, G_loss: 0.2476\n",
      "  Batch [70/1299] D_loss: -1.2001, G_loss: -2.7058\n",
      "  Batch [80/1299] D_loss: -0.8516, G_loss: -0.2597\n",
      "  Batch [90/1299] D_loss: -1.9700, G_loss: -1.5782\n",
      "  Batch [100/1299] D_loss: -0.2648, G_loss: -0.2635\n",
      "  Batch [110/1299] D_loss: -0.0340, G_loss: 0.3622\n",
      "  Batch [120/1299] D_loss: -0.1921, G_loss: 0.6636\n",
      "  Batch [130/1299] D_loss: -0.0478, G_loss: 0.6253\n",
      "  Batch [140/1299] D_loss: -0.0771, G_loss: 0.6552\n",
      "  Batch [150/1299] D_loss: -0.1008, G_loss: 0.6027\n",
      "  Batch [160/1299] D_loss: -0.0726, G_loss: 0.4855\n",
      "  Batch [170/1299] D_loss: -0.0977, G_loss: -0.8137\n",
      "  Batch [180/1299] D_loss: -0.3203, G_loss: -0.0707\n",
      "  Batch [190/1299] D_loss: -0.2124, G_loss: -1.3348\n",
      "  Batch [200/1299] D_loss: -0.1655, G_loss: -0.0901\n",
      "  Batch [210/1299] D_loss: -0.4233, G_loss: -1.2336\n",
      "  Batch [220/1299] D_loss: -1.8361, G_loss: 0.0201\n",
      "  Batch [230/1299] D_loss: -0.8499, G_loss: -0.7582\n",
      "  Batch [240/1299] D_loss: -0.0810, G_loss: 0.3235\n",
      "  Batch [250/1299] D_loss: -0.1088, G_loss: 0.5400\n",
      "  Batch [260/1299] D_loss: -0.1642, G_loss: 0.5887\n",
      "  Batch [270/1299] D_loss: -0.0649, G_loss: 0.5483\n",
      "  Batch [280/1299] D_loss: -0.0317, G_loss: 0.3013\n",
      "  Batch [290/1299] D_loss: -0.1215, G_loss: 0.4032\n",
      "  Batch [300/1299] D_loss: -1.3718, G_loss: -0.9070\n",
      "  Batch [310/1299] D_loss: -0.0269, G_loss: 0.1447\n",
      "  Batch [320/1299] D_loss: -0.0374, G_loss: 0.2241\n",
      "  Batch [330/1299] D_loss: 0.0061, G_loss: 0.3435\n",
      "  Batch [340/1299] D_loss: 0.0107, G_loss: 0.2670\n",
      "  Batch [350/1299] D_loss: -0.0609, G_loss: 0.2070\n",
      "  Batch [360/1299] D_loss: -1.1012, G_loss: -1.2598\n",
      "  Batch [370/1299] D_loss: -0.7740, G_loss: -1.2547\n",
      "  Batch [380/1299] D_loss: -0.0070, G_loss: 0.2086\n",
      "  Batch [390/1299] D_loss: -0.1073, G_loss: 0.4136\n",
      "  Batch [400/1299] D_loss: -0.0472, G_loss: 0.3598\n",
      "  Batch [410/1299] D_loss: 0.0238, G_loss: 0.3399\n",
      "  Batch [420/1299] D_loss: -0.0260, G_loss: 0.2903\n",
      "  Batch [430/1299] D_loss: -1.6082, G_loss: -2.0708\n",
      "  Batch [440/1299] D_loss: -0.3075, G_loss: 0.0669\n",
      "  Batch [450/1299] D_loss: -0.2902, G_loss: -0.1509\n",
      "  Batch [460/1299] D_loss: -0.0483, G_loss: 0.2701\n",
      "  Batch [470/1299] D_loss: -0.0967, G_loss: 0.3936\n",
      "  Batch [480/1299] D_loss: -0.0797, G_loss: 0.4464\n",
      "  Batch [490/1299] D_loss: -0.0224, G_loss: 0.4324\n",
      "  Batch [500/1299] D_loss: -0.0764, G_loss: 0.4151\n",
      "  Batch [510/1299] D_loss: -1.7831, G_loss: -3.3082\n",
      "  Batch [520/1299] D_loss: -0.6830, G_loss: -0.7049\n",
      "  Batch [530/1299] D_loss: -1.8737, G_loss: -1.7080\n",
      "  Batch [540/1299] D_loss: 0.0571, G_loss: 0.0112\n",
      "  Batch [550/1299] D_loss: -0.0228, G_loss: 0.2917\n",
      "  Batch [560/1299] D_loss: -0.0865, G_loss: 0.3765\n",
      "  Batch [570/1299] D_loss: -0.0693, G_loss: 0.4587\n",
      "  Batch [580/1299] D_loss: -0.0142, G_loss: 0.3601\n",
      "  Batch [590/1299] D_loss: -0.0155, G_loss: 0.4331\n",
      "  Batch [600/1299] D_loss: -0.0217, G_loss: 0.2396\n",
      "  Batch [610/1299] D_loss: -0.3574, G_loss: -0.0051\n",
      "  Batch [620/1299] D_loss: 0.0933, G_loss: 0.1809\n",
      "  Batch [630/1299] D_loss: -0.0447, G_loss: 0.2306\n",
      "  Batch [640/1299] D_loss: -0.0263, G_loss: 0.3705\n",
      "  Batch [650/1299] D_loss: -0.0017, G_loss: 0.3352\n",
      "  Batch [660/1299] D_loss: -0.0143, G_loss: 0.2741\n",
      "  Batch [670/1299] D_loss: -0.0885, G_loss: 0.1854\n",
      "  Batch [680/1299] D_loss: -0.0013, G_loss: 0.1491\n",
      "  Batch [690/1299] D_loss: -0.0421, G_loss: 0.2976\n",
      "  Batch [700/1299] D_loss: -0.1297, G_loss: 0.4818\n",
      "  Batch [710/1299] D_loss: -0.0639, G_loss: 0.4041\n",
      "  Batch [720/1299] D_loss: 0.0388, G_loss: 0.3065\n",
      "  Batch [730/1299] D_loss: -1.5425, G_loss: -5.3719\n",
      "  Batch [740/1299] D_loss: -0.0378, G_loss: 0.0605\n",
      "  Batch [750/1299] D_loss: -0.7721, G_loss: -0.3541\n",
      "  Batch [760/1299] D_loss: -0.0447, G_loss: 0.2066\n",
      "  Batch [770/1299] D_loss: -0.0768, G_loss: 0.3578\n",
      "  Batch [780/1299] D_loss: -0.1077, G_loss: 0.4999\n",
      "  Batch [790/1299] D_loss: -0.0028, G_loss: 0.4204\n",
      "  Batch [800/1299] D_loss: -0.0302, G_loss: 0.4036\n",
      "  Batch [810/1299] D_loss: 0.0369, G_loss: 0.2810\n",
      "  Batch [820/1299] D_loss: -1.6552, G_loss: -4.0800\n",
      "  Batch [830/1299] D_loss: -0.0428, G_loss: 0.2075\n",
      "  Batch [840/1299] D_loss: -0.0494, G_loss: 0.2977\n",
      "  Batch [850/1299] D_loss: -0.0256, G_loss: 0.2810\n",
      "  Batch [860/1299] D_loss: -0.0270, G_loss: 0.4054\n",
      "  Batch [870/1299] D_loss: -0.0422, G_loss: 0.2525\n",
      "  Batch [880/1299] D_loss: -1.1326, G_loss: -1.0137\n",
      "  Batch [890/1299] D_loss: -0.2143, G_loss: -0.1095\n",
      "  Batch [900/1299] D_loss: -0.3195, G_loss: -0.3398\n",
      "  Batch [910/1299] D_loss: -0.3505, G_loss: -0.4552\n",
      "  Batch [920/1299] D_loss: -0.0172, G_loss: 0.2378\n",
      "  Batch [930/1299] D_loss: -0.0558, G_loss: 0.3138\n",
      "  Batch [940/1299] D_loss: -0.0398, G_loss: 0.4191\n",
      "  Batch [950/1299] D_loss: -0.0776, G_loss: 0.4517\n",
      "  Batch [960/1299] D_loss: -0.0099, G_loss: 0.3045\n",
      "  Batch [970/1299] D_loss: -1.1679, G_loss: -3.1447\n",
      "  Batch [980/1299] D_loss: -0.0047, G_loss: 0.2052\n",
      "  Batch [990/1299] D_loss: -0.0733, G_loss: 0.3548\n",
      "  Batch [1000/1299] D_loss: -0.0121, G_loss: 0.4167\n",
      "  Batch [1010/1299] D_loss: -0.0505, G_loss: 0.4346\n",
      "  Batch [1020/1299] D_loss: 0.0442, G_loss: 0.3218\n",
      "  Batch [1030/1299] D_loss: -0.4583, G_loss: -0.4198\n",
      "  Batch [1040/1299] D_loss: -0.0581, G_loss: 0.0931\n",
      "  Batch [1050/1299] D_loss: -0.1627, G_loss: 0.0279\n",
      "  Batch [1060/1299] D_loss: -0.0225, G_loss: 0.1688\n",
      "  Batch [1070/1299] D_loss: -0.0450, G_loss: 0.1713\n",
      "  Batch [1080/1299] D_loss: -0.0798, G_loss: 0.3312\n",
      "  Batch [1090/1299] D_loss: -0.0544, G_loss: 0.3311\n",
      "  Batch [1100/1299] D_loss: -0.0594, G_loss: 0.1670\n",
      "  Batch [1110/1299] D_loss: -0.1099, G_loss: 0.0194\n",
      "  Batch [1120/1299] D_loss: -0.4108, G_loss: -1.2443\n",
      "  Batch [1130/1299] D_loss: -1.4072, G_loss: -2.3237\n",
      "  Batch [1140/1299] D_loss: -0.0640, G_loss: 0.1057\n",
      "  Batch [1150/1299] D_loss: -0.3707, G_loss: -0.9877\n",
      "  Batch [1160/1299] D_loss: -0.1245, G_loss: 0.3170\n",
      "  Batch [1170/1299] D_loss: -0.0399, G_loss: 0.4381\n",
      "  Batch [1180/1299] D_loss: -0.1378, G_loss: 0.5345\n",
      "  Batch [1190/1299] D_loss: -0.0004, G_loss: 0.4692\n",
      "  Batch [1200/1299] D_loss: -0.1444, G_loss: 0.4516\n",
      "  Batch [1210/1299] D_loss: -0.2728, G_loss: -1.5859\n",
      "  Batch [1220/1299] D_loss: -0.5712, G_loss: -1.3126\n",
      "  Batch [1230/1299] D_loss: 0.1353, G_loss: 0.0632\n",
      "  Batch [1240/1299] D_loss: -0.1151, G_loss: 0.0999\n",
      "  Batch [1250/1299] D_loss: -0.1538, G_loss: 0.3314\n",
      "  Batch [1260/1299] D_loss: -0.1215, G_loss: 0.5052\n",
      "  Batch [1270/1299] D_loss: -0.1527, G_loss: 0.5990\n",
      "  Batch [1280/1299] D_loss: -0.0137, G_loss: 0.5101\n",
      "  Batch [1290/1299] D_loss: -0.0155, G_loss: 0.4062\n",
      "\n",
      "Epoch 73 Summary:\n",
      "  Average D_loss: -0.1417\n",
      "  Average G_loss: -0.0546\n",
      "\n",
      "Epoch [74/100]\n",
      "  Batch [0/1299] D_loss: 0.0127, G_loss: 0.2569\n",
      "  Batch [10/1299] D_loss: -1.0493, G_loss: -4.0459\n",
      "  Batch [20/1299] D_loss: -1.0921, G_loss: -1.3775\n",
      "  Batch [30/1299] D_loss: -0.0395, G_loss: 0.0460\n",
      "  Batch [40/1299] D_loss: -0.3439, G_loss: -0.2279\n",
      "  Batch [50/1299] D_loss: -0.6867, G_loss: -2.9617\n",
      "  Batch [60/1299] D_loss: -0.0461, G_loss: 0.2837\n",
      "  Batch [70/1299] D_loss: -0.0460, G_loss: 0.4463\n",
      "  Batch [80/1299] D_loss: -0.1552, G_loss: 0.4459\n",
      "  Batch [90/1299] D_loss: -0.1154, G_loss: 0.6325\n",
      "  Batch [100/1299] D_loss: -0.0102, G_loss: 0.3181\n",
      "  Batch [110/1299] D_loss: -0.0700, G_loss: 0.3340\n",
      "  Batch [120/1299] D_loss: -2.1019, G_loss: -0.8154\n",
      "  Batch [130/1299] D_loss: -0.2667, G_loss: -0.5126\n",
      "  Batch [140/1299] D_loss: -0.0217, G_loss: 0.1739\n",
      "  Batch [150/1299] D_loss: -0.0756, G_loss: 0.2691\n",
      "  Batch [160/1299] D_loss: -0.0465, G_loss: 0.3057\n",
      "  Batch [170/1299] D_loss: -0.0993, G_loss: 0.3571\n",
      "  Batch [180/1299] D_loss: -0.0693, G_loss: 0.2740\n",
      "  Batch [190/1299] D_loss: -1.2282, G_loss: -0.5601\n",
      "  Batch [200/1299] D_loss: -0.0534, G_loss: 0.1029\n",
      "  Batch [210/1299] D_loss: 0.0050, G_loss: 0.1732\n",
      "  Batch [220/1299] D_loss: -0.0274, G_loss: 0.2908\n",
      "  Batch [230/1299] D_loss: -0.0083, G_loss: 0.2439\n",
      "  Batch [240/1299] D_loss: -0.1322, G_loss: 0.3406\n",
      "  Batch [250/1299] D_loss: -0.1055, G_loss: -0.0261\n",
      "  Batch [260/1299] D_loss: -0.9167, G_loss: -0.2708\n",
      "  Batch [270/1299] D_loss: -0.0077, G_loss: 0.1810\n",
      "  Batch [280/1299] D_loss: 0.0295, G_loss: 0.2796\n",
      "  Batch [290/1299] D_loss: 0.0298, G_loss: 0.3899\n",
      "  Batch [300/1299] D_loss: -0.0538, G_loss: 0.3726\n",
      "  Batch [310/1299] D_loss: -0.1211, G_loss: 0.2923\n",
      "  Batch [320/1299] D_loss: -0.3813, G_loss: -0.3742\n",
      "  Batch [330/1299] D_loss: -0.0155, G_loss: 0.1752\n",
      "  Batch [340/1299] D_loss: -0.2017, G_loss: 0.1662\n",
      "  Batch [350/1299] D_loss: -0.0201, G_loss: 0.1373\n",
      "  Batch [360/1299] D_loss: -0.0504, G_loss: 0.1766\n",
      "  Batch [370/1299] D_loss: -0.0827, G_loss: 0.3133\n",
      "  Batch [380/1299] D_loss: -0.0906, G_loss: 0.3578\n",
      "  Batch [390/1299] D_loss: -0.0371, G_loss: 0.6175\n",
      "  Batch [400/1299] D_loss: -0.1151, G_loss: 0.4483\n",
      "  Batch [410/1299] D_loss: -0.0739, G_loss: 0.2432\n",
      "  Batch [420/1299] D_loss: 0.0052, G_loss: 0.1553\n",
      "  Batch [430/1299] D_loss: 0.0130, G_loss: 0.2676\n",
      "  Batch [440/1299] D_loss: -0.0676, G_loss: 0.4017\n",
      "  Batch [450/1299] D_loss: -0.0146, G_loss: 0.2770\n",
      "  Batch [460/1299] D_loss: -0.1107, G_loss: 0.4329\n",
      "  Batch [470/1299] D_loss: -1.4178, G_loss: -3.4422\n",
      "  Batch [480/1299] D_loss: -0.0229, G_loss: 0.0991\n",
      "  Batch [490/1299] D_loss: -0.4677, G_loss: -1.5598\n",
      "  Batch [500/1299] D_loss: -1.8171, G_loss: -4.5838\n",
      "  Batch [510/1299] D_loss: -0.6983, G_loss: -0.6317\n",
      "  Batch [520/1299] D_loss: -0.9406, G_loss: -0.8090\n",
      "  Batch [530/1299] D_loss: -0.1241, G_loss: 0.1744\n",
      "  Batch [540/1299] D_loss: -0.1049, G_loss: 0.3399\n",
      "  Batch [550/1299] D_loss: -0.0890, G_loss: 0.4479\n",
      "  Batch [560/1299] D_loss: -0.0023, G_loss: 0.4242\n",
      "  Batch [570/1299] D_loss: -0.0547, G_loss: 0.3821\n",
      "  Batch [580/1299] D_loss: -0.0406, G_loss: 0.2508\n",
      "  Batch [590/1299] D_loss: 0.0057, G_loss: 0.0846\n",
      "  Batch [600/1299] D_loss: -0.0186, G_loss: 0.1071\n",
      "  Batch [610/1299] D_loss: -0.0082, G_loss: 0.1346\n",
      "  Batch [620/1299] D_loss: 0.0135, G_loss: 0.1603\n",
      "  Batch [630/1299] D_loss: -0.2315, G_loss: -0.2682\n",
      "  Batch [640/1299] D_loss: -0.0016, G_loss: 0.1006\n",
      "  Batch [650/1299] D_loss: -0.0263, G_loss: 0.1574\n",
      "  Batch [660/1299] D_loss: -0.0484, G_loss: 0.2037\n",
      "  Batch [670/1299] D_loss: -0.0745, G_loss: 0.1426\n",
      "  Batch [680/1299] D_loss: -0.0030, G_loss: 0.2006\n",
      "  Batch [690/1299] D_loss: 0.0232, G_loss: 0.1856\n",
      "  Batch [700/1299] D_loss: 0.0224, G_loss: 0.0941\n",
      "  Batch [710/1299] D_loss: -0.0048, G_loss: 0.1437\n",
      "  Batch [720/1299] D_loss: -0.4141, G_loss: 0.0032\n",
      "  Batch [730/1299] D_loss: -0.9455, G_loss: 0.1257\n",
      "  Batch [740/1299] D_loss: -0.0730, G_loss: 0.0329\n",
      "  Batch [750/1299] D_loss: -0.7275, G_loss: -0.1897\n",
      "  Batch [760/1299] D_loss: -0.5787, G_loss: -0.2047\n",
      "  Batch [770/1299] D_loss: -0.0445, G_loss: 0.1753\n",
      "  Batch [780/1299] D_loss: -0.1387, G_loss: 0.4612\n",
      "  Batch [790/1299] D_loss: -0.1048, G_loss: 0.6237\n",
      "  Batch [800/1299] D_loss: -0.1573, G_loss: 0.5862\n",
      "  Batch [810/1299] D_loss: -0.0953, G_loss: 0.3731\n",
      "  Batch [820/1299] D_loss: -0.0647, G_loss: 0.2129\n",
      "  Batch [830/1299] D_loss: -2.3292, G_loss: -5.3242\n",
      "  Batch [840/1299] D_loss: -0.4508, G_loss: -0.5845\n",
      "  Batch [850/1299] D_loss: -0.0371, G_loss: 0.0911\n",
      "  Batch [860/1299] D_loss: -0.0169, G_loss: 0.1765\n",
      "  Batch [870/1299] D_loss: -1.4520, G_loss: -0.2150\n",
      "  Batch [880/1299] D_loss: -0.3780, G_loss: -0.1904\n",
      "  Batch [890/1299] D_loss: -0.1049, G_loss: 0.1837\n",
      "  Batch [900/1299] D_loss: -0.0186, G_loss: 0.2400\n",
      "  Batch [910/1299] D_loss: -0.0240, G_loss: 0.3668\n",
      "  Batch [920/1299] D_loss: -0.2173, G_loss: 0.4455\n",
      "  Batch [930/1299] D_loss: -0.0435, G_loss: 0.4002\n",
      "  Batch [940/1299] D_loss: -0.1262, G_loss: 0.3287\n",
      "  Batch [950/1299] D_loss: -0.0885, G_loss: 0.2362\n",
      "  Batch [960/1299] D_loss: -0.6794, G_loss: -1.6175\n",
      "  Batch [970/1299] D_loss: -0.3226, G_loss: -0.2971\n",
      "  Batch [980/1299] D_loss: -0.2347, G_loss: -0.0181\n",
      "  Batch [990/1299] D_loss: -0.0233, G_loss: 0.1667\n",
      "  Batch [1000/1299] D_loss: -0.0848, G_loss: 0.2931\n",
      "  Batch [1010/1299] D_loss: -0.0422, G_loss: 0.3736\n",
      "  Batch [1020/1299] D_loss: -0.1043, G_loss: 0.3922\n",
      "  Batch [1030/1299] D_loss: -0.1016, G_loss: 0.3130\n",
      "  Batch [1040/1299] D_loss: -0.0114, G_loss: 0.3683\n",
      "  Batch [1050/1299] D_loss: -0.0482, G_loss: 0.2183\n",
      "  Batch [1060/1299] D_loss: -1.4460, G_loss: -3.1074\n",
      "  Batch [1070/1299] D_loss: -0.0079, G_loss: 0.2155\n",
      "  Batch [1080/1299] D_loss: -0.0630, G_loss: 0.3717\n",
      "  Batch [1090/1299] D_loss: -0.0416, G_loss: 0.6232\n",
      "  Batch [1100/1299] D_loss: -0.1663, G_loss: 0.4862\n",
      "  Batch [1110/1299] D_loss: -0.1247, G_loss: 0.3327\n",
      "  Batch [1120/1299] D_loss: -0.0958, G_loss: 0.2553\n",
      "  Batch [1130/1299] D_loss: -1.1204, G_loss: -4.9677\n",
      "  Batch [1140/1299] D_loss: -0.0409, G_loss: 0.1455\n",
      "  Batch [1150/1299] D_loss: -0.0260, G_loss: 0.2845\n",
      "  Batch [1160/1299] D_loss: -0.0377, G_loss: 0.3785\n",
      "  Batch [1170/1299] D_loss: -0.0056, G_loss: 0.4028\n",
      "  Batch [1180/1299] D_loss: -0.0256, G_loss: 0.2086\n",
      "  Batch [1190/1299] D_loss: -0.0537, G_loss: 0.1118\n",
      "  Batch [1200/1299] D_loss: -0.6564, G_loss: -0.6406\n",
      "  Batch [1210/1299] D_loss: -0.0239, G_loss: 0.0517\n",
      "  Batch [1220/1299] D_loss: -0.5232, G_loss: -1.8927\n",
      "  Batch [1230/1299] D_loss: -0.0068, G_loss: 0.0495\n",
      "  Batch [1240/1299] D_loss: -0.0104, G_loss: 0.1196\n",
      "  Batch [1250/1299] D_loss: -0.0314, G_loss: 0.3270\n",
      "  Batch [1260/1299] D_loss: -0.1024, G_loss: 0.4970\n",
      "  Batch [1270/1299] D_loss: -0.0801, G_loss: 0.4243\n",
      "  Batch [1280/1299] D_loss: -0.0681, G_loss: 0.4651\n",
      "  Batch [1290/1299] D_loss: -0.1242, G_loss: 0.3685\n",
      "\n",
      "Epoch 74 Summary:\n",
      "  Average D_loss: -0.1359\n",
      "  Average G_loss: -0.0767\n",
      "\n",
      "Epoch [75/100]\n",
      "  Batch [0/1299] D_loss: 0.0295, G_loss: 0.2496\n",
      "  Batch [10/1299] D_loss: -0.1302, G_loss: -0.0942\n",
      "  Batch [20/1299] D_loss: -0.2591, G_loss: -0.0058\n",
      "  Batch [30/1299] D_loss: -0.1284, G_loss: 0.0136\n",
      "  Batch [40/1299] D_loss: -0.2671, G_loss: -0.3381\n",
      "  Batch [50/1299] D_loss: -0.0178, G_loss: 0.1060\n",
      "  Batch [60/1299] D_loss: -0.3361, G_loss: -0.0444\n",
      "  Batch [70/1299] D_loss: -0.0539, G_loss: 0.1703\n",
      "  Batch [80/1299] D_loss: -0.0611, G_loss: 0.3358\n",
      "  Batch [90/1299] D_loss: -0.0642, G_loss: 0.4203\n",
      "  Batch [100/1299] D_loss: -0.0823, G_loss: 0.5226\n",
      "  Batch [110/1299] D_loss: -0.0705, G_loss: 0.3680\n",
      "  Batch [120/1299] D_loss: -0.0234, G_loss: 0.3218\n",
      "  Batch [130/1299] D_loss: -2.3589, G_loss: -4.4156\n",
      "  Batch [140/1299] D_loss: -0.0701, G_loss: 0.2803\n",
      "  Batch [150/1299] D_loss: 0.0058, G_loss: 0.3753\n",
      "  Batch [160/1299] D_loss: -0.0863, G_loss: 0.4259\n",
      "  Batch [170/1299] D_loss: 0.0119, G_loss: 0.4383\n",
      "  Batch [180/1299] D_loss: -0.0276, G_loss: 0.2753\n",
      "  Batch [190/1299] D_loss: 0.0147, G_loss: 0.1776\n",
      "  Batch [200/1299] D_loss: -0.5296, G_loss: 0.0361\n",
      "  Batch [210/1299] D_loss: -0.4867, G_loss: -0.7856\n",
      "  Batch [220/1299] D_loss: -0.8611, G_loss: -2.0467\n",
      "  Batch [230/1299] D_loss: -0.5509, G_loss: 0.0369\n",
      "  Batch [240/1299] D_loss: -0.3947, G_loss: 0.1211\n",
      "  Batch [250/1299] D_loss: -0.4756, G_loss: -0.8585\n",
      "  Batch [260/1299] D_loss: -0.9851, G_loss: -0.1317\n",
      "  Batch [270/1299] D_loss: -0.6349, G_loss: -0.0213\n",
      "  Batch [280/1299] D_loss: -0.3689, G_loss: 0.1480\n",
      "  Batch [290/1299] D_loss: -0.0543, G_loss: 0.4252\n",
      "  Batch [300/1299] D_loss: -0.2503, G_loss: 0.8262\n",
      "  Batch [310/1299] D_loss: 0.0040, G_loss: 0.7080\n",
      "  Batch [320/1299] D_loss: -0.0511, G_loss: 0.6236\n",
      "  Batch [330/1299] D_loss: -0.0575, G_loss: 0.4764\n",
      "  Batch [340/1299] D_loss: -2.6054, G_loss: -6.0102\n",
      "  Batch [350/1299] D_loss: -0.2780, G_loss: 0.1368\n",
      "  Batch [360/1299] D_loss: 0.0112, G_loss: 0.1349\n",
      "  Batch [370/1299] D_loss: -0.0046, G_loss: 0.3036\n",
      "  Batch [380/1299] D_loss: -0.0511, G_loss: 0.4229\n",
      "  Batch [390/1299] D_loss: -0.0290, G_loss: 0.5329\n",
      "  Batch [400/1299] D_loss: -0.0486, G_loss: 0.4296\n",
      "  Batch [410/1299] D_loss: -0.0999, G_loss: 0.4579\n",
      "  Batch [420/1299] D_loss: -0.1382, G_loss: 0.2866\n",
      "  Batch [430/1299] D_loss: -0.0357, G_loss: 0.2197\n",
      "  Batch [440/1299] D_loss: -0.1031, G_loss: 0.3135\n",
      "  Batch [450/1299] D_loss: -0.0709, G_loss: 0.3530\n",
      "  Batch [460/1299] D_loss: -0.1145, G_loss: 0.2732\n",
      "  Batch [470/1299] D_loss: -0.0624, G_loss: 0.2135\n",
      "  Batch [480/1299] D_loss: -0.0333, G_loss: -0.2127\n",
      "  Batch [490/1299] D_loss: 0.0042, G_loss: 0.1177\n",
      "  Batch [500/1299] D_loss: -0.0370, G_loss: 0.1281\n",
      "  Batch [510/1299] D_loss: -0.0442, G_loss: 0.2232\n",
      "  Batch [520/1299] D_loss: -0.0094, G_loss: 0.2135\n",
      "  Batch [530/1299] D_loss: -0.5826, G_loss: -0.6853\n",
      "  Batch [540/1299] D_loss: -0.0188, G_loss: 0.2557\n",
      "  Batch [550/1299] D_loss: -0.1011, G_loss: 0.3478\n",
      "  Batch [560/1299] D_loss: -0.0522, G_loss: 0.3756\n",
      "  Batch [570/1299] D_loss: -0.0675, G_loss: 0.4349\n",
      "  Batch [580/1299] D_loss: -0.1320, G_loss: 0.3199\n",
      "  Batch [590/1299] D_loss: -0.4432, G_loss: -0.5532\n",
      "  Batch [600/1299] D_loss: -0.0717, G_loss: 0.1752\n",
      "  Batch [610/1299] D_loss: -0.0502, G_loss: 0.3049\n",
      "  Batch [620/1299] D_loss: -0.0667, G_loss: 0.3756\n",
      "  Batch [630/1299] D_loss: -0.1937, G_loss: 0.4536\n",
      "  Batch [640/1299] D_loss: -0.0740, G_loss: 0.5011\n",
      "  Batch [650/1299] D_loss: 0.0225, G_loss: 0.2090\n",
      "  Batch [660/1299] D_loss: 0.0030, G_loss: 0.0984\n",
      "  Batch [670/1299] D_loss: -0.2236, G_loss: -0.8523\n",
      "  Batch [680/1299] D_loss: -0.0271, G_loss: 0.1836\n",
      "  Batch [690/1299] D_loss: -0.0501, G_loss: 0.2947\n",
      "  Batch [700/1299] D_loss: -0.0406, G_loss: 0.3157\n",
      "  Batch [710/1299] D_loss: -2.0411, G_loss: -1.2156\n",
      "  Batch [720/1299] D_loss: 0.0059, G_loss: 0.1389\n",
      "  Batch [730/1299] D_loss: -0.0025, G_loss: 0.2608\n",
      "  Batch [740/1299] D_loss: -0.1003, G_loss: 0.1995\n",
      "  Batch [750/1299] D_loss: -0.2699, G_loss: -0.6428\n",
      "  Batch [760/1299] D_loss: -0.1772, G_loss: -0.0378\n",
      "  Batch [770/1299] D_loss: 0.0004, G_loss: 0.2519\n",
      "  Batch [780/1299] D_loss: 0.0258, G_loss: 0.2573\n",
      "  Batch [790/1299] D_loss: -0.1157, G_loss: 0.3855\n",
      "  Batch [800/1299] D_loss: -0.1736, G_loss: 0.4208\n",
      "  Batch [810/1299] D_loss: -1.8325, G_loss: -2.0488\n",
      "  Batch [820/1299] D_loss: -0.6953, G_loss: -0.9529\n",
      "  Batch [830/1299] D_loss: -1.0805, G_loss: -3.8035\n",
      "  Batch [840/1299] D_loss: -0.0024, G_loss: 0.1771\n",
      "  Batch [850/1299] D_loss: -0.1098, G_loss: 0.3447\n",
      "  Batch [860/1299] D_loss: -0.0022, G_loss: 0.4397\n",
      "  Batch [870/1299] D_loss: -0.0703, G_loss: 0.4456\n",
      "  Batch [880/1299] D_loss: -0.0327, G_loss: 0.3544\n",
      "  Batch [890/1299] D_loss: -0.0380, G_loss: 0.2488\n",
      "  Batch [900/1299] D_loss: -0.0476, G_loss: -0.3769\n",
      "  Batch [910/1299] D_loss: -0.1951, G_loss: -0.1174\n",
      "  Batch [920/1299] D_loss: -1.0076, G_loss: -0.4581\n",
      "  Batch [930/1299] D_loss: -0.0660, G_loss: 0.3178\n",
      "  Batch [940/1299] D_loss: -0.0624, G_loss: 0.4266\n",
      "  Batch [950/1299] D_loss: 0.0248, G_loss: 0.4954\n",
      "  Batch [960/1299] D_loss: -0.0026, G_loss: 0.4002\n",
      "  Batch [970/1299] D_loss: -0.0563, G_loss: 0.3332\n",
      "  Batch [980/1299] D_loss: -0.0455, G_loss: 0.2934\n",
      "  Batch [990/1299] D_loss: -0.4486, G_loss: -1.4313\n",
      "  Batch [1000/1299] D_loss: -0.2177, G_loss: -0.6537\n",
      "  Batch [1010/1299] D_loss: -0.8048, G_loss: -1.3159\n",
      "  Batch [1020/1299] D_loss: -0.0575, G_loss: 0.0468\n",
      "  Batch [1030/1299] D_loss: -0.0701, G_loss: 0.2777\n",
      "  Batch [1040/1299] D_loss: -0.0877, G_loss: 0.3920\n",
      "  Batch [1050/1299] D_loss: -0.1096, G_loss: 0.4814\n",
      "  Batch [1060/1299] D_loss: -0.1141, G_loss: 0.4899\n",
      "  Batch [1070/1299] D_loss: -0.0427, G_loss: 0.3859\n",
      "  Batch [1080/1299] D_loss: -0.0567, G_loss: 0.2536\n",
      "  Batch [1090/1299] D_loss: -0.7221, G_loss: -0.3025\n",
      "  Batch [1100/1299] D_loss: -1.0096, G_loss: -2.6263\n",
      "  Batch [1110/1299] D_loss: -0.0548, G_loss: 0.0890\n",
      "  Batch [1120/1299] D_loss: -0.0793, G_loss: 0.3483\n",
      "  Batch [1130/1299] D_loss: -0.1238, G_loss: 0.4652\n",
      "  Batch [1140/1299] D_loss: -0.0956, G_loss: 0.4349\n",
      "  Batch [1150/1299] D_loss: 0.0056, G_loss: 0.5280\n",
      "  Batch [1160/1299] D_loss: 0.0224, G_loss: 0.2651\n",
      "  Batch [1170/1299] D_loss: -2.7772, G_loss: -5.3430\n",
      "  Batch [1180/1299] D_loss: -0.3519, G_loss: -0.0337\n",
      "  Batch [1190/1299] D_loss: -0.4974, G_loss: -0.7596\n",
      "  Batch [1200/1299] D_loss: -0.3163, G_loss: -0.5553\n",
      "  Batch [1210/1299] D_loss: -1.1016, G_loss: -2.7263\n",
      "  Batch [1220/1299] D_loss: -0.3622, G_loss: -0.1035\n",
      "  Batch [1230/1299] D_loss: -0.2707, G_loss: -0.3139\n",
      "  Batch [1240/1299] D_loss: -1.5808, G_loss: -0.9483\n",
      "  Batch [1250/1299] D_loss: -0.3137, G_loss: 0.0610\n",
      "  Batch [1260/1299] D_loss: -0.1160, G_loss: 0.0825\n",
      "  Batch [1270/1299] D_loss: -0.3905, G_loss: -0.1006\n",
      "  Batch [1280/1299] D_loss: -0.2245, G_loss: -0.1035\n",
      "  Batch [1290/1299] D_loss: -0.4312, G_loss: 0.0690\n",
      "\n",
      "Epoch 75 Summary:\n",
      "  Average D_loss: -0.1396\n",
      "  Average G_loss: -0.1008\n",
      "\n",
      "Epoch [76/100]\n",
      "  Batch [0/1299] D_loss: -0.3875, G_loss: 0.1955\n",
      "  Batch [10/1299] D_loss: -0.0928, G_loss: 0.2947\n",
      "  Batch [20/1299] D_loss: -0.2185, G_loss: 0.7602\n",
      "  Batch [30/1299] D_loss: -0.2502, G_loss: 1.1526\n",
      "  Batch [40/1299] D_loss: -0.2213, G_loss: 1.0978\n",
      "  Batch [50/1299] D_loss: -0.2272, G_loss: 0.6328\n",
      "  Batch [60/1299] D_loss: -0.1282, G_loss: 0.4661\n",
      "  Batch [70/1299] D_loss: -4.6886, G_loss: -3.9879\n",
      "  Batch [80/1299] D_loss: -0.0379, G_loss: 0.1895\n",
      "  Batch [90/1299] D_loss: -0.0641, G_loss: 0.1737\n",
      "  Batch [100/1299] D_loss: -0.1000, G_loss: 0.2926\n",
      "  Batch [110/1299] D_loss: -0.1474, G_loss: 0.2763\n",
      "  Batch [120/1299] D_loss: -0.0331, G_loss: 0.2808\n",
      "  Batch [130/1299] D_loss: -0.1297, G_loss: 0.3291\n",
      "  Batch [140/1299] D_loss: -1.6532, G_loss: -2.8577\n",
      "  Batch [150/1299] D_loss: -0.0031, G_loss: 0.1488\n",
      "  Batch [160/1299] D_loss: -0.0657, G_loss: 0.2912\n",
      "  Batch [170/1299] D_loss: 0.0418, G_loss: 0.3248\n",
      "  Batch [180/1299] D_loss: -0.1045, G_loss: 0.4965\n",
      "  Batch [190/1299] D_loss: -0.1199, G_loss: 0.4157\n",
      "  Batch [200/1299] D_loss: -0.0739, G_loss: 0.2206\n",
      "  Batch [210/1299] D_loss: -1.1696, G_loss: -1.8646\n",
      "  Batch [220/1299] D_loss: -0.0141, G_loss: 0.0680\n",
      "  Batch [230/1299] D_loss: -0.6238, G_loss: -0.0059\n",
      "  Batch [240/1299] D_loss: 0.0049, G_loss: 0.0859\n",
      "  Batch [250/1299] D_loss: -0.0023, G_loss: 0.2215\n",
      "  Batch [260/1299] D_loss: -0.1044, G_loss: 0.3649\n",
      "  Batch [270/1299] D_loss: -0.0703, G_loss: 0.4092\n",
      "  Batch [280/1299] D_loss: -0.0378, G_loss: 0.4563\n",
      "  Batch [290/1299] D_loss: -0.0857, G_loss: 0.3558\n",
      "  Batch [300/1299] D_loss: -0.0484, G_loss: 0.2814\n",
      "  Batch [310/1299] D_loss: -1.1194, G_loss: -3.0983\n",
      "  Batch [320/1299] D_loss: 0.0131, G_loss: 0.1067\n",
      "  Batch [330/1299] D_loss: -0.0196, G_loss: 0.2231\n",
      "  Batch [340/1299] D_loss: -0.0400, G_loss: 0.3144\n",
      "  Batch [350/1299] D_loss: -0.0902, G_loss: 0.3828\n",
      "  Batch [360/1299] D_loss: -0.1147, G_loss: 0.3935\n",
      "  Batch [370/1299] D_loss: -3.1953, G_loss: -4.6692\n",
      "  Batch [380/1299] D_loss: -1.0362, G_loss: -1.6458\n",
      "  Batch [390/1299] D_loss: -0.0293, G_loss: 0.2521\n",
      "  Batch [400/1299] D_loss: 0.0294, G_loss: 0.3243\n",
      "  Batch [410/1299] D_loss: -0.1169, G_loss: 0.4940\n",
      "  Batch [420/1299] D_loss: -0.0408, G_loss: 0.4319\n",
      "  Batch [430/1299] D_loss: -0.0650, G_loss: 0.4110\n",
      "  Batch [440/1299] D_loss: -0.0067, G_loss: 0.1628\n",
      "  Batch [450/1299] D_loss: -0.8935, G_loss: -1.5390\n",
      "  Batch [460/1299] D_loss: -0.0186, G_loss: 0.1707\n",
      "  Batch [470/1299] D_loss: 0.0011, G_loss: 0.2605\n",
      "  Batch [480/1299] D_loss: -0.0786, G_loss: 0.2944\n",
      "  Batch [490/1299] D_loss: -0.0079, G_loss: 0.3550\n",
      "  Batch [500/1299] D_loss: -0.0439, G_loss: 0.2787\n",
      "  Batch [510/1299] D_loss: -0.0787, G_loss: 0.2570\n",
      "  Batch [520/1299] D_loss: -0.3292, G_loss: -1.0187\n",
      "  Batch [530/1299] D_loss: -0.7884, G_loss: -0.5617\n",
      "  Batch [540/1299] D_loss: -0.3330, G_loss: -0.2100\n",
      "  Batch [550/1299] D_loss: -0.6553, G_loss: -0.8669\n",
      "  Batch [560/1299] D_loss: 0.0441, G_loss: 0.1817\n",
      "  Batch [570/1299] D_loss: -0.0963, G_loss: 0.3055\n",
      "  Batch [580/1299] D_loss: -0.1837, G_loss: 0.6286\n",
      "  Batch [590/1299] D_loss: -0.0227, G_loss: 0.5875\n",
      "  Batch [600/1299] D_loss: -0.1198, G_loss: 0.5538\n",
      "  Batch [610/1299] D_loss: 0.0141, G_loss: 0.3763\n",
      "  Batch [620/1299] D_loss: -2.5548, G_loss: -4.4524\n",
      "  Batch [630/1299] D_loss: -0.2190, G_loss: 0.0622\n",
      "  Batch [640/1299] D_loss: -1.3525, G_loss: -0.6703\n",
      "  Batch [650/1299] D_loss: -0.0318, G_loss: 0.1744\n",
      "  Batch [660/1299] D_loss: -0.0162, G_loss: 0.2208\n",
      "  Batch [670/1299] D_loss: -0.1245, G_loss: 0.4218\n",
      "  Batch [680/1299] D_loss: -0.1866, G_loss: 0.5013\n",
      "  Batch [690/1299] D_loss: -0.0716, G_loss: 0.4236\n",
      "  Batch [700/1299] D_loss: -0.0481, G_loss: 0.3596\n",
      "  Batch [710/1299] D_loss: -0.0419, G_loss: 0.3335\n",
      "  Batch [720/1299] D_loss: -1.5344, G_loss: -2.3055\n",
      "  Batch [730/1299] D_loss: -0.6133, G_loss: -1.3049\n",
      "  Batch [740/1299] D_loss: -0.6915, G_loss: -0.1668\n",
      "  Batch [750/1299] D_loss: -0.9050, G_loss: -2.9194\n",
      "  Batch [760/1299] D_loss: -0.3076, G_loss: 0.0211\n",
      "  Batch [770/1299] D_loss: -0.8357, G_loss: -0.2021\n",
      "  Batch [780/1299] D_loss: -0.2027, G_loss: 0.0492\n",
      "  Batch [790/1299] D_loss: -1.1505, G_loss: 0.0994\n",
      "  Batch [800/1299] D_loss: -0.1290, G_loss: 0.1482\n",
      "  Batch [810/1299] D_loss: -0.0379, G_loss: 0.3213\n",
      "  Batch [820/1299] D_loss: -0.1639, G_loss: 0.4156\n",
      "  Batch [830/1299] D_loss: 0.0011, G_loss: 0.5759\n",
      "  Batch [840/1299] D_loss: -0.0847, G_loss: 0.5093\n",
      "  Batch [850/1299] D_loss: -0.1828, G_loss: 0.3561\n",
      "  Batch [860/1299] D_loss: -0.1008, G_loss: 0.2803\n",
      "  Batch [870/1299] D_loss: -1.0109, G_loss: -1.0438\n",
      "  Batch [880/1299] D_loss: -0.1217, G_loss: 0.1842\n",
      "  Batch [890/1299] D_loss: -0.3437, G_loss: 0.1253\n",
      "  Batch [900/1299] D_loss: -0.1286, G_loss: 0.2235\n",
      "  Batch [910/1299] D_loss: -0.1010, G_loss: 0.5293\n",
      "  Batch [920/1299] D_loss: -0.1141, G_loss: 0.5524\n",
      "  Batch [930/1299] D_loss: -0.0097, G_loss: 0.4421\n",
      "  Batch [940/1299] D_loss: -0.0484, G_loss: 0.5383\n",
      "  Batch [950/1299] D_loss: -0.0661, G_loss: 0.2981\n",
      "  Batch [960/1299] D_loss: -0.8363, G_loss: -0.5232\n",
      "  Batch [970/1299] D_loss: -0.5172, G_loss: 0.0476\n",
      "  Batch [980/1299] D_loss: 0.0467, G_loss: 0.2138\n",
      "  Batch [990/1299] D_loss: -0.0672, G_loss: 0.3123\n",
      "  Batch [1000/1299] D_loss: 0.0202, G_loss: 0.3217\n",
      "  Batch [1010/1299] D_loss: -0.0214, G_loss: 0.3367\n",
      "  Batch [1020/1299] D_loss: -0.0764, G_loss: 0.2362\n",
      "  Batch [1030/1299] D_loss: -1.5694, G_loss: -1.8133\n",
      "  Batch [1040/1299] D_loss: -0.6225, G_loss: -0.6940\n",
      "  Batch [1050/1299] D_loss: -1.9105, G_loss: -3.2408\n",
      "  Batch [1060/1299] D_loss: -1.0849, G_loss: -1.0697\n",
      "  Batch [1070/1299] D_loss: -0.1082, G_loss: 0.0185\n",
      "  Batch [1080/1299] D_loss: -0.0859, G_loss: 0.4435\n",
      "  Batch [1090/1299] D_loss: -0.0373, G_loss: 0.5286\n",
      "  Batch [1100/1299] D_loss: -0.1822, G_loss: 0.7584\n",
      "  Batch [1110/1299] D_loss: -0.1177, G_loss: 0.6507\n",
      "  Batch [1120/1299] D_loss: -0.1080, G_loss: 0.6481\n",
      "  Batch [1130/1299] D_loss: 0.0233, G_loss: 0.4562\n",
      "  Batch [1140/1299] D_loss: -2.6568, G_loss: -7.2546\n",
      "  Batch [1150/1299] D_loss: -0.0422, G_loss: 0.0820\n",
      "  Batch [1160/1299] D_loss: -0.0564, G_loss: 0.2565\n",
      "  Batch [1170/1299] D_loss: -0.0095, G_loss: 0.3838\n",
      "  Batch [1180/1299] D_loss: -0.0418, G_loss: 0.4275\n",
      "  Batch [1190/1299] D_loss: -0.0502, G_loss: 0.4283\n",
      "  Batch [1200/1299] D_loss: -0.0847, G_loss: 0.4313\n",
      "  Batch [1210/1299] D_loss: -1.7618, G_loss: -1.4245\n",
      "  Batch [1220/1299] D_loss: -0.7840, G_loss: -1.8186\n",
      "  Batch [1230/1299] D_loss: -0.9054, G_loss: -2.3083\n",
      "  Batch [1240/1299] D_loss: -0.0681, G_loss: 0.0608\n",
      "  Batch [1250/1299] D_loss: -0.0572, G_loss: 0.2404\n",
      "  Batch [1260/1299] D_loss: -0.0675, G_loss: 0.4209\n",
      "  Batch [1270/1299] D_loss: -0.0371, G_loss: 0.3875\n",
      "  Batch [1280/1299] D_loss: -0.0169, G_loss: 0.4692\n",
      "  Batch [1290/1299] D_loss: -0.0752, G_loss: 0.4303\n",
      "\n",
      "Epoch 76 Summary:\n",
      "  Average D_loss: -0.1421\n",
      "  Average G_loss: -0.0339\n",
      "\n",
      "Epoch [77/100]\n",
      "  Batch [0/1299] D_loss: -0.0624, G_loss: 0.4187\n",
      "  Batch [10/1299] D_loss: -1.7514, G_loss: -4.7208\n",
      "  Batch [20/1299] D_loss: -0.4378, G_loss: 0.0539\n",
      "  Batch [30/1299] D_loss: -0.0078, G_loss: 0.1792\n",
      "  Batch [40/1299] D_loss: -0.0290, G_loss: 0.2644\n",
      "  Batch [50/1299] D_loss: 0.0150, G_loss: 0.3755\n",
      "  Batch [60/1299] D_loss: 0.0250, G_loss: 0.3987\n",
      "  Batch [70/1299] D_loss: 0.0034, G_loss: 0.3403\n",
      "  Batch [80/1299] D_loss: -0.0278, G_loss: 0.2093\n",
      "  Batch [90/1299] D_loss: -0.3043, G_loss: -0.4683\n",
      "  Batch [100/1299] D_loss: 0.0552, G_loss: 0.1753\n",
      "  Batch [110/1299] D_loss: -0.0609, G_loss: 0.3562\n",
      "  Batch [120/1299] D_loss: -0.0398, G_loss: 0.3381\n",
      "  Batch [130/1299] D_loss: -0.2184, G_loss: 0.3352\n",
      "  Batch [140/1299] D_loss: -0.0535, G_loss: 0.4525\n",
      "  Batch [150/1299] D_loss: -0.0927, G_loss: 0.2305\n",
      "  Batch [160/1299] D_loss: -0.3596, G_loss: -0.0927\n",
      "  Batch [170/1299] D_loss: 0.0060, G_loss: 0.1095\n",
      "  Batch [180/1299] D_loss: 0.0026, G_loss: 0.2094\n",
      "  Batch [190/1299] D_loss: -0.0456, G_loss: 0.3272\n",
      "  Batch [200/1299] D_loss: -0.0732, G_loss: 0.2928\n",
      "  Batch [210/1299] D_loss: -0.0663, G_loss: 0.3820\n",
      "  Batch [220/1299] D_loss: -0.0706, G_loss: 0.2913\n",
      "  Batch [230/1299] D_loss: -0.2022, G_loss: -0.0834\n",
      "  Batch [240/1299] D_loss: -0.2922, G_loss: -0.0442\n",
      "  Batch [250/1299] D_loss: -0.9561, G_loss: -0.0655\n",
      "  Batch [260/1299] D_loss: -0.1095, G_loss: 0.0635\n",
      "  Batch [270/1299] D_loss: -0.1542, G_loss: 0.0373\n",
      "  Batch [280/1299] D_loss: -0.7237, G_loss: -0.3848\n",
      "  Batch [290/1299] D_loss: -0.0434, G_loss: 0.1534\n",
      "  Batch [300/1299] D_loss: -0.0569, G_loss: 0.3001\n",
      "  Batch [310/1299] D_loss: -0.0845, G_loss: 0.4215\n",
      "  Batch [320/1299] D_loss: -0.8966, G_loss: -0.0760\n",
      "  Batch [330/1299] D_loss: -1.0098, G_loss: -0.6738\n",
      "  Batch [340/1299] D_loss: -0.2103, G_loss: -0.1259\n",
      "  Batch [350/1299] D_loss: -0.2248, G_loss: 0.0176\n",
      "  Batch [360/1299] D_loss: -0.1886, G_loss: 0.0387\n",
      "  Batch [370/1299] D_loss: -0.0665, G_loss: 0.2259\n",
      "  Batch [380/1299] D_loss: -0.0283, G_loss: 0.3358\n",
      "  Batch [390/1299] D_loss: -0.1549, G_loss: 0.4969\n",
      "  Batch [400/1299] D_loss: -0.0088, G_loss: 0.4327\n",
      "  Batch [410/1299] D_loss: -0.0295, G_loss: 0.3565\n",
      "  Batch [420/1299] D_loss: -1.4701, G_loss: -1.6406\n",
      "  Batch [430/1299] D_loss: 0.0030, G_loss: 0.2129\n",
      "  Batch [440/1299] D_loss: -0.0604, G_loss: 0.4349\n",
      "  Batch [450/1299] D_loss: -0.0604, G_loss: 0.4025\n",
      "  Batch [460/1299] D_loss: 0.0092, G_loss: 0.4884\n",
      "  Batch [470/1299] D_loss: -0.0597, G_loss: 0.3774\n",
      "  Batch [480/1299] D_loss: -1.3132, G_loss: -1.1944\n",
      "  Batch [490/1299] D_loss: 0.0146, G_loss: 0.1132\n",
      "  Batch [500/1299] D_loss: 0.0086, G_loss: 0.1846\n",
      "  Batch [510/1299] D_loss: -0.0147, G_loss: 0.2078\n",
      "  Batch [520/1299] D_loss: -0.0211, G_loss: 0.2618\n",
      "  Batch [530/1299] D_loss: -0.8735, G_loss: -4.0645\n",
      "  Batch [540/1299] D_loss: -0.2398, G_loss: 0.0442\n",
      "  Batch [550/1299] D_loss: -0.0505, G_loss: 0.3141\n",
      "  Batch [560/1299] D_loss: -0.0318, G_loss: 0.3172\n",
      "  Batch [570/1299] D_loss: -0.1209, G_loss: 0.2983\n",
      "  Batch [580/1299] D_loss: -1.2298, G_loss: -2.7615\n",
      "  Batch [590/1299] D_loss: -0.3617, G_loss: -0.0291\n",
      "  Batch [600/1299] D_loss: -0.7045, G_loss: -0.0936\n",
      "  Batch [610/1299] D_loss: -0.0343, G_loss: 0.1491\n",
      "  Batch [620/1299] D_loss: -0.0877, G_loss: 0.3483\n",
      "  Batch [630/1299] D_loss: -0.1998, G_loss: 0.6525\n",
      "  Batch [640/1299] D_loss: -0.1143, G_loss: 0.6040\n",
      "  Batch [650/1299] D_loss: -0.0966, G_loss: 0.6226\n",
      "  Batch [660/1299] D_loss: -0.0474, G_loss: 0.3344\n",
      "  Batch [670/1299] D_loss: -0.1210, G_loss: 0.2067\n",
      "  Batch [680/1299] D_loss: -0.9505, G_loss: -0.0006\n",
      "  Batch [690/1299] D_loss: -0.0160, G_loss: 0.1423\n",
      "  Batch [700/1299] D_loss: -0.0414, G_loss: 0.3325\n",
      "  Batch [710/1299] D_loss: -0.0521, G_loss: 0.4050\n",
      "  Batch [720/1299] D_loss: -0.0196, G_loss: 0.5820\n",
      "  Batch [730/1299] D_loss: -0.0931, G_loss: 0.2887\n",
      "  Batch [740/1299] D_loss: -0.0304, G_loss: 0.3611\n",
      "  Batch [750/1299] D_loss: -1.0589, G_loss: -5.0266\n",
      "  Batch [760/1299] D_loss: 0.0390, G_loss: 0.0948\n",
      "  Batch [770/1299] D_loss: 0.0119, G_loss: 0.1067\n",
      "  Batch [780/1299] D_loss: -0.0266, G_loss: 0.2750\n",
      "  Batch [790/1299] D_loss: -0.0457, G_loss: 0.2501\n",
      "  Batch [800/1299] D_loss: -1.5942, G_loss: -3.1219\n",
      "  Batch [810/1299] D_loss: -0.7478, G_loss: -0.0185\n",
      "  Batch [820/1299] D_loss: -0.0166, G_loss: 0.0899\n",
      "  Batch [830/1299] D_loss: 0.0163, G_loss: 0.2116\n",
      "  Batch [840/1299] D_loss: -0.0627, G_loss: 0.4639\n",
      "  Batch [850/1299] D_loss: -0.0401, G_loss: 0.4260\n",
      "  Batch [860/1299] D_loss: -0.1581, G_loss: 0.5727\n",
      "  Batch [870/1299] D_loss: -0.0816, G_loss: 0.3140\n",
      "  Batch [880/1299] D_loss: -0.9387, G_loss: -1.5636\n",
      "  Batch [890/1299] D_loss: -0.2410, G_loss: -0.0001\n",
      "  Batch [900/1299] D_loss: -0.8586, G_loss: -0.6747\n",
      "  Batch [910/1299] D_loss: -0.0273, G_loss: 0.2267\n",
      "  Batch [920/1299] D_loss: -0.0037, G_loss: 0.4750\n",
      "  Batch [930/1299] D_loss: -0.0611, G_loss: 0.5866\n",
      "  Batch [940/1299] D_loss: -0.0134, G_loss: 0.4494\n",
      "  Batch [950/1299] D_loss: -0.0350, G_loss: 0.1736\n",
      "  Batch [960/1299] D_loss: -0.9892, G_loss: -0.8115\n",
      "  Batch [970/1299] D_loss: -0.4287, G_loss: -0.1872\n",
      "  Batch [980/1299] D_loss: -0.0639, G_loss: 0.1042\n",
      "  Batch [990/1299] D_loss: -0.0782, G_loss: 0.3578\n",
      "  Batch [1000/1299] D_loss: 0.0253, G_loss: 0.4431\n",
      "  Batch [1010/1299] D_loss: -0.0732, G_loss: 0.3328\n",
      "  Batch [1020/1299] D_loss: -0.0657, G_loss: 0.3215\n",
      "  Batch [1030/1299] D_loss: -1.3390, G_loss: -1.5003\n",
      "  Batch [1040/1299] D_loss: -0.4642, G_loss: -1.1545\n",
      "  Batch [1050/1299] D_loss: -0.3052, G_loss: -0.2258\n",
      "  Batch [1060/1299] D_loss: -0.8578, G_loss: -0.5229\n",
      "  Batch [1070/1299] D_loss: -0.5815, G_loss: 0.0884\n",
      "  Batch [1080/1299] D_loss: -0.4968, G_loss: -0.3636\n",
      "  Batch [1090/1299] D_loss: -0.3676, G_loss: -0.2823\n",
      "  Batch [1100/1299] D_loss: -0.0240, G_loss: 0.2919\n",
      "  Batch [1110/1299] D_loss: -0.1441, G_loss: 0.5722\n",
      "  Batch [1120/1299] D_loss: -0.1025, G_loss: 0.4456\n",
      "  Batch [1130/1299] D_loss: -0.1033, G_loss: 0.5075\n",
      "  Batch [1140/1299] D_loss: -0.0151, G_loss: 0.4163\n",
      "  Batch [1150/1299] D_loss: 0.0180, G_loss: 0.2090\n",
      "  Batch [1160/1299] D_loss: -0.3141, G_loss: -0.6306\n",
      "  Batch [1170/1299] D_loss: -0.3936, G_loss: -0.1249\n",
      "  Batch [1180/1299] D_loss: -0.0713, G_loss: 0.2071\n",
      "  Batch [1190/1299] D_loss: -0.1013, G_loss: 0.4426\n",
      "  Batch [1200/1299] D_loss: -0.1255, G_loss: 0.5367\n",
      "  Batch [1210/1299] D_loss: -0.3005, G_loss: 0.6391\n",
      "  Batch [1220/1299] D_loss: -0.0228, G_loss: 0.5916\n",
      "  Batch [1230/1299] D_loss: -0.0583, G_loss: 0.3100\n",
      "  Batch [1240/1299] D_loss: -0.0455, G_loss: 0.2900\n",
      "  Batch [1250/1299] D_loss: -0.4382, G_loss: -0.2039\n",
      "  Batch [1260/1299] D_loss: -0.7139, G_loss: -1.6186\n",
      "  Batch [1270/1299] D_loss: -1.2296, G_loss: -1.5757\n",
      "  Batch [1280/1299] D_loss: -0.0472, G_loss: 0.1701\n",
      "  Batch [1290/1299] D_loss: -0.0809, G_loss: 0.3166\n",
      "\n",
      "Epoch 77 Summary:\n",
      "  Average D_loss: -0.1405\n",
      "  Average G_loss: -0.0790\n",
      "\n",
      "Epoch [78/100]\n",
      "  Batch [0/1299] D_loss: -0.1015, G_loss: 0.4782\n",
      "  Batch [10/1299] D_loss: -0.0286, G_loss: 0.5629\n",
      "  Batch [20/1299] D_loss: -0.1313, G_loss: 0.5297\n",
      "  Batch [30/1299] D_loss: -0.0531, G_loss: 0.5130\n",
      "  Batch [40/1299] D_loss: -0.0331, G_loss: 0.3529\n",
      "  Batch [50/1299] D_loss: -2.6502, G_loss: -3.4872\n",
      "  Batch [60/1299] D_loss: -0.0114, G_loss: 0.1115\n",
      "  Batch [70/1299] D_loss: 0.0030, G_loss: 0.1495\n",
      "  Batch [80/1299] D_loss: -0.0279, G_loss: 0.1859\n",
      "  Batch [90/1299] D_loss: -0.0472, G_loss: 0.3127\n",
      "  Batch [100/1299] D_loss: -0.0936, G_loss: 0.3029\n",
      "  Batch [110/1299] D_loss: -1.4463, G_loss: -1.5767\n",
      "  Batch [120/1299] D_loss: -0.1114, G_loss: -0.2985\n",
      "  Batch [130/1299] D_loss: -0.0412, G_loss: 0.1454\n",
      "  Batch [140/1299] D_loss: -0.0221, G_loss: 0.1459\n",
      "  Batch [150/1299] D_loss: -0.4728, G_loss: -0.8965\n",
      "  Batch [160/1299] D_loss: -0.1054, G_loss: 0.2798\n",
      "  Batch [170/1299] D_loss: -0.1026, G_loss: 0.3719\n",
      "  Batch [180/1299] D_loss: -0.1352, G_loss: 0.3841\n",
      "  Batch [190/1299] D_loss: -0.1063, G_loss: 0.4871\n",
      "  Batch [200/1299] D_loss: -0.1303, G_loss: 0.3927\n",
      "  Batch [210/1299] D_loss: -0.1823, G_loss: 0.1641\n",
      "  Batch [220/1299] D_loss: -0.4226, G_loss: -0.1062\n",
      "  Batch [230/1299] D_loss: -0.0420, G_loss: 0.1648\n",
      "  Batch [240/1299] D_loss: -0.0838, G_loss: 0.3395\n",
      "  Batch [250/1299] D_loss: -0.0414, G_loss: 0.3859\n",
      "  Batch [260/1299] D_loss: -0.0688, G_loss: 0.3577\n",
      "  Batch [270/1299] D_loss: -0.0631, G_loss: 0.3669\n",
      "  Batch [280/1299] D_loss: -0.0227, G_loss: 0.2700\n",
      "  Batch [290/1299] D_loss: -1.5767, G_loss: -0.9144\n",
      "  Batch [300/1299] D_loss: -0.2216, G_loss: -0.2344\n",
      "  Batch [310/1299] D_loss: -0.1129, G_loss: 0.0576\n",
      "  Batch [320/1299] D_loss: -0.0712, G_loss: 0.2632\n",
      "  Batch [330/1299] D_loss: -0.1713, G_loss: 0.3738\n",
      "  Batch [340/1299] D_loss: -0.0373, G_loss: 0.4996\n",
      "  Batch [350/1299] D_loss: -0.1327, G_loss: 0.4267\n",
      "  Batch [360/1299] D_loss: -0.1279, G_loss: 0.2761\n",
      "  Batch [370/1299] D_loss: -0.3669, G_loss: -1.8362\n",
      "  Batch [380/1299] D_loss: -0.0017, G_loss: 0.1275\n",
      "  Batch [390/1299] D_loss: 0.0081, G_loss: 0.2407\n",
      "  Batch [400/1299] D_loss: 0.0046, G_loss: 0.2240\n",
      "  Batch [410/1299] D_loss: -0.0601, G_loss: 0.2423\n",
      "  Batch [420/1299] D_loss: -2.5905, G_loss: -4.8812\n",
      "  Batch [430/1299] D_loss: -0.0121, G_loss: 0.1496\n",
      "  Batch [440/1299] D_loss: 0.0165, G_loss: 0.2459\n",
      "  Batch [450/1299] D_loss: -0.0120, G_loss: 0.1987\n",
      "  Batch [460/1299] D_loss: -1.5393, G_loss: -2.6625\n",
      "  Batch [470/1299] D_loss: -0.0631, G_loss: 0.2111\n",
      "  Batch [480/1299] D_loss: -0.0437, G_loss: 0.2291\n",
      "  Batch [490/1299] D_loss: 0.0153, G_loss: 0.2076\n",
      "  Batch [500/1299] D_loss: -0.0939, G_loss: 0.1916\n",
      "  Batch [510/1299] D_loss: -0.4403, G_loss: -0.2663\n",
      "  Batch [520/1299] D_loss: 0.0012, G_loss: 0.1634\n",
      "  Batch [530/1299] D_loss: -0.0039, G_loss: 0.2541\n",
      "  Batch [540/1299] D_loss: -0.0312, G_loss: 0.1856\n",
      "  Batch [550/1299] D_loss: -0.0541, G_loss: 0.1518\n",
      "  Batch [560/1299] D_loss: 0.0002, G_loss: 0.1435\n",
      "  Batch [570/1299] D_loss: -0.0110, G_loss: 0.0916\n",
      "  Batch [580/1299] D_loss: -0.0630, G_loss: 0.1035\n",
      "  Batch [590/1299] D_loss: -1.0127, G_loss: -0.0218\n",
      "  Batch [600/1299] D_loss: -0.2814, G_loss: 0.0574\n",
      "  Batch [610/1299] D_loss: -0.1854, G_loss: 0.0155\n",
      "  Batch [620/1299] D_loss: -0.9116, G_loss: -2.5568\n",
      "  Batch [630/1299] D_loss: 0.1554, G_loss: 0.0892\n",
      "  Batch [640/1299] D_loss: -0.9832, G_loss: -1.1936\n",
      "  Batch [650/1299] D_loss: -0.3196, G_loss: 0.0030\n",
      "  Batch [660/1299] D_loss: -0.1161, G_loss: 0.2972\n",
      "  Batch [670/1299] D_loss: -0.0829, G_loss: 0.4540\n",
      "  Batch [680/1299] D_loss: -0.2941, G_loss: 0.6198\n",
      "  Batch [690/1299] D_loss: -0.1222, G_loss: 0.5413\n",
      "  Batch [700/1299] D_loss: 0.0074, G_loss: 0.2581\n",
      "  Batch [710/1299] D_loss: -1.3586, G_loss: -2.6974\n",
      "  Batch [720/1299] D_loss: -0.3031, G_loss: -0.0437\n",
      "  Batch [730/1299] D_loss: -0.8262, G_loss: -0.8253\n",
      "  Batch [740/1299] D_loss: -1.1553, G_loss: -2.0809\n",
      "  Batch [750/1299] D_loss: -0.0927, G_loss: 0.1413\n",
      "  Batch [760/1299] D_loss: -1.3305, G_loss: -0.3779\n",
      "  Batch [770/1299] D_loss: -0.2172, G_loss: -0.0056\n",
      "  Batch [780/1299] D_loss: -0.0986, G_loss: 0.3869\n",
      "  Batch [790/1299] D_loss: -0.0973, G_loss: 0.3929\n",
      "  Batch [800/1299] D_loss: -0.1892, G_loss: 0.6206\n",
      "  Batch [810/1299] D_loss: -0.4091, G_loss: 0.7765\n",
      "  Batch [820/1299] D_loss: 0.0032, G_loss: 0.5197\n",
      "  Batch [830/1299] D_loss: -0.0100, G_loss: 0.3459\n",
      "  Batch [840/1299] D_loss: -1.3504, G_loss: -0.2226\n",
      "  Batch [850/1299] D_loss: -0.5689, G_loss: -0.4033\n",
      "  Batch [860/1299] D_loss: 0.0153, G_loss: 0.2064\n",
      "  Batch [870/1299] D_loss: -0.0071, G_loss: 0.2943\n",
      "  Batch [880/1299] D_loss: -0.0603, G_loss: 0.3816\n",
      "  Batch [890/1299] D_loss: -0.0672, G_loss: 0.3553\n",
      "  Batch [900/1299] D_loss: -0.0764, G_loss: 0.2835\n",
      "  Batch [910/1299] D_loss: -0.8793, G_loss: -2.8281\n",
      "  Batch [920/1299] D_loss: -0.0710, G_loss: -0.0167\n",
      "  Batch [930/1299] D_loss: -0.1890, G_loss: 0.1657\n",
      "  Batch [940/1299] D_loss: -0.1522, G_loss: 0.3709\n",
      "  Batch [950/1299] D_loss: -0.0828, G_loss: 0.4492\n",
      "  Batch [960/1299] D_loss: -0.1414, G_loss: 0.3735\n",
      "  Batch [970/1299] D_loss: -0.1008, G_loss: 0.4505\n",
      "  Batch [980/1299] D_loss: -0.1343, G_loss: 0.3455\n",
      "  Batch [990/1299] D_loss: -0.0850, G_loss: -0.1875\n",
      "  Batch [1000/1299] D_loss: -0.3008, G_loss: -0.0817\n",
      "  Batch [1010/1299] D_loss: 0.0074, G_loss: 0.0851\n",
      "  Batch [1020/1299] D_loss: -1.5087, G_loss: -1.7610\n",
      "  Batch [1030/1299] D_loss: -0.3805, G_loss: -0.1393\n",
      "  Batch [1040/1299] D_loss: -0.0275, G_loss: 0.2455\n",
      "  Batch [1050/1299] D_loss: -0.0766, G_loss: 0.4522\n",
      "  Batch [1060/1299] D_loss: -0.0794, G_loss: 0.5488\n",
      "  Batch [1070/1299] D_loss: -0.0583, G_loss: 0.4950\n",
      "  Batch [1080/1299] D_loss: -0.1089, G_loss: 0.5145\n",
      "  Batch [1090/1299] D_loss: -0.0808, G_loss: 0.3806\n",
      "  Batch [1100/1299] D_loss: -0.0817, G_loss: 0.1398\n",
      "  Batch [1110/1299] D_loss: -0.6510, G_loss: -0.3430\n",
      "  Batch [1120/1299] D_loss: 0.0177, G_loss: 0.0892\n",
      "  Batch [1130/1299] D_loss: -0.0300, G_loss: 0.2144\n",
      "  Batch [1140/1299] D_loss: -0.0617, G_loss: 0.2863\n",
      "  Batch [1150/1299] D_loss: -0.0052, G_loss: 0.2342\n",
      "  Batch [1160/1299] D_loss: -3.3214, G_loss: -1.0687\n",
      "  Batch [1170/1299] D_loss: -0.0322, G_loss: 0.1360\n",
      "  Batch [1180/1299] D_loss: -0.0357, G_loss: 0.2358\n",
      "  Batch [1190/1299] D_loss: -0.0641, G_loss: 0.2613\n",
      "  Batch [1200/1299] D_loss: -0.1082, G_loss: 0.3387\n",
      "  Batch [1210/1299] D_loss: -0.0435, G_loss: 0.2097\n",
      "  Batch [1220/1299] D_loss: -2.1848, G_loss: -2.3215\n",
      "  Batch [1230/1299] D_loss: -0.0199, G_loss: 0.0813\n",
      "  Batch [1240/1299] D_loss: -0.0474, G_loss: 0.1472\n",
      "  Batch [1250/1299] D_loss: 0.0010, G_loss: 0.1854\n",
      "  Batch [1260/1299] D_loss: -0.0327, G_loss: 0.3174\n",
      "  Batch [1270/1299] D_loss: 0.0042, G_loss: 0.2282\n",
      "  Batch [1280/1299] D_loss: -2.4151, G_loss: -0.4908\n",
      "  Batch [1290/1299] D_loss: -0.6111, G_loss: -0.1483\n",
      "\n",
      "Epoch 78 Summary:\n",
      "  Average D_loss: -0.1332\n",
      "  Average G_loss: -0.0549\n",
      "\n",
      "Epoch [79/100]\n",
      "  Batch [0/1299] D_loss: -0.1530, G_loss: 0.0566\n",
      "  Batch [10/1299] D_loss: -0.0081, G_loss: 0.1889\n",
      "  Batch [20/1299] D_loss: -0.0051, G_loss: 0.3388\n",
      "  Batch [30/1299] D_loss: -0.0546, G_loss: 0.3951\n",
      "  Batch [40/1299] D_loss: -0.0447, G_loss: 0.2364\n",
      "  Batch [50/1299] D_loss: -1.1752, G_loss: -3.1668\n",
      "  Batch [60/1299] D_loss: -0.0329, G_loss: 0.2034\n",
      "  Batch [70/1299] D_loss: -0.0495, G_loss: 0.3828\n",
      "  Batch [80/1299] D_loss: -0.0056, G_loss: 0.5327\n",
      "  Batch [90/1299] D_loss: -0.0817, G_loss: 0.6252\n",
      "  Batch [100/1299] D_loss: 0.0690, G_loss: 0.3770\n",
      "  Batch [110/1299] D_loss: -2.2979, G_loss: -2.1049\n",
      "  Batch [120/1299] D_loss: -0.0146, G_loss: 0.0891\n",
      "  Batch [130/1299] D_loss: -0.0608, G_loss: 0.1855\n",
      "  Batch [140/1299] D_loss: 0.0106, G_loss: 0.3321\n",
      "  Batch [150/1299] D_loss: -0.1589, G_loss: 0.5245\n",
      "  Batch [160/1299] D_loss: -0.0061, G_loss: 0.3706\n",
      "  Batch [170/1299] D_loss: -0.0171, G_loss: 0.2388\n",
      "  Batch [180/1299] D_loss: -1.1259, G_loss: -1.2742\n",
      "  Batch [190/1299] D_loss: -0.9487, G_loss: -0.6234\n",
      "  Batch [200/1299] D_loss: -0.3073, G_loss: 0.0216\n",
      "  Batch [210/1299] D_loss: -0.0416, G_loss: 0.1713\n",
      "  Batch [220/1299] D_loss: -0.0188, G_loss: 0.3684\n",
      "  Batch [230/1299] D_loss: -0.0246, G_loss: 0.2542\n",
      "  Batch [240/1299] D_loss: -0.0953, G_loss: 0.4305\n",
      "  Batch [250/1299] D_loss: -0.0841, G_loss: 0.3450\n",
      "  Batch [260/1299] D_loss: -1.5457, G_loss: -3.1708\n",
      "  Batch [270/1299] D_loss: -0.2568, G_loss: -0.1270\n",
      "  Batch [280/1299] D_loss: -0.3334, G_loss: -0.3546\n",
      "  Batch [290/1299] D_loss: -0.2790, G_loss: -0.0293\n",
      "  Batch [300/1299] D_loss: 0.0144, G_loss: 0.1571\n",
      "  Batch [310/1299] D_loss: -0.0541, G_loss: 0.3931\n",
      "  Batch [320/1299] D_loss: -0.0435, G_loss: 0.5549\n",
      "  Batch [330/1299] D_loss: -0.2692, G_loss: 0.4281\n",
      "  Batch [340/1299] D_loss: -0.0378, G_loss: 0.4200\n",
      "  Batch [350/1299] D_loss: -1.5417, G_loss: -0.7779\n",
      "  Batch [360/1299] D_loss: -0.7279, G_loss: -0.4738\n",
      "  Batch [370/1299] D_loss: -0.5978, G_loss: 0.0807\n",
      "  Batch [380/1299] D_loss: -0.2857, G_loss: -0.0435\n",
      "  Batch [390/1299] D_loss: -0.1895, G_loss: 0.0450\n",
      "  Batch [400/1299] D_loss: -1.4145, G_loss: -0.0408\n",
      "  Batch [410/1299] D_loss: -0.7018, G_loss: -0.7394\n",
      "  Batch [420/1299] D_loss: -0.6307, G_loss: -0.4344\n",
      "  Batch [430/1299] D_loss: -0.4314, G_loss: -0.0624\n",
      "  Batch [440/1299] D_loss: -1.2284, G_loss: -2.4359\n",
      "  Batch [450/1299] D_loss: -0.1192, G_loss: 0.3120\n",
      "  Batch [460/1299] D_loss: -0.0714, G_loss: 0.5295\n",
      "  Batch [470/1299] D_loss: -0.2233, G_loss: 0.6941\n",
      "  Batch [480/1299] D_loss: -0.2320, G_loss: 0.8660\n",
      "  Batch [490/1299] D_loss: -0.1044, G_loss: 0.7113\n",
      "  Batch [500/1299] D_loss: 0.0031, G_loss: 0.2924\n",
      "  Batch [510/1299] D_loss: -3.6647, G_loss: -6.1558\n",
      "  Batch [520/1299] D_loss: -0.2923, G_loss: -0.9388\n",
      "  Batch [530/1299] D_loss: -0.6399, G_loss: -1.0477\n",
      "  Batch [540/1299] D_loss: -0.9031, G_loss: 0.0108\n",
      "  Batch [550/1299] D_loss: -0.5378, G_loss: -0.0367\n",
      "  Batch [560/1299] D_loss: -0.0520, G_loss: 0.0824\n",
      "  Batch [570/1299] D_loss: -0.2654, G_loss: -0.4830\n",
      "  Batch [580/1299] D_loss: -0.1212, G_loss: 0.2829\n",
      "  Batch [590/1299] D_loss: -0.0841, G_loss: 0.4622\n",
      "  Batch [600/1299] D_loss: -0.1880, G_loss: 0.7579\n",
      "  Batch [610/1299] D_loss: -0.1856, G_loss: 0.6777\n",
      "  Batch [620/1299] D_loss: -0.1894, G_loss: 0.5777\n",
      "  Batch [630/1299] D_loss: -0.0165, G_loss: 0.2492\n",
      "  Batch [640/1299] D_loss: -2.0838, G_loss: -1.0992\n",
      "  Batch [650/1299] D_loss: -0.1925, G_loss: -0.0118\n",
      "  Batch [660/1299] D_loss: -0.0325, G_loss: 0.0586\n",
      "  Batch [670/1299] D_loss: -0.9179, G_loss: -1.0590\n",
      "  Batch [680/1299] D_loss: -0.0454, G_loss: 0.2204\n",
      "  Batch [690/1299] D_loss: 0.1272, G_loss: 0.3482\n",
      "  Batch [700/1299] D_loss: -0.2096, G_loss: 0.3334\n",
      "  Batch [710/1299] D_loss: -0.0256, G_loss: 0.4689\n",
      "  Batch [720/1299] D_loss: 0.0805, G_loss: 0.4047\n",
      "  Batch [730/1299] D_loss: -0.0135, G_loss: 0.2779\n",
      "  Batch [740/1299] D_loss: -0.4178, G_loss: -0.2062\n",
      "  Batch [750/1299] D_loss: -0.0644, G_loss: 0.0637\n",
      "  Batch [760/1299] D_loss: -0.6818, G_loss: -1.4427\n",
      "  Batch [770/1299] D_loss: -0.0948, G_loss: 0.0083\n",
      "  Batch [780/1299] D_loss: -0.2629, G_loss: -0.0022\n",
      "  Batch [790/1299] D_loss: -1.1101, G_loss: -1.9181\n",
      "  Batch [800/1299] D_loss: -0.9447, G_loss: 0.0904\n",
      "  Batch [810/1299] D_loss: -0.6705, G_loss: 0.1437\n",
      "  Batch [820/1299] D_loss: -0.8168, G_loss: -0.1489\n",
      "  Batch [830/1299] D_loss: 0.0057, G_loss: 0.1185\n",
      "  Batch [840/1299] D_loss: -0.0723, G_loss: 0.2899\n",
      "  Batch [850/1299] D_loss: -0.2476, G_loss: 0.6386\n",
      "  Batch [860/1299] D_loss: -0.3583, G_loss: 0.6899\n",
      "  Batch [870/1299] D_loss: -0.1482, G_loss: 0.8245\n",
      "  Batch [880/1299] D_loss: -0.0530, G_loss: 0.7170\n",
      "  Batch [890/1299] D_loss: -1.0551, G_loss: -1.4728\n",
      "  Batch [900/1299] D_loss: -0.0573, G_loss: 0.1183\n",
      "  Batch [910/1299] D_loss: -0.0641, G_loss: 0.2445\n",
      "  Batch [920/1299] D_loss: -1.5808, G_loss: -0.3630\n",
      "  Batch [930/1299] D_loss: -0.0075, G_loss: 0.0753\n",
      "  Batch [940/1299] D_loss: -0.1193, G_loss: 0.1738\n",
      "  Batch [950/1299] D_loss: -0.0639, G_loss: 0.3252\n",
      "  Batch [960/1299] D_loss: -0.0433, G_loss: 0.5714\n",
      "  Batch [970/1299] D_loss: -0.0538, G_loss: 0.6847\n",
      "  Batch [980/1299] D_loss: 0.0088, G_loss: 0.5212\n",
      "  Batch [990/1299] D_loss: -0.1049, G_loss: 0.5499\n",
      "  Batch [1000/1299] D_loss: -0.0489, G_loss: 0.2546\n",
      "  Batch [1010/1299] D_loss: -0.5819, G_loss: -1.3872\n",
      "  Batch [1020/1299] D_loss: -0.0158, G_loss: 0.2466\n",
      "  Batch [1030/1299] D_loss: 0.0008, G_loss: 0.2723\n",
      "  Batch [1040/1299] D_loss: -0.0920, G_loss: 0.3384\n",
      "  Batch [1050/1299] D_loss: -0.0551, G_loss: 0.3992\n",
      "  Batch [1060/1299] D_loss: -0.0909, G_loss: 0.3104\n",
      "  Batch [1070/1299] D_loss: -0.4784, G_loss: -0.2676\n",
      "  Batch [1080/1299] D_loss: -0.1029, G_loss: 0.2160\n",
      "  Batch [1090/1299] D_loss: 0.0231, G_loss: 0.4018\n",
      "  Batch [1100/1299] D_loss: -0.0325, G_loss: 0.4530\n",
      "  Batch [1110/1299] D_loss: -0.0851, G_loss: 0.3695\n",
      "  Batch [1120/1299] D_loss: -0.0621, G_loss: 0.2230\n",
      "  Batch [1130/1299] D_loss: -0.4671, G_loss: 0.0129\n",
      "  Batch [1140/1299] D_loss: -0.3395, G_loss: 0.0294\n",
      "  Batch [1150/1299] D_loss: -0.9481, G_loss: -2.3606\n",
      "  Batch [1160/1299] D_loss: -0.0231, G_loss: 0.0476\n",
      "  Batch [1170/1299] D_loss: -0.0777, G_loss: -0.2424\n",
      "  Batch [1180/1299] D_loss: -0.4788, G_loss: 0.1428\n",
      "  Batch [1190/1299] D_loss: -0.6876, G_loss: -0.1959\n",
      "  Batch [1200/1299] D_loss: -0.4635, G_loss: 0.0529\n",
      "  Batch [1210/1299] D_loss: -0.5295, G_loss: -0.6257\n",
      "  Batch [1220/1299] D_loss: -1.8722, G_loss: -0.8437\n",
      "  Batch [1230/1299] D_loss: -0.1158, G_loss: 0.2913\n",
      "  Batch [1240/1299] D_loss: -0.1661, G_loss: 0.4004\n",
      "  Batch [1250/1299] D_loss: -0.0642, G_loss: 0.5291\n",
      "  Batch [1260/1299] D_loss: -0.1570, G_loss: 0.5011\n",
      "  Batch [1270/1299] D_loss: -0.0637, G_loss: 0.4996\n",
      "  Batch [1280/1299] D_loss: -0.0004, G_loss: 0.2577\n",
      "  Batch [1290/1299] D_loss: -1.1028, G_loss: -0.6159\n",
      "\n",
      "Epoch 79 Summary:\n",
      "  Average D_loss: -0.1738\n",
      "  Average G_loss: -0.0802\n",
      "\n",
      "Epoch [80/100]\n",
      "  Batch [0/1299] D_loss: 0.0300, G_loss: 0.1553\n",
      "  Batch [10/1299] D_loss: -0.1615, G_loss: 0.1855\n",
      "  Batch [20/1299] D_loss: -0.0013, G_loss: 0.1162\n",
      "  Batch [30/1299] D_loss: -0.0168, G_loss: 0.2015\n",
      "  Batch [40/1299] D_loss: -0.0883, G_loss: 0.3534\n",
      "  Batch [50/1299] D_loss: -0.0034, G_loss: 0.4064\n",
      "  Batch [60/1299] D_loss: -0.0510, G_loss: 0.4446\n",
      "  Batch [70/1299] D_loss: -0.0448, G_loss: 0.4671\n",
      "  Batch [80/1299] D_loss: 0.0063, G_loss: 0.3045\n",
      "  Batch [90/1299] D_loss: -0.7075, G_loss: -0.1726\n",
      "  Batch [100/1299] D_loss: -0.0499, G_loss: 0.0718\n",
      "  Batch [110/1299] D_loss: -0.0034, G_loss: 0.1534\n",
      "  Batch [120/1299] D_loss: -0.0097, G_loss: 0.2180\n",
      "  Batch [130/1299] D_loss: -0.0187, G_loss: 0.2754\n",
      "  Batch [140/1299] D_loss: -0.1664, G_loss: 0.3630\n",
      "  Batch [150/1299] D_loss: -0.0597, G_loss: 0.1832\n",
      "  Batch [160/1299] D_loss: -0.3436, G_loss: -0.1847\n",
      "  Batch [170/1299] D_loss: -0.0174, G_loss: 0.2207\n",
      "  Batch [180/1299] D_loss: 0.0140, G_loss: 0.4104\n",
      "  Batch [190/1299] D_loss: 0.0166, G_loss: 0.3847\n",
      "  Batch [200/1299] D_loss: -0.0563, G_loss: 0.2430\n",
      "  Batch [210/1299] D_loss: -0.0816, G_loss: 0.3048\n",
      "  Batch [220/1299] D_loss: -0.4010, G_loss: -0.4961\n",
      "  Batch [230/1299] D_loss: -0.0223, G_loss: 0.2040\n",
      "  Batch [240/1299] D_loss: -0.0442, G_loss: 0.2759\n",
      "  Batch [250/1299] D_loss: -0.1098, G_loss: 0.3225\n",
      "  Batch [260/1299] D_loss: -0.0523, G_loss: 0.3024\n",
      "  Batch [270/1299] D_loss: -1.7616, G_loss: -3.8487\n",
      "  Batch [280/1299] D_loss: -0.0499, G_loss: 0.1105\n",
      "  Batch [290/1299] D_loss: -0.4629, G_loss: -1.2735\n",
      "  Batch [300/1299] D_loss: -0.0230, G_loss: 0.1674\n",
      "  Batch [310/1299] D_loss: -0.1107, G_loss: 0.4998\n",
      "  Batch [320/1299] D_loss: -0.1453, G_loss: 0.7305\n",
      "  Batch [330/1299] D_loss: -0.1729, G_loss: 0.5830\n",
      "  Batch [340/1299] D_loss: -0.1644, G_loss: 0.5434\n",
      "  Batch [350/1299] D_loss: -0.0013, G_loss: 0.3197\n",
      "  Batch [360/1299] D_loss: -0.8659, G_loss: -1.2951\n",
      "  Batch [370/1299] D_loss: -0.4005, G_loss: -0.0726\n",
      "  Batch [380/1299] D_loss: -0.0710, G_loss: 0.2635\n",
      "  Batch [390/1299] D_loss: -0.0596, G_loss: 0.4088\n",
      "  Batch [400/1299] D_loss: -0.1140, G_loss: 0.5615\n",
      "  Batch [410/1299] D_loss: -0.0652, G_loss: 0.4091\n",
      "  Batch [420/1299] D_loss: -0.0119, G_loss: 0.2333\n",
      "  Batch [430/1299] D_loss: -1.4495, G_loss: -4.3801\n",
      "  Batch [440/1299] D_loss: -0.8717, G_loss: -0.3418\n",
      "  Batch [450/1299] D_loss: 0.0229, G_loss: 0.1872\n",
      "  Batch [460/1299] D_loss: 0.0119, G_loss: 0.4214\n",
      "  Batch [470/1299] D_loss: -0.1633, G_loss: 0.6393\n",
      "  Batch [480/1299] D_loss: -0.0409, G_loss: 0.5113\n",
      "  Batch [490/1299] D_loss: -0.0793, G_loss: 0.2926\n",
      "  Batch [500/1299] D_loss: -0.0103, G_loss: 0.3469\n",
      "  Batch [510/1299] D_loss: -0.0811, G_loss: 0.2187\n",
      "  Batch [520/1299] D_loss: -0.8455, G_loss: -0.4581\n",
      "  Batch [530/1299] D_loss: -0.2715, G_loss: -0.3658\n",
      "  Batch [540/1299] D_loss: -0.1325, G_loss: -0.0810\n",
      "  Batch [550/1299] D_loss: -0.6273, G_loss: -2.6104\n",
      "  Batch [560/1299] D_loss: -1.8987, G_loss: -0.9474\n",
      "  Batch [570/1299] D_loss: -0.3907, G_loss: 0.1589\n",
      "  Batch [580/1299] D_loss: -0.0312, G_loss: 0.1909\n",
      "  Batch [590/1299] D_loss: -0.1653, G_loss: 0.4558\n",
      "  Batch [600/1299] D_loss: -0.2408, G_loss: 0.5945\n",
      "  Batch [610/1299] D_loss: -0.0276, G_loss: 0.5372\n",
      "  Batch [620/1299] D_loss: -0.1084, G_loss: 0.6880\n",
      "  Batch [630/1299] D_loss: 0.0134, G_loss: 0.4542\n",
      "  Batch [640/1299] D_loss: -0.0592, G_loss: 0.2476\n",
      "  Batch [650/1299] D_loss: -0.0080, G_loss: 0.2076\n",
      "  Batch [660/1299] D_loss: -0.0410, G_loss: 0.1530\n",
      "  Batch [670/1299] D_loss: 0.0214, G_loss: 0.2090\n",
      "  Batch [680/1299] D_loss: -0.0049, G_loss: 0.1697\n",
      "  Batch [690/1299] D_loss: -0.9347, G_loss: -0.6581\n",
      "  Batch [700/1299] D_loss: -0.1425, G_loss: -0.4837\n",
      "  Batch [710/1299] D_loss: -0.1482, G_loss: 0.0008\n",
      "  Batch [720/1299] D_loss: -0.3473, G_loss: -0.6394\n",
      "  Batch [730/1299] D_loss: -0.2188, G_loss: 0.1251\n",
      "  Batch [740/1299] D_loss: -0.1875, G_loss: 0.5351\n",
      "  Batch [750/1299] D_loss: -0.2386, G_loss: 0.7260\n",
      "  Batch [760/1299] D_loss: 0.0874, G_loss: 0.5254\n",
      "  Batch [770/1299] D_loss: -0.1337, G_loss: 0.6293\n",
      "  Batch [780/1299] D_loss: -0.0239, G_loss: 0.2740\n",
      "  Batch [790/1299] D_loss: -0.4509, G_loss: -0.4725\n",
      "  Batch [800/1299] D_loss: -0.0334, G_loss: 0.1386\n",
      "  Batch [810/1299] D_loss: 0.0311, G_loss: 0.1174\n",
      "  Batch [820/1299] D_loss: -0.6788, G_loss: -1.2255\n",
      "  Batch [830/1299] D_loss: -0.9081, G_loss: -0.9193\n",
      "  Batch [840/1299] D_loss: -1.4848, G_loss: -0.6413\n",
      "  Batch [850/1299] D_loss: -0.2152, G_loss: -0.8605\n",
      "  Batch [860/1299] D_loss: -0.1399, G_loss: 0.4069\n",
      "  Batch [870/1299] D_loss: -0.0506, G_loss: 0.4549\n",
      "  Batch [880/1299] D_loss: -0.1171, G_loss: 0.4896\n",
      "  Batch [890/1299] D_loss: -0.0849, G_loss: 0.5988\n",
      "  Batch [900/1299] D_loss: -0.0255, G_loss: 0.5040\n",
      "  Batch [910/1299] D_loss: -0.1684, G_loss: 0.2901\n",
      "  Batch [920/1299] D_loss: -0.3721, G_loss: -1.3240\n",
      "  Batch [930/1299] D_loss: -1.1099, G_loss: -0.3253\n",
      "  Batch [940/1299] D_loss: -0.2547, G_loss: -0.1674\n",
      "  Batch [950/1299] D_loss: -0.0378, G_loss: 0.2890\n",
      "  Batch [960/1299] D_loss: -0.0604, G_loss: 0.3652\n",
      "  Batch [970/1299] D_loss: -0.1447, G_loss: 0.5454\n",
      "  Batch [980/1299] D_loss: -0.2197, G_loss: 0.4974\n",
      "  Batch [990/1299] D_loss: -0.0857, G_loss: 0.4832\n",
      "  Batch [1000/1299] D_loss: -0.7071, G_loss: -2.0697\n",
      "  Batch [1010/1299] D_loss: -0.0195, G_loss: 0.1652\n",
      "  Batch [1020/1299] D_loss: -0.0531, G_loss: 0.2183\n",
      "  Batch [1030/1299] D_loss: -0.0513, G_loss: 0.3861\n",
      "  Batch [1040/1299] D_loss: -0.0093, G_loss: 0.3519\n",
      "  Batch [1050/1299] D_loss: -0.0807, G_loss: 0.1708\n",
      "  Batch [1060/1299] D_loss: -1.3199, G_loss: -5.5307\n",
      "  Batch [1070/1299] D_loss: -0.2740, G_loss: -1.0074\n",
      "  Batch [1080/1299] D_loss: -0.3439, G_loss: -0.8585\n",
      "  Batch [1090/1299] D_loss: -0.0773, G_loss: 0.0716\n",
      "  Batch [1100/1299] D_loss: -1.5744, G_loss: -0.1344\n",
      "  Batch [1110/1299] D_loss: -0.0412, G_loss: 0.2166\n",
      "  Batch [1120/1299] D_loss: -0.0624, G_loss: 0.3927\n",
      "  Batch [1130/1299] D_loss: -0.1983, G_loss: 0.6356\n",
      "  Batch [1140/1299] D_loss: -0.0557, G_loss: 0.5844\n",
      "  Batch [1150/1299] D_loss: -0.0147, G_loss: 0.5028\n",
      "  Batch [1160/1299] D_loss: -0.1442, G_loss: 0.3976\n",
      "  Batch [1170/1299] D_loss: 0.0212, G_loss: 0.1521\n",
      "  Batch [1180/1299] D_loss: 0.0336, G_loss: 0.1090\n",
      "  Batch [1190/1299] D_loss: -0.5522, G_loss: -0.5533\n",
      "  Batch [1200/1299] D_loss: -0.2982, G_loss: 0.0123\n",
      "  Batch [1210/1299] D_loss: -0.2121, G_loss: 0.0629\n",
      "  Batch [1220/1299] D_loss: -0.0996, G_loss: 0.4317\n",
      "  Batch [1230/1299] D_loss: -0.0570, G_loss: 0.4518\n",
      "  Batch [1240/1299] D_loss: -0.1404, G_loss: 0.5465\n",
      "  Batch [1250/1299] D_loss: -0.0451, G_loss: 0.2975\n",
      "  Batch [1260/1299] D_loss: 0.0022, G_loss: 0.2277\n",
      "  Batch [1270/1299] D_loss: -0.3216, G_loss: -0.0826\n",
      "  Batch [1280/1299] D_loss: -0.0683, G_loss: 0.1497\n",
      "  Batch [1290/1299] D_loss: -0.1014, G_loss: 0.4082\n",
      "\n",
      "Epoch 80 Summary:\n",
      "  Average D_loss: -0.1399\n",
      "  Average G_loss: -0.0731\n",
      "\n",
      "Epoch [81/100]\n",
      "  Batch [0/1299] D_loss: -0.0994, G_loss: 0.6284\n",
      "  Batch [10/1299] D_loss: -0.2565, G_loss: 0.7838\n",
      "  Batch [20/1299] D_loss: -0.1278, G_loss: 0.4513\n",
      "  Batch [30/1299] D_loss: 0.0528, G_loss: 0.4585\n",
      "  Batch [40/1299] D_loss: -0.0039, G_loss: 0.2474\n",
      "  Batch [50/1299] D_loss: -1.0648, G_loss: -2.6500\n",
      "  Batch [60/1299] D_loss: -1.0917, G_loss: -1.3809\n",
      "  Batch [70/1299] D_loss: 0.0262, G_loss: 0.2429\n",
      "  Batch [80/1299] D_loss: -0.0373, G_loss: 0.3564\n",
      "  Batch [90/1299] D_loss: -0.0387, G_loss: 0.3598\n",
      "  Batch [100/1299] D_loss: -0.1227, G_loss: 0.4073\n",
      "  Batch [110/1299] D_loss: -0.0556, G_loss: 0.2387\n",
      "  Batch [120/1299] D_loss: -0.5299, G_loss: -0.2966\n",
      "  Batch [130/1299] D_loss: -0.4188, G_loss: -0.5042\n",
      "  Batch [140/1299] D_loss: -0.3649, G_loss: -0.0773\n",
      "  Batch [150/1299] D_loss: -0.0392, G_loss: 0.2210\n",
      "  Batch [160/1299] D_loss: -0.1060, G_loss: 0.3809\n",
      "  Batch [170/1299] D_loss: -0.0719, G_loss: 0.3725\n",
      "  Batch [180/1299] D_loss: -0.1750, G_loss: 0.4170\n",
      "  Batch [190/1299] D_loss: -0.0459, G_loss: 0.3035\n",
      "  Batch [200/1299] D_loss: -0.4022, G_loss: -0.1980\n",
      "  Batch [210/1299] D_loss: -0.3926, G_loss: -0.2116\n",
      "  Batch [220/1299] D_loss: -0.0923, G_loss: 0.2813\n",
      "  Batch [230/1299] D_loss: -0.1442, G_loss: 0.3838\n",
      "  Batch [240/1299] D_loss: -0.0317, G_loss: 0.3560\n",
      "  Batch [250/1299] D_loss: -0.0550, G_loss: 0.5752\n",
      "  Batch [260/1299] D_loss: -0.0365, G_loss: 0.3652\n",
      "  Batch [270/1299] D_loss: -0.0604, G_loss: 0.3058\n",
      "  Batch [280/1299] D_loss: -0.7144, G_loss: -0.7276\n",
      "  Batch [290/1299] D_loss: -0.0120, G_loss: 0.2275\n",
      "  Batch [300/1299] D_loss: -0.0600, G_loss: 0.2902\n",
      "  Batch [310/1299] D_loss: -0.0707, G_loss: 0.4920\n",
      "  Batch [320/1299] D_loss: -0.0395, G_loss: 0.3539\n",
      "  Batch [330/1299] D_loss: -0.0726, G_loss: 0.2598\n",
      "  Batch [340/1299] D_loss: -0.9597, G_loss: -0.8072\n",
      "  Batch [350/1299] D_loss: -0.4156, G_loss: 0.0845\n",
      "  Batch [360/1299] D_loss: -0.0060, G_loss: 0.1520\n",
      "  Batch [370/1299] D_loss: 0.0078, G_loss: 0.2391\n",
      "  Batch [380/1299] D_loss: -0.0912, G_loss: 0.3257\n",
      "  Batch [390/1299] D_loss: -0.3141, G_loss: 0.3407\n",
      "  Batch [400/1299] D_loss: -0.0335, G_loss: 0.2583\n",
      "  Batch [410/1299] D_loss: -0.1318, G_loss: 0.0059\n",
      "  Batch [420/1299] D_loss: -0.0289, G_loss: 0.2803\n",
      "  Batch [430/1299] D_loss: -0.0427, G_loss: 0.3228\n",
      "  Batch [440/1299] D_loss: -0.0434, G_loss: 0.5233\n",
      "  Batch [450/1299] D_loss: -0.0822, G_loss: 0.3010\n",
      "  Batch [460/1299] D_loss: -1.4988, G_loss: -1.0897\n",
      "  Batch [470/1299] D_loss: -0.0420, G_loss: 0.1044\n",
      "  Batch [480/1299] D_loss: -0.0680, G_loss: 0.2852\n",
      "  Batch [490/1299] D_loss: -0.0132, G_loss: 0.4499\n",
      "  Batch [500/1299] D_loss: -0.0884, G_loss: 0.4220\n",
      "  Batch [510/1299] D_loss: -0.1213, G_loss: 0.4846\n",
      "  Batch [520/1299] D_loss: -0.0510, G_loss: 0.3640\n",
      "  Batch [530/1299] D_loss: -0.6546, G_loss: -0.8500\n",
      "  Batch [540/1299] D_loss: -0.1242, G_loss: -0.0303\n",
      "  Batch [550/1299] D_loss: -0.4357, G_loss: -0.2995\n",
      "  Batch [560/1299] D_loss: -0.8520, G_loss: 0.0672\n",
      "  Batch [570/1299] D_loss: -0.5844, G_loss: -0.3120\n",
      "  Batch [580/1299] D_loss: -0.0027, G_loss: 0.2681\n",
      "  Batch [590/1299] D_loss: -0.0233, G_loss: 0.4239\n",
      "  Batch [600/1299] D_loss: -0.1139, G_loss: 0.5004\n",
      "  Batch [610/1299] D_loss: -0.1384, G_loss: 0.4237\n",
      "  Batch [620/1299] D_loss: -0.0709, G_loss: 0.4167\n",
      "  Batch [630/1299] D_loss: -2.0583, G_loss: -1.1829\n",
      "  Batch [640/1299] D_loss: -0.2066, G_loss: -0.0016\n",
      "  Batch [650/1299] D_loss: -1.0146, G_loss: -3.4942\n",
      "  Batch [660/1299] D_loss: -0.0402, G_loss: 0.2143\n",
      "  Batch [670/1299] D_loss: -0.0078, G_loss: 0.3762\n",
      "  Batch [680/1299] D_loss: -0.0528, G_loss: 0.4654\n",
      "  Batch [690/1299] D_loss: 0.0707, G_loss: 0.5034\n",
      "  Batch [700/1299] D_loss: -0.0780, G_loss: 0.3969\n",
      "  Batch [710/1299] D_loss: -0.0014, G_loss: 0.3010\n",
      "  Batch [720/1299] D_loss: -1.2865, G_loss: -1.8201\n",
      "  Batch [730/1299] D_loss: -0.0155, G_loss: 0.2295\n",
      "  Batch [740/1299] D_loss: 0.0307, G_loss: 0.3611\n",
      "  Batch [750/1299] D_loss: -0.0006, G_loss: 0.3411\n",
      "  Batch [760/1299] D_loss: -0.0880, G_loss: 0.2844\n",
      "  Batch [770/1299] D_loss: -1.0384, G_loss: -3.8550\n",
      "  Batch [780/1299] D_loss: -0.2748, G_loss: -0.1364\n",
      "  Batch [790/1299] D_loss: -0.0666, G_loss: 0.0771\n",
      "  Batch [800/1299] D_loss: -0.0478, G_loss: 0.2551\n",
      "  Batch [810/1299] D_loss: -0.1258, G_loss: 0.4570\n",
      "  Batch [820/1299] D_loss: -0.0849, G_loss: 0.5415\n",
      "  Batch [830/1299] D_loss: -0.0025, G_loss: 0.5268\n",
      "  Batch [840/1299] D_loss: 0.0545, G_loss: 0.3433\n",
      "  Batch [850/1299] D_loss: -0.0503, G_loss: 0.3471\n",
      "  Batch [860/1299] D_loss: -0.2900, G_loss: -0.2539\n",
      "  Batch [870/1299] D_loss: -0.8761, G_loss: -1.1733\n",
      "  Batch [880/1299] D_loss: -0.4573, G_loss: -0.3914\n",
      "  Batch [890/1299] D_loss: -0.0858, G_loss: 0.2092\n",
      "  Batch [900/1299] D_loss: -0.0516, G_loss: 0.3873\n",
      "  Batch [910/1299] D_loss: 0.0179, G_loss: 0.4661\n",
      "  Batch [920/1299] D_loss: -0.0159, G_loss: 0.5180\n",
      "  Batch [930/1299] D_loss: -0.0125, G_loss: 0.3809\n",
      "  Batch [940/1299] D_loss: -1.3054, G_loss: -3.7591\n",
      "  Batch [950/1299] D_loss: -0.0013, G_loss: 0.0856\n",
      "  Batch [960/1299] D_loss: -0.0366, G_loss: 0.1859\n",
      "  Batch [970/1299] D_loss: -0.1596, G_loss: 0.4205\n",
      "  Batch [980/1299] D_loss: 0.0509, G_loss: 0.4088\n",
      "  Batch [990/1299] D_loss: 0.0413, G_loss: 0.2249\n",
      "  Batch [1000/1299] D_loss: -0.7435, G_loss: -1.0958\n",
      "  Batch [1010/1299] D_loss: -0.0251, G_loss: 0.0482\n",
      "  Batch [1020/1299] D_loss: -0.0990, G_loss: 0.0201\n",
      "  Batch [1030/1299] D_loss: -0.2373, G_loss: -0.0217\n",
      "  Batch [1040/1299] D_loss: 0.0473, G_loss: 0.1726\n",
      "  Batch [1050/1299] D_loss: -0.0387, G_loss: 0.1105\n",
      "  Batch [1060/1299] D_loss: -0.0214, G_loss: 0.1565\n",
      "  Batch [1070/1299] D_loss: -1.2596, G_loss: -0.6984\n",
      "  Batch [1080/1299] D_loss: -0.3714, G_loss: -0.4404\n",
      "  Batch [1090/1299] D_loss: -0.0557, G_loss: 0.2853\n",
      "  Batch [1100/1299] D_loss: -0.0999, G_loss: 0.4741\n",
      "  Batch [1110/1299] D_loss: -0.2824, G_loss: 0.7502\n",
      "  Batch [1120/1299] D_loss: -0.0727, G_loss: 0.5211\n",
      "  Batch [1130/1299] D_loss: -0.1658, G_loss: 0.4890\n",
      "  Batch [1140/1299] D_loss: -0.0554, G_loss: 0.1858\n",
      "  Batch [1150/1299] D_loss: -0.0233, G_loss: 0.0778\n",
      "  Batch [1160/1299] D_loss: -0.3472, G_loss: 0.0605\n",
      "  Batch [1170/1299] D_loss: -0.0509, G_loss: 0.1711\n",
      "  Batch [1180/1299] D_loss: -0.0597, G_loss: 0.2262\n",
      "  Batch [1190/1299] D_loss: -0.1305, G_loss: 0.3199\n",
      "  Batch [1200/1299] D_loss: -0.1446, G_loss: 0.3366\n",
      "  Batch [1210/1299] D_loss: 0.0307, G_loss: 0.4838\n",
      "  Batch [1220/1299] D_loss: -0.3447, G_loss: 0.3903\n",
      "  Batch [1230/1299] D_loss: -2.2594, G_loss: -5.6088\n",
      "  Batch [1240/1299] D_loss: -0.3697, G_loss: -0.6754\n",
      "  Batch [1250/1299] D_loss: -0.0431, G_loss: 0.2901\n",
      "  Batch [1260/1299] D_loss: -0.0817, G_loss: 0.3420\n",
      "  Batch [1270/1299] D_loss: -0.0905, G_loss: 0.2926\n",
      "  Batch [1280/1299] D_loss: -0.0526, G_loss: 0.3357\n",
      "  Batch [1290/1299] D_loss: -1.0541, G_loss: -4.0936\n",
      "\n",
      "Epoch 81 Summary:\n",
      "  Average D_loss: -0.1348\n",
      "  Average G_loss: -0.0663\n",
      "\n",
      "Epoch [82/100]\n",
      "  Batch [0/1299] D_loss: -0.6323, G_loss: -0.3159\n",
      "  Batch [10/1299] D_loss: -0.0381, G_loss: 0.2168\n",
      "  Batch [20/1299] D_loss: -0.1914, G_loss: 0.4058\n",
      "  Batch [30/1299] D_loss: -0.0622, G_loss: 0.5071\n",
      "  Batch [40/1299] D_loss: -0.1920, G_loss: 0.4701\n",
      "  Batch [50/1299] D_loss: -0.0347, G_loss: 0.4900\n",
      "  Batch [60/1299] D_loss: -0.0259, G_loss: 0.2295\n",
      "  Batch [70/1299] D_loss: -0.0525, G_loss: 0.2281\n",
      "  Batch [80/1299] D_loss: -1.2852, G_loss: -0.9963\n",
      "  Batch [90/1299] D_loss: -0.9981, G_loss: -0.0177\n",
      "  Batch [100/1299] D_loss: -0.3980, G_loss: -0.4978\n",
      "  Batch [110/1299] D_loss: -0.0487, G_loss: 0.1655\n",
      "  Batch [120/1299] D_loss: -0.0628, G_loss: 0.3028\n",
      "  Batch [130/1299] D_loss: -0.0234, G_loss: 0.2870\n",
      "  Batch [140/1299] D_loss: -0.0792, G_loss: 0.3809\n",
      "  Batch [150/1299] D_loss: -0.0768, G_loss: 0.2910\n",
      "  Batch [160/1299] D_loss: -0.3537, G_loss: -0.7428\n",
      "  Batch [170/1299] D_loss: -0.8745, G_loss: -1.7676\n",
      "  Batch [180/1299] D_loss: -1.8661, G_loss: -3.4943\n",
      "  Batch [190/1299] D_loss: -0.0214, G_loss: 0.0957\n",
      "  Batch [200/1299] D_loss: -0.0544, G_loss: 0.2646\n",
      "  Batch [210/1299] D_loss: -0.1142, G_loss: 0.4351\n",
      "  Batch [220/1299] D_loss: -0.2446, G_loss: 0.5193\n",
      "  Batch [230/1299] D_loss: -0.0559, G_loss: 0.4748\n",
      "  Batch [240/1299] D_loss: -0.0550, G_loss: 0.3500\n",
      "  Batch [250/1299] D_loss: -1.8784, G_loss: -4.4068\n",
      "  Batch [260/1299] D_loss: -0.8224, G_loss: -0.0939\n",
      "  Batch [270/1299] D_loss: -0.0192, G_loss: 0.1954\n",
      "  Batch [280/1299] D_loss: -0.0625, G_loss: 0.4705\n",
      "  Batch [290/1299] D_loss: -0.0899, G_loss: 0.4317\n",
      "  Batch [300/1299] D_loss: -0.1395, G_loss: 0.5880\n",
      "  Batch [310/1299] D_loss: -0.0089, G_loss: 0.2855\n",
      "  Batch [320/1299] D_loss: -0.1713, G_loss: 0.3517\n",
      "  Batch [330/1299] D_loss: -1.2923, G_loss: -0.1949\n",
      "  Batch [340/1299] D_loss: -0.5871, G_loss: -0.0388\n",
      "  Batch [350/1299] D_loss: -0.0130, G_loss: 0.2378\n",
      "  Batch [360/1299] D_loss: -0.0741, G_loss: 0.4015\n",
      "  Batch [370/1299] D_loss: -0.0830, G_loss: 0.5116\n",
      "  Batch [380/1299] D_loss: -0.0361, G_loss: 0.4120\n",
      "  Batch [390/1299] D_loss: 0.0035, G_loss: 0.3175\n",
      "  Batch [400/1299] D_loss: -0.0690, G_loss: 0.2608\n",
      "  Batch [410/1299] D_loss: -0.2464, G_loss: -0.2458\n",
      "  Batch [420/1299] D_loss: -0.6234, G_loss: -0.4888\n",
      "  Batch [430/1299] D_loss: 0.0038, G_loss: 0.1513\n",
      "  Batch [440/1299] D_loss: 0.0119, G_loss: 0.3179\n",
      "  Batch [450/1299] D_loss: -0.0632, G_loss: 0.3428\n",
      "  Batch [460/1299] D_loss: -0.0634, G_loss: 0.4493\n",
      "  Batch [470/1299] D_loss: -0.0497, G_loss: 0.2397\n",
      "  Batch [480/1299] D_loss: -0.6680, G_loss: -0.3580\n",
      "  Batch [490/1299] D_loss: 0.0018, G_loss: 0.0591\n",
      "  Batch [500/1299] D_loss: -0.0083, G_loss: 0.0876\n",
      "  Batch [510/1299] D_loss: -0.0086, G_loss: 0.0823\n",
      "  Batch [520/1299] D_loss: -0.0178, G_loss: 0.1295\n",
      "  Batch [530/1299] D_loss: -1.1303, G_loss: -2.5632\n",
      "  Batch [540/1299] D_loss: -0.4613, G_loss: -1.4668\n",
      "  Batch [550/1299] D_loss: -0.2220, G_loss: 0.1100\n",
      "  Batch [560/1299] D_loss: -0.2256, G_loss: 0.2031\n",
      "  Batch [570/1299] D_loss: -0.0862, G_loss: 0.4870\n",
      "  Batch [580/1299] D_loss: -0.1699, G_loss: 0.5992\n",
      "  Batch [590/1299] D_loss: -0.0791, G_loss: 0.5959\n",
      "  Batch [600/1299] D_loss: -0.1688, G_loss: 0.3681\n",
      "  Batch [610/1299] D_loss: -0.0772, G_loss: 0.3748\n",
      "  Batch [620/1299] D_loss: -1.4853, G_loss: -1.3157\n",
      "  Batch [630/1299] D_loss: -1.1906, G_loss: -1.8527\n",
      "  Batch [640/1299] D_loss: -0.4731, G_loss: -0.2156\n",
      "  Batch [650/1299] D_loss: -0.1262, G_loss: -0.0730\n",
      "  Batch [660/1299] D_loss: -0.6151, G_loss: -0.3299\n",
      "  Batch [670/1299] D_loss: -0.1657, G_loss: 0.4133\n",
      "  Batch [680/1299] D_loss: -0.1719, G_loss: 0.5081\n",
      "  Batch [690/1299] D_loss: -0.0180, G_loss: 0.5221\n",
      "  Batch [700/1299] D_loss: -0.1253, G_loss: 0.5274\n",
      "  Batch [710/1299] D_loss: -0.1184, G_loss: 0.6986\n",
      "  Batch [720/1299] D_loss: -0.0554, G_loss: 0.4043\n",
      "  Batch [730/1299] D_loss: -0.0947, G_loss: -0.0637\n",
      "  Batch [740/1299] D_loss: 0.0027, G_loss: 0.1116\n",
      "  Batch [750/1299] D_loss: -0.0218, G_loss: 0.1642\n",
      "  Batch [760/1299] D_loss: -1.9579, G_loss: -1.6954\n",
      "  Batch [770/1299] D_loss: -0.2320, G_loss: -0.4012\n",
      "  Batch [780/1299] D_loss: -0.4560, G_loss: -0.5442\n",
      "  Batch [790/1299] D_loss: -0.0235, G_loss: 0.1503\n",
      "  Batch [800/1299] D_loss: -0.0067, G_loss: 0.2783\n",
      "  Batch [810/1299] D_loss: -0.1477, G_loss: 0.4633\n",
      "  Batch [820/1299] D_loss: 0.0013, G_loss: 0.5622\n",
      "  Batch [830/1299] D_loss: -0.0264, G_loss: 0.4485\n",
      "  Batch [840/1299] D_loss: -0.1087, G_loss: 0.4361\n",
      "  Batch [850/1299] D_loss: -0.0612, G_loss: 0.3457\n",
      "  Batch [860/1299] D_loss: -0.6211, G_loss: -0.5834\n",
      "  Batch [870/1299] D_loss: -0.6068, G_loss: -1.3799\n",
      "  Batch [880/1299] D_loss: -0.5603, G_loss: -0.7869\n",
      "  Batch [890/1299] D_loss: -0.4601, G_loss: 0.0847\n",
      "  Batch [900/1299] D_loss: -1.1159, G_loss: -1.0864\n",
      "  Batch [910/1299] D_loss: -0.5624, G_loss: -0.0149\n",
      "  Batch [920/1299] D_loss: -0.0818, G_loss: 0.3665\n",
      "  Batch [930/1299] D_loss: -0.1232, G_loss: 0.5686\n",
      "  Batch [940/1299] D_loss: -0.1483, G_loss: 0.5694\n",
      "  Batch [950/1299] D_loss: -0.0903, G_loss: 0.6781\n",
      "  Batch [960/1299] D_loss: -0.0294, G_loss: 0.5782\n",
      "  Batch [970/1299] D_loss: -0.0833, G_loss: 0.3019\n",
      "  Batch [980/1299] D_loss: -0.4025, G_loss: -1.0944\n",
      "  Batch [990/1299] D_loss: -0.0258, G_loss: 0.0940\n",
      "  Batch [1000/1299] D_loss: -0.0170, G_loss: 0.1608\n",
      "  Batch [1010/1299] D_loss: -0.0602, G_loss: 0.2665\n",
      "  Batch [1020/1299] D_loss: -0.0647, G_loss: 0.3466\n",
      "  Batch [1030/1299] D_loss: -0.0818, G_loss: 0.3055\n",
      "  Batch [1040/1299] D_loss: -0.3456, G_loss: -1.7427\n",
      "  Batch [1050/1299] D_loss: 0.0104, G_loss: 0.0957\n",
      "  Batch [1060/1299] D_loss: -0.0205, G_loss: 0.2355\n",
      "  Batch [1070/1299] D_loss: -0.0307, G_loss: 0.5177\n",
      "  Batch [1080/1299] D_loss: -0.0267, G_loss: 0.4096\n",
      "  Batch [1090/1299] D_loss: -0.0493, G_loss: 0.5169\n",
      "  Batch [1100/1299] D_loss: -0.1451, G_loss: 0.3089\n",
      "  Batch [1110/1299] D_loss: -0.3802, G_loss: 0.1635\n",
      "  Batch [1120/1299] D_loss: 0.0146, G_loss: 0.1311\n",
      "  Batch [1130/1299] D_loss: -0.9959, G_loss: -1.6403\n",
      "  Batch [1140/1299] D_loss: -0.1610, G_loss: -0.0041\n",
      "  Batch [1150/1299] D_loss: -0.3679, G_loss: -0.3266\n",
      "  Batch [1160/1299] D_loss: -0.2835, G_loss: -0.1331\n",
      "  Batch [1170/1299] D_loss: -0.0151, G_loss: 0.2578\n",
      "  Batch [1180/1299] D_loss: -0.0971, G_loss: 0.3128\n",
      "  Batch [1190/1299] D_loss: -0.0730, G_loss: 0.3844\n",
      "  Batch [1200/1299] D_loss: -0.1426, G_loss: 0.4268\n",
      "  Batch [1210/1299] D_loss: -0.1482, G_loss: 0.4532\n",
      "  Batch [1220/1299] D_loss: -0.0020, G_loss: -0.0775\n",
      "  Batch [1230/1299] D_loss: -0.6294, G_loss: -0.1318\n",
      "  Batch [1240/1299] D_loss: -0.0206, G_loss: 0.1332\n",
      "  Batch [1250/1299] D_loss: -0.0837, G_loss: 0.2257\n",
      "  Batch [1260/1299] D_loss: -0.0888, G_loss: 0.4994\n",
      "  Batch [1270/1299] D_loss: -0.1016, G_loss: 0.4779\n",
      "  Batch [1280/1299] D_loss: -0.0697, G_loss: 0.8084\n",
      "  Batch [1290/1299] D_loss: -0.0475, G_loss: 0.3798\n",
      "\n",
      "Epoch 82 Summary:\n",
      "  Average D_loss: -0.1412\n",
      "  Average G_loss: -0.0522\n",
      "\n",
      "Epoch [83/100]\n",
      "  Batch [0/1299] D_loss: -0.0977, G_loss: 0.3002\n",
      "  Batch [10/1299] D_loss: -0.1275, G_loss: -0.0797\n",
      "  Batch [20/1299] D_loss: -0.1554, G_loss: 0.1896\n",
      "  Batch [30/1299] D_loss: -0.1557, G_loss: 0.4932\n",
      "  Batch [40/1299] D_loss: -0.1101, G_loss: 0.5891\n",
      "  Batch [50/1299] D_loss: -0.1014, G_loss: 0.4879\n",
      "  Batch [60/1299] D_loss: 0.1307, G_loss: 0.2621\n",
      "  Batch [70/1299] D_loss: -0.3374, G_loss: -1.6158\n",
      "  Batch [80/1299] D_loss: -0.0216, G_loss: 0.1841\n",
      "  Batch [90/1299] D_loss: 0.0078, G_loss: 0.1958\n",
      "  Batch [100/1299] D_loss: -0.0022, G_loss: 0.3484\n",
      "  Batch [110/1299] D_loss: 0.0854, G_loss: 0.4114\n",
      "  Batch [120/1299] D_loss: -0.0344, G_loss: 0.2463\n",
      "  Batch [130/1299] D_loss: -0.8052, G_loss: -0.4502\n",
      "  Batch [140/1299] D_loss: -0.5669, G_loss: -0.3942\n",
      "  Batch [150/1299] D_loss: -0.3932, G_loss: 0.0603\n",
      "  Batch [160/1299] D_loss: -0.4319, G_loss: 0.0996\n",
      "  Batch [170/1299] D_loss: -0.1052, G_loss: 0.0692\n",
      "  Batch [180/1299] D_loss: -0.0760, G_loss: 0.1784\n",
      "  Batch [190/1299] D_loss: -0.0774, G_loss: 0.3773\n",
      "  Batch [200/1299] D_loss: -0.0389, G_loss: 0.4104\n",
      "  Batch [210/1299] D_loss: -0.0859, G_loss: 0.5751\n",
      "  Batch [220/1299] D_loss: -0.0456, G_loss: 0.4613\n",
      "  Batch [230/1299] D_loss: -0.0343, G_loss: 0.3728\n",
      "  Batch [240/1299] D_loss: -1.0455, G_loss: -1.1577\n",
      "  Batch [250/1299] D_loss: -0.0899, G_loss: 0.0956\n",
      "  Batch [260/1299] D_loss: -0.0552, G_loss: 0.1864\n",
      "  Batch [270/1299] D_loss: -0.0811, G_loss: 0.2906\n",
      "  Batch [280/1299] D_loss: -0.0840, G_loss: 0.4776\n",
      "  Batch [290/1299] D_loss: -0.0326, G_loss: 0.4954\n",
      "  Batch [300/1299] D_loss: -0.1410, G_loss: 0.4078\n",
      "  Batch [310/1299] D_loss: -0.1622, G_loss: 0.4252\n",
      "  Batch [320/1299] D_loss: 0.0115, G_loss: 0.1141\n",
      "  Batch [330/1299] D_loss: -0.0602, G_loss: 0.1505\n",
      "  Batch [340/1299] D_loss: -0.0379, G_loss: 0.2802\n",
      "  Batch [350/1299] D_loss: -0.3686, G_loss: -0.1412\n",
      "  Batch [360/1299] D_loss: -0.0305, G_loss: 0.1595\n",
      "  Batch [370/1299] D_loss: -0.1059, G_loss: 0.1810\n",
      "  Batch [380/1299] D_loss: -0.1547, G_loss: 0.4692\n",
      "  Batch [390/1299] D_loss: -0.0293, G_loss: 0.3786\n",
      "  Batch [400/1299] D_loss: -0.1224, G_loss: 0.2117\n",
      "  Batch [410/1299] D_loss: -0.1056, G_loss: 0.0797\n",
      "  Batch [420/1299] D_loss: -0.3507, G_loss: -1.3130\n",
      "  Batch [430/1299] D_loss: -0.3621, G_loss: -0.7621\n",
      "  Batch [440/1299] D_loss: -0.0138, G_loss: 0.2816\n",
      "  Batch [450/1299] D_loss: -0.1289, G_loss: 0.5069\n",
      "  Batch [460/1299] D_loss: -0.0760, G_loss: 0.5782\n",
      "  Batch [470/1299] D_loss: -0.0405, G_loss: 0.4627\n",
      "  Batch [480/1299] D_loss: -0.1119, G_loss: 0.4053\n",
      "  Batch [490/1299] D_loss: -1.9018, G_loss: -2.6076\n",
      "  Batch [500/1299] D_loss: -0.0357, G_loss: 0.2675\n",
      "  Batch [510/1299] D_loss: -0.0335, G_loss: 0.2512\n",
      "  Batch [520/1299] D_loss: -0.1945, G_loss: 0.3241\n",
      "  Batch [530/1299] D_loss: -0.0110, G_loss: 0.2066\n",
      "  Batch [540/1299] D_loss: -2.0244, G_loss: -0.8769\n",
      "  Batch [550/1299] D_loss: -0.0131, G_loss: 0.1382\n",
      "  Batch [560/1299] D_loss: -0.4369, G_loss: -1.6378\n",
      "  Batch [570/1299] D_loss: -0.3450, G_loss: -0.5133\n",
      "  Batch [580/1299] D_loss: -0.0194, G_loss: 0.2524\n",
      "  Batch [590/1299] D_loss: 0.0195, G_loss: 0.2244\n",
      "  Batch [600/1299] D_loss: -0.1313, G_loss: 0.3904\n",
      "  Batch [610/1299] D_loss: -0.0183, G_loss: 0.2250\n",
      "  Batch [620/1299] D_loss: -0.9047, G_loss: -1.0376\n",
      "  Batch [630/1299] D_loss: -0.1024, G_loss: 0.2733\n",
      "  Batch [640/1299] D_loss: -0.0079, G_loss: 0.2776\n",
      "  Batch [650/1299] D_loss: -0.0453, G_loss: 0.3199\n",
      "  Batch [660/1299] D_loss: -1.8796, G_loss: -2.9969\n",
      "  Batch [670/1299] D_loss: -0.0323, G_loss: 0.1659\n",
      "  Batch [680/1299] D_loss: -0.3189, G_loss: -0.3462\n",
      "  Batch [690/1299] D_loss: -0.0722, G_loss: 0.0935\n",
      "  Batch [700/1299] D_loss: -0.0352, G_loss: 0.3198\n",
      "  Batch [710/1299] D_loss: -0.0775, G_loss: 0.3509\n",
      "  Batch [720/1299] D_loss: -0.0821, G_loss: 0.4473\n",
      "  Batch [730/1299] D_loss: -0.0664, G_loss: 0.4387\n",
      "  Batch [740/1299] D_loss: -0.1011, G_loss: 0.3938\n",
      "  Batch [750/1299] D_loss: -0.9393, G_loss: -0.8620\n",
      "  Batch [760/1299] D_loss: 0.0021, G_loss: 0.0836\n",
      "  Batch [770/1299] D_loss: -0.0491, G_loss: 0.1751\n",
      "  Batch [780/1299] D_loss: -0.0510, G_loss: 0.2930\n",
      "  Batch [790/1299] D_loss: -0.1676, G_loss: 0.3756\n",
      "  Batch [800/1299] D_loss: -0.0418, G_loss: 0.3148\n",
      "  Batch [810/1299] D_loss: -1.0551, G_loss: -2.5512\n",
      "  Batch [820/1299] D_loss: 0.0007, G_loss: 0.2721\n",
      "  Batch [830/1299] D_loss: 0.0144, G_loss: 0.2958\n",
      "  Batch [840/1299] D_loss: -0.0428, G_loss: 0.3404\n",
      "  Batch [850/1299] D_loss: -0.0575, G_loss: 0.4536\n",
      "  Batch [860/1299] D_loss: -0.0605, G_loss: 0.2046\n",
      "  Batch [870/1299] D_loss: -1.9533, G_loss: -3.6769\n",
      "  Batch [880/1299] D_loss: -0.3034, G_loss: -0.2900\n",
      "  Batch [890/1299] D_loss: -0.7310, G_loss: -0.4115\n",
      "  Batch [900/1299] D_loss: -0.5319, G_loss: -0.3388\n",
      "  Batch [910/1299] D_loss: -0.3808, G_loss: -0.4230\n",
      "  Batch [920/1299] D_loss: -0.1962, G_loss: 0.2133\n",
      "  Batch [930/1299] D_loss: -0.0691, G_loss: 0.2698\n",
      "  Batch [940/1299] D_loss: -0.0158, G_loss: 0.2295\n",
      "  Batch [950/1299] D_loss: -0.0179, G_loss: 0.4926\n",
      "  Batch [960/1299] D_loss: -0.1021, G_loss: 0.5102\n",
      "  Batch [970/1299] D_loss: -0.0994, G_loss: 0.2991\n",
      "  Batch [980/1299] D_loss: -0.0717, G_loss: 0.2096\n",
      "  Batch [990/1299] D_loss: -0.3867, G_loss: -0.2566\n",
      "  Batch [1000/1299] D_loss: -0.0244, G_loss: 0.0412\n",
      "  Batch [1010/1299] D_loss: -0.4883, G_loss: -0.8229\n",
      "  Batch [1020/1299] D_loss: -1.0957, G_loss: -1.5686\n",
      "  Batch [1030/1299] D_loss: -0.2738, G_loss: 0.1370\n",
      "  Batch [1040/1299] D_loss: -0.9423, G_loss: -1.3615\n",
      "  Batch [1050/1299] D_loss: -0.0526, G_loss: 0.1690\n",
      "  Batch [1060/1299] D_loss: -0.0582, G_loss: 0.3581\n",
      "  Batch [1070/1299] D_loss: -0.1303, G_loss: 0.5342\n",
      "  Batch [1080/1299] D_loss: -0.1624, G_loss: 0.5368\n",
      "  Batch [1090/1299] D_loss: -0.0514, G_loss: 0.5602\n",
      "  Batch [1100/1299] D_loss: -0.0251, G_loss: 0.4169\n",
      "  Batch [1110/1299] D_loss: -0.0381, G_loss: 0.2944\n",
      "  Batch [1120/1299] D_loss: 0.0280, G_loss: 0.1016\n",
      "  Batch [1130/1299] D_loss: -0.0289, G_loss: 0.0633\n",
      "  Batch [1140/1299] D_loss: -0.0961, G_loss: 0.0212\n",
      "  Batch [1150/1299] D_loss: -0.5817, G_loss: -0.5161\n",
      "  Batch [1160/1299] D_loss: -0.8982, G_loss: -1.0877\n",
      "  Batch [1170/1299] D_loss: -1.2500, G_loss: -0.5644\n",
      "  Batch [1180/1299] D_loss: -0.0651, G_loss: 0.2274\n",
      "  Batch [1190/1299] D_loss: -0.1088, G_loss: 0.5105\n",
      "  Batch [1200/1299] D_loss: -0.2804, G_loss: 0.7373\n",
      "  Batch [1210/1299] D_loss: -0.1670, G_loss: 0.5450\n",
      "  Batch [1220/1299] D_loss: 0.0189, G_loss: 0.5120\n",
      "  Batch [1230/1299] D_loss: -0.0112, G_loss: 0.3639\n",
      "  Batch [1240/1299] D_loss: -0.6990, G_loss: -1.7458\n",
      "  Batch [1250/1299] D_loss: -0.7412, G_loss: -0.0453\n",
      "  Batch [1260/1299] D_loss: -0.1214, G_loss: 0.0387\n",
      "  Batch [1270/1299] D_loss: -0.0694, G_loss: 0.1979\n",
      "  Batch [1280/1299] D_loss: -0.1363, G_loss: 0.4397\n",
      "  Batch [1290/1299] D_loss: -0.1404, G_loss: 0.6045\n",
      "\n",
      "Epoch 83 Summary:\n",
      "  Average D_loss: -0.1255\n",
      "  Average G_loss: -0.0677\n",
      "\n",
      "Epoch [84/100]\n",
      "  Batch [0/1299] D_loss: -0.1316, G_loss: 0.5567\n",
      "  Batch [10/1299] D_loss: -0.0012, G_loss: 0.6634\n",
      "  Batch [20/1299] D_loss: -0.0432, G_loss: 0.3526\n",
      "  Batch [30/1299] D_loss: -0.0869, G_loss: 0.2259\n",
      "  Batch [40/1299] D_loss: 0.0859, G_loss: 0.0035\n",
      "  Batch [50/1299] D_loss: -0.0086, G_loss: 0.1275\n",
      "  Batch [60/1299] D_loss: -0.0482, G_loss: 0.1798\n",
      "  Batch [70/1299] D_loss: -0.0443, G_loss: 0.2337\n",
      "  Batch [80/1299] D_loss: -0.1527, G_loss: 0.3955\n",
      "  Batch [90/1299] D_loss: -0.1003, G_loss: 0.3381\n",
      "  Batch [100/1299] D_loss: -4.6105, G_loss: -8.4179\n",
      "  Batch [110/1299] D_loss: -1.0646, G_loss: -1.2946\n",
      "  Batch [120/1299] D_loss: -0.0313, G_loss: 0.1251\n",
      "  Batch [130/1299] D_loss: -0.0607, G_loss: 0.4670\n",
      "  Batch [140/1299] D_loss: -0.0365, G_loss: 0.6421\n",
      "  Batch [150/1299] D_loss: -0.0207, G_loss: 0.4059\n",
      "  Batch [160/1299] D_loss: -0.0311, G_loss: 0.4767\n",
      "  Batch [170/1299] D_loss: -0.0505, G_loss: 0.2609\n",
      "  Batch [180/1299] D_loss: -3.3148, G_loss: -6.9775\n",
      "  Batch [190/1299] D_loss: 0.0025, G_loss: 0.0380\n",
      "  Batch [200/1299] D_loss: -0.8964, G_loss: -1.5000\n",
      "  Batch [210/1299] D_loss: -0.2238, G_loss: 0.0170\n",
      "  Batch [220/1299] D_loss: 0.0084, G_loss: 0.1315\n",
      "  Batch [230/1299] D_loss: -0.0448, G_loss: 0.3026\n",
      "  Batch [240/1299] D_loss: 0.0243, G_loss: 0.4060\n",
      "  Batch [250/1299] D_loss: -0.0429, G_loss: 0.3082\n",
      "  Batch [260/1299] D_loss: -0.0846, G_loss: 0.3969\n",
      "  Batch [270/1299] D_loss: -0.0492, G_loss: 0.2682\n",
      "  Batch [280/1299] D_loss: -3.3740, G_loss: -4.4073\n",
      "  Batch [290/1299] D_loss: -0.0993, G_loss: 0.1147\n",
      "  Batch [300/1299] D_loss: -0.1324, G_loss: -0.1216\n",
      "  Batch [310/1299] D_loss: -0.7463, G_loss: -0.7250\n",
      "  Batch [320/1299] D_loss: -0.2467, G_loss: -0.1452\n",
      "  Batch [330/1299] D_loss: -0.0510, G_loss: 0.1481\n",
      "  Batch [340/1299] D_loss: -0.0386, G_loss: 0.3508\n",
      "  Batch [350/1299] D_loss: -0.1014, G_loss: 0.4348\n",
      "  Batch [360/1299] D_loss: -0.1063, G_loss: 0.5085\n",
      "  Batch [370/1299] D_loss: -0.0709, G_loss: 0.2714\n",
      "  Batch [380/1299] D_loss: -0.0703, G_loss: 0.3337\n",
      "  Batch [390/1299] D_loss: -0.5467, G_loss: -2.7657\n",
      "  Batch [400/1299] D_loss: -0.0317, G_loss: 0.1397\n",
      "  Batch [410/1299] D_loss: -0.1030, G_loss: 0.2896\n",
      "  Batch [420/1299] D_loss: -0.0405, G_loss: 0.4449\n",
      "  Batch [430/1299] D_loss: -0.1108, G_loss: 0.4253\n",
      "  Batch [440/1299] D_loss: -0.0887, G_loss: 0.5004\n",
      "  Batch [450/1299] D_loss: 0.0379, G_loss: 0.3734\n",
      "  Batch [460/1299] D_loss: -0.2401, G_loss: -0.1477\n",
      "  Batch [470/1299] D_loss: -0.0162, G_loss: 0.1303\n",
      "  Batch [480/1299] D_loss: 0.0366, G_loss: 0.2047\n",
      "  Batch [490/1299] D_loss: -0.0552, G_loss: 0.1444\n",
      "  Batch [500/1299] D_loss: -0.8776, G_loss: -1.6992\n",
      "  Batch [510/1299] D_loss: -1.1029, G_loss: -0.2979\n",
      "  Batch [520/1299] D_loss: -0.0193, G_loss: 0.1561\n",
      "  Batch [530/1299] D_loss: -0.5107, G_loss: -0.8078\n",
      "  Batch [540/1299] D_loss: -0.0315, G_loss: 0.1731\n",
      "  Batch [550/1299] D_loss: -0.1986, G_loss: 0.4883\n",
      "  Batch [560/1299] D_loss: -0.1513, G_loss: 0.7421\n",
      "  Batch [570/1299] D_loss: -0.1395, G_loss: 0.6520\n",
      "  Batch [580/1299] D_loss: -0.2314, G_loss: 0.6732\n",
      "  Batch [590/1299] D_loss: -0.1526, G_loss: 0.5341\n",
      "  Batch [600/1299] D_loss: -0.0419, G_loss: 0.1376\n",
      "  Batch [610/1299] D_loss: -0.0019, G_loss: 0.1256\n",
      "  Batch [620/1299] D_loss: -0.0167, G_loss: 0.1994\n",
      "  Batch [630/1299] D_loss: -0.0276, G_loss: 0.1846\n",
      "  Batch [640/1299] D_loss: -0.0124, G_loss: 0.2077\n",
      "  Batch [650/1299] D_loss: -0.1443, G_loss: 0.2700\n",
      "  Batch [660/1299] D_loss: -0.7246, G_loss: -1.3837\n",
      "  Batch [670/1299] D_loss: -0.3360, G_loss: -0.0308\n",
      "  Batch [680/1299] D_loss: -0.9483, G_loss: -0.9429\n",
      "  Batch [690/1299] D_loss: -0.0309, G_loss: 0.2072\n",
      "  Batch [700/1299] D_loss: -0.1081, G_loss: 0.3579\n",
      "  Batch [710/1299] D_loss: 0.0776, G_loss: 0.3450\n",
      "  Batch [720/1299] D_loss: -0.0024, G_loss: 0.3723\n",
      "  Batch [730/1299] D_loss: 0.0435, G_loss: 0.3731\n",
      "  Batch [740/1299] D_loss: -0.5807, G_loss: -0.5452\n",
      "  Batch [750/1299] D_loss: -0.3947, G_loss: -0.3197\n",
      "  Batch [760/1299] D_loss: -0.3690, G_loss: 0.0418\n",
      "  Batch [770/1299] D_loss: -0.0465, G_loss: 0.1922\n",
      "  Batch [780/1299] D_loss: -0.0763, G_loss: 0.3171\n",
      "  Batch [790/1299] D_loss: -0.0315, G_loss: 0.4084\n",
      "  Batch [800/1299] D_loss: -0.0803, G_loss: 0.3461\n",
      "  Batch [810/1299] D_loss: -0.0911, G_loss: 0.2066\n",
      "  Batch [820/1299] D_loss: -0.4758, G_loss: 0.0330\n",
      "  Batch [830/1299] D_loss: -0.0425, G_loss: 0.2218\n",
      "  Batch [840/1299] D_loss: -0.0906, G_loss: 0.2680\n",
      "  Batch [850/1299] D_loss: -0.1495, G_loss: 0.4283\n",
      "  Batch [860/1299] D_loss: -0.0009, G_loss: 0.2075\n",
      "  Batch [870/1299] D_loss: -0.0366, G_loss: 0.1343\n",
      "  Batch [880/1299] D_loss: 0.0206, G_loss: 0.1747\n",
      "  Batch [890/1299] D_loss: 0.0289, G_loss: 0.2123\n",
      "  Batch [900/1299] D_loss: -0.0415, G_loss: 0.2215\n",
      "  Batch [910/1299] D_loss: -0.0433, G_loss: 0.1872\n",
      "  Batch [920/1299] D_loss: -1.0821, G_loss: -1.3382\n",
      "  Batch [930/1299] D_loss: -0.1752, G_loss: 0.0478\n",
      "  Batch [940/1299] D_loss: -0.0727, G_loss: 0.1903\n",
      "  Batch [950/1299] D_loss: -0.1963, G_loss: 0.4756\n",
      "  Batch [960/1299] D_loss: 0.0967, G_loss: 0.5531\n",
      "  Batch [970/1299] D_loss: -0.0479, G_loss: 0.3162\n",
      "  Batch [980/1299] D_loss: -2.7282, G_loss: -2.6050\n",
      "  Batch [990/1299] D_loss: -0.7059, G_loss: -1.8385\n",
      "  Batch [1000/1299] D_loss: -0.5587, G_loss: -1.2459\n",
      "  Batch [1010/1299] D_loss: -0.1343, G_loss: -0.0877\n",
      "  Batch [1020/1299] D_loss: -0.3077, G_loss: 0.0326\n",
      "  Batch [1030/1299] D_loss: -0.1460, G_loss: 0.2705\n",
      "  Batch [1040/1299] D_loss: -0.0692, G_loss: 0.5444\n",
      "  Batch [1050/1299] D_loss: -0.0348, G_loss: 0.6170\n",
      "  Batch [1060/1299] D_loss: -0.0057, G_loss: 0.5295\n",
      "  Batch [1070/1299] D_loss: 0.0730, G_loss: 0.5116\n",
      "  Batch [1080/1299] D_loss: -0.0527, G_loss: 0.3264\n",
      "  Batch [1090/1299] D_loss: -0.0940, G_loss: 0.2770\n",
      "  Batch [1100/1299] D_loss: -1.0398, G_loss: -2.3408\n",
      "  Batch [1110/1299] D_loss: -1.0773, G_loss: -0.5186\n",
      "  Batch [1120/1299] D_loss: -0.9408, G_loss: -1.0702\n",
      "  Batch [1130/1299] D_loss: -0.0250, G_loss: 0.2207\n",
      "  Batch [1140/1299] D_loss: 0.0621, G_loss: 0.2800\n",
      "  Batch [1150/1299] D_loss: -0.0503, G_loss: 0.3666\n",
      "  Batch [1160/1299] D_loss: 0.0163, G_loss: 0.3773\n",
      "  Batch [1170/1299] D_loss: -0.0702, G_loss: 0.2878\n",
      "  Batch [1180/1299] D_loss: -0.0257, G_loss: 0.0828\n",
      "  Batch [1190/1299] D_loss: -0.0311, G_loss: 0.1806\n",
      "  Batch [1200/1299] D_loss: -0.0012, G_loss: 0.2075\n",
      "  Batch [1210/1299] D_loss: -0.0430, G_loss: 0.4439\n",
      "  Batch [1220/1299] D_loss: -0.0239, G_loss: 0.4010\n",
      "  Batch [1230/1299] D_loss: -3.1077, G_loss: -3.0247\n",
      "  Batch [1240/1299] D_loss: -0.0211, G_loss: 0.1415\n",
      "  Batch [1250/1299] D_loss: -0.0730, G_loss: 0.3850\n",
      "  Batch [1260/1299] D_loss: -0.1545, G_loss: 0.4241\n",
      "  Batch [1270/1299] D_loss: -0.0615, G_loss: 0.3820\n",
      "  Batch [1280/1299] D_loss: 0.0528, G_loss: 0.3564\n",
      "  Batch [1290/1299] D_loss: -0.3234, G_loss: -0.8722\n",
      "\n",
      "Epoch 84 Summary:\n",
      "  Average D_loss: -0.1172\n",
      "  Average G_loss: -0.0579\n",
      "\n",
      "Epoch [85/100]\n",
      "  Batch [0/1299] D_loss: -0.0517, G_loss: 0.0799\n",
      "  Batch [10/1299] D_loss: -0.0644, G_loss: 0.2262\n",
      "  Batch [20/1299] D_loss: -0.0680, G_loss: 0.3377\n",
      "  Batch [30/1299] D_loss: -0.0747, G_loss: 0.2892\n",
      "  Batch [40/1299] D_loss: 0.0362, G_loss: 0.2338\n",
      "  Batch [50/1299] D_loss: 0.0328, G_loss: 0.4297\n",
      "  Batch [60/1299] D_loss: -0.6388, G_loss: -0.4853\n",
      "  Batch [70/1299] D_loss: -0.8500, G_loss: -0.9454\n",
      "  Batch [80/1299] D_loss: -0.2581, G_loss: 0.0627\n",
      "  Batch [90/1299] D_loss: -0.2695, G_loss: -0.0043\n",
      "  Batch [100/1299] D_loss: -0.3170, G_loss: -0.0746\n",
      "  Batch [110/1299] D_loss: -0.0647, G_loss: 0.2200\n",
      "  Batch [120/1299] D_loss: -0.0779, G_loss: 0.5523\n",
      "  Batch [130/1299] D_loss: -0.1379, G_loss: 0.5773\n",
      "  Batch [140/1299] D_loss: -0.1477, G_loss: 0.4790\n",
      "  Batch [150/1299] D_loss: -0.0798, G_loss: 0.4304\n",
      "  Batch [160/1299] D_loss: -0.9894, G_loss: -2.1085\n",
      "  Batch [170/1299] D_loss: -0.0105, G_loss: 0.1109\n",
      "  Batch [180/1299] D_loss: -0.5683, G_loss: -0.1785\n",
      "  Batch [190/1299] D_loss: 0.0230, G_loss: 0.0486\n",
      "  Batch [200/1299] D_loss: -0.1116, G_loss: 0.0053\n",
      "  Batch [210/1299] D_loss: -0.1106, G_loss: 0.0988\n",
      "  Batch [220/1299] D_loss: -1.3570, G_loss: -2.1557\n",
      "  Batch [230/1299] D_loss: -0.1140, G_loss: 0.1965\n",
      "  Batch [240/1299] D_loss: 0.0232, G_loss: 0.2955\n",
      "  Batch [250/1299] D_loss: -0.1368, G_loss: 0.4836\n",
      "  Batch [260/1299] D_loss: -0.0922, G_loss: 0.5025\n",
      "  Batch [270/1299] D_loss: -0.0962, G_loss: 0.4072\n",
      "  Batch [280/1299] D_loss: -0.0728, G_loss: 0.3936\n",
      "  Batch [290/1299] D_loss: -1.7694, G_loss: -8.2774\n",
      "  Batch [300/1299] D_loss: -0.6381, G_loss: -0.5182\n",
      "  Batch [310/1299] D_loss: -0.5292, G_loss: -0.4989\n",
      "  Batch [320/1299] D_loss: -0.5731, G_loss: 0.0985\n",
      "  Batch [330/1299] D_loss: 0.0047, G_loss: 0.1195\n",
      "  Batch [340/1299] D_loss: -0.0209, G_loss: 0.2978\n",
      "  Batch [350/1299] D_loss: -0.0999, G_loss: 0.3194\n",
      "  Batch [360/1299] D_loss: -0.0309, G_loss: 0.3499\n",
      "  Batch [370/1299] D_loss: -0.0930, G_loss: 0.3930\n",
      "  Batch [380/1299] D_loss: 0.0053, G_loss: 0.3806\n",
      "  Batch [390/1299] D_loss: -0.0481, G_loss: 0.2997\n",
      "  Batch [400/1299] D_loss: -0.5956, G_loss: -0.1775\n",
      "  Batch [410/1299] D_loss: -0.0641, G_loss: 0.3380\n",
      "  Batch [420/1299] D_loss: -0.0263, G_loss: 0.4112\n",
      "  Batch [430/1299] D_loss: -0.0815, G_loss: 0.5184\n",
      "  Batch [440/1299] D_loss: -0.1161, G_loss: 0.3607\n",
      "  Batch [450/1299] D_loss: -0.1168, G_loss: 0.3453\n",
      "  Batch [460/1299] D_loss: -0.9215, G_loss: -0.1863\n",
      "  Batch [470/1299] D_loss: -1.0104, G_loss: -1.2561\n",
      "  Batch [480/1299] D_loss: -0.7622, G_loss: -0.3578\n",
      "  Batch [490/1299] D_loss: -0.3062, G_loss: 0.1107\n",
      "  Batch [500/1299] D_loss: -0.1105, G_loss: 0.3136\n",
      "  Batch [510/1299] D_loss: -0.0490, G_loss: 0.4081\n",
      "  Batch [520/1299] D_loss: 0.0365, G_loss: 0.4188\n",
      "  Batch [530/1299] D_loss: -0.1158, G_loss: 0.5086\n",
      "  Batch [540/1299] D_loss: -0.0206, G_loss: 0.3924\n",
      "  Batch [550/1299] D_loss: -0.0867, G_loss: 0.2246\n",
      "  Batch [560/1299] D_loss: -0.6696, G_loss: -0.3020\n",
      "  Batch [570/1299] D_loss: -0.5780, G_loss: -1.0839\n",
      "  Batch [580/1299] D_loss: -0.2835, G_loss: -0.4559\n",
      "  Batch [590/1299] D_loss: -0.1340, G_loss: 0.3744\n",
      "  Batch [600/1299] D_loss: -0.0404, G_loss: 0.5434\n",
      "  Batch [610/1299] D_loss: -0.1399, G_loss: 0.5608\n",
      "  Batch [620/1299] D_loss: -0.0401, G_loss: 0.7363\n",
      "  Batch [630/1299] D_loss: -0.0392, G_loss: 0.5208\n",
      "  Batch [640/1299] D_loss: -0.0518, G_loss: 0.2903\n",
      "  Batch [650/1299] D_loss: -0.2156, G_loss: -0.2749\n",
      "  Batch [660/1299] D_loss: -1.2580, G_loss: -2.0038\n",
      "  Batch [670/1299] D_loss: -0.4191, G_loss: -0.4768\n",
      "  Batch [680/1299] D_loss: -0.3240, G_loss: -0.5875\n",
      "  Batch [690/1299] D_loss: -0.0622, G_loss: 0.3213\n",
      "  Batch [700/1299] D_loss: -0.0795, G_loss: 0.5157\n",
      "  Batch [710/1299] D_loss: -0.1048, G_loss: 0.5114\n",
      "  Batch [720/1299] D_loss: -0.1430, G_loss: 0.3788\n",
      "  Batch [730/1299] D_loss: -0.1016, G_loss: 0.4360\n",
      "  Batch [740/1299] D_loss: -0.0644, G_loss: 0.1042\n",
      "  Batch [750/1299] D_loss: -0.0800, G_loss: 0.0589\n",
      "  Batch [760/1299] D_loss: -0.2978, G_loss: 0.0507\n",
      "  Batch [770/1299] D_loss: -0.3933, G_loss: -0.1102\n",
      "  Batch [780/1299] D_loss: -0.2810, G_loss: -0.0677\n",
      "  Batch [790/1299] D_loss: -0.0940, G_loss: -0.2756\n",
      "  Batch [800/1299] D_loss: -0.0755, G_loss: 0.3163\n",
      "  Batch [810/1299] D_loss: 0.0288, G_loss: 0.5680\n",
      "  Batch [820/1299] D_loss: -0.0608, G_loss: 0.4779\n",
      "  Batch [830/1299] D_loss: -0.0955, G_loss: 0.5170\n",
      "  Batch [840/1299] D_loss: -0.1279, G_loss: 0.4296\n",
      "  Batch [850/1299] D_loss: -0.2167, G_loss: -0.0921\n",
      "  Batch [860/1299] D_loss: -0.1373, G_loss: 0.1207\n",
      "  Batch [870/1299] D_loss: -0.0870, G_loss: 0.3072\n",
      "  Batch [880/1299] D_loss: 0.0340, G_loss: 0.4149\n",
      "  Batch [890/1299] D_loss: -0.0999, G_loss: 0.3962\n",
      "  Batch [900/1299] D_loss: -0.1993, G_loss: 0.3213\n",
      "  Batch [910/1299] D_loss: -0.0275, G_loss: 0.1743\n",
      "  Batch [920/1299] D_loss: -0.2418, G_loss: -0.1813\n",
      "  Batch [930/1299] D_loss: 0.0487, G_loss: 0.1433\n",
      "  Batch [940/1299] D_loss: -0.3250, G_loss: -0.0933\n",
      "  Batch [950/1299] D_loss: -0.8773, G_loss: -1.1220\n",
      "  Batch [960/1299] D_loss: -0.0488, G_loss: 0.2136\n",
      "  Batch [970/1299] D_loss: 0.0129, G_loss: 0.3513\n",
      "  Batch [980/1299] D_loss: -0.1449, G_loss: 0.3407\n",
      "  Batch [990/1299] D_loss: -0.0382, G_loss: 0.4568\n",
      "  Batch [1000/1299] D_loss: 0.0091, G_loss: 0.3221\n",
      "  Batch [1010/1299] D_loss: -0.0666, G_loss: 0.2747\n",
      "  Batch [1020/1299] D_loss: -1.5808, G_loss: -3.6922\n",
      "  Batch [1030/1299] D_loss: 0.0017, G_loss: 0.1235\n",
      "  Batch [1040/1299] D_loss: -0.0587, G_loss: 0.2292\n",
      "  Batch [1050/1299] D_loss: -0.0858, G_loss: 0.3164\n",
      "  Batch [1060/1299] D_loss: -0.2498, G_loss: 0.5516\n",
      "  Batch [1070/1299] D_loss: -0.2861, G_loss: 0.5163\n",
      "  Batch [1080/1299] D_loss: -2.0041, G_loss: -2.6915\n",
      "  Batch [1090/1299] D_loss: -0.3890, G_loss: -1.0713\n",
      "  Batch [1100/1299] D_loss: -0.2607, G_loss: -0.2455\n",
      "  Batch [1110/1299] D_loss: -0.7793, G_loss: -0.4742\n",
      "  Batch [1120/1299] D_loss: -0.0987, G_loss: 0.3688\n",
      "  Batch [1130/1299] D_loss: -0.0456, G_loss: 0.4783\n",
      "  Batch [1140/1299] D_loss: -0.0849, G_loss: 0.5128\n",
      "  Batch [1150/1299] D_loss: -0.0278, G_loss: 0.3574\n",
      "  Batch [1160/1299] D_loss: 0.0010, G_loss: 0.3084\n",
      "  Batch [1170/1299] D_loss: -1.2363, G_loss: -1.8668\n",
      "  Batch [1180/1299] D_loss: 0.0312, G_loss: 0.1805\n",
      "  Batch [1190/1299] D_loss: -0.0828, G_loss: 0.3487\n",
      "  Batch [1200/1299] D_loss: -0.0257, G_loss: 0.4045\n",
      "  Batch [1210/1299] D_loss: -0.0261, G_loss: 0.4060\n",
      "  Batch [1220/1299] D_loss: -0.0821, G_loss: 0.2730\n",
      "  Batch [1230/1299] D_loss: -3.2954, G_loss: -6.5299\n",
      "  Batch [1240/1299] D_loss: -0.0992, G_loss: 0.0657\n",
      "  Batch [1250/1299] D_loss: -0.0706, G_loss: 0.1744\n",
      "  Batch [1260/1299] D_loss: -0.0029, G_loss: 0.2046\n",
      "  Batch [1270/1299] D_loss: -0.1152, G_loss: 0.3441\n",
      "  Batch [1280/1299] D_loss: -0.0877, G_loss: 0.3444\n",
      "  Batch [1290/1299] D_loss: -3.0095, G_loss: -0.4503\n",
      "\n",
      "Epoch 85 Summary:\n",
      "  Average D_loss: -0.1347\n",
      "  Average G_loss: -0.0711\n",
      "\n",
      "Epoch [86/100]\n",
      "  Batch [0/1299] D_loss: -0.2411, G_loss: 0.1263\n",
      "  Batch [10/1299] D_loss: -0.1352, G_loss: 0.0434\n",
      "  Batch [20/1299] D_loss: -0.7878, G_loss: -2.3655\n",
      "  Batch [30/1299] D_loss: -0.5930, G_loss: 0.0632\n",
      "  Batch [40/1299] D_loss: -0.0662, G_loss: 0.1761\n",
      "  Batch [50/1299] D_loss: -1.0047, G_loss: -0.7784\n",
      "  Batch [60/1299] D_loss: -0.2618, G_loss: -0.0459\n",
      "  Batch [70/1299] D_loss: -0.0778, G_loss: 0.2878\n",
      "  Batch [80/1299] D_loss: -0.1794, G_loss: 0.5302\n",
      "  Batch [90/1299] D_loss: -0.2073, G_loss: 0.6989\n",
      "  Batch [100/1299] D_loss: -0.2403, G_loss: 0.6442\n",
      "  Batch [110/1299] D_loss: 0.0395, G_loss: 0.5718\n",
      "  Batch [120/1299] D_loss: -0.1027, G_loss: 0.4638\n",
      "  Batch [130/1299] D_loss: -0.9230, G_loss: -0.6720\n",
      "  Batch [140/1299] D_loss: -0.5700, G_loss: -0.9079\n",
      "  Batch [150/1299] D_loss: -0.0284, G_loss: 0.1417\n",
      "  Batch [160/1299] D_loss: -0.1247, G_loss: 0.3984\n",
      "  Batch [170/1299] D_loss: -0.1359, G_loss: 0.3986\n",
      "  Batch [180/1299] D_loss: -0.1016, G_loss: 0.4608\n",
      "  Batch [190/1299] D_loss: -0.1928, G_loss: 0.4326\n",
      "  Batch [200/1299] D_loss: -0.0371, G_loss: 0.2694\n",
      "  Batch [210/1299] D_loss: -0.3378, G_loss: -0.5459\n",
      "  Batch [220/1299] D_loss: -0.5493, G_loss: -0.4793\n",
      "  Batch [230/1299] D_loss: -0.6857, G_loss: -0.4390\n",
      "  Batch [240/1299] D_loss: -0.0078, G_loss: 0.3273\n",
      "  Batch [250/1299] D_loss: -0.2256, G_loss: 0.5906\n",
      "  Batch [260/1299] D_loss: -0.0734, G_loss: 0.6013\n",
      "  Batch [270/1299] D_loss: -0.1230, G_loss: 0.5419\n",
      "  Batch [280/1299] D_loss: -0.0472, G_loss: 0.3541\n",
      "  Batch [290/1299] D_loss: -0.1962, G_loss: 0.3934\n",
      "  Batch [300/1299] D_loss: -1.2801, G_loss: -0.6813\n",
      "  Batch [310/1299] D_loss: -0.2784, G_loss: 0.0865\n",
      "  Batch [320/1299] D_loss: -0.4357, G_loss: -0.0688\n",
      "  Batch [330/1299] D_loss: 0.0126, G_loss: -0.0426\n",
      "  Batch [340/1299] D_loss: -0.6309, G_loss: -0.9693\n",
      "  Batch [350/1299] D_loss: -0.3741, G_loss: 0.0175\n",
      "  Batch [360/1299] D_loss: -0.0187, G_loss: 0.1533\n",
      "  Batch [370/1299] D_loss: -0.1533, G_loss: 0.1567\n",
      "  Batch [380/1299] D_loss: -0.9874, G_loss: -0.3314\n",
      "  Batch [390/1299] D_loss: -0.2213, G_loss: 0.1587\n",
      "  Batch [400/1299] D_loss: -0.6503, G_loss: -1.5967\n",
      "  Batch [410/1299] D_loss: -0.6870, G_loss: 0.0552\n",
      "  Batch [420/1299] D_loss: -0.2374, G_loss: 0.1877\n",
      "  Batch [430/1299] D_loss: -0.7147, G_loss: 0.0097\n",
      "  Batch [440/1299] D_loss: -0.1792, G_loss: 0.4964\n",
      "  Batch [450/1299] D_loss: -0.2954, G_loss: 0.4967\n",
      "  Batch [460/1299] D_loss: -0.0502, G_loss: 0.8660\n",
      "  Batch [470/1299] D_loss: -0.0512, G_loss: 0.5438\n",
      "  Batch [480/1299] D_loss: -0.0926, G_loss: 0.3561\n",
      "  Batch [490/1299] D_loss: -0.1420, G_loss: 0.2584\n",
      "  Batch [500/1299] D_loss: -0.0785, G_loss: 0.0441\n",
      "  Batch [510/1299] D_loss: -0.2057, G_loss: 0.1633\n",
      "  Batch [520/1299] D_loss: -0.9139, G_loss: -0.8003\n",
      "  Batch [530/1299] D_loss: -0.0243, G_loss: 0.2370\n",
      "  Batch [540/1299] D_loss: -0.0968, G_loss: 0.2626\n",
      "  Batch [550/1299] D_loss: -0.0954, G_loss: 0.4117\n",
      "  Batch [560/1299] D_loss: 0.0066, G_loss: 0.3497\n",
      "  Batch [570/1299] D_loss: -0.1306, G_loss: 0.4076\n",
      "  Batch [580/1299] D_loss: -0.0452, G_loss: 0.3382\n",
      "  Batch [590/1299] D_loss: -1.6141, G_loss: -4.1262\n",
      "  Batch [600/1299] D_loss: -0.8024, G_loss: -0.3624\n",
      "  Batch [610/1299] D_loss: -0.1086, G_loss: 0.3048\n",
      "  Batch [620/1299] D_loss: -0.0989, G_loss: 0.4489\n",
      "  Batch [630/1299] D_loss: -0.0926, G_loss: 0.4652\n",
      "  Batch [640/1299] D_loss: -0.0870, G_loss: 0.5061\n",
      "  Batch [650/1299] D_loss: -0.0155, G_loss: 0.3294\n",
      "  Batch [660/1299] D_loss: -0.0482, G_loss: 0.1888\n",
      "  Batch [670/1299] D_loss: -0.4733, G_loss: -1.1842\n",
      "  Batch [680/1299] D_loss: -0.1020, G_loss: 0.0439\n",
      "  Batch [690/1299] D_loss: -0.8298, G_loss: -0.7281\n",
      "  Batch [700/1299] D_loss: -0.6617, G_loss: -1.5677\n",
      "  Batch [710/1299] D_loss: -0.1035, G_loss: 0.3748\n",
      "  Batch [720/1299] D_loss: -0.1148, G_loss: 0.4936\n",
      "  Batch [730/1299] D_loss: -0.1196, G_loss: 0.6324\n",
      "  Batch [740/1299] D_loss: -0.0261, G_loss: 0.7328\n",
      "  Batch [750/1299] D_loss: -0.2078, G_loss: 0.6819\n",
      "  Batch [760/1299] D_loss: 0.0113, G_loss: 0.3837\n",
      "  Batch [770/1299] D_loss: -0.1188, G_loss: 0.1525\n",
      "  Batch [780/1299] D_loss: -0.0289, G_loss: 0.1858\n",
      "  Batch [790/1299] D_loss: -0.0721, G_loss: 0.3017\n",
      "  Batch [800/1299] D_loss: -0.0704, G_loss: 0.5156\n",
      "  Batch [810/1299] D_loss: -0.0913, G_loss: 0.3132\n",
      "  Batch [820/1299] D_loss: -0.0857, G_loss: 0.3260\n",
      "  Batch [830/1299] D_loss: -0.1166, G_loss: 0.3970\n",
      "  Batch [840/1299] D_loss: -1.4804, G_loss: -1.8412\n",
      "  Batch [850/1299] D_loss: -0.0454, G_loss: 0.0132\n",
      "  Batch [860/1299] D_loss: -0.1123, G_loss: 0.0100\n",
      "  Batch [870/1299] D_loss: -1.0904, G_loss: -0.7531\n",
      "  Batch [880/1299] D_loss: -0.0257, G_loss: 0.1992\n",
      "  Batch [890/1299] D_loss: -0.0404, G_loss: 0.5131\n",
      "  Batch [900/1299] D_loss: -0.2075, G_loss: 0.6023\n",
      "  Batch [910/1299] D_loss: -0.2561, G_loss: 0.9128\n",
      "  Batch [920/1299] D_loss: 0.0023, G_loss: 0.7160\n",
      "  Batch [930/1299] D_loss: 0.0041, G_loss: 0.3716\n",
      "  Batch [940/1299] D_loss: -1.2157, G_loss: -3.6875\n",
      "  Batch [950/1299] D_loss: -0.7409, G_loss: -0.8916\n",
      "  Batch [960/1299] D_loss: -0.3199, G_loss: -0.3007\n",
      "  Batch [970/1299] D_loss: -0.0262, G_loss: 0.1366\n",
      "  Batch [980/1299] D_loss: 0.0348, G_loss: 0.3071\n",
      "  Batch [990/1299] D_loss: -0.1532, G_loss: 0.3700\n",
      "  Batch [1000/1299] D_loss: -0.0445, G_loss: 0.5346\n",
      "  Batch [1010/1299] D_loss: -0.0713, G_loss: 0.4009\n",
      "  Batch [1020/1299] D_loss: -0.0908, G_loss: 0.4404\n",
      "  Batch [1030/1299] D_loss: -0.1838, G_loss: 0.3012\n",
      "  Batch [1040/1299] D_loss: -4.2852, G_loss: -10.0210\n",
      "  Batch [1050/1299] D_loss: -0.0693, G_loss: -0.0646\n",
      "  Batch [1060/1299] D_loss: -1.0226, G_loss: -0.1667\n",
      "  Batch [1070/1299] D_loss: -0.5994, G_loss: -1.8572\n",
      "  Batch [1080/1299] D_loss: -0.1995, G_loss: 0.4032\n",
      "  Batch [1090/1299] D_loss: -0.1405, G_loss: 0.5028\n",
      "  Batch [1100/1299] D_loss: -0.0434, G_loss: 0.3699\n",
      "  Batch [1110/1299] D_loss: -0.0435, G_loss: 0.4223\n",
      "  Batch [1120/1299] D_loss: -0.1062, G_loss: 0.4104\n",
      "  Batch [1130/1299] D_loss: -2.3954, G_loss: -3.8007\n",
      "  Batch [1140/1299] D_loss: -0.3764, G_loss: -0.2662\n",
      "  Batch [1150/1299] D_loss: -0.0243, G_loss: 0.3117\n",
      "  Batch [1160/1299] D_loss: -0.1045, G_loss: 0.3586\n",
      "  Batch [1170/1299] D_loss: -0.1743, G_loss: 0.5789\n",
      "  Batch [1180/1299] D_loss: -0.0659, G_loss: 0.5367\n",
      "  Batch [1190/1299] D_loss: 0.0370, G_loss: 0.3004\n",
      "  Batch [1200/1299] D_loss: -0.0900, G_loss: 0.1641\n",
      "  Batch [1210/1299] D_loss: -0.0729, G_loss: 0.1703\n",
      "  Batch [1220/1299] D_loss: -0.1223, G_loss: 0.2826\n",
      "  Batch [1230/1299] D_loss: -0.0522, G_loss: 0.2736\n",
      "  Batch [1240/1299] D_loss: -0.1427, G_loss: 0.3332\n",
      "  Batch [1250/1299] D_loss: -0.0497, G_loss: 0.1486\n",
      "  Batch [1260/1299] D_loss: -0.0227, G_loss: 0.1412\n",
      "  Batch [1270/1299] D_loss: 0.0175, G_loss: 0.1495\n",
      "  Batch [1280/1299] D_loss: -0.0366, G_loss: 0.2911\n",
      "  Batch [1290/1299] D_loss: -0.0624, G_loss: 0.3618\n",
      "\n",
      "Epoch 86 Summary:\n",
      "  Average D_loss: -0.1530\n",
      "  Average G_loss: -0.0752\n",
      "\n",
      "Epoch [87/100]\n",
      "  Batch [0/1299] D_loss: -0.0043, G_loss: 0.3443\n",
      "  Batch [10/1299] D_loss: -2.1687, G_loss: -0.4899\n",
      "  Batch [20/1299] D_loss: -0.0081, G_loss: 0.1485\n",
      "  Batch [30/1299] D_loss: -0.0010, G_loss: 0.1326\n",
      "  Batch [40/1299] D_loss: -0.0135, G_loss: 0.1925\n",
      "  Batch [50/1299] D_loss: -0.0540, G_loss: 0.1680\n",
      "  Batch [60/1299] D_loss: -0.0267, G_loss: 0.0784\n",
      "  Batch [70/1299] D_loss: -0.0317, G_loss: 0.2214\n",
      "  Batch [80/1299] D_loss: -0.0863, G_loss: 0.4134\n",
      "  Batch [90/1299] D_loss: -0.0961, G_loss: 0.3516\n",
      "  Batch [100/1299] D_loss: -0.1603, G_loss: 0.4823\n",
      "  Batch [110/1299] D_loss: -0.1040, G_loss: 0.2977\n",
      "  Batch [120/1299] D_loss: -0.1113, G_loss: 0.2636\n",
      "  Batch [130/1299] D_loss: -0.0390, G_loss: 0.0769\n",
      "  Batch [140/1299] D_loss: 0.0047, G_loss: 0.0862\n",
      "  Batch [150/1299] D_loss: -0.6596, G_loss: -0.1618\n",
      "  Batch [160/1299] D_loss: -0.0792, G_loss: 0.2058\n",
      "  Batch [170/1299] D_loss: -0.1981, G_loss: 0.4654\n",
      "  Batch [180/1299] D_loss: -0.1405, G_loss: 0.6186\n",
      "  Batch [190/1299] D_loss: -0.0444, G_loss: 0.4739\n",
      "  Batch [200/1299] D_loss: -0.0341, G_loss: 0.3057\n",
      "  Batch [210/1299] D_loss: -1.6042, G_loss: -0.8168\n",
      "  Batch [220/1299] D_loss: -0.0496, G_loss: 0.0452\n",
      "  Batch [230/1299] D_loss: -0.0074, G_loss: 0.1964\n",
      "  Batch [240/1299] D_loss: -0.0847, G_loss: 0.2564\n",
      "  Batch [250/1299] D_loss: -0.2518, G_loss: 0.5019\n",
      "  Batch [260/1299] D_loss: -0.0030, G_loss: 0.1890\n",
      "  Batch [270/1299] D_loss: -0.0003, G_loss: -0.1191\n",
      "  Batch [280/1299] D_loss: -0.0375, G_loss: 0.2370\n",
      "  Batch [290/1299] D_loss: -0.0966, G_loss: 0.4484\n",
      "  Batch [300/1299] D_loss: -0.0956, G_loss: 0.4988\n",
      "  Batch [310/1299] D_loss: -0.1737, G_loss: 0.5574\n",
      "  Batch [320/1299] D_loss: -0.1295, G_loss: 0.4957\n",
      "  Batch [330/1299] D_loss: -2.4725, G_loss: -2.9656\n",
      "  Batch [340/1299] D_loss: -0.0259, G_loss: 0.2286\n",
      "  Batch [350/1299] D_loss: -1.8783, G_loss: -1.6001\n",
      "  Batch [360/1299] D_loss: -0.0497, G_loss: 0.0011\n",
      "  Batch [370/1299] D_loss: -0.0317, G_loss: 0.2334\n",
      "  Batch [380/1299] D_loss: -0.0581, G_loss: 0.4956\n",
      "  Batch [390/1299] D_loss: 0.0773, G_loss: 0.2883\n",
      "  Batch [400/1299] D_loss: 0.0055, G_loss: 0.3645\n",
      "  Batch [410/1299] D_loss: 0.0628, G_loss: 0.3334\n",
      "  Batch [420/1299] D_loss: -0.6439, G_loss: -1.5312\n",
      "  Batch [430/1299] D_loss: -0.3771, G_loss: 0.1017\n",
      "  Batch [440/1299] D_loss: -0.2730, G_loss: 0.0639\n",
      "  Batch [450/1299] D_loss: -0.0164, G_loss: 0.1610\n",
      "  Batch [460/1299] D_loss: -0.0126, G_loss: 0.2736\n",
      "  Batch [470/1299] D_loss: 0.0216, G_loss: 0.4893\n",
      "  Batch [480/1299] D_loss: -0.1136, G_loss: 0.6709\n",
      "  Batch [490/1299] D_loss: -0.1999, G_loss: 0.6219\n",
      "  Batch [500/1299] D_loss: 0.0086, G_loss: 0.4128\n",
      "  Batch [510/1299] D_loss: -0.0888, G_loss: 0.1586\n",
      "  Batch [520/1299] D_loss: -0.0114, G_loss: 0.0617\n",
      "  Batch [530/1299] D_loss: -0.0536, G_loss: 0.1749\n",
      "  Batch [540/1299] D_loss: -0.0619, G_loss: 0.2304\n",
      "  Batch [550/1299] D_loss: -0.0753, G_loss: 0.3553\n",
      "  Batch [560/1299] D_loss: -1.4966, G_loss: -0.3915\n",
      "  Batch [570/1299] D_loss: -0.3139, G_loss: -0.1822\n",
      "  Batch [580/1299] D_loss: -0.6510, G_loss: -0.1810\n",
      "  Batch [590/1299] D_loss: -0.0547, G_loss: 0.0920\n",
      "  Batch [600/1299] D_loss: -0.7292, G_loss: 0.0813\n",
      "  Batch [610/1299] D_loss: -0.0893, G_loss: 0.1933\n",
      "  Batch [620/1299] D_loss: -0.1610, G_loss: 0.4185\n",
      "  Batch [630/1299] D_loss: -0.1412, G_loss: 0.5230\n",
      "  Batch [640/1299] D_loss: 0.0740, G_loss: 0.5217\n",
      "  Batch [650/1299] D_loss: -0.0221, G_loss: 0.5866\n",
      "  Batch [660/1299] D_loss: 0.0660, G_loss: 0.3449\n",
      "  Batch [670/1299] D_loss: -0.0640, G_loss: 0.2198\n",
      "  Batch [680/1299] D_loss: -0.3043, G_loss: -0.6857\n",
      "  Batch [690/1299] D_loss: -0.0319, G_loss: 0.1525\n",
      "  Batch [700/1299] D_loss: 0.0020, G_loss: 0.2179\n",
      "  Batch [710/1299] D_loss: -0.9675, G_loss: -2.9685\n",
      "  Batch [720/1299] D_loss: 0.0060, G_loss: 0.1987\n",
      "  Batch [730/1299] D_loss: -0.0641, G_loss: 0.3048\n",
      "  Batch [740/1299] D_loss: -0.1799, G_loss: 0.3969\n",
      "  Batch [750/1299] D_loss: -0.0846, G_loss: 0.6304\n",
      "  Batch [760/1299] D_loss: -0.1574, G_loss: 0.4200\n",
      "  Batch [770/1299] D_loss: 0.0267, G_loss: 0.3993\n",
      "  Batch [780/1299] D_loss: -0.1302, G_loss: 0.3571\n",
      "  Batch [790/1299] D_loss: -0.4063, G_loss: -0.1854\n",
      "  Batch [800/1299] D_loss: -0.4365, G_loss: -1.3732\n",
      "  Batch [810/1299] D_loss: -0.2349, G_loss: -0.1925\n",
      "  Batch [820/1299] D_loss: -0.8548, G_loss: -1.0287\n",
      "  Batch [830/1299] D_loss: -0.7152, G_loss: -0.7279\n",
      "  Batch [840/1299] D_loss: -0.0647, G_loss: 0.2784\n",
      "  Batch [850/1299] D_loss: -0.1775, G_loss: 0.3211\n",
      "  Batch [860/1299] D_loss: -0.1037, G_loss: 0.4485\n",
      "  Batch [870/1299] D_loss: -0.1443, G_loss: 0.6711\n",
      "  Batch [880/1299] D_loss: -0.0975, G_loss: 0.5296\n",
      "  Batch [890/1299] D_loss: -2.3301, G_loss: -2.1584\n",
      "  Batch [900/1299] D_loss: -0.0626, G_loss: -0.0408\n",
      "  Batch [910/1299] D_loss: -0.1146, G_loss: 0.0734\n",
      "  Batch [920/1299] D_loss: -0.0733, G_loss: 0.1941\n",
      "  Batch [930/1299] D_loss: -0.1077, G_loss: 0.2955\n",
      "  Batch [940/1299] D_loss: -0.0144, G_loss: 0.5240\n",
      "  Batch [950/1299] D_loss: -0.0365, G_loss: 0.5473\n",
      "  Batch [960/1299] D_loss: -0.2221, G_loss: 0.5407\n",
      "  Batch [970/1299] D_loss: -0.0857, G_loss: 0.4235\n",
      "  Batch [980/1299] D_loss: -0.2406, G_loss: -0.2066\n",
      "  Batch [990/1299] D_loss: -0.2582, G_loss: -0.0776\n",
      "  Batch [1000/1299] D_loss: -0.8039, G_loss: -0.6261\n",
      "  Batch [1010/1299] D_loss: -0.3934, G_loss: 0.0795\n",
      "  Batch [1020/1299] D_loss: -0.7607, G_loss: -0.3588\n",
      "  Batch [1030/1299] D_loss: -0.5364, G_loss: -0.6887\n",
      "  Batch [1040/1299] D_loss: -0.0126, G_loss: 0.3155\n",
      "  Batch [1050/1299] D_loss: 0.0649, G_loss: 0.3839\n",
      "  Batch [1060/1299] D_loss: -0.0318, G_loss: 0.5210\n",
      "  Batch [1070/1299] D_loss: 0.0318, G_loss: 0.5052\n",
      "  Batch [1080/1299] D_loss: -0.0445, G_loss: 0.4225\n",
      "  Batch [1090/1299] D_loss: -0.0899, G_loss: 0.3319\n",
      "  Batch [1100/1299] D_loss: -1.6284, G_loss: -0.7600\n",
      "  Batch [1110/1299] D_loss: -0.0202, G_loss: 0.1767\n",
      "  Batch [1120/1299] D_loss: -0.0212, G_loss: 0.2048\n",
      "  Batch [1130/1299] D_loss: -0.0531, G_loss: 0.2669\n",
      "  Batch [1140/1299] D_loss: -0.0933, G_loss: 0.3609\n",
      "  Batch [1150/1299] D_loss: -3.2976, G_loss: -4.7360\n",
      "  Batch [1160/1299] D_loss: -0.0440, G_loss: 0.1317\n",
      "  Batch [1170/1299] D_loss: -0.1195, G_loss: 0.4287\n",
      "  Batch [1180/1299] D_loss: -0.1442, G_loss: 0.5006\n",
      "  Batch [1190/1299] D_loss: 0.0026, G_loss: 0.3498\n",
      "  Batch [1200/1299] D_loss: 0.0785, G_loss: 0.3430\n",
      "  Batch [1210/1299] D_loss: -0.2079, G_loss: -1.1163\n",
      "  Batch [1220/1299] D_loss: -0.0250, G_loss: 0.1260\n",
      "  Batch [1230/1299] D_loss: -0.0023, G_loss: 0.1974\n",
      "  Batch [1240/1299] D_loss: -0.0436, G_loss: 0.3264\n",
      "  Batch [1250/1299] D_loss: -0.0216, G_loss: 0.2829\n",
      "  Batch [1260/1299] D_loss: -0.8625, G_loss: -3.7255\n",
      "  Batch [1270/1299] D_loss: -0.0227, G_loss: 0.1976\n",
      "  Batch [1280/1299] D_loss: -0.0586, G_loss: 0.2655\n",
      "  Batch [1290/1299] D_loss: -0.0404, G_loss: 0.2677\n",
      "\n",
      "Epoch 87 Summary:\n",
      "  Average D_loss: -0.1224\n",
      "  Average G_loss: -0.0528\n",
      "\n",
      "Epoch [88/100]\n",
      "  Batch [0/1299] D_loss: 0.0079, G_loss: 0.2687\n",
      "  Batch [10/1299] D_loss: 0.0110, G_loss: 0.3138\n",
      "  Batch [20/1299] D_loss: -1.1240, G_loss: -1.4081\n",
      "  Batch [30/1299] D_loss: -1.4022, G_loss: -2.7222\n",
      "  Batch [40/1299] D_loss: -0.0723, G_loss: -0.0109\n",
      "  Batch [50/1299] D_loss: -0.3106, G_loss: -0.0334\n",
      "  Batch [60/1299] D_loss: -0.1376, G_loss: 0.2681\n",
      "  Batch [70/1299] D_loss: -0.0212, G_loss: 0.3636\n",
      "  Batch [80/1299] D_loss: -0.0980, G_loss: 0.4702\n",
      "  Batch [90/1299] D_loss: -0.0793, G_loss: 0.6598\n",
      "  Batch [100/1299] D_loss: 0.0677, G_loss: 0.6268\n",
      "  Batch [110/1299] D_loss: -0.1175, G_loss: 0.5262\n",
      "  Batch [120/1299] D_loss: 0.0016, G_loss: 0.2280\n",
      "  Batch [130/1299] D_loss: -0.8144, G_loss: -1.6563\n",
      "  Batch [140/1299] D_loss: -0.0693, G_loss: 0.2349\n",
      "  Batch [150/1299] D_loss: 0.0311, G_loss: 0.3304\n",
      "  Batch [160/1299] D_loss: -0.1319, G_loss: 0.5449\n",
      "  Batch [170/1299] D_loss: -0.0942, G_loss: 0.4125\n",
      "  Batch [180/1299] D_loss: -0.0599, G_loss: 0.2225\n",
      "  Batch [190/1299] D_loss: -0.0389, G_loss: 0.1615\n",
      "  Batch [200/1299] D_loss: -0.4698, G_loss: -0.1107\n",
      "  Batch [210/1299] D_loss: -0.0117, G_loss: 0.1403\n",
      "  Batch [220/1299] D_loss: -0.0959, G_loss: 0.2905\n",
      "  Batch [230/1299] D_loss: -0.0117, G_loss: 0.3329\n",
      "  Batch [240/1299] D_loss: -0.0306, G_loss: 0.4358\n",
      "  Batch [250/1299] D_loss: -0.1435, G_loss: 0.4729\n",
      "  Batch [260/1299] D_loss: 0.0104, G_loss: 0.0935\n",
      "  Batch [270/1299] D_loss: -0.0492, G_loss: 0.0496\n",
      "  Batch [280/1299] D_loss: -0.0276, G_loss: 0.1951\n",
      "  Batch [290/1299] D_loss: -0.0547, G_loss: 0.2221\n",
      "  Batch [300/1299] D_loss: -0.1855, G_loss: 0.2171\n",
      "  Batch [310/1299] D_loss: -0.0122, G_loss: 0.3327\n",
      "  Batch [320/1299] D_loss: 0.0792, G_loss: 0.1444\n",
      "  Batch [330/1299] D_loss: -0.7070, G_loss: -0.4249\n",
      "  Batch [340/1299] D_loss: -1.4243, G_loss: -2.7319\n",
      "  Batch [350/1299] D_loss: -0.5850, G_loss: 0.0278\n",
      "  Batch [360/1299] D_loss: 0.0034, G_loss: 0.2220\n",
      "  Batch [370/1299] D_loss: -0.0025, G_loss: 0.4557\n",
      "  Batch [380/1299] D_loss: -0.0088, G_loss: 0.3674\n",
      "  Batch [390/1299] D_loss: 0.0254, G_loss: 0.2829\n",
      "  Batch [400/1299] D_loss: -0.0480, G_loss: 0.2402\n",
      "  Batch [410/1299] D_loss: 0.0003, G_loss: 0.2668\n",
      "  Batch [420/1299] D_loss: 0.3087, G_loss: -0.8093\n",
      "  Batch [430/1299] D_loss: -0.0036, G_loss: 0.0625\n",
      "  Batch [440/1299] D_loss: 0.0080, G_loss: 0.0931\n",
      "  Batch [450/1299] D_loss: -0.0069, G_loss: 0.1083\n",
      "  Batch [460/1299] D_loss: 0.0036, G_loss: 0.1671\n",
      "  Batch [470/1299] D_loss: -0.0684, G_loss: 0.1783\n",
      "  Batch [480/1299] D_loss: -0.1612, G_loss: 0.4487\n",
      "  Batch [490/1299] D_loss: -0.1486, G_loss: -0.1004\n",
      "  Batch [500/1299] D_loss: -0.4608, G_loss: -0.3072\n",
      "  Batch [510/1299] D_loss: -0.6048, G_loss: -0.8323\n",
      "  Batch [520/1299] D_loss: -0.1655, G_loss: 0.0409\n",
      "  Batch [530/1299] D_loss: -0.6788, G_loss: -0.3805\n",
      "  Batch [540/1299] D_loss: -0.3696, G_loss: -0.5705\n",
      "  Batch [550/1299] D_loss: -0.1476, G_loss: 0.3488\n",
      "  Batch [560/1299] D_loss: -0.0348, G_loss: 0.4457\n",
      "  Batch [570/1299] D_loss: -0.0999, G_loss: 0.5479\n",
      "  Batch [580/1299] D_loss: -0.0879, G_loss: 0.4738\n",
      "  Batch [590/1299] D_loss: -0.0540, G_loss: 0.5732\n",
      "  Batch [600/1299] D_loss: -1.0426, G_loss: -0.6509\n",
      "  Batch [610/1299] D_loss: -1.2067, G_loss: -0.3590\n",
      "  Batch [620/1299] D_loss: -1.4120, G_loss: -3.7548\n",
      "  Batch [630/1299] D_loss: -0.0695, G_loss: 0.1978\n",
      "  Batch [640/1299] D_loss: -0.0494, G_loss: 0.4070\n",
      "  Batch [650/1299] D_loss: 0.0197, G_loss: 0.4083\n",
      "  Batch [660/1299] D_loss: -0.0910, G_loss: 0.6423\n",
      "  Batch [670/1299] D_loss: -0.1799, G_loss: 0.6858\n",
      "  Batch [680/1299] D_loss: 0.0844, G_loss: 0.4947\n",
      "  Batch [690/1299] D_loss: -0.0699, G_loss: 0.4106\n",
      "  Batch [700/1299] D_loss: -1.9533, G_loss: -6.0231\n",
      "  Batch [710/1299] D_loss: -0.0207, G_loss: 0.0948\n",
      "  Batch [720/1299] D_loss: 0.0025, G_loss: 0.2412\n",
      "  Batch [730/1299] D_loss: -0.1313, G_loss: 0.3881\n",
      "  Batch [740/1299] D_loss: 0.0522, G_loss: 0.4210\n",
      "  Batch [750/1299] D_loss: -0.2489, G_loss: 0.6469\n",
      "  Batch [760/1299] D_loss: 0.0429, G_loss: 0.3268\n",
      "  Batch [770/1299] D_loss: -0.0612, G_loss: 0.3816\n",
      "  Batch [780/1299] D_loss: -1.1587, G_loss: -3.0577\n",
      "  Batch [790/1299] D_loss: -0.3453, G_loss: -0.1276\n",
      "  Batch [800/1299] D_loss: -0.6089, G_loss: 0.0712\n",
      "  Batch [810/1299] D_loss: -0.5722, G_loss: -0.0063\n",
      "  Batch [820/1299] D_loss: -0.1014, G_loss: 0.2703\n",
      "  Batch [830/1299] D_loss: -0.1200, G_loss: 0.5881\n",
      "  Batch [840/1299] D_loss: 0.0211, G_loss: 0.3670\n",
      "  Batch [850/1299] D_loss: -0.2470, G_loss: 0.1239\n",
      "  Batch [860/1299] D_loss: -0.0425, G_loss: 0.3092\n",
      "  Batch [870/1299] D_loss: -0.0769, G_loss: 0.2302\n",
      "  Batch [880/1299] D_loss: -0.0978, G_loss: 0.0489\n",
      "  Batch [890/1299] D_loss: -1.0665, G_loss: -1.1038\n",
      "  Batch [900/1299] D_loss: -0.0149, G_loss: 0.1231\n",
      "  Batch [910/1299] D_loss: -0.0237, G_loss: 0.1504\n",
      "  Batch [920/1299] D_loss: -0.1247, G_loss: 0.3033\n",
      "  Batch [930/1299] D_loss: -0.0941, G_loss: 0.2628\n",
      "  Batch [940/1299] D_loss: -0.0573, G_loss: 0.1587\n",
      "  Batch [950/1299] D_loss: -1.1770, G_loss: -1.5692\n",
      "  Batch [960/1299] D_loss: -0.0347, G_loss: 0.1337\n",
      "  Batch [970/1299] D_loss: -0.0577, G_loss: 0.2160\n",
      "  Batch [980/1299] D_loss: -0.0332, G_loss: 0.4285\n",
      "  Batch [990/1299] D_loss: -0.1145, G_loss: 0.5176\n",
      "  Batch [1000/1299] D_loss: 0.0505, G_loss: 0.5548\n",
      "  Batch [1010/1299] D_loss: 0.0129, G_loss: 0.2409\n",
      "  Batch [1020/1299] D_loss: -0.1384, G_loss: 0.0633\n",
      "  Batch [1030/1299] D_loss: -0.3119, G_loss: -0.6292\n",
      "  Batch [1040/1299] D_loss: -0.3029, G_loss: -0.2560\n",
      "  Batch [1050/1299] D_loss: -0.7354, G_loss: -0.4061\n",
      "  Batch [1060/1299] D_loss: -0.1432, G_loss: 0.0072\n",
      "  Batch [1070/1299] D_loss: -0.2839, G_loss: -0.1058\n",
      "  Batch [1080/1299] D_loss: -0.0482, G_loss: 0.1300\n",
      "  Batch [1090/1299] D_loss: -0.0751, G_loss: 0.4261\n",
      "  Batch [1100/1299] D_loss: -0.1461, G_loss: 0.6436\n",
      "  Batch [1110/1299] D_loss: 0.0347, G_loss: 0.2783\n",
      "  Batch [1120/1299] D_loss: -0.1090, G_loss: 0.4062\n",
      "  Batch [1130/1299] D_loss: -0.2145, G_loss: -0.0834\n",
      "  Batch [1140/1299] D_loss: -0.0023, G_loss: 0.1470\n",
      "  Batch [1150/1299] D_loss: 0.0192, G_loss: 0.3076\n",
      "  Batch [1160/1299] D_loss: -0.1203, G_loss: 0.4403\n",
      "  Batch [1170/1299] D_loss: 0.0029, G_loss: 0.4269\n",
      "  Batch [1180/1299] D_loss: -0.1508, G_loss: 0.4592\n",
      "  Batch [1190/1299] D_loss: -0.1021, G_loss: 0.1829\n",
      "  Batch [1200/1299] D_loss: -0.0169, G_loss: 0.0507\n",
      "  Batch [1210/1299] D_loss: -0.1775, G_loss: -0.1070\n",
      "  Batch [1220/1299] D_loss: -0.2983, G_loss: -0.0851\n",
      "  Batch [1230/1299] D_loss: -0.1707, G_loss: -0.0296\n",
      "  Batch [1240/1299] D_loss: -0.0663, G_loss: 0.3304\n",
      "  Batch [1250/1299] D_loss: -0.0594, G_loss: 0.5123\n",
      "  Batch [1260/1299] D_loss: -0.1432, G_loss: 0.4872\n",
      "  Batch [1270/1299] D_loss: -0.0668, G_loss: 0.4389\n",
      "  Batch [1280/1299] D_loss: -0.0351, G_loss: 0.3874\n",
      "  Batch [1290/1299] D_loss: -3.8309, G_loss: -3.2134\n",
      "\n",
      "Epoch 88 Summary:\n",
      "  Average D_loss: -0.1267\n",
      "  Average G_loss: -0.0806\n",
      "\n",
      "Epoch [89/100]\n",
      "  Batch [0/1299] D_loss: -0.4241, G_loss: -0.1707\n",
      "  Batch [10/1299] D_loss: -0.0963, G_loss: 0.2949\n",
      "  Batch [20/1299] D_loss: 0.0206, G_loss: 0.5058\n",
      "  Batch [30/1299] D_loss: -0.2179, G_loss: 0.4450\n",
      "  Batch [40/1299] D_loss: -0.0078, G_loss: 0.6423\n",
      "  Batch [50/1299] D_loss: -0.0999, G_loss: 0.3483\n",
      "  Batch [60/1299] D_loss: -0.1529, G_loss: -0.1711\n",
      "  Batch [70/1299] D_loss: -0.0083, G_loss: 0.1677\n",
      "  Batch [80/1299] D_loss: -0.0197, G_loss: 0.1444\n",
      "  Batch [90/1299] D_loss: -1.2379, G_loss: -1.6852\n",
      "  Batch [100/1299] D_loss: -0.0417, G_loss: 0.2078\n",
      "  Batch [110/1299] D_loss: -0.0834, G_loss: 0.2990\n",
      "  Batch [120/1299] D_loss: -0.1226, G_loss: 0.2319\n",
      "  Batch [130/1299] D_loss: -0.0803, G_loss: 0.1715\n",
      "  Batch [140/1299] D_loss: -0.5863, G_loss: 0.0001\n",
      "  Batch [150/1299] D_loss: 0.0209, G_loss: 0.2084\n",
      "  Batch [160/1299] D_loss: -0.0142, G_loss: 0.1577\n",
      "  Batch [170/1299] D_loss: -0.0192, G_loss: 0.1298\n",
      "  Batch [180/1299] D_loss: -0.9287, G_loss: -0.7669\n",
      "  Batch [190/1299] D_loss: -0.1536, G_loss: 0.0121\n",
      "  Batch [200/1299] D_loss: -0.0032, G_loss: 0.3131\n",
      "  Batch [210/1299] D_loss: 0.0147, G_loss: 0.4019\n",
      "  Batch [220/1299] D_loss: -0.0460, G_loss: 0.3065\n",
      "  Batch [230/1299] D_loss: -0.0844, G_loss: 0.1915\n",
      "  Batch [240/1299] D_loss: -0.7218, G_loss: -1.0449\n",
      "  Batch [250/1299] D_loss: -0.1195, G_loss: 0.0403\n",
      "  Batch [260/1299] D_loss: -0.3576, G_loss: -0.4728\n",
      "  Batch [270/1299] D_loss: -0.5087, G_loss: 0.0471\n",
      "  Batch [280/1299] D_loss: -0.4613, G_loss: -0.0499\n",
      "  Batch [290/1299] D_loss: -0.2190, G_loss: -0.0066\n",
      "  Batch [300/1299] D_loss: 0.0636, G_loss: 0.1399\n",
      "  Batch [310/1299] D_loss: -0.0981, G_loss: 0.3249\n",
      "  Batch [320/1299] D_loss: 0.0192, G_loss: 0.4757\n",
      "  Batch [330/1299] D_loss: -0.0504, G_loss: 0.4285\n",
      "  Batch [340/1299] D_loss: -0.1864, G_loss: 0.4843\n",
      "  Batch [350/1299] D_loss: -0.4339, G_loss: -0.9805\n",
      "  Batch [360/1299] D_loss: -1.0113, G_loss: -1.8728\n",
      "  Batch [370/1299] D_loss: -0.2298, G_loss: 0.0472\n",
      "  Batch [380/1299] D_loss: -0.0426, G_loss: 0.1912\n",
      "  Batch [390/1299] D_loss: -0.2179, G_loss: 0.2384\n",
      "  Batch [400/1299] D_loss: -0.0639, G_loss: 0.5193\n",
      "  Batch [410/1299] D_loss: -0.0232, G_loss: 0.5429\n",
      "  Batch [420/1299] D_loss: -0.0697, G_loss: 0.3577\n",
      "  Batch [430/1299] D_loss: -0.1681, G_loss: 0.1930\n",
      "  Batch [440/1299] D_loss: -0.1441, G_loss: -0.6046\n",
      "  Batch [450/1299] D_loss: -0.0356, G_loss: 0.0806\n",
      "  Batch [460/1299] D_loss: -0.0972, G_loss: 0.1979\n",
      "  Batch [470/1299] D_loss: -0.0481, G_loss: 0.2695\n",
      "  Batch [480/1299] D_loss: -0.1414, G_loss: 0.2039\n",
      "  Batch [490/1299] D_loss: -0.0044, G_loss: 0.2535\n",
      "  Batch [500/1299] D_loss: -3.3135, G_loss: -10.5767\n",
      "  Batch [510/1299] D_loss: -0.0529, G_loss: 0.0858\n",
      "  Batch [520/1299] D_loss: -0.1278, G_loss: 0.1549\n",
      "  Batch [530/1299] D_loss: -0.0844, G_loss: 0.2059\n",
      "  Batch [540/1299] D_loss: -0.0315, G_loss: 0.4113\n",
      "  Batch [550/1299] D_loss: -0.1724, G_loss: 0.3370\n",
      "  Batch [560/1299] D_loss: 0.0820, G_loss: 0.1548\n",
      "  Batch [570/1299] D_loss: -1.0055, G_loss: -1.9743\n",
      "  Batch [580/1299] D_loss: -0.8894, G_loss: -2.5645\n",
      "  Batch [590/1299] D_loss: -0.9314, G_loss: -0.4467\n",
      "  Batch [600/1299] D_loss: -0.5845, G_loss: -0.0283\n",
      "  Batch [610/1299] D_loss: -0.0595, G_loss: 0.0247\n",
      "  Batch [620/1299] D_loss: -0.7091, G_loss: -0.2217\n",
      "  Batch [630/1299] D_loss: -0.0574, G_loss: 0.0350\n",
      "  Batch [640/1299] D_loss: -0.3216, G_loss: -0.3472\n",
      "  Batch [650/1299] D_loss: -0.2166, G_loss: 0.0928\n",
      "  Batch [660/1299] D_loss: -0.2344, G_loss: 0.5298\n",
      "  Batch [670/1299] D_loss: -0.0966, G_loss: 0.7268\n",
      "  Batch [680/1299] D_loss: -0.0720, G_loss: 0.4597\n",
      "  Batch [690/1299] D_loss: -0.1552, G_loss: 0.6245\n",
      "  Batch [700/1299] D_loss: -0.0110, G_loss: 0.3051\n",
      "  Batch [710/1299] D_loss: -1.1034, G_loss: -1.0767\n",
      "  Batch [720/1299] D_loss: -0.0077, G_loss: 0.0293\n",
      "  Batch [730/1299] D_loss: -0.4688, G_loss: -0.4821\n",
      "  Batch [740/1299] D_loss: -1.0554, G_loss: -2.2224\n",
      "  Batch [750/1299] D_loss: -0.6371, G_loss: -1.6199\n",
      "  Batch [760/1299] D_loss: -0.0116, G_loss: 0.2916\n",
      "  Batch [770/1299] D_loss: -0.0692, G_loss: 0.3368\n",
      "  Batch [780/1299] D_loss: -0.0209, G_loss: 0.5863\n",
      "  Batch [790/1299] D_loss: -0.0306, G_loss: 0.4614\n",
      "  Batch [800/1299] D_loss: 0.0043, G_loss: 0.2922\n",
      "  Batch [810/1299] D_loss: -3.0577, G_loss: -6.5213\n",
      "  Batch [820/1299] D_loss: -0.2126, G_loss: 0.0757\n",
      "  Batch [830/1299] D_loss: 0.0178, G_loss: 0.0711\n",
      "  Batch [840/1299] D_loss: -0.9363, G_loss: -0.3804\n",
      "  Batch [850/1299] D_loss: -0.1508, G_loss: -0.1342\n",
      "  Batch [860/1299] D_loss: -0.6580, G_loss: -0.6253\n",
      "  Batch [870/1299] D_loss: -0.3364, G_loss: 0.0463\n",
      "  Batch [880/1299] D_loss: -0.1568, G_loss: 0.3441\n",
      "  Batch [890/1299] D_loss: -0.0780, G_loss: 0.3727\n",
      "  Batch [900/1299] D_loss: -0.1419, G_loss: 0.4516\n",
      "  Batch [910/1299] D_loss: -0.0739, G_loss: 0.3165\n",
      "  Batch [920/1299] D_loss: -0.0456, G_loss: 0.3047\n",
      "  Batch [930/1299] D_loss: -0.2893, G_loss: -0.0581\n",
      "  Batch [940/1299] D_loss: 0.0120, G_loss: 0.0753\n",
      "  Batch [950/1299] D_loss: 0.0131, G_loss: 0.2340\n",
      "  Batch [960/1299] D_loss: -0.0350, G_loss: 0.2459\n",
      "  Batch [970/1299] D_loss: -0.1349, G_loss: 0.3477\n",
      "  Batch [980/1299] D_loss: -0.1106, G_loss: 0.3016\n",
      "  Batch [990/1299] D_loss: -2.8475, G_loss: -3.9726\n",
      "  Batch [1000/1299] D_loss: -0.0404, G_loss: 0.1670\n",
      "  Batch [1010/1299] D_loss: -0.0611, G_loss: 0.1681\n",
      "  Batch [1020/1299] D_loss: -0.0188, G_loss: 0.4371\n",
      "  Batch [1030/1299] D_loss: -0.0888, G_loss: 0.4030\n",
      "  Batch [1040/1299] D_loss: 0.0741, G_loss: 0.4141\n",
      "  Batch [1050/1299] D_loss: -1.0917, G_loss: -8.5518\n",
      "  Batch [1060/1299] D_loss: -0.0454, G_loss: 0.1656\n",
      "  Batch [1070/1299] D_loss: -0.0401, G_loss: 0.2973\n",
      "  Batch [1080/1299] D_loss: -0.1396, G_loss: 0.2776\n",
      "  Batch [1090/1299] D_loss: 0.0206, G_loss: 0.3974\n",
      "  Batch [1100/1299] D_loss: -0.1341, G_loss: 0.4425\n",
      "  Batch [1110/1299] D_loss: -2.1357, G_loss: -2.1430\n",
      "  Batch [1120/1299] D_loss: -0.0077, G_loss: 0.1325\n",
      "  Batch [1130/1299] D_loss: -0.0381, G_loss: 0.2166\n",
      "  Batch [1140/1299] D_loss: -0.0787, G_loss: 0.2724\n",
      "  Batch [1150/1299] D_loss: -0.0610, G_loss: 0.2246\n",
      "  Batch [1160/1299] D_loss: -1.2059, G_loss: -3.1999\n",
      "  Batch [1170/1299] D_loss: -0.0878, G_loss: -0.0010\n",
      "  Batch [1180/1299] D_loss: -0.0352, G_loss: 0.1207\n",
      "  Batch [1190/1299] D_loss: -0.3066, G_loss: 0.0778\n",
      "  Batch [1200/1299] D_loss: -0.0370, G_loss: 0.1481\n",
      "  Batch [1210/1299] D_loss: 0.0590, G_loss: 0.3376\n",
      "  Batch [1220/1299] D_loss: 0.0722, G_loss: 0.6715\n",
      "  Batch [1230/1299] D_loss: -0.0205, G_loss: 0.3710\n",
      "  Batch [1240/1299] D_loss: -0.1038, G_loss: 0.2996\n",
      "  Batch [1250/1299] D_loss: -0.9649, G_loss: -1.5715\n",
      "  Batch [1260/1299] D_loss: -0.9434, G_loss: -0.9266\n",
      "  Batch [1270/1299] D_loss: -0.2298, G_loss: 0.0342\n",
      "  Batch [1280/1299] D_loss: -0.8182, G_loss: -0.0843\n",
      "  Batch [1290/1299] D_loss: -0.0182, G_loss: 0.2329\n",
      "\n",
      "Epoch 89 Summary:\n",
      "  Average D_loss: -0.1289\n",
      "  Average G_loss: -0.0620\n",
      "\n",
      "Epoch [90/100]\n",
      "  Batch [0/1299] D_loss: -0.1046, G_loss: 0.4265\n",
      "  Batch [10/1299] D_loss: -0.0931, G_loss: 0.6604\n",
      "  Batch [20/1299] D_loss: -0.1208, G_loss: 0.6586\n",
      "  Batch [30/1299] D_loss: 0.0061, G_loss: 0.4912\n",
      "  Batch [40/1299] D_loss: -0.0021, G_loss: 0.2530\n",
      "  Batch [50/1299] D_loss: -1.3223, G_loss: -1.0547\n",
      "  Batch [60/1299] D_loss: 0.0468, G_loss: 0.1492\n",
      "  Batch [70/1299] D_loss: 0.0391, G_loss: 0.0491\n",
      "  Batch [80/1299] D_loss: -1.2443, G_loss: -1.6682\n",
      "  Batch [90/1299] D_loss: -0.4279, G_loss: -4.8278\n",
      "  Batch [100/1299] D_loss: -0.0196, G_loss: 0.2181\n",
      "  Batch [110/1299] D_loss: 0.0169, G_loss: 0.3804\n",
      "  Batch [120/1299] D_loss: -0.0400, G_loss: 0.4075\n",
      "  Batch [130/1299] D_loss: -0.2198, G_loss: 0.4269\n",
      "  Batch [140/1299] D_loss: -0.0491, G_loss: 0.3324\n",
      "  Batch [150/1299] D_loss: -0.0297, G_loss: 0.2606\n",
      "  Batch [160/1299] D_loss: 0.0343, G_loss: 0.1429\n",
      "  Batch [170/1299] D_loss: -0.0396, G_loss: 0.1341\n",
      "  Batch [180/1299] D_loss: -0.0047, G_loss: 0.1262\n",
      "  Batch [190/1299] D_loss: 0.0175, G_loss: 0.0814\n",
      "  Batch [200/1299] D_loss: -0.7864, G_loss: -1.6288\n",
      "  Batch [210/1299] D_loss: -0.0512, G_loss: 0.0469\n",
      "  Batch [220/1299] D_loss: 0.0006, G_loss: 0.1675\n",
      "  Batch [230/1299] D_loss: -0.0131, G_loss: 0.3809\n",
      "  Batch [240/1299] D_loss: -0.0446, G_loss: 0.4203\n",
      "  Batch [250/1299] D_loss: -0.1213, G_loss: 0.5030\n",
      "  Batch [260/1299] D_loss: -0.0474, G_loss: 0.2748\n",
      "  Batch [270/1299] D_loss: -0.1621, G_loss: 0.3502\n",
      "  Batch [280/1299] D_loss: -1.3569, G_loss: -2.2077\n",
      "  Batch [290/1299] D_loss: -0.3090, G_loss: -0.0800\n",
      "  Batch [300/1299] D_loss: -0.3630, G_loss: -0.2827\n",
      "  Batch [310/1299] D_loss: -0.1101, G_loss: 0.0178\n",
      "  Batch [320/1299] D_loss: -0.0469, G_loss: 0.3388\n",
      "  Batch [330/1299] D_loss: -0.0643, G_loss: 0.4546\n",
      "  Batch [340/1299] D_loss: -0.0860, G_loss: 0.5445\n",
      "  Batch [350/1299] D_loss: -0.0631, G_loss: 0.3482\n",
      "  Batch [360/1299] D_loss: -0.0426, G_loss: 0.2111\n",
      "  Batch [370/1299] D_loss: -0.5783, G_loss: -0.5659\n",
      "  Batch [380/1299] D_loss: -0.2764, G_loss: -0.3803\n",
      "  Batch [390/1299] D_loss: -0.5420, G_loss: -0.7476\n",
      "  Batch [400/1299] D_loss: -0.4004, G_loss: -0.0456\n",
      "  Batch [410/1299] D_loss: -0.8707, G_loss: -2.2646\n",
      "  Batch [420/1299] D_loss: -0.0517, G_loss: 0.2489\n",
      "  Batch [430/1299] D_loss: -0.2115, G_loss: 0.6839\n",
      "  Batch [440/1299] D_loss: -0.1546, G_loss: 0.7490\n",
      "  Batch [450/1299] D_loss: -0.0431, G_loss: 0.6700\n",
      "  Batch [460/1299] D_loss: -0.1248, G_loss: 0.5110\n",
      "  Batch [470/1299] D_loss: -0.0528, G_loss: 0.2678\n",
      "  Batch [480/1299] D_loss: -3.9049, G_loss: -6.7719\n",
      "  Batch [490/1299] D_loss: -0.4570, G_loss: -0.7305\n",
      "  Batch [500/1299] D_loss: -1.1962, G_loss: -1.3897\n",
      "  Batch [510/1299] D_loss: -0.4891, G_loss: -1.1965\n",
      "  Batch [520/1299] D_loss: -0.0225, G_loss: 0.1466\n",
      "  Batch [530/1299] D_loss: -0.0853, G_loss: 0.4159\n",
      "  Batch [540/1299] D_loss: -0.2020, G_loss: 0.5947\n",
      "  Batch [550/1299] D_loss: -0.2534, G_loss: 0.7454\n",
      "  Batch [560/1299] D_loss: -0.1291, G_loss: 0.6098\n",
      "  Batch [570/1299] D_loss: -0.0765, G_loss: 0.3684\n",
      "  Batch [580/1299] D_loss: -0.0548, G_loss: 0.3301\n",
      "  Batch [590/1299] D_loss: -1.4817, G_loss: -2.1373\n",
      "  Batch [600/1299] D_loss: -0.4450, G_loss: -0.7324\n",
      "  Batch [610/1299] D_loss: 0.0057, G_loss: 0.2193\n",
      "  Batch [620/1299] D_loss: -0.0445, G_loss: 0.3127\n",
      "  Batch [630/1299] D_loss: -0.1089, G_loss: 0.5829\n",
      "  Batch [640/1299] D_loss: -0.1677, G_loss: 0.5153\n",
      "  Batch [650/1299] D_loss: -0.0707, G_loss: 0.3177\n",
      "  Batch [660/1299] D_loss: -0.1128, G_loss: 0.2675\n",
      "  Batch [670/1299] D_loss: 0.0086, G_loss: 0.0769\n",
      "  Batch [680/1299] D_loss: -0.0696, G_loss: 0.2157\n",
      "  Batch [690/1299] D_loss: -0.0840, G_loss: 0.2329\n",
      "  Batch [700/1299] D_loss: -0.1429, G_loss: 0.4705\n",
      "  Batch [710/1299] D_loss: -0.1634, G_loss: 0.6164\n",
      "  Batch [720/1299] D_loss: -0.0241, G_loss: 0.4016\n",
      "  Batch [730/1299] D_loss: -0.2241, G_loss: 0.3886\n",
      "  Batch [740/1299] D_loss: -0.6980, G_loss: -0.2614\n",
      "  Batch [750/1299] D_loss: 0.0367, G_loss: 0.1644\n",
      "  Batch [760/1299] D_loss: -0.0132, G_loss: 0.1755\n",
      "  Batch [770/1299] D_loss: -0.3551, G_loss: -0.1217\n",
      "  Batch [780/1299] D_loss: -0.1350, G_loss: -0.2431\n",
      "  Batch [790/1299] D_loss: -0.0861, G_loss: 0.2918\n",
      "  Batch [800/1299] D_loss: 0.0063, G_loss: 0.2859\n",
      "  Batch [810/1299] D_loss: -0.0952, G_loss: 0.3428\n",
      "  Batch [820/1299] D_loss: -2.2440, G_loss: -2.1400\n",
      "  Batch [830/1299] D_loss: 0.0121, G_loss: -0.2009\n",
      "  Batch [840/1299] D_loss: -0.0440, G_loss: 0.1732\n",
      "  Batch [850/1299] D_loss: -0.0180, G_loss: 0.3525\n",
      "  Batch [860/1299] D_loss: -0.0569, G_loss: 0.4727\n",
      "  Batch [870/1299] D_loss: -0.0963, G_loss: 0.3648\n",
      "  Batch [880/1299] D_loss: -0.1500, G_loss: 0.4086\n",
      "  Batch [890/1299] D_loss: -0.0544, G_loss: 0.1062\n",
      "  Batch [900/1299] D_loss: -0.6420, G_loss: -1.9127\n",
      "  Batch [910/1299] D_loss: -1.0611, G_loss: -0.8603\n",
      "  Batch [920/1299] D_loss: -0.0833, G_loss: 0.0765\n",
      "  Batch [930/1299] D_loss: -0.2226, G_loss: 0.2177\n",
      "  Batch [940/1299] D_loss: -0.1436, G_loss: 0.3106\n",
      "  Batch [950/1299] D_loss: -0.1786, G_loss: 0.3351\n",
      "  Batch [960/1299] D_loss: -0.0644, G_loss: 0.3585\n",
      "  Batch [970/1299] D_loss: -0.0194, G_loss: 0.3729\n",
      "  Batch [980/1299] D_loss: -0.0397, G_loss: 0.1886\n",
      "  Batch [990/1299] D_loss: 0.0310, G_loss: 0.1675\n",
      "  Batch [1000/1299] D_loss: 0.0023, G_loss: 0.1588\n",
      "  Batch [1010/1299] D_loss: 0.0561, G_loss: 0.2085\n",
      "  Batch [1020/1299] D_loss: -0.0437, G_loss: 0.2684\n",
      "  Batch [1030/1299] D_loss: -0.1017, G_loss: 0.1634\n",
      "  Batch [1040/1299] D_loss: -0.7283, G_loss: -3.2172\n",
      "  Batch [1050/1299] D_loss: -0.1696, G_loss: 0.1020\n",
      "  Batch [1060/1299] D_loss: 0.0222, G_loss: 0.2020\n",
      "  Batch [1070/1299] D_loss: -0.0655, G_loss: 0.3297\n",
      "  Batch [1080/1299] D_loss: -0.0684, G_loss: 0.4030\n",
      "  Batch [1090/1299] D_loss: -0.0188, G_loss: 0.3339\n",
      "  Batch [1100/1299] D_loss: -0.5679, G_loss: -2.2874\n",
      "  Batch [1110/1299] D_loss: -0.3123, G_loss: 0.0404\n",
      "  Batch [1120/1299] D_loss: 0.0117, G_loss: 0.1585\n",
      "  Batch [1130/1299] D_loss: -0.0886, G_loss: 0.2397\n",
      "  Batch [1140/1299] D_loss: -0.0532, G_loss: 0.2475\n",
      "  Batch [1150/1299] D_loss: -2.1895, G_loss: -1.6948\n",
      "  Batch [1160/1299] D_loss: -0.0342, G_loss: 0.2149\n",
      "  Batch [1170/1299] D_loss: 0.0331, G_loss: 0.1601\n",
      "  Batch [1180/1299] D_loss: -0.0190, G_loss: 0.2044\n",
      "  Batch [1190/1299] D_loss: 0.0795, G_loss: 0.1558\n",
      "  Batch [1200/1299] D_loss: -0.1601, G_loss: 0.0766\n",
      "  Batch [1210/1299] D_loss: -0.3328, G_loss: -0.3985\n",
      "  Batch [1220/1299] D_loss: 0.0474, G_loss: 0.2387\n",
      "  Batch [1230/1299] D_loss: -0.0516, G_loss: 0.3440\n",
      "  Batch [1240/1299] D_loss: -0.0931, G_loss: 0.4099\n",
      "  Batch [1250/1299] D_loss: -0.0512, G_loss: 0.2500\n",
      "  Batch [1260/1299] D_loss: -0.1480, G_loss: -0.1044\n",
      "  Batch [1270/1299] D_loss: -0.0375, G_loss: 0.1615\n",
      "  Batch [1280/1299] D_loss: -0.0163, G_loss: 0.2678\n",
      "  Batch [1290/1299] D_loss: -0.0919, G_loss: 0.4681\n",
      "\n",
      "Epoch 90 Summary:\n",
      "  Average D_loss: -0.1154\n",
      "  Average G_loss: -0.0289\n",
      "\n",
      "Epoch [91/100]\n",
      "  Batch [0/1299] D_loss: -0.3626, G_loss: 0.4293\n",
      "  Batch [10/1299] D_loss: -0.0946, G_loss: 0.2696\n",
      "  Batch [20/1299] D_loss: -0.4497, G_loss: -0.4608\n",
      "  Batch [30/1299] D_loss: -0.0376, G_loss: 0.2507\n",
      "  Batch [40/1299] D_loss: 0.0388, G_loss: 0.2630\n",
      "  Batch [50/1299] D_loss: -0.1021, G_loss: 0.3029\n",
      "  Batch [60/1299] D_loss: -0.0076, G_loss: 0.2942\n",
      "  Batch [70/1299] D_loss: -0.0806, G_loss: 0.2353\n",
      "  Batch [80/1299] D_loss: -0.0969, G_loss: 0.3027\n",
      "  Batch [90/1299] D_loss: -0.1082, G_loss: 0.2232\n",
      "  Batch [100/1299] D_loss: -0.0147, G_loss: 0.2730\n",
      "  Batch [110/1299] D_loss: -0.0045, G_loss: 0.1545\n",
      "  Batch [120/1299] D_loss: -2.4727, G_loss: -3.8624\n",
      "  Batch [130/1299] D_loss: -0.3475, G_loss: -1.1952\n",
      "  Batch [140/1299] D_loss: -0.0075, G_loss: 0.2632\n",
      "  Batch [150/1299] D_loss: -0.0386, G_loss: 0.2491\n",
      "  Batch [160/1299] D_loss: -0.0377, G_loss: 0.2804\n",
      "  Batch [170/1299] D_loss: -0.0558, G_loss: 0.1542\n",
      "  Batch [180/1299] D_loss: -0.0933, G_loss: -0.0238\n",
      "  Batch [190/1299] D_loss: -1.0415, G_loss: -0.5422\n",
      "  Batch [200/1299] D_loss: -0.8259, G_loss: -0.6881\n",
      "  Batch [210/1299] D_loss: -0.4951, G_loss: 0.0580\n",
      "  Batch [220/1299] D_loss: -0.2878, G_loss: 0.1398\n",
      "  Batch [230/1299] D_loss: -0.7497, G_loss: 0.0915\n",
      "  Batch [240/1299] D_loss: -0.2259, G_loss: 0.0868\n",
      "  Batch [250/1299] D_loss: -0.9460, G_loss: 0.2225\n",
      "  Batch [260/1299] D_loss: -0.0647, G_loss: 0.3461\n",
      "  Batch [270/1299] D_loss: -0.0147, G_loss: 0.5553\n",
      "  Batch [280/1299] D_loss: -0.1494, G_loss: 0.6857\n",
      "  Batch [290/1299] D_loss: -0.0857, G_loss: 0.7466\n",
      "  Batch [300/1299] D_loss: -0.1201, G_loss: 0.4994\n",
      "  Batch [310/1299] D_loss: -0.1004, G_loss: 0.4015\n",
      "  Batch [320/1299] D_loss: -0.5194, G_loss: -1.3851\n",
      "  Batch [330/1299] D_loss: -0.3049, G_loss: 0.0951\n",
      "  Batch [340/1299] D_loss: -0.3720, G_loss: -0.7542\n",
      "  Batch [350/1299] D_loss: 0.0201, G_loss: 0.2198\n",
      "  Batch [360/1299] D_loss: -0.0320, G_loss: 0.4213\n",
      "  Batch [370/1299] D_loss: -0.0476, G_loss: 0.4316\n",
      "  Batch [380/1299] D_loss: -0.0821, G_loss: 0.5719\n",
      "  Batch [390/1299] D_loss: -0.2049, G_loss: 0.3955\n",
      "  Batch [400/1299] D_loss: -0.1291, G_loss: 0.3620\n",
      "  Batch [410/1299] D_loss: -2.1544, G_loss: -5.9319\n",
      "  Batch [420/1299] D_loss: -1.0667, G_loss: -0.1145\n",
      "  Batch [430/1299] D_loss: -0.1849, G_loss: -0.0174\n",
      "  Batch [440/1299] D_loss: -0.5912, G_loss: 0.0937\n",
      "  Batch [450/1299] D_loss: -1.0994, G_loss: -0.8297\n",
      "  Batch [460/1299] D_loss: -0.1680, G_loss: -0.2307\n",
      "  Batch [470/1299] D_loss: 0.0504, G_loss: 0.1132\n",
      "  Batch [480/1299] D_loss: -0.1732, G_loss: 0.0533\n",
      "  Batch [490/1299] D_loss: -0.8562, G_loss: -0.0056\n",
      "  Batch [500/1299] D_loss: -0.0527, G_loss: 0.2689\n",
      "  Batch [510/1299] D_loss: -0.1543, G_loss: 0.1190\n",
      "  Batch [520/1299] D_loss: -0.0000, G_loss: 0.3798\n",
      "  Batch [530/1299] D_loss: -0.1898, G_loss: 0.7556\n",
      "  Batch [540/1299] D_loss: -0.0187, G_loss: 0.6633\n",
      "  Batch [550/1299] D_loss: -0.0676, G_loss: 0.4177\n",
      "  Batch [560/1299] D_loss: -0.0531, G_loss: 0.3660\n",
      "  Batch [570/1299] D_loss: -0.0989, G_loss: 0.3275\n",
      "  Batch [580/1299] D_loss: -2.2868, G_loss: -2.2469\n",
      "  Batch [590/1299] D_loss: -0.8595, G_loss: -0.5880\n",
      "  Batch [600/1299] D_loss: -0.0108, G_loss: 0.1331\n",
      "  Batch [610/1299] D_loss: -0.0075, G_loss: 0.2424\n",
      "  Batch [620/1299] D_loss: -0.1105, G_loss: 0.4092\n",
      "  Batch [630/1299] D_loss: -0.1342, G_loss: 0.5048\n",
      "  Batch [640/1299] D_loss: -0.2292, G_loss: 0.7021\n",
      "  Batch [650/1299] D_loss: -0.0490, G_loss: 0.5125\n",
      "  Batch [660/1299] D_loss: -0.0685, G_loss: 0.2932\n",
      "  Batch [670/1299] D_loss: -1.2369, G_loss: -2.0165\n",
      "  Batch [680/1299] D_loss: -0.1503, G_loss: 0.0923\n",
      "  Batch [690/1299] D_loss: -0.6005, G_loss: -0.0389\n",
      "  Batch [700/1299] D_loss: -0.2269, G_loss: -0.5987\n",
      "  Batch [710/1299] D_loss: -0.2372, G_loss: -0.0023\n",
      "  Batch [720/1299] D_loss: -0.0140, G_loss: 0.1618\n",
      "  Batch [730/1299] D_loss: -0.1102, G_loss: 0.3689\n",
      "  Batch [740/1299] D_loss: -0.2271, G_loss: 0.6898\n",
      "  Batch [750/1299] D_loss: -0.2384, G_loss: 0.7648\n",
      "  Batch [760/1299] D_loss: -0.0872, G_loss: 0.6597\n",
      "  Batch [770/1299] D_loss: 0.0514, G_loss: 0.6090\n",
      "  Batch [780/1299] D_loss: -0.0249, G_loss: 0.2434\n",
      "  Batch [790/1299] D_loss: -0.9540, G_loss: -1.6499\n",
      "  Batch [800/1299] D_loss: 0.0041, G_loss: 0.1262\n",
      "  Batch [810/1299] D_loss: 0.0174, G_loss: 0.2400\n",
      "  Batch [820/1299] D_loss: -0.0763, G_loss: 0.3970\n",
      "  Batch [830/1299] D_loss: -0.0639, G_loss: 0.4183\n",
      "  Batch [840/1299] D_loss: -0.0555, G_loss: 0.3690\n",
      "  Batch [850/1299] D_loss: -0.5151, G_loss: -1.7680\n",
      "  Batch [860/1299] D_loss: -0.0020, G_loss: 0.1794\n",
      "  Batch [870/1299] D_loss: -0.0176, G_loss: 0.2229\n",
      "  Batch [880/1299] D_loss: -0.2145, G_loss: 0.4219\n",
      "  Batch [890/1299] D_loss: -0.0499, G_loss: 0.3525\n",
      "  Batch [900/1299] D_loss: -0.1338, G_loss: 0.3410\n",
      "  Batch [910/1299] D_loss: -0.7313, G_loss: -1.4121\n",
      "  Batch [920/1299] D_loss: -0.2515, G_loss: -0.2185\n",
      "  Batch [930/1299] D_loss: -0.9066, G_loss: -1.1756\n",
      "  Batch [940/1299] D_loss: 0.0473, G_loss: 0.3776\n",
      "  Batch [950/1299] D_loss: 0.2001, G_loss: 0.4047\n",
      "  Batch [960/1299] D_loss: -0.1398, G_loss: 0.6391\n",
      "  Batch [970/1299] D_loss: -0.0162, G_loss: 0.6338\n",
      "  Batch [980/1299] D_loss: 0.0065, G_loss: 0.2884\n",
      "  Batch [990/1299] D_loss: -0.5989, G_loss: -0.4554\n",
      "  Batch [1000/1299] D_loss: -0.7417, G_loss: -1.0995\n",
      "  Batch [1010/1299] D_loss: -0.8880, G_loss: -0.2380\n",
      "  Batch [1020/1299] D_loss: -1.0095, G_loss: 0.0643\n",
      "  Batch [1030/1299] D_loss: -0.1857, G_loss: 0.0179\n",
      "  Batch [1040/1299] D_loss: -0.0913, G_loss: 0.1400\n",
      "  Batch [1050/1299] D_loss: -0.0644, G_loss: 0.4009\n",
      "  Batch [1060/1299] D_loss: -0.0202, G_loss: 0.4731\n",
      "  Batch [1070/1299] D_loss: -0.0768, G_loss: 0.4883\n",
      "  Batch [1080/1299] D_loss: -0.1170, G_loss: 0.4396\n",
      "  Batch [1090/1299] D_loss: -0.0601, G_loss: 0.3471\n",
      "  Batch [1100/1299] D_loss: -0.8707, G_loss: -0.1213\n",
      "  Batch [1110/1299] D_loss: -0.0249, G_loss: 0.3260\n",
      "  Batch [1120/1299] D_loss: -0.1009, G_loss: 0.4228\n",
      "  Batch [1130/1299] D_loss: -0.0587, G_loss: 0.3592\n",
      "  Batch [1140/1299] D_loss: -0.0562, G_loss: 0.4418\n",
      "  Batch [1150/1299] D_loss: -0.0666, G_loss: 0.2097\n",
      "  Batch [1160/1299] D_loss: -0.2908, G_loss: -0.0920\n",
      "  Batch [1170/1299] D_loss: -0.0065, G_loss: 0.1662\n",
      "  Batch [1180/1299] D_loss: -0.0249, G_loss: 0.2239\n",
      "  Batch [1190/1299] D_loss: -0.0888, G_loss: 0.2084\n",
      "  Batch [1200/1299] D_loss: -0.8272, G_loss: -0.6817\n",
      "  Batch [1210/1299] D_loss: -0.2546, G_loss: -0.6656\n",
      "  Batch [1220/1299] D_loss: 0.0005, G_loss: 0.1792\n",
      "  Batch [1230/1299] D_loss: -0.0722, G_loss: 0.3560\n",
      "  Batch [1240/1299] D_loss: 0.0544, G_loss: 0.3963\n",
      "  Batch [1250/1299] D_loss: -0.0419, G_loss: 0.4882\n",
      "  Batch [1260/1299] D_loss: -0.0486, G_loss: 0.4027\n",
      "  Batch [1270/1299] D_loss: -0.0391, G_loss: 0.2658\n",
      "  Batch [1280/1299] D_loss: -1.8821, G_loss: -2.8091\n",
      "  Batch [1290/1299] D_loss: 0.0081, G_loss: 0.0650\n",
      "\n",
      "Epoch 91 Summary:\n",
      "  Average D_loss: -0.1572\n",
      "  Average G_loss: -0.0698\n",
      "\n",
      "Epoch [92/100]\n",
      "  Batch [0/1299] D_loss: 0.0277, G_loss: 0.1710\n",
      "  Batch [10/1299] D_loss: 0.0073, G_loss: 0.2369\n",
      "  Batch [20/1299] D_loss: -0.0382, G_loss: 0.2838\n",
      "  Batch [30/1299] D_loss: 0.0078, G_loss: 0.2688\n",
      "  Batch [40/1299] D_loss: -0.0240, G_loss: 0.2997\n",
      "  Batch [50/1299] D_loss: -1.9463, G_loss: -5.0356\n",
      "  Batch [60/1299] D_loss: -0.0648, G_loss: 0.2300\n",
      "  Batch [70/1299] D_loss: -0.1419, G_loss: 0.3250\n",
      "  Batch [80/1299] D_loss: 0.0046, G_loss: 0.4082\n",
      "  Batch [90/1299] D_loss: 0.0330, G_loss: 0.3007\n",
      "  Batch [100/1299] D_loss: -0.0338, G_loss: 0.2713\n",
      "  Batch [110/1299] D_loss: -0.9817, G_loss: -1.5144\n",
      "  Batch [120/1299] D_loss: -0.6348, G_loss: -1.3855\n",
      "  Batch [130/1299] D_loss: -0.0264, G_loss: 0.1848\n",
      "  Batch [140/1299] D_loss: -0.0545, G_loss: 0.2617\n",
      "  Batch [150/1299] D_loss: -0.0149, G_loss: 0.2999\n",
      "  Batch [160/1299] D_loss: -0.0791, G_loss: 0.4400\n",
      "  Batch [170/1299] D_loss: -0.1080, G_loss: 0.4226\n",
      "  Batch [180/1299] D_loss: -2.9202, G_loss: -5.9039\n",
      "  Batch [190/1299] D_loss: -0.2595, G_loss: -0.6388\n",
      "  Batch [200/1299] D_loss: -0.0633, G_loss: 0.2120\n",
      "  Batch [210/1299] D_loss: -0.0109, G_loss: 0.3263\n",
      "  Batch [220/1299] D_loss: -0.0520, G_loss: 0.3111\n",
      "  Batch [230/1299] D_loss: 0.0082, G_loss: 0.2941\n",
      "  Batch [240/1299] D_loss: -1.3168, G_loss: -2.2705\n",
      "  Batch [250/1299] D_loss: 0.0041, G_loss: 0.1663\n",
      "  Batch [260/1299] D_loss: -0.0670, G_loss: 0.3085\n",
      "  Batch [270/1299] D_loss: -0.0178, G_loss: 0.5068\n",
      "  Batch [280/1299] D_loss: -0.1124, G_loss: 0.4757\n",
      "  Batch [290/1299] D_loss: -0.0380, G_loss: 0.4207\n",
      "  Batch [300/1299] D_loss: -0.1230, G_loss: 0.4137\n",
      "  Batch [310/1299] D_loss: -0.1016, G_loss: 0.1801\n",
      "  Batch [320/1299] D_loss: -0.7148, G_loss: -3.0152\n",
      "  Batch [330/1299] D_loss: -0.1631, G_loss: -0.2994\n",
      "  Batch [340/1299] D_loss: -0.7566, G_loss: -0.1107\n",
      "  Batch [350/1299] D_loss: -0.0457, G_loss: 0.1198\n",
      "  Batch [360/1299] D_loss: -0.0934, G_loss: 0.3619\n",
      "  Batch [370/1299] D_loss: -0.0841, G_loss: 0.5424\n",
      "  Batch [380/1299] D_loss: -0.0953, G_loss: 0.7246\n",
      "  Batch [390/1299] D_loss: -0.0630, G_loss: 0.5526\n",
      "  Batch [400/1299] D_loss: 0.0321, G_loss: 0.4328\n",
      "  Batch [410/1299] D_loss: 0.0635, G_loss: 0.1930\n",
      "  Batch [420/1299] D_loss: -0.7302, G_loss: -0.2970\n",
      "  Batch [430/1299] D_loss: -0.0277, G_loss: 0.1762\n",
      "  Batch [440/1299] D_loss: -0.0506, G_loss: 0.3697\n",
      "  Batch [450/1299] D_loss: -0.0356, G_loss: 0.3111\n",
      "  Batch [460/1299] D_loss: -0.0270, G_loss: 0.2635\n",
      "  Batch [470/1299] D_loss: -3.7572, G_loss: -5.6590\n",
      "  Batch [480/1299] D_loss: -0.2132, G_loss: -0.2198\n",
      "  Batch [490/1299] D_loss: -0.0709, G_loss: 0.1555\n",
      "  Batch [500/1299] D_loss: -0.0407, G_loss: 0.2411\n",
      "  Batch [510/1299] D_loss: -0.0262, G_loss: 0.4274\n",
      "  Batch [520/1299] D_loss: -0.1173, G_loss: 0.4386\n",
      "  Batch [530/1299] D_loss: -0.0427, G_loss: 0.4242\n",
      "  Batch [540/1299] D_loss: -0.0778, G_loss: 0.2505\n",
      "  Batch [550/1299] D_loss: -0.4257, G_loss: -0.1781\n",
      "  Batch [560/1299] D_loss: -0.0176, G_loss: 0.2117\n",
      "  Batch [570/1299] D_loss: -0.1130, G_loss: 0.3345\n",
      "  Batch [580/1299] D_loss: -0.0934, G_loss: 0.3049\n",
      "  Batch [590/1299] D_loss: -0.2443, G_loss: -0.7072\n",
      "  Batch [600/1299] D_loss: -0.0015, G_loss: 0.0784\n",
      "  Batch [610/1299] D_loss: -1.6454, G_loss: -0.1397\n",
      "  Batch [620/1299] D_loss: -0.7481, G_loss: -0.7160\n",
      "  Batch [630/1299] D_loss: -0.0227, G_loss: 0.2205\n",
      "  Batch [640/1299] D_loss: -0.0269, G_loss: 0.1976\n",
      "  Batch [650/1299] D_loss: 0.0126, G_loss: 0.2984\n",
      "  Batch [660/1299] D_loss: -0.0894, G_loss: 0.3964\n",
      "  Batch [670/1299] D_loss: -0.1719, G_loss: 0.4445\n",
      "  Batch [680/1299] D_loss: -0.7934, G_loss: -4.0476\n",
      "  Batch [690/1299] D_loss: -0.3526, G_loss: -0.2277\n",
      "  Batch [700/1299] D_loss: -0.8239, G_loss: -0.7492\n",
      "  Batch [710/1299] D_loss: -0.8188, G_loss: -1.3517\n",
      "  Batch [720/1299] D_loss: -0.1782, G_loss: 0.1387\n",
      "  Batch [730/1299] D_loss: -0.8180, G_loss: -0.0617\n",
      "  Batch [740/1299] D_loss: -0.0302, G_loss: 0.2723\n",
      "  Batch [750/1299] D_loss: -0.1532, G_loss: 0.5899\n",
      "  Batch [760/1299] D_loss: -0.1738, G_loss: 0.7090\n",
      "  Batch [770/1299] D_loss: 0.0057, G_loss: 0.7482\n",
      "  Batch [780/1299] D_loss: -0.0168, G_loss: 0.2506\n",
      "  Batch [790/1299] D_loss: -1.1331, G_loss: -3.8866\n",
      "  Batch [800/1299] D_loss: -0.0006, G_loss: 0.1007\n",
      "  Batch [810/1299] D_loss: -0.9677, G_loss: -1.1629\n",
      "  Batch [820/1299] D_loss: 0.0033, G_loss: 0.1858\n",
      "  Batch [830/1299] D_loss: -0.0784, G_loss: 0.3152\n",
      "  Batch [840/1299] D_loss: -0.0937, G_loss: 0.3903\n",
      "  Batch [850/1299] D_loss: 0.0128, G_loss: 0.3740\n",
      "  Batch [860/1299] D_loss: -0.1029, G_loss: 0.4106\n",
      "  Batch [870/1299] D_loss: -0.0487, G_loss: 0.3034\n",
      "  Batch [880/1299] D_loss: -0.5467, G_loss: -1.1803\n",
      "  Batch [890/1299] D_loss: -0.7944, G_loss: -0.7191\n",
      "  Batch [900/1299] D_loss: -0.2185, G_loss: 0.1071\n",
      "  Batch [910/1299] D_loss: -0.5927, G_loss: -0.0192\n",
      "  Batch [920/1299] D_loss: -0.2593, G_loss: -0.8447\n",
      "  Batch [930/1299] D_loss: 0.0058, G_loss: 0.1548\n",
      "  Batch [940/1299] D_loss: -0.0207, G_loss: 0.3972\n",
      "  Batch [950/1299] D_loss: 0.1380, G_loss: 0.5300\n",
      "  Batch [960/1299] D_loss: -0.0174, G_loss: 0.4408\n",
      "  Batch [970/1299] D_loss: -0.0435, G_loss: 0.3354\n",
      "  Batch [980/1299] D_loss: -0.0704, G_loss: 0.2262\n",
      "  Batch [990/1299] D_loss: -0.2824, G_loss: -1.2932\n",
      "  Batch [1000/1299] D_loss: -0.2158, G_loss: -0.1446\n",
      "  Batch [1010/1299] D_loss: -0.0342, G_loss: 0.3079\n",
      "  Batch [1020/1299] D_loss: -0.0959, G_loss: 0.3245\n",
      "  Batch [1030/1299] D_loss: -0.0206, G_loss: 0.2143\n",
      "  Batch [1040/1299] D_loss: -0.0485, G_loss: 0.2611\n",
      "  Batch [1050/1299] D_loss: -0.4762, G_loss: -0.9693\n",
      "  Batch [1060/1299] D_loss: -0.0547, G_loss: -0.0264\n",
      "  Batch [1070/1299] D_loss: -0.7665, G_loss: -2.0158\n",
      "  Batch [1080/1299] D_loss: -0.1134, G_loss: 0.3261\n",
      "  Batch [1090/1299] D_loss: -0.1958, G_loss: 0.6867\n",
      "  Batch [1100/1299] D_loss: 0.0476, G_loss: 0.6899\n",
      "  Batch [1110/1299] D_loss: -0.1341, G_loss: 0.7055\n",
      "  Batch [1120/1299] D_loss: -0.0234, G_loss: 0.3239\n",
      "  Batch [1130/1299] D_loss: 0.0256, G_loss: 0.2266\n",
      "  Batch [1140/1299] D_loss: -0.4098, G_loss: -0.8428\n",
      "  Batch [1150/1299] D_loss: 0.0386, G_loss: 0.1322\n",
      "  Batch [1160/1299] D_loss: -0.0566, G_loss: 0.1831\n",
      "  Batch [1170/1299] D_loss: 0.0448, G_loss: 0.2740\n",
      "  Batch [1180/1299] D_loss: -0.0312, G_loss: 0.3220\n",
      "  Batch [1190/1299] D_loss: -0.7726, G_loss: -1.9388\n",
      "  Batch [1200/1299] D_loss: -0.0846, G_loss: 0.0208\n",
      "  Batch [1210/1299] D_loss: -0.4105, G_loss: 0.0179\n",
      "  Batch [1220/1299] D_loss: -0.2678, G_loss: -0.0954\n",
      "  Batch [1230/1299] D_loss: -0.0713, G_loss: 0.3347\n",
      "  Batch [1240/1299] D_loss: 0.0383, G_loss: 0.3768\n",
      "  Batch [1250/1299] D_loss: -0.0692, G_loss: 0.2979\n",
      "  Batch [1260/1299] D_loss: -0.1404, G_loss: 0.4945\n",
      "  Batch [1270/1299] D_loss: 0.0536, G_loss: 0.2493\n",
      "  Batch [1280/1299] D_loss: -1.0534, G_loss: -0.9216\n",
      "  Batch [1290/1299] D_loss: -0.6796, G_loss: -0.9540\n",
      "\n",
      "Epoch 92 Summary:\n",
      "  Average D_loss: -0.1236\n",
      "  Average G_loss: -0.0788\n",
      "\n",
      "Epoch [93/100]\n",
      "  Batch [0/1299] D_loss: -0.0701, G_loss: 0.1059\n",
      "  Batch [10/1299] D_loss: -0.0875, G_loss: 0.2290\n",
      "  Batch [20/1299] D_loss: -0.0681, G_loss: 0.3608\n",
      "  Batch [30/1299] D_loss: -0.0730, G_loss: 0.5316\n",
      "  Batch [40/1299] D_loss: 0.0501, G_loss: 0.4004\n",
      "  Batch [50/1299] D_loss: -2.1271, G_loss: -4.1911\n",
      "  Batch [60/1299] D_loss: -0.1079, G_loss: -0.0428\n",
      "  Batch [70/1299] D_loss: 0.0006, G_loss: 0.2117\n",
      "  Batch [80/1299] D_loss: -0.1320, G_loss: 0.4222\n",
      "  Batch [90/1299] D_loss: -0.3313, G_loss: 0.6825\n",
      "  Batch [100/1299] D_loss: 0.0956, G_loss: 0.5888\n",
      "  Batch [110/1299] D_loss: -0.0582, G_loss: 0.6553\n",
      "  Batch [120/1299] D_loss: -0.0730, G_loss: 0.2836\n",
      "  Batch [130/1299] D_loss: 0.0284, G_loss: -0.5346\n",
      "  Batch [140/1299] D_loss: -0.5056, G_loss: -0.1766\n",
      "  Batch [150/1299] D_loss: -0.0594, G_loss: 0.0915\n",
      "  Batch [160/1299] D_loss: -1.1282, G_loss: -1.1950\n",
      "  Batch [170/1299] D_loss: -0.0760, G_loss: 0.4893\n",
      "  Batch [180/1299] D_loss: -0.1152, G_loss: 0.4543\n",
      "  Batch [190/1299] D_loss: 0.1049, G_loss: 0.2896\n",
      "  Batch [200/1299] D_loss: -0.0388, G_loss: 0.3438\n",
      "  Batch [210/1299] D_loss: -1.6659, G_loss: -1.6915\n",
      "  Batch [220/1299] D_loss: -0.3720, G_loss: -0.0368\n",
      "  Batch [230/1299] D_loss: -0.1562, G_loss: -0.0132\n",
      "  Batch [240/1299] D_loss: -0.7268, G_loss: -2.1358\n",
      "  Batch [250/1299] D_loss: -0.0513, G_loss: 0.2531\n",
      "  Batch [260/1299] D_loss: -0.0978, G_loss: 0.4221\n",
      "  Batch [270/1299] D_loss: -0.0468, G_loss: 0.4396\n",
      "  Batch [280/1299] D_loss: -0.1253, G_loss: 0.3251\n",
      "  Batch [290/1299] D_loss: -0.0058, G_loss: 0.2776\n",
      "  Batch [300/1299] D_loss: -0.3407, G_loss: 0.0000\n",
      "  Batch [310/1299] D_loss: -0.0473, G_loss: 0.2044\n",
      "  Batch [320/1299] D_loss: -0.1115, G_loss: 0.3601\n",
      "  Batch [330/1299] D_loss: -0.0064, G_loss: 0.2498\n",
      "  Batch [340/1299] D_loss: -0.0685, G_loss: 0.4324\n",
      "  Batch [350/1299] D_loss: -0.0603, G_loss: 0.2332\n",
      "  Batch [360/1299] D_loss: -0.9981, G_loss: -2.0849\n",
      "  Batch [370/1299] D_loss: -0.0374, G_loss: 0.1654\n",
      "  Batch [380/1299] D_loss: -0.0925, G_loss: 0.3475\n",
      "  Batch [390/1299] D_loss: -0.0280, G_loss: 0.2597\n",
      "  Batch [400/1299] D_loss: -0.1780, G_loss: 0.3359\n",
      "  Batch [410/1299] D_loss: -0.0640, G_loss: 0.1796\n",
      "  Batch [420/1299] D_loss: 0.0178, G_loss: 0.2253\n",
      "  Batch [430/1299] D_loss: -2.2525, G_loss: -4.4951\n",
      "  Batch [440/1299] D_loss: -1.0972, G_loss: -1.1554\n",
      "  Batch [450/1299] D_loss: -0.1019, G_loss: 0.1187\n",
      "  Batch [460/1299] D_loss: -0.0066, G_loss: 0.0920\n",
      "  Batch [470/1299] D_loss: -1.2614, G_loss: -0.8701\n",
      "  Batch [480/1299] D_loss: -0.0083, G_loss: 0.2023\n",
      "  Batch [490/1299] D_loss: -0.0124, G_loss: 0.3373\n",
      "  Batch [500/1299] D_loss: -0.1954, G_loss: 0.5766\n",
      "  Batch [510/1299] D_loss: -0.0777, G_loss: 0.6383\n",
      "  Batch [520/1299] D_loss: -0.0351, G_loss: 0.6781\n",
      "  Batch [530/1299] D_loss: -0.0713, G_loss: 0.5179\n",
      "  Batch [540/1299] D_loss: -0.1514, G_loss: 0.4686\n",
      "  Batch [550/1299] D_loss: 0.0275, G_loss: 0.2035\n",
      "  Batch [560/1299] D_loss: -0.5146, G_loss: -0.9905\n",
      "  Batch [570/1299] D_loss: -0.1258, G_loss: 0.2104\n",
      "  Batch [580/1299] D_loss: -0.0270, G_loss: 0.2635\n",
      "  Batch [590/1299] D_loss: -0.0551, G_loss: 0.3018\n",
      "  Batch [600/1299] D_loss: -0.1536, G_loss: 0.4198\n",
      "  Batch [610/1299] D_loss: -0.2160, G_loss: 0.3592\n",
      "  Batch [620/1299] D_loss: -0.0221, G_loss: 0.2999\n",
      "  Batch [630/1299] D_loss: -0.0406, G_loss: 0.0549\n",
      "  Batch [640/1299] D_loss: -0.0159, G_loss: 0.1450\n",
      "  Batch [650/1299] D_loss: -0.0516, G_loss: 0.2159\n",
      "  Batch [660/1299] D_loss: -0.0229, G_loss: 0.1967\n",
      "  Batch [670/1299] D_loss: -0.2459, G_loss: 0.2024\n",
      "  Batch [680/1299] D_loss: -1.1799, G_loss: -1.4668\n",
      "  Batch [690/1299] D_loss: 0.0147, G_loss: 0.2814\n",
      "  Batch [700/1299] D_loss: -0.1600, G_loss: 0.4244\n",
      "  Batch [710/1299] D_loss: -0.2457, G_loss: 0.6872\n",
      "  Batch [720/1299] D_loss: -0.1801, G_loss: 0.4959\n",
      "  Batch [730/1299] D_loss: -0.0319, G_loss: 0.4034\n",
      "  Batch [740/1299] D_loss: -0.0374, G_loss: 0.4216\n",
      "  Batch [750/1299] D_loss: -4.8547, G_loss: -8.9139\n",
      "  Batch [760/1299] D_loss: -0.4945, G_loss: 0.1173\n",
      "  Batch [770/1299] D_loss: 0.0125, G_loss: 0.2389\n",
      "  Batch [780/1299] D_loss: -0.0078, G_loss: 0.3256\n",
      "  Batch [790/1299] D_loss: 0.0732, G_loss: 0.2987\n",
      "  Batch [800/1299] D_loss: -0.1899, G_loss: 0.3013\n",
      "  Batch [810/1299] D_loss: -0.1537, G_loss: 0.4254\n",
      "  Batch [820/1299] D_loss: -0.7333, G_loss: -2.4463\n",
      "  Batch [830/1299] D_loss: -1.0409, G_loss: -3.7282\n",
      "  Batch [840/1299] D_loss: -0.0738, G_loss: 0.1632\n",
      "  Batch [850/1299] D_loss: 0.0663, G_loss: 0.3617\n",
      "  Batch [860/1299] D_loss: -0.1820, G_loss: 0.5351\n",
      "  Batch [870/1299] D_loss: -0.0995, G_loss: 0.5718\n",
      "  Batch [880/1299] D_loss: -0.1679, G_loss: 0.5702\n",
      "  Batch [890/1299] D_loss: -0.1189, G_loss: 0.4693\n",
      "  Batch [900/1299] D_loss: -0.7306, G_loss: -0.4516\n",
      "  Batch [910/1299] D_loss: -1.1909, G_loss: -2.4536\n",
      "  Batch [920/1299] D_loss: -0.2028, G_loss: -0.0818\n",
      "  Batch [930/1299] D_loss: 0.0280, G_loss: 0.2100\n",
      "  Batch [940/1299] D_loss: 0.0070, G_loss: 0.3306\n",
      "  Batch [950/1299] D_loss: -0.0907, G_loss: 0.3490\n",
      "  Batch [960/1299] D_loss: -0.1390, G_loss: 0.4604\n",
      "  Batch [970/1299] D_loss: -0.1149, G_loss: 0.3500\n",
      "  Batch [980/1299] D_loss: -0.0437, G_loss: 0.2513\n",
      "  Batch [990/1299] D_loss: -0.2542, G_loss: -0.4747\n",
      "  Batch [1000/1299] D_loss: 0.0167, G_loss: 0.1602\n",
      "  Batch [1010/1299] D_loss: -0.2395, G_loss: 0.1850\n",
      "  Batch [1020/1299] D_loss: -0.0371, G_loss: 0.2585\n",
      "  Batch [1030/1299] D_loss: -0.0750, G_loss: 0.6099\n",
      "  Batch [1040/1299] D_loss: -0.0314, G_loss: 0.3206\n",
      "  Batch [1050/1299] D_loss: -0.1102, G_loss: 0.3147\n",
      "  Batch [1060/1299] D_loss: -0.0493, G_loss: 0.2228\n",
      "  Batch [1070/1299] D_loss: -0.5721, G_loss: -2.7546\n",
      "  Batch [1080/1299] D_loss: -0.5874, G_loss: -0.0207\n",
      "  Batch [1090/1299] D_loss: -0.7229, G_loss: -2.8272\n",
      "  Batch [1100/1299] D_loss: -1.0853, G_loss: -2.8893\n",
      "  Batch [1110/1299] D_loss: -0.1124, G_loss: 0.2481\n",
      "  Batch [1120/1299] D_loss: -0.0821, G_loss: 0.3640\n",
      "  Batch [1130/1299] D_loss: -0.1000, G_loss: 0.4794\n",
      "  Batch [1140/1299] D_loss: -0.0860, G_loss: 0.3248\n",
      "  Batch [1150/1299] D_loss: -0.0074, G_loss: 0.4281\n",
      "  Batch [1160/1299] D_loss: -0.1931, G_loss: 0.3697\n",
      "  Batch [1170/1299] D_loss: -1.6359, G_loss: -4.4465\n",
      "  Batch [1180/1299] D_loss: -0.6955, G_loss: -0.5948\n",
      "  Batch [1190/1299] D_loss: -0.0308, G_loss: 0.1621\n",
      "  Batch [1200/1299] D_loss: -0.0784, G_loss: 0.2261\n",
      "  Batch [1210/1299] D_loss: 0.0024, G_loss: 0.3071\n",
      "  Batch [1220/1299] D_loss: -0.0606, G_loss: 0.4750\n",
      "  Batch [1230/1299] D_loss: -0.0393, G_loss: 0.4176\n",
      "  Batch [1240/1299] D_loss: -0.0737, G_loss: 0.2609\n",
      "  Batch [1250/1299] D_loss: -0.5521, G_loss: -0.4719\n",
      "  Batch [1260/1299] D_loss: -0.8904, G_loss: -0.5279\n",
      "  Batch [1270/1299] D_loss: 0.0009, G_loss: 0.1469\n",
      "  Batch [1280/1299] D_loss: -0.0024, G_loss: 0.2158\n",
      "  Batch [1290/1299] D_loss: -0.0543, G_loss: 0.2208\n",
      "\n",
      "Epoch 93 Summary:\n",
      "  Average D_loss: -0.1287\n",
      "  Average G_loss: -0.0510\n",
      "\n",
      "Epoch [94/100]\n",
      "  Batch [0/1299] D_loss: -1.0371, G_loss: -0.8480\n",
      "  Batch [10/1299] D_loss: 0.0261, G_loss: 0.2089\n",
      "  Batch [20/1299] D_loss: -0.0398, G_loss: 0.2918\n",
      "  Batch [30/1299] D_loss: -0.0414, G_loss: 0.2962\n",
      "  Batch [40/1299] D_loss: -0.0459, G_loss: 0.3215\n",
      "  Batch [50/1299] D_loss: -0.6168, G_loss: -0.4117\n",
      "  Batch [60/1299] D_loss: 0.0416, G_loss: 0.2535\n",
      "  Batch [70/1299] D_loss: -0.0831, G_loss: 0.4030\n",
      "  Batch [80/1299] D_loss: -0.1028, G_loss: 0.2659\n",
      "  Batch [90/1299] D_loss: -0.0672, G_loss: 0.3947\n",
      "  Batch [100/1299] D_loss: -0.9861, G_loss: -0.1788\n",
      "  Batch [110/1299] D_loss: -0.0525, G_loss: -0.0064\n",
      "  Batch [120/1299] D_loss: -0.6376, G_loss: -0.1767\n",
      "  Batch [130/1299] D_loss: -0.6482, G_loss: -0.3699\n",
      "  Batch [140/1299] D_loss: -0.4725, G_loss: -0.3835\n",
      "  Batch [150/1299] D_loss: -0.4143, G_loss: -2.0944\n",
      "  Batch [160/1299] D_loss: -0.3685, G_loss: -0.1319\n",
      "  Batch [170/1299] D_loss: -0.1044, G_loss: 0.2045\n",
      "  Batch [180/1299] D_loss: -0.0750, G_loss: 0.4480\n",
      "  Batch [190/1299] D_loss: -0.0908, G_loss: 0.4032\n",
      "  Batch [200/1299] D_loss: -0.1731, G_loss: 0.5525\n",
      "  Batch [210/1299] D_loss: -0.1019, G_loss: 0.3640\n",
      "  Batch [220/1299] D_loss: -0.1516, G_loss: 0.2911\n",
      "  Batch [230/1299] D_loss: -0.8393, G_loss: -0.1346\n",
      "  Batch [240/1299] D_loss: 0.0313, G_loss: 0.1349\n",
      "  Batch [250/1299] D_loss: -0.0504, G_loss: 0.3462\n",
      "  Batch [260/1299] D_loss: 0.0509, G_loss: 0.4100\n",
      "  Batch [270/1299] D_loss: -0.0225, G_loss: 0.2874\n",
      "  Batch [280/1299] D_loss: -0.0380, G_loss: 0.3119\n",
      "  Batch [290/1299] D_loss: 0.0080, G_loss: 0.2413\n",
      "  Batch [300/1299] D_loss: -0.1821, G_loss: 0.2946\n",
      "  Batch [310/1299] D_loss: -0.4738, G_loss: -0.4806\n",
      "  Batch [320/1299] D_loss: 0.0451, G_loss: 0.2639\n",
      "  Batch [330/1299] D_loss: -0.0313, G_loss: 0.3512\n",
      "  Batch [340/1299] D_loss: 0.0100, G_loss: 0.2417\n",
      "  Batch [350/1299] D_loss: -2.3510, G_loss: -6.1790\n",
      "  Batch [360/1299] D_loss: -0.0659, G_loss: 0.1773\n",
      "  Batch [370/1299] D_loss: 0.0279, G_loss: 0.2724\n",
      "  Batch [380/1299] D_loss: -0.1205, G_loss: 0.4752\n",
      "  Batch [390/1299] D_loss: 0.0417, G_loss: 0.5724\n",
      "  Batch [400/1299] D_loss: -0.2223, G_loss: 0.5743\n",
      "  Batch [410/1299] D_loss: 0.1387, G_loss: 0.4951\n",
      "  Batch [420/1299] D_loss: 0.0175, G_loss: 0.1492\n",
      "  Batch [430/1299] D_loss: -0.0706, G_loss: 0.0510\n",
      "  Batch [440/1299] D_loss: -0.5422, G_loss: -0.7671\n",
      "  Batch [450/1299] D_loss: -0.4035, G_loss: -0.4217\n",
      "  Batch [460/1299] D_loss: -0.6599, G_loss: -0.9101\n",
      "  Batch [470/1299] D_loss: -0.1124, G_loss: 0.1319\n",
      "  Batch [480/1299] D_loss: -0.0353, G_loss: 0.1785\n",
      "  Batch [490/1299] D_loss: -0.0862, G_loss: 0.3997\n",
      "  Batch [500/1299] D_loss: -0.0815, G_loss: 0.3702\n",
      "  Batch [510/1299] D_loss: -0.2022, G_loss: 0.5020\n",
      "  Batch [520/1299] D_loss: -0.0762, G_loss: 0.4241\n",
      "  Batch [530/1299] D_loss: -0.0464, G_loss: 0.3174\n",
      "  Batch [540/1299] D_loss: -1.1696, G_loss: -4.1114\n",
      "  Batch [550/1299] D_loss: -0.1361, G_loss: 0.0657\n",
      "  Batch [560/1299] D_loss: -0.8965, G_loss: -1.6285\n",
      "  Batch [570/1299] D_loss: 0.0030, G_loss: 0.1266\n",
      "  Batch [580/1299] D_loss: -0.0755, G_loss: 0.1298\n",
      "  Batch [590/1299] D_loss: -0.0323, G_loss: 0.3249\n",
      "  Batch [600/1299] D_loss: 0.0159, G_loss: 0.3844\n",
      "  Batch [610/1299] D_loss: -0.1747, G_loss: 0.5383\n",
      "  Batch [620/1299] D_loss: 0.1299, G_loss: 0.3525\n",
      "  Batch [630/1299] D_loss: -0.1480, G_loss: 0.4931\n",
      "  Batch [640/1299] D_loss: -1.5680, G_loss: -1.3156\n",
      "  Batch [650/1299] D_loss: -0.0161, G_loss: 0.0522\n",
      "  Batch [660/1299] D_loss: -0.0180, G_loss: 0.1425\n",
      "  Batch [670/1299] D_loss: -0.0480, G_loss: 0.1678\n",
      "  Batch [680/1299] D_loss: -0.0860, G_loss: 0.1892\n",
      "  Batch [690/1299] D_loss: -0.0114, G_loss: 0.2809\n",
      "  Batch [700/1299] D_loss: -0.4422, G_loss: -1.2190\n",
      "  Batch [710/1299] D_loss: -0.0399, G_loss: 0.0717\n",
      "  Batch [720/1299] D_loss: -0.1312, G_loss: -0.4408\n",
      "  Batch [730/1299] D_loss: -1.0103, G_loss: -1.9593\n",
      "  Batch [740/1299] D_loss: -1.0480, G_loss: -0.4191\n",
      "  Batch [750/1299] D_loss: -1.2108, G_loss: -2.2016\n",
      "  Batch [760/1299] D_loss: -0.3476, G_loss: -0.1100\n",
      "  Batch [770/1299] D_loss: -0.2586, G_loss: -0.0934\n",
      "  Batch [780/1299] D_loss: -0.5315, G_loss: 0.1058\n",
      "  Batch [790/1299] D_loss: -0.4017, G_loss: 0.0695\n",
      "  Batch [800/1299] D_loss: -0.7086, G_loss: 0.1205\n",
      "  Batch [810/1299] D_loss: -0.0484, G_loss: 0.2214\n",
      "  Batch [820/1299] D_loss: -0.1889, G_loss: 0.2338\n",
      "  Batch [830/1299] D_loss: -0.0832, G_loss: 0.5597\n",
      "  Batch [840/1299] D_loss: -0.0110, G_loss: 0.5397\n",
      "  Batch [850/1299] D_loss: -0.1303, G_loss: 0.5265\n",
      "  Batch [860/1299] D_loss: -0.0738, G_loss: 0.3972\n",
      "  Batch [870/1299] D_loss: -0.8849, G_loss: -1.0509\n",
      "  Batch [880/1299] D_loss: -0.8634, G_loss: -1.6591\n",
      "  Batch [890/1299] D_loss: -1.3073, G_loss: -0.3242\n",
      "  Batch [900/1299] D_loss: -1.2120, G_loss: -1.2020\n",
      "  Batch [910/1299] D_loss: -0.0417, G_loss: 0.2487\n",
      "  Batch [920/1299] D_loss: -0.1682, G_loss: 0.4799\n",
      "  Batch [930/1299] D_loss: -0.1223, G_loss: 0.6740\n",
      "  Batch [940/1299] D_loss: 0.2094, G_loss: 0.6876\n",
      "  Batch [950/1299] D_loss: -0.2831, G_loss: 0.7147\n",
      "  Batch [960/1299] D_loss: 0.0303, G_loss: 0.2975\n",
      "  Batch [970/1299] D_loss: -0.0596, G_loss: 0.2902\n",
      "  Batch [980/1299] D_loss: 0.2713, G_loss: -1.0108\n",
      "  Batch [990/1299] D_loss: -0.3531, G_loss: -0.0026\n",
      "  Batch [1000/1299] D_loss: -0.4833, G_loss: -1.0690\n",
      "  Batch [1010/1299] D_loss: -1.1922, G_loss: -1.1232\n",
      "  Batch [1020/1299] D_loss: -0.7311, G_loss: -0.3330\n",
      "  Batch [1030/1299] D_loss: -0.1285, G_loss: 0.3134\n",
      "  Batch [1040/1299] D_loss: -0.0957, G_loss: 0.5678\n",
      "  Batch [1050/1299] D_loss: -0.0039, G_loss: 0.8547\n",
      "  Batch [1060/1299] D_loss: -0.0447, G_loss: 0.3892\n",
      "  Batch [1070/1299] D_loss: 0.0083, G_loss: 0.6976\n",
      "  Batch [1080/1299] D_loss: -0.1137, G_loss: 0.6140\n",
      "  Batch [1090/1299] D_loss: -0.0530, G_loss: 0.2718\n",
      "  Batch [1100/1299] D_loss: -0.0260, G_loss: 0.0910\n",
      "  Batch [1110/1299] D_loss: -0.0236, G_loss: 0.1569\n",
      "  Batch [1120/1299] D_loss: -0.0335, G_loss: 0.2826\n",
      "  Batch [1130/1299] D_loss: -0.1452, G_loss: 0.2843\n",
      "  Batch [1140/1299] D_loss: -0.0492, G_loss: 0.3502\n",
      "  Batch [1150/1299] D_loss: -0.0090, G_loss: 0.3896\n",
      "  Batch [1160/1299] D_loss: -0.0366, G_loss: 0.1978\n",
      "  Batch [1170/1299] D_loss: -0.7648, G_loss: -1.5435\n",
      "  Batch [1180/1299] D_loss: -0.1612, G_loss: 0.1057\n",
      "  Batch [1190/1299] D_loss: -0.1933, G_loss: -0.7331\n",
      "  Batch [1200/1299] D_loss: -0.5616, G_loss: -0.6158\n",
      "  Batch [1210/1299] D_loss: -0.2384, G_loss: -0.3866\n",
      "  Batch [1220/1299] D_loss: -0.0620, G_loss: 0.2525\n",
      "  Batch [1230/1299] D_loss: -0.1036, G_loss: 0.4456\n",
      "  Batch [1240/1299] D_loss: -0.1013, G_loss: 0.6491\n",
      "  Batch [1250/1299] D_loss: -0.0352, G_loss: 0.4754\n",
      "  Batch [1260/1299] D_loss: -0.1520, G_loss: 0.4211\n",
      "  Batch [1270/1299] D_loss: -0.0628, G_loss: 0.3346\n",
      "  Batch [1280/1299] D_loss: -0.2720, G_loss: -0.2578\n",
      "  Batch [1290/1299] D_loss: -0.1617, G_loss: -0.3537\n",
      "\n",
      "Epoch 94 Summary:\n",
      "  Average D_loss: -0.1500\n",
      "  Average G_loss: -0.0785\n",
      "\n",
      "Epoch [95/100]\n",
      "  Batch [0/1299] D_loss: -0.0051, G_loss: 0.2955\n",
      "  Batch [10/1299] D_loss: -0.1249, G_loss: 0.2848\n",
      "  Batch [20/1299] D_loss: -0.2434, G_loss: 0.4167\n",
      "  Batch [30/1299] D_loss: -0.3214, G_loss: 0.6275\n",
      "  Batch [40/1299] D_loss: -0.1425, G_loss: 0.4487\n",
      "  Batch [50/1299] D_loss: -0.0149, G_loss: 0.1642\n",
      "  Batch [60/1299] D_loss: -0.0319, G_loss: 0.0352\n",
      "  Batch [70/1299] D_loss: -0.6591, G_loss: -1.5949\n",
      "  Batch [80/1299] D_loss: -0.2584, G_loss: 0.0648\n",
      "  Batch [90/1299] D_loss: -0.4292, G_loss: -0.4893\n",
      "  Batch [100/1299] D_loss: -3.5822, G_loss: -4.9107\n",
      "  Batch [110/1299] D_loss: -0.0332, G_loss: 0.1734\n",
      "  Batch [120/1299] D_loss: -0.0581, G_loss: 0.4240\n",
      "  Batch [130/1299] D_loss: -0.1640, G_loss: 0.4882\n",
      "  Batch [140/1299] D_loss: -0.0846, G_loss: 0.6824\n",
      "  Batch [150/1299] D_loss: 0.0553, G_loss: 0.5023\n",
      "  Batch [160/1299] D_loss: -0.0003, G_loss: 0.5222\n",
      "  Batch [170/1299] D_loss: -0.0060, G_loss: 0.2602\n",
      "  Batch [180/1299] D_loss: -0.1065, G_loss: -0.0414\n",
      "  Batch [190/1299] D_loss: -0.4772, G_loss: -0.0265\n",
      "  Batch [200/1299] D_loss: -0.1089, G_loss: 0.2972\n",
      "  Batch [210/1299] D_loss: -0.2275, G_loss: 0.5763\n",
      "  Batch [220/1299] D_loss: -0.2534, G_loss: 0.5445\n",
      "  Batch [230/1299] D_loss: 0.0220, G_loss: 0.4068\n",
      "  Batch [240/1299] D_loss: -0.0380, G_loss: 0.4829\n",
      "  Batch [250/1299] D_loss: -0.0186, G_loss: 0.3361\n",
      "  Batch [260/1299] D_loss: -0.4687, G_loss: -4.8772\n",
      "  Batch [270/1299] D_loss: -0.0372, G_loss: -0.1413\n",
      "  Batch [280/1299] D_loss: -0.0112, G_loss: 0.1667\n",
      "  Batch [290/1299] D_loss: 0.0161, G_loss: 0.3251\n",
      "  Batch [300/1299] D_loss: -0.0384, G_loss: 0.3541\n",
      "  Batch [310/1299] D_loss: -0.1043, G_loss: 0.4456\n",
      "  Batch [320/1299] D_loss: -0.0764, G_loss: 0.5522\n",
      "  Batch [330/1299] D_loss: -0.0483, G_loss: 0.3439\n",
      "  Batch [340/1299] D_loss: -0.1187, G_loss: 0.2754\n",
      "  Batch [350/1299] D_loss: -1.9352, G_loss: -3.9085\n",
      "  Batch [360/1299] D_loss: -0.0782, G_loss: 0.1741\n",
      "  Batch [370/1299] D_loss: -0.0542, G_loss: 0.1745\n",
      "  Batch [380/1299] D_loss: -0.1847, G_loss: 0.0640\n",
      "  Batch [390/1299] D_loss: -0.0328, G_loss: -0.1464\n",
      "  Batch [400/1299] D_loss: -0.0076, G_loss: 0.0843\n",
      "  Batch [410/1299] D_loss: -0.0692, G_loss: 0.2300\n",
      "  Batch [420/1299] D_loss: -0.0711, G_loss: 0.3498\n",
      "  Batch [430/1299] D_loss: -0.1279, G_loss: 0.4297\n",
      "  Batch [440/1299] D_loss: -0.0298, G_loss: 0.3265\n",
      "  Batch [450/1299] D_loss: -0.0718, G_loss: 0.2243\n",
      "  Batch [460/1299] D_loss: -0.0199, G_loss: 0.1176\n",
      "  Batch [470/1299] D_loss: -0.0208, G_loss: 0.0930\n",
      "  Batch [480/1299] D_loss: -0.0144, G_loss: 0.0828\n",
      "  Batch [490/1299] D_loss: -0.0429, G_loss: 0.1906\n",
      "  Batch [500/1299] D_loss: -0.0666, G_loss: 0.2247\n",
      "  Batch [510/1299] D_loss: -2.1710, G_loss: -3.6402\n",
      "  Batch [520/1299] D_loss: 0.0234, G_loss: 0.1295\n",
      "  Batch [530/1299] D_loss: -0.0683, G_loss: 0.2627\n",
      "  Batch [540/1299] D_loss: 0.0698, G_loss: 0.2213\n",
      "  Batch [550/1299] D_loss: 0.0177, G_loss: 0.2808\n",
      "  Batch [560/1299] D_loss: -0.9989, G_loss: -3.6779\n",
      "  Batch [570/1299] D_loss: 0.0182, G_loss: 0.0865\n",
      "  Batch [580/1299] D_loss: 0.0060, G_loss: 0.1136\n",
      "  Batch [590/1299] D_loss: -0.0450, G_loss: 0.1815\n",
      "  Batch [600/1299] D_loss: -0.0395, G_loss: 0.1457\n",
      "  Batch [610/1299] D_loss: -0.7560, G_loss: -0.0381\n",
      "  Batch [620/1299] D_loss: 0.0250, G_loss: 0.1317\n",
      "  Batch [630/1299] D_loss: -0.0113, G_loss: 0.1373\n",
      "  Batch [640/1299] D_loss: -1.1502, G_loss: -2.1306\n",
      "  Batch [650/1299] D_loss: 0.0461, G_loss: 0.1602\n",
      "  Batch [660/1299] D_loss: -0.9352, G_loss: -0.8108\n",
      "  Batch [670/1299] D_loss: -0.2336, G_loss: -0.0605\n",
      "  Batch [680/1299] D_loss: -0.3988, G_loss: -0.9125\n",
      "  Batch [690/1299] D_loss: -0.0296, G_loss: 0.0410\n",
      "  Batch [700/1299] D_loss: -0.0940, G_loss: 0.2954\n",
      "  Batch [710/1299] D_loss: -0.2754, G_loss: 0.6680\n",
      "  Batch [720/1299] D_loss: -0.1854, G_loss: 0.4764\n",
      "  Batch [730/1299] D_loss: -0.0419, G_loss: 0.3454\n",
      "  Batch [740/1299] D_loss: -0.0118, G_loss: 0.3861\n",
      "  Batch [750/1299] D_loss: -0.0571, G_loss: 0.2714\n",
      "  Batch [760/1299] D_loss: -0.6336, G_loss: -1.0035\n",
      "  Batch [770/1299] D_loss: -0.0233, G_loss: 0.1037\n",
      "  Batch [780/1299] D_loss: -0.0303, G_loss: 0.2108\n",
      "  Batch [790/1299] D_loss: 0.0232, G_loss: 0.3139\n",
      "  Batch [800/1299] D_loss: 0.0384, G_loss: 0.3360\n",
      "  Batch [810/1299] D_loss: -0.0779, G_loss: 0.4243\n",
      "  Batch [820/1299] D_loss: -0.0583, G_loss: 0.2048\n",
      "  Batch [830/1299] D_loss: -0.2341, G_loss: -0.5165\n",
      "  Batch [840/1299] D_loss: -1.2418, G_loss: -1.5411\n",
      "  Batch [850/1299] D_loss: -0.8707, G_loss: -1.2324\n",
      "  Batch [860/1299] D_loss: -0.0351, G_loss: 0.1181\n",
      "  Batch [870/1299] D_loss: -0.6061, G_loss: -1.4960\n",
      "  Batch [880/1299] D_loss: -0.6249, G_loss: -0.1212\n",
      "  Batch [890/1299] D_loss: -0.8805, G_loss: 0.0712\n",
      "  Batch [900/1299] D_loss: -0.6960, G_loss: -0.6405\n",
      "  Batch [910/1299] D_loss: -0.0604, G_loss: 0.1537\n",
      "  Batch [920/1299] D_loss: -0.1311, G_loss: 0.4550\n",
      "  Batch [930/1299] D_loss: -0.2828, G_loss: 0.8747\n",
      "  Batch [940/1299] D_loss: -0.1125, G_loss: 0.9058\n",
      "  Batch [950/1299] D_loss: -0.0259, G_loss: 0.7689\n",
      "  Batch [960/1299] D_loss: -0.0015, G_loss: 0.7033\n",
      "  Batch [970/1299] D_loss: -0.1083, G_loss: 0.5297\n",
      "  Batch [980/1299] D_loss: -0.0394, G_loss: 0.0792\n",
      "  Batch [990/1299] D_loss: -0.1520, G_loss: -0.2556\n",
      "  Batch [1000/1299] D_loss: -1.1439, G_loss: -2.3863\n",
      "  Batch [1010/1299] D_loss: -0.6225, G_loss: -0.7462\n",
      "  Batch [1020/1299] D_loss: -0.1188, G_loss: 0.2428\n",
      "  Batch [1030/1299] D_loss: -0.0444, G_loss: 0.3601\n",
      "  Batch [1040/1299] D_loss: -0.1234, G_loss: 0.4534\n",
      "  Batch [1050/1299] D_loss: -0.1223, G_loss: 0.4766\n",
      "  Batch [1060/1299] D_loss: -0.0623, G_loss: 0.5527\n",
      "  Batch [1070/1299] D_loss: -0.0642, G_loss: 0.2672\n",
      "  Batch [1080/1299] D_loss: -1.0965, G_loss: -1.5721\n",
      "  Batch [1090/1299] D_loss: -0.0944, G_loss: 0.1746\n",
      "  Batch [1100/1299] D_loss: -0.1113, G_loss: 0.3551\n",
      "  Batch [1110/1299] D_loss: -0.0440, G_loss: 0.4185\n",
      "  Batch [1120/1299] D_loss: 0.0359, G_loss: 0.2809\n",
      "  Batch [1130/1299] D_loss: -0.0204, G_loss: 0.3211\n",
      "  Batch [1140/1299] D_loss: -0.0875, G_loss: 0.1541\n",
      "  Batch [1150/1299] D_loss: -0.6630, G_loss: -0.4493\n",
      "  Batch [1160/1299] D_loss: -0.1880, G_loss: 0.1038\n",
      "  Batch [1170/1299] D_loss: -0.0507, G_loss: 0.0753\n",
      "  Batch [1180/1299] D_loss: -2.1860, G_loss: -1.0294\n",
      "  Batch [1190/1299] D_loss: -0.5697, G_loss: -0.1198\n",
      "  Batch [1200/1299] D_loss: -0.2497, G_loss: -0.1615\n",
      "  Batch [1210/1299] D_loss: -0.2037, G_loss: 0.0536\n",
      "  Batch [1220/1299] D_loss: -0.5068, G_loss: -0.4345\n",
      "  Batch [1230/1299] D_loss: -1.1105, G_loss: -0.8660\n",
      "  Batch [1240/1299] D_loss: -0.5053, G_loss: -0.7405\n",
      "  Batch [1250/1299] D_loss: -0.1558, G_loss: 0.0617\n",
      "  Batch [1260/1299] D_loss: -0.0436, G_loss: 0.1345\n",
      "  Batch [1270/1299] D_loss: -0.0877, G_loss: 0.2598\n",
      "  Batch [1280/1299] D_loss: -0.2737, G_loss: 0.5225\n",
      "  Batch [1290/1299] D_loss: -0.1573, G_loss: 0.6692\n",
      "\n",
      "Epoch 95 Summary:\n",
      "  Average D_loss: -0.1326\n",
      "  Average G_loss: -0.0604\n",
      "\n",
      "Epoch [96/100]\n",
      "  Batch [0/1299] D_loss: -0.3777, G_loss: 0.8388\n",
      "  Batch [10/1299] D_loss: -0.0942, G_loss: 0.3938\n",
      "  Batch [20/1299] D_loss: -0.7004, G_loss: -0.3384\n",
      "  Batch [30/1299] D_loss: -0.0258, G_loss: 0.1064\n",
      "  Batch [40/1299] D_loss: -0.1118, G_loss: 0.2615\n",
      "  Batch [50/1299] D_loss: -0.2460, G_loss: 0.6639\n",
      "  Batch [60/1299] D_loss: 0.0254, G_loss: 0.5355\n",
      "  Batch [70/1299] D_loss: -0.0187, G_loss: 0.6399\n",
      "  Batch [80/1299] D_loss: -0.2456, G_loss: 0.7913\n",
      "  Batch [90/1299] D_loss: 0.0730, G_loss: 0.3035\n",
      "  Batch [100/1299] D_loss: -0.0631, G_loss: 0.3500\n",
      "  Batch [110/1299] D_loss: -0.4870, G_loss: -0.5152\n",
      "  Batch [120/1299] D_loss: -0.9733, G_loss: -1.5093\n",
      "  Batch [130/1299] D_loss: -0.2423, G_loss: 0.0626\n",
      "  Batch [140/1299] D_loss: -0.7726, G_loss: -0.7200\n",
      "  Batch [150/1299] D_loss: -0.0482, G_loss: 0.2164\n",
      "  Batch [160/1299] D_loss: -0.0427, G_loss: 0.2414\n",
      "  Batch [170/1299] D_loss: -0.1073, G_loss: 0.5037\n",
      "  Batch [180/1299] D_loss: -0.1163, G_loss: 0.4554\n",
      "  Batch [190/1299] D_loss: -0.2477, G_loss: 0.4545\n",
      "  Batch [200/1299] D_loss: -0.8698, G_loss: -1.9538\n",
      "  Batch [210/1299] D_loss: -0.0151, G_loss: 0.1665\n",
      "  Batch [220/1299] D_loss: -0.0885, G_loss: 0.3612\n",
      "  Batch [230/1299] D_loss: -0.0996, G_loss: 0.7331\n",
      "  Batch [240/1299] D_loss: -0.2284, G_loss: 0.7427\n",
      "  Batch [250/1299] D_loss: 0.0012, G_loss: 0.4116\n",
      "  Batch [260/1299] D_loss: 0.0706, G_loss: 0.3533\n",
      "  Batch [270/1299] D_loss: -0.1382, G_loss: 0.2515\n",
      "  Batch [280/1299] D_loss: 0.6007, G_loss: -2.2373\n",
      "  Batch [290/1299] D_loss: -1.6162, G_loss: -2.2112\n",
      "  Batch [300/1299] D_loss: 0.0081, G_loss: 0.1530\n",
      "  Batch [310/1299] D_loss: -0.0840, G_loss: 0.2989\n",
      "  Batch [320/1299] D_loss: -0.1428, G_loss: 0.5968\n",
      "  Batch [330/1299] D_loss: -0.1204, G_loss: 0.6296\n",
      "  Batch [340/1299] D_loss: -0.1323, G_loss: 0.5832\n",
      "  Batch [350/1299] D_loss: -0.1767, G_loss: 0.4455\n",
      "  Batch [360/1299] D_loss: -0.1168, G_loss: 0.3085\n",
      "  Batch [370/1299] D_loss: -3.4988, G_loss: -9.2276\n",
      "  Batch [380/1299] D_loss: -0.0287, G_loss: 0.1295\n",
      "  Batch [390/1299] D_loss: -0.1026, G_loss: 0.2839\n",
      "  Batch [400/1299] D_loss: -0.1066, G_loss: 0.4251\n",
      "  Batch [410/1299] D_loss: -0.0281, G_loss: 0.4454\n",
      "  Batch [420/1299] D_loss: 0.0567, G_loss: 0.3894\n",
      "  Batch [430/1299] D_loss: -0.0442, G_loss: 0.3136\n",
      "  Batch [440/1299] D_loss: -0.1265, G_loss: 0.3687\n",
      "  Batch [450/1299] D_loss: -1.3916, G_loss: -2.5348\n",
      "  Batch [460/1299] D_loss: -0.0357, G_loss: 0.0112\n",
      "  Batch [470/1299] D_loss: -0.2893, G_loss: -0.1259\n",
      "  Batch [480/1299] D_loss: -1.3977, G_loss: -0.3889\n",
      "  Batch [490/1299] D_loss: -0.0760, G_loss: -0.0427\n",
      "  Batch [500/1299] D_loss: -0.3745, G_loss: -0.1566\n",
      "  Batch [510/1299] D_loss: -0.0089, G_loss: 0.1228\n",
      "  Batch [520/1299] D_loss: -0.0533, G_loss: 0.2652\n",
      "  Batch [530/1299] D_loss: -0.1726, G_loss: 0.3148\n",
      "  Batch [540/1299] D_loss: -0.0478, G_loss: 0.2805\n",
      "  Batch [550/1299] D_loss: -0.0202, G_loss: 0.4233\n",
      "  Batch [560/1299] D_loss: -0.0661, G_loss: 0.2488\n",
      "  Batch [570/1299] D_loss: -0.0828, G_loss: 0.0004\n",
      "  Batch [580/1299] D_loss: -0.0174, G_loss: 0.2065\n",
      "  Batch [590/1299] D_loss: -0.0928, G_loss: 0.2991\n",
      "  Batch [600/1299] D_loss: -0.0798, G_loss: 0.3011\n",
      "  Batch [610/1299] D_loss: -0.0242, G_loss: 0.3546\n",
      "  Batch [620/1299] D_loss: -0.0269, G_loss: 0.1967\n",
      "  Batch [630/1299] D_loss: -0.3957, G_loss: -0.2973\n",
      "  Batch [640/1299] D_loss: -0.0482, G_loss: 0.2042\n",
      "  Batch [650/1299] D_loss: -0.0173, G_loss: 0.2177\n",
      "  Batch [660/1299] D_loss: -0.0828, G_loss: 0.3580\n",
      "  Batch [670/1299] D_loss: -0.0996, G_loss: 0.3210\n",
      "  Batch [680/1299] D_loss: 0.0359, G_loss: 0.3137\n",
      "  Batch [690/1299] D_loss: -1.9050, G_loss: -5.3469\n",
      "  Batch [700/1299] D_loss: -0.0333, G_loss: 0.0792\n",
      "  Batch [710/1299] D_loss: -0.0233, G_loss: 0.1405\n",
      "  Batch [720/1299] D_loss: -0.1314, G_loss: 0.2143\n",
      "  Batch [730/1299] D_loss: -0.2144, G_loss: 0.2836\n",
      "  Batch [740/1299] D_loss: 0.1020, G_loss: 0.2951\n",
      "  Batch [750/1299] D_loss: -0.1262, G_loss: 0.3120\n",
      "  Batch [760/1299] D_loss: -0.4232, G_loss: -1.6530\n",
      "  Batch [770/1299] D_loss: -1.2061, G_loss: -1.4498\n",
      "  Batch [780/1299] D_loss: -0.0703, G_loss: 0.1858\n",
      "  Batch [790/1299] D_loss: -0.0104, G_loss: 0.1626\n",
      "  Batch [800/1299] D_loss: -0.0673, G_loss: 0.3141\n",
      "  Batch [810/1299] D_loss: 0.0181, G_loss: 0.2847\n",
      "  Batch [820/1299] D_loss: -0.0988, G_loss: 0.3219\n",
      "  Batch [830/1299] D_loss: -0.0421, G_loss: 0.2063\n",
      "  Batch [840/1299] D_loss: -0.9004, G_loss: -3.0342\n",
      "  Batch [850/1299] D_loss: -0.4429, G_loss: -0.3438\n",
      "  Batch [860/1299] D_loss: -0.0124, G_loss: 0.0569\n",
      "  Batch [870/1299] D_loss: -0.0989, G_loss: 0.2372\n",
      "  Batch [880/1299] D_loss: -0.0304, G_loss: 0.1822\n",
      "  Batch [890/1299] D_loss: -0.0576, G_loss: 0.2742\n",
      "  Batch [900/1299] D_loss: -1.5981, G_loss: -3.5273\n",
      "  Batch [910/1299] D_loss: -0.0075, G_loss: 0.3090\n",
      "  Batch [920/1299] D_loss: -0.1750, G_loss: 0.3840\n",
      "  Batch [930/1299] D_loss: 0.0371, G_loss: 0.5545\n",
      "  Batch [940/1299] D_loss: -0.0496, G_loss: 0.5249\n",
      "  Batch [950/1299] D_loss: -0.1318, G_loss: 0.5665\n",
      "  Batch [960/1299] D_loss: -0.0498, G_loss: 0.1361\n",
      "  Batch [970/1299] D_loss: -2.3779, G_loss: -1.3193\n",
      "  Batch [980/1299] D_loss: -0.0188, G_loss: 0.1391\n",
      "  Batch [990/1299] D_loss: -0.0013, G_loss: 0.1701\n",
      "  Batch [1000/1299] D_loss: 0.0131, G_loss: 0.1267\n",
      "  Batch [1010/1299] D_loss: -1.9527, G_loss: -1.3293\n",
      "  Batch [1020/1299] D_loss: -0.3786, G_loss: 0.1290\n",
      "  Batch [1030/1299] D_loss: -0.4252, G_loss: -0.4727\n",
      "  Batch [1040/1299] D_loss: -0.0613, G_loss: 0.1677\n",
      "  Batch [1050/1299] D_loss: -0.0976, G_loss: 0.2794\n",
      "  Batch [1060/1299] D_loss: -0.0802, G_loss: 0.4092\n",
      "  Batch [1070/1299] D_loss: -0.0942, G_loss: 0.3460\n",
      "  Batch [1080/1299] D_loss: -0.0744, G_loss: 0.2622\n",
      "  Batch [1090/1299] D_loss: -0.0786, G_loss: 0.2237\n",
      "  Batch [1100/1299] D_loss: -1.3949, G_loss: -2.2389\n",
      "  Batch [1110/1299] D_loss: -0.4297, G_loss: -0.0967\n",
      "  Batch [1120/1299] D_loss: -1.2405, G_loss: -0.8537\n",
      "  Batch [1130/1299] D_loss: -0.0081, G_loss: -0.6646\n",
      "  Batch [1140/1299] D_loss: -0.5902, G_loss: -0.1469\n",
      "  Batch [1150/1299] D_loss: -0.0120, G_loss: 0.1265\n",
      "  Batch [1160/1299] D_loss: -0.0792, G_loss: 0.3141\n",
      "  Batch [1170/1299] D_loss: -0.0526, G_loss: 0.4838\n",
      "  Batch [1180/1299] D_loss: -0.1912, G_loss: 0.6239\n",
      "  Batch [1190/1299] D_loss: 0.0431, G_loss: 0.5450\n",
      "  Batch [1200/1299] D_loss: -0.0151, G_loss: 0.3182\n",
      "  Batch [1210/1299] D_loss: -0.6111, G_loss: -3.0336\n",
      "  Batch [1220/1299] D_loss: -0.2885, G_loss: 0.0916\n",
      "  Batch [1230/1299] D_loss: -0.5858, G_loss: -1.1199\n",
      "  Batch [1240/1299] D_loss: -0.0491, G_loss: 0.2773\n",
      "  Batch [1250/1299] D_loss: -0.1521, G_loss: 0.6169\n",
      "  Batch [1260/1299] D_loss: -0.0754, G_loss: 0.4529\n",
      "  Batch [1270/1299] D_loss: -0.0316, G_loss: 0.3641\n",
      "  Batch [1280/1299] D_loss: -0.1349, G_loss: 0.4542\n",
      "  Batch [1290/1299] D_loss: -0.8448, G_loss: -2.6450\n",
      "\n",
      "Epoch 96 Summary:\n",
      "  Average D_loss: -0.1277\n",
      "  Average G_loss: -0.0685\n",
      "\n",
      "Epoch [97/100]\n",
      "  Batch [0/1299] D_loss: -0.0277, G_loss: 0.0609\n",
      "  Batch [10/1299] D_loss: -0.0848, G_loss: 0.0616\n",
      "  Batch [20/1299] D_loss: -0.0756, G_loss: 0.1687\n",
      "  Batch [30/1299] D_loss: -0.1407, G_loss: 0.4134\n",
      "  Batch [40/1299] D_loss: -0.0904, G_loss: 0.3297\n",
      "  Batch [50/1299] D_loss: -1.7032, G_loss: -2.1887\n",
      "  Batch [60/1299] D_loss: 0.0249, G_loss: 0.1216\n",
      "  Batch [70/1299] D_loss: -0.0645, G_loss: 0.3202\n",
      "  Batch [80/1299] D_loss: -0.1467, G_loss: 0.2135\n",
      "  Batch [90/1299] D_loss: 0.0200, G_loss: 0.2938\n",
      "  Batch [100/1299] D_loss: -0.2159, G_loss: 0.4982\n",
      "  Batch [110/1299] D_loss: 0.0357, G_loss: 0.2842\n",
      "  Batch [120/1299] D_loss: -0.3351, G_loss: -0.3105\n",
      "  Batch [130/1299] D_loss: 0.0183, G_loss: 0.0900\n",
      "  Batch [140/1299] D_loss: -0.0168, G_loss: 0.1289\n",
      "  Batch [150/1299] D_loss: -3.0689, G_loss: -4.3234\n",
      "  Batch [160/1299] D_loss: -0.0417, G_loss: 0.0269\n",
      "  Batch [170/1299] D_loss: -0.6322, G_loss: -0.7013\n",
      "  Batch [180/1299] D_loss: -0.0574, G_loss: 0.3524\n",
      "  Batch [190/1299] D_loss: 0.0568, G_loss: 0.5207\n",
      "  Batch [200/1299] D_loss: 0.0757, G_loss: 0.4565\n",
      "  Batch [210/1299] D_loss: -0.0075, G_loss: 0.6615\n",
      "  Batch [220/1299] D_loss: -0.0517, G_loss: 0.6927\n",
      "  Batch [230/1299] D_loss: -0.0325, G_loss: 0.4225\n",
      "  Batch [240/1299] D_loss: 0.0530, G_loss: 0.1856\n",
      "  Batch [250/1299] D_loss: -0.0275, G_loss: 0.0621\n",
      "  Batch [260/1299] D_loss: 0.0053, G_loss: 0.1481\n",
      "  Batch [270/1299] D_loss: -0.0609, G_loss: 0.1453\n",
      "  Batch [280/1299] D_loss: 0.0549, G_loss: 0.2767\n",
      "  Batch [290/1299] D_loss: 0.0143, G_loss: 0.3394\n",
      "  Batch [300/1299] D_loss: 0.0061, G_loss: 0.1779\n",
      "  Batch [310/1299] D_loss: -0.1723, G_loss: 0.3997\n",
      "  Batch [320/1299] D_loss: -0.0566, G_loss: -0.0691\n",
      "  Batch [330/1299] D_loss: -0.1688, G_loss: 0.0060\n",
      "  Batch [340/1299] D_loss: -1.1009, G_loss: -1.1942\n",
      "  Batch [350/1299] D_loss: -0.5574, G_loss: -0.1281\n",
      "  Batch [360/1299] D_loss: -0.1105, G_loss: 0.0599\n",
      "  Batch [370/1299] D_loss: -0.0346, G_loss: 0.3055\n",
      "  Batch [380/1299] D_loss: -0.1955, G_loss: 0.5403\n",
      "  Batch [390/1299] D_loss: 0.0120, G_loss: 0.4047\n",
      "  Batch [400/1299] D_loss: -0.0777, G_loss: 0.4501\n",
      "  Batch [410/1299] D_loss: -0.0831, G_loss: 0.5857\n",
      "  Batch [420/1299] D_loss: 0.0455, G_loss: 0.3368\n",
      "  Batch [430/1299] D_loss: -1.6380, G_loss: -0.3523\n",
      "  Batch [440/1299] D_loss: 0.0033, G_loss: 0.0969\n",
      "  Batch [450/1299] D_loss: -0.4343, G_loss: 0.0189\n",
      "  Batch [460/1299] D_loss: -1.1262, G_loss: -3.3550\n",
      "  Batch [470/1299] D_loss: -0.6421, G_loss: -0.0223\n",
      "  Batch [480/1299] D_loss: -0.0771, G_loss: -0.0461\n",
      "  Batch [490/1299] D_loss: -0.1410, G_loss: -0.1000\n",
      "  Batch [500/1299] D_loss: -0.1095, G_loss: 0.4344\n",
      "  Batch [510/1299] D_loss: -0.1491, G_loss: 0.5518\n",
      "  Batch [520/1299] D_loss: 0.0438, G_loss: 0.5268\n",
      "  Batch [530/1299] D_loss: -0.1860, G_loss: 0.6163\n",
      "  Batch [540/1299] D_loss: 0.2769, G_loss: 0.4448\n",
      "  Batch [550/1299] D_loss: 0.0502, G_loss: 0.2741\n",
      "  Batch [560/1299] D_loss: -0.1135, G_loss: -0.0015\n",
      "  Batch [570/1299] D_loss: -0.0346, G_loss: 0.0172\n",
      "  Batch [580/1299] D_loss: -1.3634, G_loss: -1.2895\n",
      "  Batch [590/1299] D_loss: -0.9649, G_loss: -0.4976\n",
      "  Batch [600/1299] D_loss: -0.5535, G_loss: -0.1291\n",
      "  Batch [610/1299] D_loss: -0.0958, G_loss: 0.2827\n",
      "  Batch [620/1299] D_loss: -0.1727, G_loss: 0.4846\n",
      "  Batch [630/1299] D_loss: -0.1623, G_loss: 0.7072\n",
      "  Batch [640/1299] D_loss: -0.0316, G_loss: 0.4720\n",
      "  Batch [650/1299] D_loss: -0.0325, G_loss: 0.3374\n",
      "  Batch [660/1299] D_loss: -0.2019, G_loss: -0.0950\n",
      "  Batch [670/1299] D_loss: -0.0693, G_loss: 0.0038\n",
      "  Batch [680/1299] D_loss: -0.0232, G_loss: 0.2015\n",
      "  Batch [690/1299] D_loss: -0.0751, G_loss: 0.2855\n",
      "  Batch [700/1299] D_loss: 0.0096, G_loss: 0.3177\n",
      "  Batch [710/1299] D_loss: -0.0022, G_loss: 0.2559\n",
      "  Batch [720/1299] D_loss: -0.2782, G_loss: -0.7180\n",
      "  Batch [730/1299] D_loss: 0.0363, G_loss: 0.1802\n",
      "  Batch [740/1299] D_loss: 0.0130, G_loss: 0.3560\n",
      "  Batch [750/1299] D_loss: -0.0935, G_loss: 0.6098\n",
      "  Batch [760/1299] D_loss: -0.0322, G_loss: 0.6821\n",
      "  Batch [770/1299] D_loss: -0.2224, G_loss: 0.6403\n",
      "  Batch [780/1299] D_loss: -0.1561, G_loss: 0.5508\n",
      "  Batch [790/1299] D_loss: -0.1197, G_loss: 0.3231\n",
      "  Batch [800/1299] D_loss: -2.7505, G_loss: -1.3986\n",
      "  Batch [810/1299] D_loss: -0.0231, G_loss: 0.1085\n",
      "  Batch [820/1299] D_loss: -0.0184, G_loss: 0.1369\n",
      "  Batch [830/1299] D_loss: -0.8995, G_loss: -2.6080\n",
      "  Batch [840/1299] D_loss: -0.0034, G_loss: 0.0797\n",
      "  Batch [850/1299] D_loss: -1.2563, G_loss: -1.0986\n",
      "  Batch [860/1299] D_loss: -0.6839, G_loss: -2.6535\n",
      "  Batch [870/1299] D_loss: -0.0384, G_loss: 0.1815\n",
      "  Batch [880/1299] D_loss: -0.1081, G_loss: 0.4214\n",
      "  Batch [890/1299] D_loss: -0.1706, G_loss: 0.5270\n",
      "  Batch [900/1299] D_loss: -0.0715, G_loss: 0.5877\n",
      "  Batch [910/1299] D_loss: 0.0328, G_loss: 0.3954\n",
      "  Batch [920/1299] D_loss: -0.0877, G_loss: 0.3405\n",
      "  Batch [930/1299] D_loss: -0.6387, G_loss: -2.3537\n",
      "  Batch [940/1299] D_loss: -0.0549, G_loss: 0.1006\n",
      "  Batch [950/1299] D_loss: 0.0084, G_loss: 0.2022\n",
      "  Batch [960/1299] D_loss: -0.0536, G_loss: 0.1842\n",
      "  Batch [970/1299] D_loss: -0.0482, G_loss: 0.3598\n",
      "  Batch [980/1299] D_loss: -0.1065, G_loss: 0.3754\n",
      "  Batch [990/1299] D_loss: 0.0112, G_loss: 0.3201\n",
      "  Batch [1000/1299] D_loss: -1.5343, G_loss: -2.9792\n",
      "  Batch [1010/1299] D_loss: -0.3419, G_loss: -0.0576\n",
      "  Batch [1020/1299] D_loss: -1.0861, G_loss: -2.4541\n",
      "  Batch [1030/1299] D_loss: -0.0412, G_loss: 0.0700\n",
      "  Batch [1040/1299] D_loss: -0.0739, G_loss: 0.1668\n",
      "  Batch [1050/1299] D_loss: -0.0341, G_loss: 0.5010\n",
      "  Batch [1060/1299] D_loss: -0.1843, G_loss: 0.6997\n",
      "  Batch [1070/1299] D_loss: -0.3738, G_loss: 0.6019\n",
      "  Batch [1080/1299] D_loss: -0.0279, G_loss: 0.3344\n",
      "  Batch [1090/1299] D_loss: -0.5556, G_loss: -0.5146\n",
      "  Batch [1100/1299] D_loss: -0.3488, G_loss: -0.4856\n",
      "  Batch [1110/1299] D_loss: -0.3531, G_loss: -0.2344\n",
      "  Batch [1120/1299] D_loss: -0.1483, G_loss: 0.0451\n",
      "  Batch [1130/1299] D_loss: -0.9547, G_loss: -0.6545\n",
      "  Batch [1140/1299] D_loss: -0.3378, G_loss: -0.4464\n",
      "  Batch [1150/1299] D_loss: 0.0074, G_loss: 0.3154\n",
      "  Batch [1160/1299] D_loss: -0.0744, G_loss: 0.4016\n",
      "  Batch [1170/1299] D_loss: -0.1783, G_loss: 0.6092\n",
      "  Batch [1180/1299] D_loss: -0.0483, G_loss: 0.5328\n",
      "  Batch [1190/1299] D_loss: 0.0655, G_loss: 0.6090\n",
      "  Batch [1200/1299] D_loss: -0.1575, G_loss: 0.4187\n",
      "  Batch [1210/1299] D_loss: -0.1685, G_loss: -1.2044\n",
      "  Batch [1220/1299] D_loss: 0.0011, G_loss: 0.0736\n",
      "  Batch [1230/1299] D_loss: -0.0387, G_loss: 0.1287\n",
      "  Batch [1240/1299] D_loss: -0.0946, G_loss: 0.2508\n",
      "  Batch [1250/1299] D_loss: 0.0375, G_loss: 0.3779\n",
      "  Batch [1260/1299] D_loss: -0.1805, G_loss: 0.3154\n",
      "  Batch [1270/1299] D_loss: -0.1017, G_loss: 0.3148\n",
      "  Batch [1280/1299] D_loss: -1.2187, G_loss: -2.1543\n",
      "  Batch [1290/1299] D_loss: -0.6565, G_loss: -0.1706\n",
      "\n",
      "Epoch 97 Summary:\n",
      "  Average D_loss: -0.1403\n",
      "  Average G_loss: -0.0724\n",
      "\n",
      "Epoch [98/100]\n",
      "  Batch [0/1299] D_loss: -0.9817, G_loss: -0.6916\n",
      "  Batch [10/1299] D_loss: -0.1497, G_loss: 0.1386\n",
      "  Batch [20/1299] D_loss: -0.5856, G_loss: -0.7494\n",
      "  Batch [30/1299] D_loss: -0.1630, G_loss: 0.5578\n",
      "  Batch [40/1299] D_loss: -0.1565, G_loss: 0.7697\n",
      "  Batch [50/1299] D_loss: 0.0215, G_loss: 0.5097\n",
      "  Batch [60/1299] D_loss: -0.0683, G_loss: 0.5939\n",
      "  Batch [70/1299] D_loss: -0.0068, G_loss: 0.2286\n",
      "  Batch [80/1299] D_loss: -0.6425, G_loss: -0.3495\n",
      "  Batch [90/1299] D_loss: 0.0062, G_loss: 0.0777\n",
      "  Batch [100/1299] D_loss: -0.8052, G_loss: 0.1109\n",
      "  Batch [110/1299] D_loss: -0.5247, G_loss: -0.2575\n",
      "  Batch [120/1299] D_loss: -0.7565, G_loss: -0.4873\n",
      "  Batch [130/1299] D_loss: -0.0650, G_loss: 0.1530\n",
      "  Batch [140/1299] D_loss: -0.0656, G_loss: 0.2349\n",
      "  Batch [150/1299] D_loss: -0.1220, G_loss: 0.4654\n",
      "  Batch [160/1299] D_loss: -0.0927, G_loss: 0.4671\n",
      "  Batch [170/1299] D_loss: -0.1493, G_loss: 0.5086\n",
      "  Batch [180/1299] D_loss: -0.0679, G_loss: 0.3566\n",
      "  Batch [190/1299] D_loss: -1.1479, G_loss: -5.7046\n",
      "  Batch [200/1299] D_loss: -0.0099, G_loss: 0.1448\n",
      "  Batch [210/1299] D_loss: -0.1156, G_loss: 0.1999\n",
      "  Batch [220/1299] D_loss: -0.0723, G_loss: 0.2885\n",
      "  Batch [230/1299] D_loss: -0.0440, G_loss: 0.3016\n",
      "  Batch [240/1299] D_loss: -0.0965, G_loss: 0.4387\n",
      "  Batch [250/1299] D_loss: -0.0502, G_loss: 0.2858\n",
      "  Batch [260/1299] D_loss: 0.0145, G_loss: 0.2009\n",
      "  Batch [270/1299] D_loss: -1.0763, G_loss: -3.3620\n",
      "  Batch [280/1299] D_loss: -0.0725, G_loss: -0.2376\n",
      "  Batch [290/1299] D_loss: -0.9396, G_loss: -0.5934\n",
      "  Batch [300/1299] D_loss: -0.1717, G_loss: 0.3217\n",
      "  Batch [310/1299] D_loss: 0.0239, G_loss: 0.4973\n",
      "  Batch [320/1299] D_loss: -0.1998, G_loss: 0.6111\n",
      "  Batch [330/1299] D_loss: -0.1615, G_loss: 0.6785\n",
      "  Batch [340/1299] D_loss: -0.1592, G_loss: 0.6017\n",
      "  Batch [350/1299] D_loss: -0.0966, G_loss: 0.5128\n",
      "  Batch [360/1299] D_loss: 0.0367, G_loss: 0.2541\n",
      "  Batch [370/1299] D_loss: -0.9502, G_loss: -1.3522\n",
      "  Batch [380/1299] D_loss: -0.0028, G_loss: -0.1073\n",
      "  Batch [390/1299] D_loss: -0.0973, G_loss: 0.0659\n",
      "  Batch [400/1299] D_loss: -0.0542, G_loss: 0.0344\n",
      "  Batch [410/1299] D_loss: 0.0539, G_loss: 0.2470\n",
      "  Batch [420/1299] D_loss: -0.0202, G_loss: 0.5224\n",
      "  Batch [430/1299] D_loss: 0.1092, G_loss: 0.3744\n",
      "  Batch [440/1299] D_loss: -0.1830, G_loss: 0.7363\n",
      "  Batch [450/1299] D_loss: -0.0517, G_loss: 0.5646\n",
      "  Batch [460/1299] D_loss: -0.0797, G_loss: 0.3930\n",
      "  Batch [470/1299] D_loss: 0.0183, G_loss: 0.0863\n",
      "  Batch [480/1299] D_loss: -0.0062, G_loss: 0.1431\n",
      "  Batch [490/1299] D_loss: -0.1111, G_loss: 0.3245\n",
      "  Batch [500/1299] D_loss: -0.1321, G_loss: 0.4446\n",
      "  Batch [510/1299] D_loss: 0.0029, G_loss: 0.1805\n",
      "  Batch [520/1299] D_loss: -0.0608, G_loss: 0.2989\n",
      "  Batch [530/1299] D_loss: -1.7372, G_loss: -3.1482\n",
      "  Batch [540/1299] D_loss: -0.0594, G_loss: 0.0323\n",
      "  Batch [550/1299] D_loss: -0.4878, G_loss: 0.0976\n",
      "  Batch [560/1299] D_loss: -0.5884, G_loss: -1.8951\n",
      "  Batch [570/1299] D_loss: -0.4125, G_loss: -0.3552\n",
      "  Batch [580/1299] D_loss: 0.0239, G_loss: 0.3154\n",
      "  Batch [590/1299] D_loss: -0.1113, G_loss: 0.4433\n",
      "  Batch [600/1299] D_loss: 0.0179, G_loss: 0.6084\n",
      "  Batch [610/1299] D_loss: -0.1229, G_loss: 0.4385\n",
      "  Batch [620/1299] D_loss: -0.1605, G_loss: 0.5378\n",
      "  Batch [630/1299] D_loss: -0.1519, G_loss: 0.5717\n",
      "  Batch [640/1299] D_loss: -0.2104, G_loss: 0.4218\n",
      "  Batch [650/1299] D_loss: 0.0507, G_loss: -2.8719\n",
      "  Batch [660/1299] D_loss: -0.0863, G_loss: -0.1463\n",
      "  Batch [670/1299] D_loss: -0.4923, G_loss: 0.1070\n",
      "  Batch [680/1299] D_loss: -0.6371, G_loss: -0.9793\n",
      "  Batch [690/1299] D_loss: 0.0239, G_loss: 0.1792\n",
      "  Batch [700/1299] D_loss: -0.1195, G_loss: 0.3653\n",
      "  Batch [710/1299] D_loss: -0.0883, G_loss: 0.4719\n",
      "  Batch [720/1299] D_loss: -0.0586, G_loss: 0.6459\n",
      "  Batch [730/1299] D_loss: -0.1372, G_loss: 0.6335\n",
      "  Batch [740/1299] D_loss: 0.1450, G_loss: 0.4443\n",
      "  Batch [750/1299] D_loss: -0.0855, G_loss: 0.2322\n",
      "  Batch [760/1299] D_loss: -0.0473, G_loss: 0.1472\n",
      "  Batch [770/1299] D_loss: -1.3507, G_loss: -3.5031\n",
      "  Batch [780/1299] D_loss: -0.1169, G_loss: 0.0250\n",
      "  Batch [790/1299] D_loss: 0.0242, G_loss: 0.1643\n",
      "  Batch [800/1299] D_loss: -0.0760, G_loss: 0.2633\n",
      "  Batch [810/1299] D_loss: -0.1982, G_loss: 0.3471\n",
      "  Batch [820/1299] D_loss: 0.0658, G_loss: 0.4222\n",
      "  Batch [830/1299] D_loss: -0.1543, G_loss: 0.2939\n",
      "  Batch [840/1299] D_loss: -0.0926, G_loss: 0.4222\n",
      "  Batch [850/1299] D_loss: -0.1153, G_loss: -0.2730\n",
      "  Batch [860/1299] D_loss: -0.8217, G_loss: -0.9852\n",
      "  Batch [870/1299] D_loss: -0.1603, G_loss: -0.2859\n",
      "  Batch [880/1299] D_loss: -0.0066, G_loss: 0.1440\n",
      "  Batch [890/1299] D_loss: -2.3601, G_loss: -0.7572\n",
      "  Batch [900/1299] D_loss: -2.2845, G_loss: -4.7133\n",
      "  Batch [910/1299] D_loss: -0.0122, G_loss: 0.0715\n",
      "  Batch [920/1299] D_loss: -0.0746, G_loss: 0.1975\n",
      "  Batch [930/1299] D_loss: -0.1376, G_loss: 0.4645\n",
      "  Batch [940/1299] D_loss: -0.2145, G_loss: 0.5739\n",
      "  Batch [950/1299] D_loss: -0.1145, G_loss: 0.6646\n",
      "  Batch [960/1299] D_loss: -0.1461, G_loss: 0.7649\n",
      "  Batch [970/1299] D_loss: -0.1900, G_loss: 0.5201\n",
      "  Batch [980/1299] D_loss: -0.0382, G_loss: 0.1611\n",
      "  Batch [990/1299] D_loss: -0.0336, G_loss: -0.9098\n",
      "  Batch [1000/1299] D_loss: -0.4420, G_loss: -2.5091\n",
      "  Batch [1010/1299] D_loss: 0.0396, G_loss: -0.0110\n",
      "  Batch [1020/1299] D_loss: -0.1190, G_loss: 0.2513\n",
      "  Batch [1030/1299] D_loss: -0.1314, G_loss: 0.4516\n",
      "  Batch [1040/1299] D_loss: -0.0692, G_loss: 0.5859\n",
      "  Batch [1050/1299] D_loss: -0.0529, G_loss: 0.4675\n",
      "  Batch [1060/1299] D_loss: -0.1273, G_loss: 0.5051\n",
      "  Batch [1070/1299] D_loss: 0.0877, G_loss: 0.4593\n",
      "  Batch [1080/1299] D_loss: -0.3039, G_loss: -0.4357\n",
      "  Batch [1090/1299] D_loss: -0.2999, G_loss: -0.7052\n",
      "  Batch [1100/1299] D_loss: -0.1632, G_loss: 0.0204\n",
      "  Batch [1110/1299] D_loss: -0.2776, G_loss: 0.0210\n",
      "  Batch [1120/1299] D_loss: -0.6888, G_loss: -2.3807\n",
      "  Batch [1130/1299] D_loss: 0.0041, G_loss: 0.1105\n",
      "  Batch [1140/1299] D_loss: 0.0000, G_loss: 0.1907\n",
      "  Batch [1150/1299] D_loss: -0.1081, G_loss: 0.3361\n",
      "  Batch [1160/1299] D_loss: -0.0651, G_loss: 0.5003\n",
      "  Batch [1170/1299] D_loss: -0.1472, G_loss: 0.4751\n",
      "  Batch [1180/1299] D_loss: -0.1036, G_loss: 0.3569\n",
      "  Batch [1190/1299] D_loss: -0.0467, G_loss: 0.1856\n",
      "  Batch [1200/1299] D_loss: 0.0065, G_loss: 0.0744\n",
      "  Batch [1210/1299] D_loss: -0.0754, G_loss: 0.1924\n",
      "  Batch [1220/1299] D_loss: -0.0583, G_loss: 0.1645\n",
      "  Batch [1230/1299] D_loss: -0.0468, G_loss: 0.2380\n",
      "  Batch [1240/1299] D_loss: -0.1077, G_loss: 0.3596\n",
      "  Batch [1250/1299] D_loss: -0.0726, G_loss: 0.1927\n",
      "  Batch [1260/1299] D_loss: -0.1934, G_loss: -0.2375\n",
      "  Batch [1270/1299] D_loss: -0.3653, G_loss: -0.8652\n",
      "  Batch [1280/1299] D_loss: -1.7952, G_loss: -1.2723\n",
      "  Batch [1290/1299] D_loss: -0.7021, G_loss: -0.4350\n",
      "\n",
      "Epoch 98 Summary:\n",
      "  Average D_loss: -0.1257\n",
      "  Average G_loss: -0.0588\n",
      "\n",
      "Epoch [99/100]\n",
      "  Batch [0/1299] D_loss: -0.0151, G_loss: 0.2298\n",
      "  Batch [10/1299] D_loss: -0.0027, G_loss: 0.3407\n",
      "  Batch [20/1299] D_loss: -0.1236, G_loss: 0.4914\n",
      "  Batch [30/1299] D_loss: -0.1531, G_loss: 0.4815\n",
      "  Batch [40/1299] D_loss: -0.1488, G_loss: 0.3313\n",
      "  Batch [50/1299] D_loss: -0.0630, G_loss: 0.2311\n",
      "  Batch [60/1299] D_loss: -0.1350, G_loss: 0.0052\n",
      "  Batch [70/1299] D_loss: -0.3738, G_loss: -0.0340\n",
      "  Batch [80/1299] D_loss: -0.8986, G_loss: -0.1731\n",
      "  Batch [90/1299] D_loss: -0.1128, G_loss: 0.2492\n",
      "  Batch [100/1299] D_loss: -0.0400, G_loss: 0.3713\n",
      "  Batch [110/1299] D_loss: -0.0049, G_loss: 0.2919\n",
      "  Batch [120/1299] D_loss: -0.0800, G_loss: 0.3246\n",
      "  Batch [130/1299] D_loss: -0.0247, G_loss: 0.2596\n",
      "  Batch [140/1299] D_loss: -0.5602, G_loss: -3.6684\n",
      "  Batch [150/1299] D_loss: -0.3901, G_loss: -1.3428\n",
      "  Batch [160/1299] D_loss: -1.1720, G_loss: -1.3436\n",
      "  Batch [170/1299] D_loss: -0.0240, G_loss: 0.0346\n",
      "  Batch [180/1299] D_loss: -0.0503, G_loss: 0.1683\n",
      "  Batch [190/1299] D_loss: -0.1023, G_loss: 0.2832\n",
      "  Batch [200/1299] D_loss: -0.1692, G_loss: 0.4742\n",
      "  Batch [210/1299] D_loss: -0.1104, G_loss: 0.5097\n",
      "  Batch [220/1299] D_loss: -0.1231, G_loss: 0.4961\n",
      "  Batch [230/1299] D_loss: 0.1068, G_loss: 0.4597\n",
      "  Batch [240/1299] D_loss: -0.0281, G_loss: 0.3054\n",
      "  Batch [250/1299] D_loss: -0.5064, G_loss: -0.1172\n",
      "  Batch [260/1299] D_loss: -0.1339, G_loss: -0.1281\n",
      "  Batch [270/1299] D_loss: -0.0422, G_loss: 0.1714\n",
      "  Batch [280/1299] D_loss: -0.0258, G_loss: 0.2710\n",
      "  Batch [290/1299] D_loss: -0.0923, G_loss: 0.3012\n",
      "  Batch [300/1299] D_loss: -0.0390, G_loss: 0.4266\n",
      "  Batch [310/1299] D_loss: -0.0671, G_loss: 0.3709\n",
      "  Batch [320/1299] D_loss: -0.0550, G_loss: 0.1660\n",
      "  Batch [330/1299] D_loss: -0.0896, G_loss: -0.1621\n",
      "  Batch [340/1299] D_loss: -0.8564, G_loss: -1.5106\n",
      "  Batch [350/1299] D_loss: -0.1377, G_loss: -0.0073\n",
      "  Batch [360/1299] D_loss: -0.0907, G_loss: -0.0065\n",
      "  Batch [370/1299] D_loss: -0.1054, G_loss: 0.1243\n",
      "  Batch [380/1299] D_loss: -0.0473, G_loss: 0.3286\n",
      "  Batch [390/1299] D_loss: -0.0779, G_loss: 0.3277\n",
      "  Batch [400/1299] D_loss: -0.0596, G_loss: 0.2520\n",
      "  Batch [410/1299] D_loss: 0.0177, G_loss: 0.5231\n",
      "  Batch [420/1299] D_loss: -0.2605, G_loss: 0.3038\n",
      "  Batch [430/1299] D_loss: 0.0046, G_loss: 0.3197\n",
      "  Batch [440/1299] D_loss: -0.0077, G_loss: 0.2838\n",
      "  Batch [450/1299] D_loss: -0.5016, G_loss: -0.2343\n",
      "  Batch [460/1299] D_loss: -0.1073, G_loss: 0.0614\n",
      "  Batch [470/1299] D_loss: -0.8118, G_loss: -0.3339\n",
      "  Batch [480/1299] D_loss: -0.1667, G_loss: 0.2216\n",
      "  Batch [490/1299] D_loss: -0.1429, G_loss: 0.5033\n",
      "  Batch [500/1299] D_loss: -0.0452, G_loss: 0.4866\n",
      "  Batch [510/1299] D_loss: -0.1175, G_loss: 0.5772\n",
      "  Batch [520/1299] D_loss: -0.0963, G_loss: 0.5360\n",
      "  Batch [530/1299] D_loss: -0.0841, G_loss: 0.4293\n",
      "  Batch [540/1299] D_loss: -0.1481, G_loss: 0.2590\n",
      "  Batch [550/1299] D_loss: -0.0073, G_loss: 0.0722\n",
      "  Batch [560/1299] D_loss: -1.1318, G_loss: -3.7356\n",
      "  Batch [570/1299] D_loss: -0.3070, G_loss: -0.5517\n",
      "  Batch [580/1299] D_loss: 0.0208, G_loss: 0.1924\n",
      "  Batch [590/1299] D_loss: 0.0045, G_loss: 0.2864\n",
      "  Batch [600/1299] D_loss: -0.0110, G_loss: 0.3569\n",
      "  Batch [610/1299] D_loss: -0.0524, G_loss: 0.4147\n",
      "  Batch [620/1299] D_loss: 0.0158, G_loss: 0.3687\n",
      "  Batch [630/1299] D_loss: -0.0598, G_loss: 0.4049\n",
      "  Batch [640/1299] D_loss: -0.1387, G_loss: 0.1425\n",
      "  Batch [650/1299] D_loss: 0.0589, G_loss: 0.2125\n",
      "  Batch [660/1299] D_loss: -0.0760, G_loss: 0.4128\n",
      "  Batch [670/1299] D_loss: 0.0617, G_loss: 0.4672\n",
      "  Batch [680/1299] D_loss: -0.0840, G_loss: 0.4023\n",
      "  Batch [690/1299] D_loss: -0.0369, G_loss: 0.4442\n",
      "  Batch [700/1299] D_loss: -0.0421, G_loss: 0.1720\n",
      "  Batch [710/1299] D_loss: -0.4060, G_loss: -0.9854\n",
      "  Batch [720/1299] D_loss: -0.4713, G_loss: -0.3297\n",
      "  Batch [730/1299] D_loss: -0.7053, G_loss: 0.0044\n",
      "  Batch [740/1299] D_loss: -0.8304, G_loss: -0.4087\n",
      "  Batch [750/1299] D_loss: -0.5394, G_loss: -0.0743\n",
      "  Batch [760/1299] D_loss: -0.7303, G_loss: -0.3045\n",
      "  Batch [770/1299] D_loss: 0.0179, G_loss: 0.1540\n",
      "  Batch [780/1299] D_loss: 0.0387, G_loss: 0.3935\n",
      "  Batch [790/1299] D_loss: -0.1662, G_loss: 0.4443\n",
      "  Batch [800/1299] D_loss: -0.0672, G_loss: 0.4693\n",
      "  Batch [810/1299] D_loss: 0.0584, G_loss: 0.4229\n",
      "  Batch [820/1299] D_loss: -0.1385, G_loss: 0.4543\n",
      "  Batch [830/1299] D_loss: -0.0580, G_loss: 0.4097\n",
      "  Batch [840/1299] D_loss: -0.0435, G_loss: 0.2177\n",
      "  Batch [850/1299] D_loss: 0.1133, G_loss: -0.1367\n",
      "  Batch [860/1299] D_loss: -0.7628, G_loss: -0.9474\n",
      "  Batch [870/1299] D_loss: -0.6543, G_loss: -0.4233\n",
      "  Batch [880/1299] D_loss: -0.1358, G_loss: 0.1354\n",
      "  Batch [890/1299] D_loss: -0.1647, G_loss: 0.1093\n",
      "  Batch [900/1299] D_loss: -0.3543, G_loss: -0.7726\n",
      "  Batch [910/1299] D_loss: 0.0476, G_loss: 0.1513\n",
      "  Batch [920/1299] D_loss: -0.2852, G_loss: 0.2156\n",
      "  Batch [930/1299] D_loss: -0.2021, G_loss: 0.5099\n",
      "  Batch [940/1299] D_loss: -0.0780, G_loss: 0.4095\n",
      "  Batch [950/1299] D_loss: 0.0339, G_loss: 0.5688\n",
      "  Batch [960/1299] D_loss: 0.0129, G_loss: 0.5632\n",
      "  Batch [970/1299] D_loss: 0.0047, G_loss: 0.5641\n",
      "  Batch [980/1299] D_loss: -0.1512, G_loss: 0.3739\n",
      "  Batch [990/1299] D_loss: -1.0066, G_loss: -2.1751\n",
      "  Batch [1000/1299] D_loss: 0.0489, G_loss: 0.0714\n",
      "  Batch [1010/1299] D_loss: -0.0365, G_loss: 0.2183\n",
      "  Batch [1020/1299] D_loss: -0.1147, G_loss: 0.3348\n",
      "  Batch [1030/1299] D_loss: -0.0256, G_loss: 0.5488\n",
      "  Batch [1040/1299] D_loss: -0.0774, G_loss: 0.5235\n",
      "  Batch [1050/1299] D_loss: 0.0420, G_loss: 0.6232\n",
      "  Batch [1060/1299] D_loss: 0.0235, G_loss: 0.3353\n",
      "  Batch [1070/1299] D_loss: -0.1082, G_loss: 0.2689\n",
      "  Batch [1080/1299] D_loss: -2.0056, G_loss: -4.2665\n",
      "  Batch [1090/1299] D_loss: -1.0633, G_loss: -1.3168\n",
      "  Batch [1100/1299] D_loss: -0.9858, G_loss: -0.6308\n",
      "  Batch [1110/1299] D_loss: -0.2087, G_loss: -0.9010\n",
      "  Batch [1120/1299] D_loss: -0.0601, G_loss: 0.2821\n",
      "  Batch [1130/1299] D_loss: -0.0147, G_loss: 0.4388\n",
      "  Batch [1140/1299] D_loss: 0.0162, G_loss: 0.3889\n",
      "  Batch [1150/1299] D_loss: -0.0736, G_loss: 0.2965\n",
      "  Batch [1160/1299] D_loss: -0.1669, G_loss: 0.4928\n",
      "  Batch [1170/1299] D_loss: -0.5187, G_loss: -2.0205\n",
      "  Batch [1180/1299] D_loss: -0.8901, G_loss: -0.6962\n",
      "  Batch [1190/1299] D_loss: -0.3469, G_loss: -0.1287\n",
      "  Batch [1200/1299] D_loss: -0.0301, G_loss: 0.3283\n",
      "  Batch [1210/1299] D_loss: 0.0573, G_loss: 0.2392\n",
      "  Batch [1220/1299] D_loss: -0.0062, G_loss: 0.3807\n",
      "  Batch [1230/1299] D_loss: -0.0211, G_loss: 0.2825\n",
      "  Batch [1240/1299] D_loss: -0.0397, G_loss: 0.2414\n",
      "  Batch [1250/1299] D_loss: -0.0249, G_loss: 0.2041\n",
      "  Batch [1260/1299] D_loss: -0.3114, G_loss: -0.5976\n",
      "  Batch [1270/1299] D_loss: -0.7398, G_loss: 0.0122\n",
      "  Batch [1280/1299] D_loss: -0.6942, G_loss: -0.1462\n",
      "  Batch [1290/1299] D_loss: -0.5229, G_loss: -1.6066\n",
      "\n",
      "Epoch 99 Summary:\n",
      "  Average D_loss: -0.1423\n",
      "  Average G_loss: -0.0903\n",
      "\n",
      "Epoch [100/100]\n",
      "  Batch [0/1299] D_loss: -0.5542, G_loss: -1.1262\n",
      "  Batch [10/1299] D_loss: -0.6715, G_loss: -0.3185\n",
      "  Batch [20/1299] D_loss: -0.6535, G_loss: -0.0449\n",
      "  Batch [30/1299] D_loss: -0.5226, G_loss: 0.0771\n",
      "  Batch [40/1299] D_loss: -0.1781, G_loss: 0.0452\n",
      "  Batch [50/1299] D_loss: -0.2301, G_loss: 0.0677\n",
      "  Batch [60/1299] D_loss: -0.1897, G_loss: 0.2908\n",
      "  Batch [70/1299] D_loss: 0.0281, G_loss: 0.8880\n",
      "  Batch [80/1299] D_loss: -0.3370, G_loss: 0.9105\n",
      "  Batch [90/1299] D_loss: 0.0522, G_loss: 0.5756\n",
      "  Batch [100/1299] D_loss: 0.0015, G_loss: 0.3996\n",
      "  Batch [110/1299] D_loss: -1.6120, G_loss: -3.1919\n",
      "  Batch [120/1299] D_loss: -1.0745, G_loss: -0.4966\n",
      "  Batch [130/1299] D_loss: -0.1097, G_loss: -0.3943\n",
      "  Batch [140/1299] D_loss: -0.0146, G_loss: 0.1831\n",
      "  Batch [150/1299] D_loss: -0.0626, G_loss: 0.2486\n",
      "  Batch [160/1299] D_loss: -0.1435, G_loss: 0.4274\n",
      "  Batch [170/1299] D_loss: -0.1209, G_loss: 0.3758\n",
      "  Batch [180/1299] D_loss: 0.0056, G_loss: 0.4241\n",
      "  Batch [190/1299] D_loss: -0.0655, G_loss: 0.3673\n",
      "  Batch [200/1299] D_loss: -1.5184, G_loss: -3.5229\n",
      "  Batch [210/1299] D_loss: -0.4381, G_loss: 0.0179\n",
      "  Batch [220/1299] D_loss: 0.0382, G_loss: 0.1934\n",
      "  Batch [230/1299] D_loss: -2.9666, G_loss: -1.9726\n",
      "  Batch [240/1299] D_loss: -0.9966, G_loss: -0.3367\n",
      "  Batch [250/1299] D_loss: -0.5044, G_loss: -0.1327\n",
      "  Batch [260/1299] D_loss: -0.0694, G_loss: 0.1511\n",
      "  Batch [270/1299] D_loss: -0.9887, G_loss: 0.0674\n",
      "  Batch [280/1299] D_loss: -0.8238, G_loss: -0.1918\n",
      "  Batch [290/1299] D_loss: -0.0752, G_loss: 0.2580\n",
      "  Batch [300/1299] D_loss: -0.0157, G_loss: 0.4848\n",
      "  Batch [310/1299] D_loss: -0.1189, G_loss: 0.6248\n",
      "  Batch [320/1299] D_loss: -0.0268, G_loss: 0.4060\n",
      "  Batch [330/1299] D_loss: -0.1175, G_loss: 0.5400\n",
      "  Batch [340/1299] D_loss: -0.0902, G_loss: 0.3540\n",
      "  Batch [350/1299] D_loss: -0.3770, G_loss: -1.3314\n",
      "  Batch [360/1299] D_loss: -0.0026, G_loss: 0.1976\n",
      "  Batch [370/1299] D_loss: -0.1059, G_loss: 0.2986\n",
      "  Batch [380/1299] D_loss: -0.0611, G_loss: 0.3824\n",
      "  Batch [390/1299] D_loss: -0.0520, G_loss: 0.3537\n",
      "  Batch [400/1299] D_loss: -0.0684, G_loss: 0.2671\n",
      "  Batch [410/1299] D_loss: -0.0603, G_loss: 0.3305\n",
      "  Batch [420/1299] D_loss: -0.7741, G_loss: -1.8975\n",
      "  Batch [430/1299] D_loss: -1.3407, G_loss: -1.3546\n",
      "  Batch [440/1299] D_loss: -0.0641, G_loss: 0.1699\n",
      "  Batch [450/1299] D_loss: -0.0655, G_loss: 0.3120\n",
      "  Batch [460/1299] D_loss: 0.0253, G_loss: 0.6156\n",
      "  Batch [470/1299] D_loss: -0.0663, G_loss: 0.5797\n",
      "  Batch [480/1299] D_loss: -0.0805, G_loss: 0.4664\n",
      "  Batch [490/1299] D_loss: 0.0673, G_loss: 0.3757\n",
      "  Batch [500/1299] D_loss: -0.1474, G_loss: 0.4363\n",
      "  Batch [510/1299] D_loss: -2.9047, G_loss: -7.0435\n",
      "  Batch [520/1299] D_loss: -0.0003, G_loss: 0.0754\n",
      "  Batch [530/1299] D_loss: -0.1450, G_loss: 0.3483\n",
      "  Batch [540/1299] D_loss: -0.0069, G_loss: 0.4714\n",
      "  Batch [550/1299] D_loss: -0.0651, G_loss: 0.4418\n",
      "  Batch [560/1299] D_loss: -0.0628, G_loss: 0.6551\n",
      "  Batch [570/1299] D_loss: -0.1921, G_loss: 0.5037\n",
      "  Batch [580/1299] D_loss: -0.3371, G_loss: -0.2638\n",
      "  Batch [590/1299] D_loss: -0.8492, G_loss: -1.2700\n",
      "  Batch [600/1299] D_loss: -2.0429, G_loss: -2.8064\n",
      "  Batch [610/1299] D_loss: -0.2844, G_loss: -0.0678\n",
      "  Batch [620/1299] D_loss: -0.9567, G_loss: -0.7257\n",
      "  Batch [630/1299] D_loss: -0.2007, G_loss: 0.0746\n",
      "  Batch [640/1299] D_loss: 0.0191, G_loss: 0.0936\n",
      "  Batch [650/1299] D_loss: -0.0233, G_loss: 0.1240\n",
      "  Batch [660/1299] D_loss: -0.1731, G_loss: 0.3577\n",
      "  Batch [670/1299] D_loss: -0.2883, G_loss: 0.5123\n",
      "  Batch [680/1299] D_loss: -0.0904, G_loss: 0.5488\n",
      "  Batch [690/1299] D_loss: -0.2273, G_loss: 0.7187\n",
      "  Batch [700/1299] D_loss: 0.0055, G_loss: 0.4232\n",
      "  Batch [710/1299] D_loss: -1.2424, G_loss: -1.2939\n",
      "  Batch [720/1299] D_loss: -0.0459, G_loss: 0.1853\n",
      "  Batch [730/1299] D_loss: -0.0058, G_loss: 0.2348\n",
      "  Batch [740/1299] D_loss: 0.0425, G_loss: 0.2312\n",
      "  Batch [750/1299] D_loss: -0.0679, G_loss: 0.2866\n",
      "  Batch [760/1299] D_loss: -0.0639, G_loss: 0.1339\n",
      "  Batch [770/1299] D_loss: -0.1479, G_loss: -0.2609\n",
      "  Batch [780/1299] D_loss: -0.4486, G_loss: -0.0311\n",
      "  Batch [790/1299] D_loss: -0.0199, G_loss: 0.0727\n",
      "  Batch [800/1299] D_loss: -0.2686, G_loss: 0.0655\n",
      "  Batch [810/1299] D_loss: -0.1856, G_loss: 0.0233\n",
      "  Batch [820/1299] D_loss: -0.0952, G_loss: -0.0431\n",
      "  Batch [830/1299] D_loss: -0.5032, G_loss: -0.2485\n",
      "  Batch [840/1299] D_loss: -0.3206, G_loss: -0.4081\n",
      "  Batch [850/1299] D_loss: -1.0452, G_loss: -0.5297\n",
      "  Batch [860/1299] D_loss: 0.1075, G_loss: 0.3464\n",
      "  Batch [870/1299] D_loss: -0.1025, G_loss: 0.7504\n",
      "  Batch [880/1299] D_loss: -0.0547, G_loss: 0.4832\n",
      "  Batch [890/1299] D_loss: -0.0400, G_loss: 0.4949\n",
      "  Batch [900/1299] D_loss: -0.1098, G_loss: 0.5799\n",
      "  Batch [910/1299] D_loss: -0.0866, G_loss: 0.3272\n",
      "  Batch [920/1299] D_loss: 0.7912, G_loss: -0.5364\n",
      "  Batch [930/1299] D_loss: -0.1104, G_loss: 0.0021\n",
      "  Batch [940/1299] D_loss: -0.7361, G_loss: -0.1289\n",
      "  Batch [950/1299] D_loss: -0.1575, G_loss: 0.3075\n",
      "  Batch [960/1299] D_loss: -0.1726, G_loss: 0.5147\n",
      "  Batch [970/1299] D_loss: -0.2056, G_loss: 0.5234\n",
      "  Batch [980/1299] D_loss: -0.2438, G_loss: 0.5989\n",
      "  Batch [990/1299] D_loss: -0.1825, G_loss: 0.5578\n",
      "  Batch [1000/1299] D_loss: -0.0154, G_loss: 0.3939\n",
      "  Batch [1010/1299] D_loss: 0.0367, G_loss: 0.3514\n",
      "  Batch [1020/1299] D_loss: -2.1134, G_loss: -6.4481\n",
      "  Batch [1030/1299] D_loss: -0.4063, G_loss: -0.5991\n",
      "  Batch [1040/1299] D_loss: -0.1050, G_loss: 0.2091\n",
      "  Batch [1050/1299] D_loss: -0.0621, G_loss: 0.1936\n",
      "  Batch [1060/1299] D_loss: -0.0585, G_loss: 0.4565\n",
      "  Batch [1070/1299] D_loss: -0.0863, G_loss: 0.3389\n",
      "  Batch [1080/1299] D_loss: 0.0264, G_loss: 0.2615\n",
      "  Batch [1090/1299] D_loss: -0.0802, G_loss: 0.2343\n",
      "  Batch [1100/1299] D_loss: -0.1721, G_loss: 0.2977\n",
      "  Batch [1110/1299] D_loss: -0.0955, G_loss: 0.3777\n",
      "  Batch [1120/1299] D_loss: -0.0341, G_loss: 0.3768\n",
      "  Batch [1130/1299] D_loss: -1.5448, G_loss: -1.2698\n",
      "  Batch [1140/1299] D_loss: -0.0646, G_loss: 0.1560\n",
      "  Batch [1150/1299] D_loss: -0.2038, G_loss: 0.1845\n",
      "  Batch [1160/1299] D_loss: -0.0179, G_loss: 0.1474\n",
      "  Batch [1170/1299] D_loss: -0.0115, G_loss: 0.1913\n",
      "  Batch [1180/1299] D_loss: -0.0770, G_loss: 0.4511\n",
      "  Batch [1190/1299] D_loss: -0.0429, G_loss: 0.3507\n",
      "  Batch [1200/1299] D_loss: -0.1161, G_loss: 0.1900\n",
      "  Batch [1210/1299] D_loss: -1.1519, G_loss: -3.2311\n",
      "  Batch [1220/1299] D_loss: -3.4597, G_loss: -3.4128\n",
      "  Batch [1230/1299] D_loss: -0.0037, G_loss: 0.1291\n",
      "  Batch [1240/1299] D_loss: -0.1491, G_loss: 0.3559\n",
      "  Batch [1250/1299] D_loss: -0.1317, G_loss: 0.6741\n",
      "  Batch [1260/1299] D_loss: -0.2409, G_loss: 0.8486\n",
      "  Batch [1270/1299] D_loss: -0.0931, G_loss: 0.5457\n",
      "  Batch [1280/1299] D_loss: -0.0347, G_loss: 0.2944\n",
      "  Batch [1290/1299] D_loss: -1.6712, G_loss: -2.7276\n",
      "\n",
      "Epoch 100 Summary:\n",
      "  Average D_loss: -0.1484\n",
      "  Average G_loss: -0.0627\n"
     ]
    }
   ],
   "source": [
    "def main(selected_categories=None):\n",
    "    \"\"\"\n",
    "    Train the GAN with selected categorical variables\n",
    "    Args:\n",
    "        selected_categories: List of column names to use as categorical variables.\n",
    "                           If None, uses all columns except 'cell_id'\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'epochs': 100,\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 32,\n",
    "        'nb_layers': 3,\n",
    "        'hdim': 256,\n",
    "        'lr': 5e-4,\n",
    "        'nb_critic': 5\n",
    "    }\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/\"\n",
    "    \n",
    "    # Load expression matrix\n",
    "    with h5py.File(data_path+'combined_normalized_data.h5', 'r') as f:\n",
    "        x_train = f['matrix'][:].T\n",
    "    \n",
    "    # Load all categorical variables from single file\n",
    "    cat_data = pd.read_csv(data_path+'combined_metadata.csv', sep=';')\n",
    "    print(\"Categorical data shape:\", cat_data.shape)\n",
    "    print(\"Available categorical variables:\", [col for col in cat_data.columns if col != 'cell_id'])\n",
    "    \n",
    "    # Determine which categories to use\n",
    "    if selected_categories is None:\n",
    "        # Use all columns except cell_id\n",
    "        categories_to_use = [col for col in cat_data.columns if col != 'cell_id']\n",
    "    else:\n",
    "        # Validate selected categories\n",
    "        invalid_categories = [cat for cat in selected_categories if cat not in cat_data.columns]\n",
    "        if invalid_categories:\n",
    "            raise ValueError(f\"Invalid categories: {invalid_categories}\")\n",
    "        categories_to_use = selected_categories\n",
    "    \n",
    "    print(f\"\\nUsing categorical variables: {categories_to_use}\")\n",
    "    \n",
    "    # Create dictionaries and inverse mappings for categorical variables\n",
    "    cat_dicts = []\n",
    "    encoded_covs = []\n",
    "    \n",
    "    # Process each selected column as a categorical variable\n",
    "    for column in categories_to_use:\n",
    "        # Get the column data\n",
    "        cat_vec = cat_data[column]\n",
    "        print(f\"\\nProcessing categorical variable: {column}\")\n",
    "        \n",
    "        # Create list of unique category names, sorted\n",
    "        dict_inv = np.array(list(sorted(set(cat_vec.values))))\n",
    "        dict_map = {t: i for i, t in enumerate(dict_inv)}\n",
    "        cat_dicts.append(dict_inv)\n",
    "        \n",
    "        # Convert categorical variables to integers\n",
    "        encoded = np.vectorize(lambda t: dict_map[t])(cat_vec)\n",
    "        encoded = encoded.reshape(-1, 1)  # Reshape to column vector\n",
    "        encoded_covs.append(encoded)\n",
    "        \n",
    "        print(f\"Categories in {column}:\", dict_inv)\n",
    "        print(f\"Number of categories:\", len(dict_inv))\n",
    "    \n",
    "    # Combine all categorical covariates\n",
    "    cat_covs = np.hstack(encoded_covs)\n",
    "    print(\"\\nCombined categorical covariates shape:\", cat_covs.shape)\n",
    "    \n",
    "    # Load numerical covariates (currently empty)\n",
    "    num_covs = np.zeros((x_train.shape[0], 0))\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to device\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)  # Keep on CPU for DataLoader\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(x_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    vocab_sizes = [len(c) for c in cat_dicts]\n",
    "    print(\"\\nVocabulary sizes for categorical variables:\", vocab_sizes)\n",
    "    nb_numeric = num_covs.shape[-1]\n",
    "    x_dim = x_train.shape[-1]\n",
    "    ##\n",
    "    #print(\"x_train: \"+str(x_train))\n",
    "    #print(\"x_dim: \"+str(x_dim))\n",
    "    #print(\"vocab_sizes: \"+str(vocab_sizes))\n",
    "    \n",
    "    generator = Generator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "        z_dim=CONFIG['latent_dim']\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Define save function\n",
    "    def save_models(generator, discriminator, epoch):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # create save directory\n",
    "        categories_str = \"+\".join(categories_to_use)\n",
    "        save_dir = os.path.join(data_path, \"saved_models\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Create run folder\n",
    "        run_dir = os.path.join(save_dir, f\"run_{timestamp}_{categories_str}\")\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "        # Save model initialization parameters\n",
    "        model_config = {\n",
    "            'x_dim': x_dim,\n",
    "            'vocab_sizes': vocab_sizes,\n",
    "            'nb_numeric': nb_numeric,\n",
    "            'h_dims': [CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "            'z_dim': CONFIG['latent_dim'],\n",
    "            'categories': categories_to_use,\n",
    "            'training_config': CONFIG}\n",
    "        config_path = os.path.join(run_dir, 'model_config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(model_config, f, indent=4)\n",
    "        \n",
    "        # Save generator\n",
    "        generator_path = os.path.join(run_dir, f\"generator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(generator.state_dict(), generator_path)\n",
    "        \n",
    "        \n",
    "        # Save discriminator\n",
    "        discriminator_path = os.path.join(run_dir, f\"discriminator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(discriminator.state_dict(), discriminator_path)\n",
    "        \n",
    "        print(f\"\\nModels saved at epoch {epoch + 1}:\")\n",
    "        print(f\"Generator: {generator_path}\")\n",
    "        print(f\"Discriminator: {discriminator_path}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        if wandb.run is not None:\n",
    "            wandb.save(generator_path)\n",
    "            wandb.save(discriminator_path)\n",
    "\n",
    "    # Initialize wandb with unique run name\n",
    "    run_name = f\"run_{int(time.time())}\"  # Uses timestamp for unique name\n",
    "    wandb.init(\n",
    "        project='adversarial_gene_expr',\n",
    "        config=CONFIG,\n",
    "        name=run_name,\n",
    "        reinit=True  # Ensures new run each time\n",
    "    )\n",
    "    \n",
    "    # Add selected categories to wandb config\n",
    "    wandb.config.update({'selected_categories': categories_to_use})\n",
    "    \n",
    "    # Train model\n",
    "    train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=train_loader,\n",
    "        cat_covs=cat_covs,\n",
    "        num_covs=num_covs,\n",
    "        config=CONFIG,\n",
    "        device=device,\n",
    "        save_fn=save_models\n",
    "        #save_fn=None\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    # Use specific categories:\n",
    "    main(selected_categories=['dataset'])\n",
    "    \n",
    "    # Or use all available categories:\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for data gneration\n",
    "def inspect_generator_dims(generator):\n",
    "    \"\"\"\n",
    "    Inspect the generator's dimensions and architecture\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Generator model\n",
    "    \n",
    "    Returns:\n",
    "        dict containing dimension information\n",
    "    \"\"\"\n",
    "    # Get embedding dimensions\n",
    "    embedding_dims = [emb.embedding_dim for emb in generator.embeddings]\n",
    "    total_embedding_dim = sum(embedding_dims)\n",
    "    \n",
    "    # Get first layer dimension\n",
    "    first_layer_in_dim = generator.network[0].in_features\n",
    "    \n",
    "    return {\n",
    "        'embedding_dims': embedding_dims,\n",
    "        'total_embedding_dim': total_embedding_dim,\n",
    "        'first_layer_in_dim': first_layer_in_dim,\n",
    "        'recommended_latent_dim': first_layer_in_dim - total_embedding_dim\n",
    "    }\n",
    "\n",
    "def generate_expression_profiles(generator, n_samples, dataset_category, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate gene expression profiles using the trained cWGAN generator\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained Generator model\n",
    "        n_samples: Number of profiles to generate\n",
    "        dataset_category: Integer indicating which dataset category to generate (0-6 for dataset1-dataset7)\n",
    "        device: Device to run generation on ('cuda', 'mps', or 'cpu')\n",
    "        debug: If True, print debugging information\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of generated expression profiles with shape (n_samples, n_genes)\n",
    "    \"\"\"\n",
    "    # Set generator to eval mode\n",
    "    generator.eval()\n",
    "    \n",
    "    # Inspect dimensions\n",
    "    dims = inspect_generator_dims(generator)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Generator dimensions:\")\n",
    "        for k, v in dims.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    \n",
    "    # Create latent vectors\n",
    "    latent_dim = dims['recommended_latent_dim']\n",
    "    z = torch.randn(n_samples, latent_dim, device=device)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nLatent vector shape: {z.shape}\")\n",
    "    \n",
    "    # Create categorical condition tensor\n",
    "    cat_covs = torch.full((n_samples, 1), dataset_category, dtype=torch.long, device=device)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Categorical covariates shape: {cat_covs.shape}\")\n",
    "    \n",
    "    # Create empty numeric covariates tensor\n",
    "    num_covs = torch.zeros((n_samples, 0), device=device)\n",
    "    \n",
    "    # Generate samples\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings\n",
    "            embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(generator.embeddings)]\n",
    "            embedded = torch.cat(embeddings, dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Embedded shape: {embedded.shape}\")\n",
    "            \n",
    "            # Concatenate inputs\n",
    "            gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Generator input shape: {gen_input.shape}\")\n",
    "                print(f\"First layer input dim: {generator.network[0].in_features}\")\n",
    "                print(f\"First layer weight shape: {generator.network[0].weight.shape}\")\n",
    "            \n",
    "            # Generate samples\n",
    "            fake_samples = generator.network(gen_input)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(\"\\nError during generation:\")\n",
    "        print(e)\n",
    "        print(\"\\nGenerator architecture:\")\n",
    "        print(generator)\n",
    "        raise\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    return fake_samples.cpu().numpy()\n",
    "\n",
    "def generate_and_save_profiles(generator, n_samples_per_category, save_path, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate expression profiles for all dataset categories and save to file\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained Generator model\n",
    "        n_samples_per_category: Number of samples to generate per dataset category\n",
    "        save_path: Path to save the generated profiles\n",
    "        device: Device to run generation on ('cuda', 'mps', or 'cpu')\n",
    "        debug: If True, print debugging information\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "    all_categories = []\n",
    "    \n",
    "    # Generate samples for each dataset category\n",
    "    for category in range(7):  # 7 datasets (dataset1-dataset7)\n",
    "        if debug:\n",
    "            print(f\"\\nGenerating samples for dataset{category+1}\")\n",
    "        \n",
    "        samples = generate_expression_profiles(\n",
    "            generator, \n",
    "            n_samples_per_category, \n",
    "            category, \n",
    "            device,\n",
    "            debug=debug\n",
    "        )\n",
    "        all_samples.append(samples)\n",
    "        all_categories.extend([f'dataset{category+1}'] * n_samples_per_category)\n",
    "    # Print saved data path\n",
    "    print(\"Save location: \"+str(save_path))\n",
    "\n",
    "    # Combine all samples\n",
    "    all_samples = np.vstack(all_samples)\n",
    "    \n",
    "    # Save generated profiles\n",
    "    np.save(f'{save_path}_profiles.npy', all_samples)\n",
    "    \n",
    "    # Save category labels\n",
    "    with open(f'{save_path}_categories.txt', 'w') as f:\n",
    "        for category in all_categories:\n",
    "            f.write(f'{category}\\n')\n",
    "            \n",
    "    return all_samples, all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Save location: /Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/saved_models/run_20250113_135205_dataset/generated_data\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "\n",
    "# Set directories\n",
    "# 2 hidden layers\n",
    "#run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/saved_models/run_20250113_114232_dataset/\"\n",
    "#generator_model = \"generator_20250113_114232_dataset.pt\"\n",
    "# 3 hidden layers\n",
    "run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/13_1_2025__normalized/saved_models/run_20250113_135205_dataset/\"\n",
    "generator_model = \"generator_20250113_135205_dataset_epoch_51.pt\"\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(run_dir, 'model_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "    \n",
    "# Initialize models with saved configuration\n",
    "generator = Generator(\n",
    "    x_dim=model_config['x_dim'],\n",
    "    vocab_sizes=model_config['vocab_sizes'],\n",
    "    nb_numeric=model_config['nb_numeric'],\n",
    "    h_dims=model_config['h_dims'],\n",
    "    z_dim=model_config['z_dim']).to(device)\n",
    "    \n",
    "discriminator = Discriminator(\n",
    "    x_dim=model_config['x_dim'],\n",
    "    vocab_sizes=model_config['vocab_sizes'],\n",
    "    nb_numeric=model_config['nb_numeric'],\n",
    "    h_dims=model_config['h_dims']).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#discriminator_path = os.path.join(run_dir, \"discriminator.pt\")\n",
    "#discriminator.load_state_dict(torch.load(discriminator_path, map_location=device, weights_only=True))\n",
    "generator_path = os.path.join(run_dir, generator_model)\n",
    "generator.load_state_dict(torch.load(generator_path, map_location=device, weights_only=True))\n",
    "\n",
    "\n",
    "all_samples, categories = generate_and_save_profiles(\n",
    "    generator,\n",
    "    n_samples_per_category=1000,\n",
    "    save_path=run_dir+'generated_data',\n",
    "    debug=False  # Enable debug output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.320732</td>\n",
       "      <td>0.105634</td>\n",
       "      <td>0.059878</td>\n",
       "      <td>-0.033626</td>\n",
       "      <td>0.315196</td>\n",
       "      <td>0.355873</td>\n",
       "      <td>0.098052</td>\n",
       "      <td>1.415885</td>\n",
       "      <td>0.138555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038606</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>0.049724</td>\n",
       "      <td>0.033215</td>\n",
       "      <td>0.174404</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.013922</td>\n",
       "      <td>0.012524</td>\n",
       "      <td>-0.024358</td>\n",
       "      <td>dataset1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.795014</td>\n",
       "      <td>11.064900</td>\n",
       "      <td>7.698052</td>\n",
       "      <td>6.852327</td>\n",
       "      <td>0.702508</td>\n",
       "      <td>1.186466</td>\n",
       "      <td>0.304192</td>\n",
       "      <td>6.257954</td>\n",
       "      <td>3.651256</td>\n",
       "      <td>5.293654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029625</td>\n",
       "      <td>-0.164511</td>\n",
       "      <td>-0.066902</td>\n",
       "      <td>-0.005725</td>\n",
       "      <td>-0.053271</td>\n",
       "      <td>0.016862</td>\n",
       "      <td>0.056550</td>\n",
       "      <td>-0.123670</td>\n",
       "      <td>-0.149712</td>\n",
       "      <td>dataset1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100273</td>\n",
       "      <td>2.477551</td>\n",
       "      <td>1.751069</td>\n",
       "      <td>1.583150</td>\n",
       "      <td>-0.010378</td>\n",
       "      <td>0.883565</td>\n",
       "      <td>0.253218</td>\n",
       "      <td>1.477944</td>\n",
       "      <td>1.379792</td>\n",
       "      <td>1.206260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014636</td>\n",
       "      <td>-0.010550</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>-0.004031</td>\n",
       "      <td>-0.040558</td>\n",
       "      <td>0.042852</td>\n",
       "      <td>0.056843</td>\n",
       "      <td>-0.005354</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>dataset1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.034639</td>\n",
       "      <td>1.667600</td>\n",
       "      <td>1.251640</td>\n",
       "      <td>1.145788</td>\n",
       "      <td>-0.147350</td>\n",
       "      <td>-0.009043</td>\n",
       "      <td>0.021922</td>\n",
       "      <td>1.158313</td>\n",
       "      <td>0.585592</td>\n",
       "      <td>0.775481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008856</td>\n",
       "      <td>-0.005037</td>\n",
       "      <td>-0.006993</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.022809</td>\n",
       "      <td>0.020079</td>\n",
       "      <td>0.043271</td>\n",
       "      <td>-0.002714</td>\n",
       "      <td>0.044753</td>\n",
       "      <td>dataset1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.099730</td>\n",
       "      <td>2.358865</td>\n",
       "      <td>1.671180</td>\n",
       "      <td>1.510806</td>\n",
       "      <td>-0.010824</td>\n",
       "      <td>0.802133</td>\n",
       "      <td>0.232048</td>\n",
       "      <td>1.419516</td>\n",
       "      <td>1.297776</td>\n",
       "      <td>1.144986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013183</td>\n",
       "      <td>-0.012915</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>-0.003016</td>\n",
       "      <td>-0.038905</td>\n",
       "      <td>0.038836</td>\n",
       "      <td>0.054853</td>\n",
       "      <td>-0.007304</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>dataset1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>0.168555</td>\n",
       "      <td>3.913768</td>\n",
       "      <td>2.765483</td>\n",
       "      <td>2.494067</td>\n",
       "      <td>-0.015362</td>\n",
       "      <td>0.981787</td>\n",
       "      <td>0.322876</td>\n",
       "      <td>2.348282</td>\n",
       "      <td>1.871573</td>\n",
       "      <td>1.913221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>-0.019285</td>\n",
       "      <td>-0.025493</td>\n",
       "      <td>-0.022838</td>\n",
       "      <td>-0.026965</td>\n",
       "      <td>0.032246</td>\n",
       "      <td>0.085965</td>\n",
       "      <td>0.015422</td>\n",
       "      <td>0.032537</td>\n",
       "      <td>dataset7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>2.949266</td>\n",
       "      <td>6.560752</td>\n",
       "      <td>4.519499</td>\n",
       "      <td>4.007252</td>\n",
       "      <td>0.760103</td>\n",
       "      <td>-5.030645</td>\n",
       "      <td>0.517229</td>\n",
       "      <td>4.105344</td>\n",
       "      <td>0.180469</td>\n",
       "      <td>3.276859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082215</td>\n",
       "      <td>-0.041214</td>\n",
       "      <td>0.152816</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>0.466038</td>\n",
       "      <td>-0.133744</td>\n",
       "      <td>-0.015720</td>\n",
       "      <td>-0.038499</td>\n",
       "      <td>0.360135</td>\n",
       "      <td>dataset7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>0.381713</td>\n",
       "      <td>5.291831</td>\n",
       "      <td>3.660735</td>\n",
       "      <td>3.238459</td>\n",
       "      <td>0.432806</td>\n",
       "      <td>1.683521</td>\n",
       "      <td>0.287335</td>\n",
       "      <td>2.875701</td>\n",
       "      <td>2.568585</td>\n",
       "      <td>2.648107</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003065</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>-0.005536</td>\n",
       "      <td>-0.044282</td>\n",
       "      <td>0.051976</td>\n",
       "      <td>0.068840</td>\n",
       "      <td>0.014409</td>\n",
       "      <td>-0.000424</td>\n",
       "      <td>-0.000549</td>\n",
       "      <td>dataset7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>-1.652966</td>\n",
       "      <td>-0.480020</td>\n",
       "      <td>-0.284708</td>\n",
       "      <td>-0.158320</td>\n",
       "      <td>-0.800277</td>\n",
       "      <td>0.109397</td>\n",
       "      <td>-0.567243</td>\n",
       "      <td>-0.265171</td>\n",
       "      <td>-0.451420</td>\n",
       "      <td>0.389981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055769</td>\n",
       "      <td>0.094848</td>\n",
       "      <td>0.044443</td>\n",
       "      <td>-0.125318</td>\n",
       "      <td>-0.165968</td>\n",
       "      <td>0.050426</td>\n",
       "      <td>0.132208</td>\n",
       "      <td>0.117812</td>\n",
       "      <td>-0.112642</td>\n",
       "      <td>dataset7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>-0.334864</td>\n",
       "      <td>0.917253</td>\n",
       "      <td>0.526947</td>\n",
       "      <td>0.508436</td>\n",
       "      <td>-0.073659</td>\n",
       "      <td>0.692343</td>\n",
       "      <td>-0.012822</td>\n",
       "      <td>0.410956</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>0.455444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>-0.006103</td>\n",
       "      <td>-0.023581</td>\n",
       "      <td>-0.063090</td>\n",
       "      <td>0.033996</td>\n",
       "      <td>0.107796</td>\n",
       "      <td>-0.004826</td>\n",
       "      <td>-0.040344</td>\n",
       "      <td>dataset7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2         3         4         5         6  \\\n",
       "0     0.029197   0.320732  0.105634  0.059878 -0.033626  0.315196  0.355873   \n",
       "1     0.795014  11.064900  7.698052  6.852327  0.702508  1.186466  0.304192   \n",
       "2     0.100273   2.477551  1.751069  1.583150 -0.010378  0.883565  0.253218   \n",
       "3    -0.034639   1.667600  1.251640  1.145788 -0.147350 -0.009043  0.021922   \n",
       "4     0.099730   2.358865  1.671180  1.510806 -0.010824  0.802133  0.232048   \n",
       "...        ...        ...       ...       ...       ...       ...       ...   \n",
       "6995  0.168555   3.913768  2.765483  2.494067 -0.015362  0.981787  0.322876   \n",
       "6996  2.949266   6.560752  4.519499  4.007252  0.760103 -5.030645  0.517229   \n",
       "6997  0.381713   5.291831  3.660735  3.238459  0.432806  1.683521  0.287335   \n",
       "6998 -1.652966  -0.480020 -0.284708 -0.158320 -0.800277  0.109397 -0.567243   \n",
       "6999 -0.334864   0.917253  0.526947  0.508436 -0.073659  0.692343 -0.012822   \n",
       "\n",
       "             7         8         9  ...       991       992       993  \\\n",
       "0     0.098052  1.415885  0.138555  ...  0.038606  0.017874  0.049724   \n",
       "1     6.257954  3.651256  5.293654  ... -0.029625 -0.164511 -0.066902   \n",
       "2     1.477944  1.379792  1.206260  ...  0.014636 -0.010550  0.019563   \n",
       "3     1.158313  0.585592  0.775481  ... -0.008856 -0.005037 -0.006993   \n",
       "4     1.419516  1.297776  1.144986  ...  0.013183 -0.012915  0.016879   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6995  2.348282  1.871573  1.913221  ...  0.005220 -0.019285 -0.025493   \n",
       "6996  4.105344  0.180469  3.276859  ... -0.082215 -0.041214  0.152816   \n",
       "6997  2.875701  2.568585  2.648107  ... -0.003065  0.001313 -0.005536   \n",
       "6998 -0.265171 -0.451420  0.389981  ... -0.055769  0.094848  0.044443   \n",
       "6999  0.410956  0.590551  0.455444  ...  0.000125  0.006495 -0.006103   \n",
       "\n",
       "           994       995       996       997       998       999   dataset  \n",
       "0     0.033215  0.174404  0.011404  0.013922  0.012524 -0.024358  dataset1  \n",
       "1    -0.005725 -0.053271  0.016862  0.056550 -0.123670 -0.149712  dataset1  \n",
       "2    -0.004031 -0.040558  0.042852  0.056843 -0.005354  0.013900  dataset1  \n",
       "3     0.000234  0.022809  0.020079  0.043271 -0.002714  0.044753  dataset1  \n",
       "4    -0.003016 -0.038905  0.038836  0.054853 -0.007304  0.010891  dataset1  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6995 -0.022838 -0.026965  0.032246  0.085965  0.015422  0.032537  dataset7  \n",
       "6996  0.310174  0.466038 -0.133744 -0.015720 -0.038499  0.360135  dataset7  \n",
       "6997 -0.044282  0.051976  0.068840  0.014409 -0.000424 -0.000549  dataset7  \n",
       "6998 -0.125318 -0.165968  0.050426  0.132208  0.117812 -0.112642  dataset7  \n",
       "6999 -0.023581 -0.063090  0.033996  0.107796 -0.004826 -0.040344  dataset7  \n",
       "\n",
       "[7000 rows x 1001 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load generated data\n",
    "# Load the generated profiles\n",
    "profiles = np.load(run_dir + 'generated_data_profiles.npy')\n",
    "\n",
    "# Load categories\n",
    "with open(run_dir + 'generated_data_categories.txt', 'r') as f:\n",
    "    categories = [line.strip() for line in f]\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(profiles)\n",
    "\n",
    "# Add categories as a column\n",
    "df['dataset'] = categories\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
