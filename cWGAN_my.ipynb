{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb\n",
    "import h5py\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "# Load expression matrix (csv file with rows as cells and columns as genes)\n",
    "data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/\"\n",
    "# Load expression matrix\n",
    "with h5py.File(data_path+'train_data_1dataset.h5', 'r') as f:\n",
    "    matrix = f['matrix'][:]\n",
    "\n",
    "# each row is a cell\n",
    "# matrix[:,0]\n",
    "\n",
    "x_train = matrix\n",
    "\n",
    "\n",
    "# Load cluster info\n",
    "cluster_vec = pd.read_csv(data_path+'train_data_1dataset_cluster.csv').T\n",
    "cluster_vec\n",
    "# Load numerical covariates\n",
    "num_covs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(cluster_vec.values.flatten())\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example raw data\n",
    "# tissues = np.array(['liver', 'brain', 'liver', 'kidney', 'brain'])\n",
    "\n",
    "# Create dictionaries and inverse mappings\n",
    "cat_dicts = []\n",
    "\n",
    "## Cluster covariate\n",
    "# Create list of unique cluster names, sorted\n",
    "cluster_dict_inv = np.array(list(sorted(set(cluster_vec.values.flatten()))))  # ['brain', 'kidney', 'liver']\n",
    "cluster_dict = {t: i for i, t in enumerate(cluster_dict_inv)}  # {'brain': 0, 'kidney': 1, 'liver': 2}\n",
    "cat_dicts.append(cluster_dict_inv)\n",
    "\n",
    "# Convert categorical variables to integers\n",
    "clusters_encoded = np.vectorize(lambda t: cluster_dict[t])(cluster_vec)\n",
    "\n",
    "print(\"Original clusters:\", cluster_vec)\n",
    "print(\"Encoded clusters:\", clusters_encoded)\n",
    "print(\"Cluster mapping:\", cluster_dict)\n",
    "\n",
    "print(\"\\ncat_dicts:\", cat_dicts)\n",
    "\n",
    "# This gives us vocab_sizes for model initialization\n",
    "vocab_sizes = [len(c) for c in cat_dicts]  # [3, 2] (3 tissue types, 2 dataset types)\n",
    "print(\"\\nvocab_sizes:\", vocab_sizes)\n",
    "\n",
    "# assign categorical covariates\n",
    "cat_covs = clusters_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims, z_dim):\n",
    "        \"\"\"\n",
    "        Generator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of output data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "            z_dim: Dimension of latent noise vector\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size)) \n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is latent dim + embedding dim + numeric covariates\n",
    "        input_dim = z_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build generator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, x_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims):\n",
    "        \"\"\"\n",
    "        Discriminator network for conditional GAN\n",
    "        Args:\n",
    "            x_dim: Dimension of input data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            h_dims: List of hidden dimensions\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size))\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is data dim + embedding dim + numeric covariates\n",
    "        input_dim = x_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Build discriminator network\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for h_dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, h_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            current_dim = h_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        disc_input = torch.cat([x, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.network(disc_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, dataloader, cat_covs, num_covs, \n",
    "              config, device, score_fn=None, save_fn=None):\n",
    "    \"\"\"\n",
    "    Train the conditional GAN with progress tracking and proper device handling\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    g_optimizer = optim.RMSprop(generator.parameters(), lr=config['lr'])\n",
    "    d_optimizer = optim.RMSprop(discriminator.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Convert covariates to tensors and move to device\n",
    "    cat_covs = torch.tensor(cat_covs, dtype=torch.long).to(device)\n",
    "    num_covs = torch.tensor(num_covs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "    print(f\"Total batches per epoch: {total_batches}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        print(f\"\\nEpoch [{epoch+1}/{config['epochs']}]\")\n",
    "        \n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Move real data to device\n",
    "            real_data = real_data.to(device)\n",
    "            \n",
    "            # Get random batch of categorical and numerical covariates\n",
    "            batch_indices = torch.randint(0, cat_covs.size(0), (batch_size,))\n",
    "            batch_cat_covs = cat_covs[batch_indices]\n",
    "            batch_num_covs = num_covs[batch_indices]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(config['nb_critic']):\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "                fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate discriminator loss\n",
    "                real_validity = discriminator(real_data, batch_cat_covs, batch_num_covs)\n",
    "                fake_validity = discriminator(fake_data.detach(), batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                d_loss = -(torch.mean(real_validity) - torch.mean(fake_validity))\n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                # Clip discriminator weights (Wasserstein GAN)\n",
    "                for p in discriminator.parameters():\n",
    "                    p.data.clamp_(-0.01, 0.01)\n",
    "                    \n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "            fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "            \n",
    "            # Calculate generator loss\n",
    "            fake_validity = discriminator(fake_data, batch_cat_covs, batch_num_covs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            g_losses.append(g_loss.item())\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Batch [{batch_idx}/{total_batches}] \" \\\n",
    "                      f\"D_loss: {d_loss.item():.4f}, \" \\\n",
    "                      f\"G_loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average D_loss: {avg_d_loss:.4f}\")\n",
    "        print(f\"  Average G_loss: {avg_g_loss:.4f}\")\n",
    "        \n",
    "        # Log metrics\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'd_loss': np.mean(d_losses),\n",
    "                'g_loss': np.mean(g_losses)\n",
    "            })\n",
    "        \n",
    "        # Evaluate and save model if needed\n",
    "        if score_fn is not None and epoch % 10 == 0:\n",
    "            score = score_fn(generator)\n",
    "            print(f'Epoch {epoch}: Score = {score:.4f}')\n",
    "            \n",
    "        if save_fn is not None and epoch % 100 == 0:\n",
    "            save_fn(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Categorical data shape: (41588, 3)\n",
      "Available categorical variables: ['dataset', 'cluster']\n",
      "\n",
      "Using categorical variables: ['dataset']\n",
      "\n",
      "Processing categorical variable: dataset\n",
      "Categories in dataset: ['dataset1' 'dataset2' 'dataset3' 'dataset4' 'dataset5' 'dataset6'\n",
      " 'dataset7']\n",
      "Number of categories: 7\n",
      "\n",
      "Combined categorical covariates shape: (41588, 1)\n",
      "\n",
      "Vocabulary sizes for categorical variables: [7]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>d_loss</td><td>▇██████████████████████████████████▁▂▂▃▃</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▆▆▆▇▇▇▇▇████▁▂▂▂▂</td></tr><tr><td>g_loss</td><td>▃▃▃▃▂▃▂▃▂▃▂▃▂▁▃▂▂▃▂▃▃▂▃▂▂▂▃▂▃▂▃▃▂▁▂█▄█▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>d_loss</td><td>-14.09647</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>g_loss</td><td>8.20882</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-wood-1</strong> at: <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/rcmbqqgj' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/rcmbqqgj</a><br> View project at: <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250109_113219-rcmbqqgj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/guyshani/Documents/PHD/Aim_2/cycle_GAN/wandb/run-20250109_150554-m7w27hzr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/m7w27hzr' target=\"_blank\">run_1736427954</a></strong> to <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/m7w27hzr' target=\"_blank\">https://wandb.ai/guyshani-tel-aviv-university/adversarial_gene_expr/runs/m7w27hzr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 200 epochs...\n",
      "Total batches per epoch: 31\n",
      "Using device: mps\n",
      "\n",
      "Epoch [1/200]\n",
      "  Batch [0/31] D_loss: -163.3172, G_loss: 78.1496\n",
      "  Batch [10/31] D_loss: -7.6385, G_loss: 18.5354\n",
      "  Batch [20/31] D_loss: -76.1167, G_loss: 54.2831\n",
      "  Batch [30/31] D_loss: -53.5064, G_loss: 14.7118\n",
      "\n",
      "Epoch 1 Summary:\n",
      "  Average D_loss: -24.2392\n",
      "  Average G_loss: 19.4420\n",
      "\n",
      "Epoch [2/200]\n",
      "  Batch [0/31] D_loss: -63.6787, G_loss: 28.0441\n",
      "  Batch [10/31] D_loss: -55.8071, G_loss: -8.5930\n",
      "  Batch [20/31] D_loss: -40.7650, G_loss: -5.0894\n",
      "  Batch [30/31] D_loss: -39.9867, G_loss: -0.8309\n",
      "\n",
      "Epoch 2 Summary:\n",
      "  Average D_loss: -30.0696\n",
      "  Average G_loss: 5.1760\n",
      "\n",
      "Epoch [3/200]\n",
      "  Batch [0/31] D_loss: -31.2131, G_loss: -1.8148\n",
      "  Batch [10/31] D_loss: -31.2604, G_loss: 13.7603\n",
      "  Batch [20/31] D_loss: -52.0506, G_loss: 10.1782\n",
      "  Batch [30/31] D_loss: -31.7023, G_loss: 0.6628\n",
      "\n",
      "Epoch 3 Summary:\n",
      "  Average D_loss: -21.7641\n",
      "  Average G_loss: 5.8327\n",
      "\n",
      "Epoch [4/200]\n",
      "  Batch [0/31] D_loss: -52.2966, G_loss: 12.2898\n",
      "  Batch [10/31] D_loss: -63.7557, G_loss: 15.8009\n",
      "  Batch [20/31] D_loss: -43.9516, G_loss: 4.9688\n",
      "  Batch [30/31] D_loss: -38.3666, G_loss: 14.7762\n",
      "\n",
      "Epoch 4 Summary:\n",
      "  Average D_loss: -30.3228\n",
      "  Average G_loss: 9.1537\n",
      "\n",
      "Epoch [5/200]\n",
      "  Batch [0/31] D_loss: -35.5220, G_loss: 14.5946\n",
      "  Batch [10/31] D_loss: -40.6504, G_loss: 3.3625\n",
      "  Batch [20/31] D_loss: -40.6221, G_loss: 29.6440\n",
      "  Batch [30/31] D_loss: -51.8643, G_loss: 12.5452\n",
      "\n",
      "Epoch 5 Summary:\n",
      "  Average D_loss: -22.3016\n",
      "  Average G_loss: 12.2570\n",
      "\n",
      "Epoch [6/200]\n",
      "  Batch [0/31] D_loss: -47.1220, G_loss: 19.0980\n",
      "  Batch [10/31] D_loss: -49.9147, G_loss: -18.2604\n",
      "  Batch [20/31] D_loss: -40.0723, G_loss: 17.5937\n",
      "  Batch [30/31] D_loss: -41.2892, G_loss: 21.4609\n",
      "\n",
      "Epoch 6 Summary:\n",
      "  Average D_loss: -25.0002\n",
      "  Average G_loss: 9.8698\n",
      "\n",
      "Epoch [7/200]\n",
      "  Batch [0/31] D_loss: -39.4054, G_loss: 27.7336\n",
      "  Batch [10/31] D_loss: -35.9280, G_loss: 11.6942\n",
      "  Batch [20/31] D_loss: -48.6948, G_loss: 19.0923\n",
      "  Batch [30/31] D_loss: -58.2405, G_loss: 2.1867\n",
      "\n",
      "Epoch 7 Summary:\n",
      "  Average D_loss: -24.8093\n",
      "  Average G_loss: 10.7103\n",
      "\n",
      "Epoch [8/200]\n",
      "  Batch [0/31] D_loss: -45.7975, G_loss: -6.1455\n",
      "  Batch [10/31] D_loss: -43.7187, G_loss: 0.3525\n",
      "  Batch [20/31] D_loss: -46.1283, G_loss: 35.6687\n",
      "  Batch [30/31] D_loss: -33.0630, G_loss: -10.7205\n",
      "\n",
      "Epoch 8 Summary:\n",
      "  Average D_loss: -23.5469\n",
      "  Average G_loss: 2.6900\n",
      "\n",
      "Epoch [9/200]\n",
      "  Batch [0/31] D_loss: -35.6179, G_loss: -10.1282\n",
      "  Batch [10/31] D_loss: -44.6870, G_loss: 18.7028\n",
      "  Batch [20/31] D_loss: -38.9277, G_loss: 20.9402\n",
      "  Batch [30/31] D_loss: -34.1597, G_loss: -41.7801\n",
      "\n",
      "Epoch 9 Summary:\n",
      "  Average D_loss: -21.1306\n",
      "  Average G_loss: 4.1794\n",
      "\n",
      "Epoch [10/200]\n",
      "  Batch [0/31] D_loss: -33.8470, G_loss: -28.1326\n",
      "  Batch [10/31] D_loss: -30.7114, G_loss: 9.3024\n",
      "  Batch [20/31] D_loss: -42.4074, G_loss: -25.0707\n",
      "  Batch [30/31] D_loss: -35.2317, G_loss: 23.5701\n",
      "\n",
      "Epoch 10 Summary:\n",
      "  Average D_loss: -21.2866\n",
      "  Average G_loss: 11.6279\n",
      "\n",
      "Epoch [11/200]\n",
      "  Batch [0/31] D_loss: -41.6615, G_loss: 22.7161\n",
      "  Batch [10/31] D_loss: -37.9014, G_loss: 1.3886\n",
      "  Batch [20/31] D_loss: -46.9602, G_loss: 23.0844\n",
      "  Batch [30/31] D_loss: -49.7910, G_loss: 9.1138\n",
      "\n",
      "Epoch 11 Summary:\n",
      "  Average D_loss: -22.1174\n",
      "  Average G_loss: 4.6311\n",
      "\n",
      "Epoch [12/200]\n",
      "  Batch [0/31] D_loss: -47.3586, G_loss: 2.4543\n",
      "  Batch [10/31] D_loss: -38.9636, G_loss: 19.0459\n",
      "  Batch [20/31] D_loss: -49.1122, G_loss: 8.7609\n",
      "  Batch [30/31] D_loss: -47.7019, G_loss: -22.6711\n",
      "\n",
      "Epoch 12 Summary:\n",
      "  Average D_loss: -23.2609\n",
      "  Average G_loss: 6.2593\n",
      "\n",
      "Epoch [13/200]\n",
      "  Batch [0/31] D_loss: -45.9149, G_loss: -8.8264\n",
      "  Batch [10/31] D_loss: -48.9913, G_loss: 24.1568\n",
      "  Batch [20/31] D_loss: -42.1885, G_loss: -12.4994\n",
      "  Batch [30/31] D_loss: -35.3744, G_loss: 18.9804\n",
      "\n",
      "Epoch 13 Summary:\n",
      "  Average D_loss: -20.2083\n",
      "  Average G_loss: 6.2682\n",
      "\n",
      "Epoch [14/200]\n",
      "  Batch [0/31] D_loss: -39.2206, G_loss: 10.6150\n",
      "  Batch [10/31] D_loss: -37.3761, G_loss: 6.7905\n",
      "  Batch [20/31] D_loss: -27.6810, G_loss: -17.8251\n",
      "  Batch [30/31] D_loss: -57.6821, G_loss: -10.8859\n",
      "\n",
      "Epoch 14 Summary:\n",
      "  Average D_loss: -17.8357\n",
      "  Average G_loss: 7.6322\n",
      "\n",
      "Epoch [15/200]\n",
      "  Batch [0/31] D_loss: -48.9507, G_loss: -0.8754\n",
      "  Batch [10/31] D_loss: -44.1625, G_loss: 14.9897\n",
      "  Batch [20/31] D_loss: -34.0439, G_loss: 30.2515\n",
      "  Batch [30/31] D_loss: -38.2262, G_loss: -2.0636\n",
      "\n",
      "Epoch 15 Summary:\n",
      "  Average D_loss: -18.0365\n",
      "  Average G_loss: 9.8242\n",
      "\n",
      "Epoch [16/200]\n",
      "  Batch [0/31] D_loss: -33.8454, G_loss: -12.2284\n",
      "  Batch [10/31] D_loss: -38.3456, G_loss: -3.1424\n",
      "  Batch [20/31] D_loss: -33.0062, G_loss: 23.9133\n",
      "  Batch [30/31] D_loss: -35.5377, G_loss: 0.6288\n",
      "\n",
      "Epoch 16 Summary:\n",
      "  Average D_loss: -17.0734\n",
      "  Average G_loss: 5.4807\n",
      "\n",
      "Epoch [17/200]\n",
      "  Batch [0/31] D_loss: -40.3978, G_loss: -11.1970\n",
      "  Batch [10/31] D_loss: -40.8194, G_loss: 12.2694\n",
      "  Batch [20/31] D_loss: -39.3564, G_loss: -5.1984\n",
      "  Batch [30/31] D_loss: -45.9309, G_loss: 10.7997\n",
      "\n",
      "Epoch 17 Summary:\n",
      "  Average D_loss: -18.1187\n",
      "  Average G_loss: 5.6033\n",
      "\n",
      "Epoch [18/200]\n",
      "  Batch [0/31] D_loss: -43.1009, G_loss: 23.1873\n",
      "  Batch [10/31] D_loss: -33.6991, G_loss: -7.8567\n",
      "  Batch [20/31] D_loss: -39.7647, G_loss: 12.3676\n",
      "  Batch [30/31] D_loss: -47.3663, G_loss: 1.0645\n",
      "\n",
      "Epoch 18 Summary:\n",
      "  Average D_loss: -18.4139\n",
      "  Average G_loss: 9.8736\n",
      "\n",
      "Epoch [19/200]\n",
      "  Batch [0/31] D_loss: -42.2246, G_loss: 9.9397\n",
      "  Batch [10/31] D_loss: -38.2942, G_loss: -0.4865\n",
      "  Batch [20/31] D_loss: -35.1014, G_loss: 12.0289\n",
      "  Batch [30/31] D_loss: -41.3877, G_loss: -15.5441\n",
      "\n",
      "Epoch 19 Summary:\n",
      "  Average D_loss: -18.8746\n",
      "  Average G_loss: 5.6851\n",
      "\n",
      "Epoch [20/200]\n",
      "  Batch [0/31] D_loss: -42.9695, G_loss: -18.7760\n",
      "  Batch [10/31] D_loss: -23.8670, G_loss: 24.4076\n",
      "  Batch [20/31] D_loss: -35.1788, G_loss: -7.0120\n",
      "  Batch [30/31] D_loss: -60.1151, G_loss: 50.8105\n",
      "\n",
      "Epoch 20 Summary:\n",
      "  Average D_loss: -19.3679\n",
      "  Average G_loss: 6.5704\n",
      "\n",
      "Epoch [21/200]\n",
      "  Batch [0/31] D_loss: -47.1625, G_loss: 48.4164\n",
      "  Batch [10/31] D_loss: -22.9491, G_loss: -31.7593\n",
      "  Batch [20/31] D_loss: -33.3655, G_loss: 9.8832\n",
      "  Batch [30/31] D_loss: -40.3067, G_loss: 3.1324\n",
      "\n",
      "Epoch 21 Summary:\n",
      "  Average D_loss: -17.5771\n",
      "  Average G_loss: 7.1193\n",
      "\n",
      "Epoch [22/200]\n",
      "  Batch [0/31] D_loss: -39.7423, G_loss: -11.0410\n",
      "  Batch [10/31] D_loss: -37.1990, G_loss: 17.0270\n",
      "  Batch [20/31] D_loss: -39.3701, G_loss: 0.3584\n",
      "  Batch [30/31] D_loss: -35.0516, G_loss: -12.5934\n",
      "\n",
      "Epoch 22 Summary:\n",
      "  Average D_loss: -18.4731\n",
      "  Average G_loss: 3.1354\n",
      "\n",
      "Epoch [23/200]\n",
      "  Batch [0/31] D_loss: -21.8022, G_loss: -20.7376\n",
      "  Batch [10/31] D_loss: -36.9570, G_loss: 8.6809\n",
      "  Batch [20/31] D_loss: -37.4289, G_loss: -6.9162\n",
      "  Batch [30/31] D_loss: -35.1951, G_loss: 4.2621\n",
      "\n",
      "Epoch 23 Summary:\n",
      "  Average D_loss: -18.3849\n",
      "  Average G_loss: 6.1895\n",
      "\n",
      "Epoch [24/200]\n",
      "  Batch [0/31] D_loss: -46.0527, G_loss: 22.7799\n",
      "  Batch [10/31] D_loss: -41.9360, G_loss: 11.3469\n",
      "  Batch [20/31] D_loss: -42.3607, G_loss: -18.7167\n",
      "  Batch [30/31] D_loss: -37.0452, G_loss: 34.0712\n",
      "\n",
      "Epoch 24 Summary:\n",
      "  Average D_loss: -20.1522\n",
      "  Average G_loss: 8.3260\n",
      "\n",
      "Epoch [25/200]\n",
      "  Batch [0/31] D_loss: -37.7915, G_loss: 29.8483\n",
      "  Batch [10/31] D_loss: -42.7560, G_loss: -25.8102\n",
      "  Batch [20/31] D_loss: -26.9812, G_loss: 1.4853\n",
      "  Batch [30/31] D_loss: -31.3625, G_loss: 7.8067\n",
      "\n",
      "Epoch 25 Summary:\n",
      "  Average D_loss: -18.8632\n",
      "  Average G_loss: 3.5521\n",
      "\n",
      "Epoch [26/200]\n",
      "  Batch [0/31] D_loss: -41.2368, G_loss: 30.3638\n",
      "  Batch [10/31] D_loss: -38.0269, G_loss: 0.7145\n",
      "  Batch [20/31] D_loss: -33.7188, G_loss: 10.7552\n",
      "  Batch [30/31] D_loss: -30.0673, G_loss: -5.6796\n",
      "\n",
      "Epoch 26 Summary:\n",
      "  Average D_loss: -18.1887\n",
      "  Average G_loss: 7.7037\n",
      "\n",
      "Epoch [27/200]\n",
      "  Batch [0/31] D_loss: -32.0023, G_loss: 0.7549\n",
      "  Batch [10/31] D_loss: -28.9782, G_loss: 27.7011\n",
      "  Batch [20/31] D_loss: -37.8940, G_loss: -8.2378\n",
      "  Batch [30/31] D_loss: -44.1951, G_loss: 48.9603\n",
      "\n",
      "Epoch 27 Summary:\n",
      "  Average D_loss: -20.1182\n",
      "  Average G_loss: 5.6790\n",
      "\n",
      "Epoch [28/200]\n",
      "  Batch [0/31] D_loss: -12.0187, G_loss: 25.0399\n",
      "  Batch [10/31] D_loss: -28.6528, G_loss: 1.0653\n",
      "  Batch [20/31] D_loss: -31.6385, G_loss: -18.7856\n",
      "  Batch [30/31] D_loss: -36.9499, G_loss: 1.9415\n",
      "\n",
      "Epoch 28 Summary:\n",
      "  Average D_loss: -17.4294\n",
      "  Average G_loss: 5.2388\n",
      "\n",
      "Epoch [29/200]\n",
      "  Batch [0/31] D_loss: -35.8518, G_loss: 7.6433\n",
      "  Batch [10/31] D_loss: -41.7737, G_loss: 6.1274\n",
      "  Batch [20/31] D_loss: -36.7483, G_loss: 4.3770\n",
      "  Batch [30/31] D_loss: -42.7741, G_loss: 5.6913\n",
      "\n",
      "Epoch 29 Summary:\n",
      "  Average D_loss: -18.6607\n",
      "  Average G_loss: 6.2057\n",
      "\n",
      "Epoch [30/200]\n",
      "  Batch [0/31] D_loss: -41.3927, G_loss: -6.4050\n",
      "  Batch [10/31] D_loss: -38.9582, G_loss: 24.5913\n",
      "  Batch [20/31] D_loss: -35.7603, G_loss: -17.0054\n",
      "  Batch [30/31] D_loss: -32.6492, G_loss: -1.3127\n",
      "\n",
      "Epoch 30 Summary:\n",
      "  Average D_loss: -18.4267\n",
      "  Average G_loss: 5.9003\n",
      "\n",
      "Epoch [31/200]\n",
      "  Batch [0/31] D_loss: -29.7041, G_loss: 2.0572\n",
      "  Batch [10/31] D_loss: -38.9398, G_loss: 16.8457\n",
      "  Batch [20/31] D_loss: -33.6770, G_loss: -7.4095\n",
      "  Batch [30/31] D_loss: -43.3703, G_loss: 4.2261\n",
      "\n",
      "Epoch 31 Summary:\n",
      "  Average D_loss: -18.6855\n",
      "  Average G_loss: 1.2056\n",
      "\n",
      "Epoch [32/200]\n",
      "  Batch [0/31] D_loss: -39.5440, G_loss: 17.0396\n",
      "  Batch [10/31] D_loss: -38.1677, G_loss: 11.4237\n",
      "  Batch [20/31] D_loss: -35.2797, G_loss: 15.9178\n",
      "  Batch [30/31] D_loss: -42.0964, G_loss: 21.7102\n",
      "\n",
      "Epoch 32 Summary:\n",
      "  Average D_loss: -17.1501\n",
      "  Average G_loss: 11.9676\n",
      "\n",
      "Epoch [33/200]\n",
      "  Batch [0/31] D_loss: -30.0860, G_loss: 3.4860\n",
      "  Batch [10/31] D_loss: -30.8949, G_loss: 3.0010\n",
      "  Batch [20/31] D_loss: -37.3192, G_loss: 13.9424\n",
      "  Batch [30/31] D_loss: -44.4446, G_loss: -9.6189\n",
      "\n",
      "Epoch 33 Summary:\n",
      "  Average D_loss: -18.7713\n",
      "  Average G_loss: 5.2863\n",
      "\n",
      "Epoch [34/200]\n",
      "  Batch [0/31] D_loss: -44.6386, G_loss: -29.4385\n",
      "  Batch [10/31] D_loss: -34.3918, G_loss: 11.5660\n",
      "  Batch [20/31] D_loss: -29.8503, G_loss: -23.7849\n",
      "  Batch [30/31] D_loss: -34.7890, G_loss: 9.9357\n",
      "\n",
      "Epoch 34 Summary:\n",
      "  Average D_loss: -18.1328\n",
      "  Average G_loss: 4.2400\n",
      "\n",
      "Epoch [35/200]\n",
      "  Batch [0/31] D_loss: -35.4395, G_loss: 4.6370\n",
      "  Batch [10/31] D_loss: -33.7495, G_loss: -5.9098\n",
      "  Batch [20/31] D_loss: -37.9607, G_loss: -6.0459\n",
      "  Batch [30/31] D_loss: -31.5806, G_loss: 4.8886\n",
      "\n",
      "Epoch 35 Summary:\n",
      "  Average D_loss: -17.7671\n",
      "  Average G_loss: 6.2564\n",
      "\n",
      "Epoch [36/200]\n",
      "  Batch [0/31] D_loss: -42.6091, G_loss: 14.7117\n",
      "  Batch [10/31] D_loss: -36.3158, G_loss: -0.8477\n",
      "  Batch [20/31] D_loss: -32.3238, G_loss: 19.6063\n",
      "  Batch [30/31] D_loss: -35.2651, G_loss: 24.1220\n",
      "\n",
      "Epoch 36 Summary:\n",
      "  Average D_loss: -18.3649\n",
      "  Average G_loss: 2.2802\n",
      "\n",
      "Epoch [37/200]\n",
      "  Batch [0/31] D_loss: -35.7210, G_loss: 28.1831\n",
      "  Batch [10/31] D_loss: -29.7984, G_loss: -3.6531\n",
      "  Batch [20/31] D_loss: -36.1543, G_loss: -0.0321\n",
      "  Batch [30/31] D_loss: -37.3765, G_loss: 37.0384\n",
      "\n",
      "Epoch 37 Summary:\n",
      "  Average D_loss: -18.7699\n",
      "  Average G_loss: 8.1432\n",
      "\n",
      "Epoch [38/200]\n",
      "  Batch [0/31] D_loss: -32.1432, G_loss: 23.9272\n",
      "  Batch [10/31] D_loss: -34.6801, G_loss: -24.2898\n",
      "  Batch [20/31] D_loss: -34.7464, G_loss: -14.9524\n",
      "  Batch [30/31] D_loss: -31.6392, G_loss: 5.8637\n",
      "\n",
      "Epoch 38 Summary:\n",
      "  Average D_loss: -17.3220\n",
      "  Average G_loss: 6.7225\n",
      "\n",
      "Epoch [39/200]\n",
      "  Batch [0/31] D_loss: -37.2443, G_loss: -14.0262\n",
      "  Batch [10/31] D_loss: -29.4298, G_loss: 6.9512\n",
      "  Batch [20/31] D_loss: -35.4353, G_loss: -1.8064\n",
      "  Batch [30/31] D_loss: -39.7574, G_loss: 0.7501\n",
      "\n",
      "Epoch 39 Summary:\n",
      "  Average D_loss: -17.4591\n",
      "  Average G_loss: 4.4383\n",
      "\n",
      "Epoch [40/200]\n",
      "  Batch [0/31] D_loss: -37.6683, G_loss: 19.2381\n",
      "  Batch [10/31] D_loss: -36.8609, G_loss: 31.4121\n",
      "  Batch [20/31] D_loss: -42.1433, G_loss: -11.7384\n",
      "  Batch [30/31] D_loss: -40.3427, G_loss: 16.2083\n",
      "\n",
      "Epoch 40 Summary:\n",
      "  Average D_loss: -17.6490\n",
      "  Average G_loss: 3.3111\n",
      "\n",
      "Epoch [41/200]\n",
      "  Batch [0/31] D_loss: -40.5048, G_loss: 18.6035\n",
      "  Batch [10/31] D_loss: -36.3410, G_loss: 9.0552\n",
      "  Batch [20/31] D_loss: -38.7373, G_loss: 0.8317\n",
      "  Batch [30/31] D_loss: -33.6517, G_loss: -8.9773\n",
      "\n",
      "Epoch 41 Summary:\n",
      "  Average D_loss: -17.5269\n",
      "  Average G_loss: 7.1818\n",
      "\n",
      "Epoch [42/200]\n",
      "  Batch [0/31] D_loss: -30.0839, G_loss: -5.4714\n",
      "  Batch [10/31] D_loss: -39.2705, G_loss: 11.9060\n",
      "  Batch [20/31] D_loss: -32.9973, G_loss: 5.6770\n",
      "  Batch [30/31] D_loss: -28.3280, G_loss: -10.4286\n",
      "\n",
      "Epoch 42 Summary:\n",
      "  Average D_loss: -18.3016\n",
      "  Average G_loss: -0.1880\n",
      "\n",
      "Epoch [43/200]\n",
      "  Batch [0/31] D_loss: -32.7670, G_loss: 21.8570\n",
      "  Batch [10/31] D_loss: -29.7144, G_loss: 3.6864\n",
      "  Batch [20/31] D_loss: -36.3242, G_loss: 1.2884\n",
      "  Batch [30/31] D_loss: -42.8655, G_loss: -41.0465\n",
      "\n",
      "Epoch 43 Summary:\n",
      "  Average D_loss: -17.1954\n",
      "  Average G_loss: 6.4478\n",
      "\n",
      "Epoch [44/200]\n",
      "  Batch [0/31] D_loss: -32.2944, G_loss: -38.8121\n",
      "  Batch [10/31] D_loss: -20.6673, G_loss: 14.4106\n",
      "  Batch [20/31] D_loss: -34.9753, G_loss: -10.1443\n",
      "  Batch [30/31] D_loss: -33.3402, G_loss: 14.9853\n",
      "\n",
      "Epoch 44 Summary:\n",
      "  Average D_loss: -16.0333\n",
      "  Average G_loss: 9.0643\n",
      "\n",
      "Epoch [45/200]\n",
      "  Batch [0/31] D_loss: -35.4943, G_loss: 4.2582\n",
      "  Batch [10/31] D_loss: -36.2684, G_loss: 32.0131\n",
      "  Batch [20/31] D_loss: -41.4705, G_loss: 20.0712\n",
      "  Batch [30/31] D_loss: -36.1827, G_loss: -21.3600\n",
      "\n",
      "Epoch 45 Summary:\n",
      "  Average D_loss: -16.9249\n",
      "  Average G_loss: 1.2796\n",
      "\n",
      "Epoch [46/200]\n",
      "  Batch [0/31] D_loss: -36.9855, G_loss: -19.7259\n",
      "  Batch [10/31] D_loss: -33.3855, G_loss: -11.7630\n",
      "  Batch [20/31] D_loss: -32.0605, G_loss: 27.0114\n",
      "  Batch [30/31] D_loss: -46.4471, G_loss: -22.6573\n",
      "\n",
      "Epoch 46 Summary:\n",
      "  Average D_loss: -16.5263\n",
      "  Average G_loss: 7.3189\n",
      "\n",
      "Epoch [47/200]\n",
      "  Batch [0/31] D_loss: -37.9545, G_loss: -22.6483\n",
      "  Batch [10/31] D_loss: -26.8808, G_loss: 32.3820\n",
      "  Batch [20/31] D_loss: -34.3967, G_loss: 5.4276\n",
      "  Batch [30/31] D_loss: -28.4611, G_loss: -6.9050\n",
      "\n",
      "Epoch 47 Summary:\n",
      "  Average D_loss: -13.5384\n",
      "  Average G_loss: -0.2231\n",
      "\n",
      "Epoch [48/200]\n",
      "  Batch [0/31] D_loss: -40.0217, G_loss: 0.3378\n",
      "  Batch [10/31] D_loss: -19.8266, G_loss: 20.7149\n",
      "  Batch [20/31] D_loss: -28.6969, G_loss: -58.9697\n",
      "  Batch [30/31] D_loss: -42.9934, G_loss: 33.6335\n",
      "\n",
      "Epoch 48 Summary:\n",
      "  Average D_loss: -12.2782\n",
      "  Average G_loss: 1.8883\n",
      "\n",
      "Epoch [49/200]\n",
      "  Batch [0/31] D_loss: -41.4270, G_loss: 53.6689\n",
      "  Batch [10/31] D_loss: -31.0451, G_loss: 0.4765\n",
      "  Batch [20/31] D_loss: -14.3995, G_loss: -2.0924\n",
      "  Batch [30/31] D_loss: -10.6764, G_loss: 43.6530\n",
      "\n",
      "Epoch 49 Summary:\n",
      "  Average D_loss: -12.5709\n",
      "  Average G_loss: 12.0462\n",
      "\n",
      "Epoch [50/200]\n",
      "  Batch [0/31] D_loss: -15.4351, G_loss: 37.4522\n",
      "  Batch [10/31] D_loss: -29.9030, G_loss: -67.9906\n",
      "  Batch [20/31] D_loss: -41.7128, G_loss: 38.7413\n",
      "  Batch [30/31] D_loss: -44.1731, G_loss: 24.6931\n",
      "\n",
      "Epoch 50 Summary:\n",
      "  Average D_loss: -10.2584\n",
      "  Average G_loss: 1.4028\n",
      "\n",
      "Epoch [51/200]\n",
      "  Batch [0/31] D_loss: -38.7778, G_loss: 26.2830\n",
      "  Batch [10/31] D_loss: -19.0364, G_loss: -38.9981\n",
      "  Batch [20/31] D_loss: -25.6496, G_loss: 17.7375\n",
      "  Batch [30/31] D_loss: -33.0713, G_loss: -2.2912\n",
      "\n",
      "Epoch 51 Summary:\n",
      "  Average D_loss: -13.2393\n",
      "  Average G_loss: 2.8243\n",
      "\n",
      "Epoch [52/200]\n",
      "  Batch [0/31] D_loss: -27.1475, G_loss: -15.3914\n",
      "  Batch [10/31] D_loss: -20.6942, G_loss: 13.4858\n",
      "  Batch [20/31] D_loss: -27.1641, G_loss: -12.1429\n",
      "  Batch [30/31] D_loss: -6.5117, G_loss: 14.6306\n",
      "\n",
      "Epoch 52 Summary:\n",
      "  Average D_loss: -12.7460\n",
      "  Average G_loss: 11.2165\n",
      "\n",
      "Epoch [53/200]\n",
      "  Batch [0/31] D_loss: -11.6353, G_loss: 13.6519\n",
      "  Batch [10/31] D_loss: -15.6814, G_loss: -34.2013\n",
      "  Batch [20/31] D_loss: -25.1551, G_loss: -11.3538\n",
      "  Batch [30/31] D_loss: -38.2675, G_loss: -36.8768\n",
      "\n",
      "Epoch 53 Summary:\n",
      "  Average D_loss: -11.7612\n",
      "  Average G_loss: 0.6468\n",
      "\n",
      "Epoch [54/200]\n",
      "  Batch [0/31] D_loss: -25.4256, G_loss: -38.0958\n",
      "  Batch [10/31] D_loss: -31.7403, G_loss: 56.6072\n",
      "  Batch [20/31] D_loss: -23.5351, G_loss: 4.9217\n",
      "  Batch [30/31] D_loss: -36.4272, G_loss: 16.5145\n",
      "\n",
      "Epoch 54 Summary:\n",
      "  Average D_loss: -9.4630\n",
      "  Average G_loss: 1.1114\n",
      "\n",
      "Epoch [55/200]\n",
      "  Batch [0/31] D_loss: -42.7315, G_loss: 38.9562\n",
      "  Batch [10/31] D_loss: -37.4151, G_loss: 6.2387\n",
      "  Batch [20/31] D_loss: -35.9225, G_loss: 26.2732\n",
      "  Batch [30/31] D_loss: -47.4802, G_loss: -1.8127\n",
      "\n",
      "Epoch 55 Summary:\n",
      "  Average D_loss: -13.6743\n",
      "  Average G_loss: 8.9345\n",
      "\n",
      "Epoch [56/200]\n",
      "  Batch [0/31] D_loss: -48.9615, G_loss: -23.1994\n",
      "  Batch [10/31] D_loss: -32.9812, G_loss: -15.9293\n",
      "  Batch [20/31] D_loss: -33.4095, G_loss: -18.9666\n",
      "  Batch [30/31] D_loss: -41.2271, G_loss: 1.0312\n",
      "\n",
      "Epoch 56 Summary:\n",
      "  Average D_loss: -13.0857\n",
      "  Average G_loss: 4.6343\n",
      "\n",
      "Epoch [57/200]\n",
      "  Batch [0/31] D_loss: -36.9406, G_loss: 9.4637\n",
      "  Batch [10/31] D_loss: -35.4921, G_loss: 3.6754\n",
      "  Batch [20/31] D_loss: -31.3745, G_loss: 1.3139\n",
      "  Batch [30/31] D_loss: -36.3488, G_loss: -31.1294\n",
      "\n",
      "Epoch 57 Summary:\n",
      "  Average D_loss: -13.3812\n",
      "  Average G_loss: 2.1314\n",
      "\n",
      "Epoch [58/200]\n",
      "  Batch [0/31] D_loss: -24.7380, G_loss: -0.1566\n",
      "  Batch [10/31] D_loss: -28.3247, G_loss: -4.9613\n",
      "  Batch [20/31] D_loss: -39.4247, G_loss: 18.0502\n",
      "  Batch [30/31] D_loss: -27.1616, G_loss: -1.3289\n",
      "\n",
      "Epoch 58 Summary:\n",
      "  Average D_loss: -13.3408\n",
      "  Average G_loss: 5.4196\n",
      "\n",
      "Epoch [59/200]\n",
      "  Batch [0/31] D_loss: -30.4098, G_loss: 33.6542\n",
      "  Batch [10/31] D_loss: -27.3342, G_loss: 7.1613\n",
      "  Batch [20/31] D_loss: -36.0299, G_loss: 33.9234\n",
      "  Batch [30/31] D_loss: -33.6840, G_loss: 31.1570\n",
      "\n",
      "Epoch 59 Summary:\n",
      "  Average D_loss: -12.9127\n",
      "  Average G_loss: 11.8903\n",
      "\n",
      "Epoch [60/200]\n",
      "  Batch [0/31] D_loss: -26.3921, G_loss: 18.3895\n",
      "  Batch [10/31] D_loss: -25.4777, G_loss: -26.3405\n",
      "  Batch [20/31] D_loss: -3.1393, G_loss: 33.4343\n",
      "  Batch [30/31] D_loss: -18.4705, G_loss: -29.6177\n",
      "\n",
      "Epoch 60 Summary:\n",
      "  Average D_loss: -11.1130\n",
      "  Average G_loss: 2.8001\n",
      "\n",
      "Epoch [61/200]\n",
      "  Batch [0/31] D_loss: -27.3054, G_loss: -38.3004\n",
      "  Batch [10/31] D_loss: -36.6801, G_loss: 14.8949\n",
      "  Batch [20/31] D_loss: -34.8482, G_loss: 28.0373\n",
      "  Batch [30/31] D_loss: -30.4870, G_loss: -15.4698\n",
      "\n",
      "Epoch 61 Summary:\n",
      "  Average D_loss: -11.0196\n",
      "  Average G_loss: -1.3286\n",
      "\n",
      "Epoch [62/200]\n",
      "  Batch [0/31] D_loss: -35.3688, G_loss: -4.8512\n",
      "  Batch [10/31] D_loss: -39.2308, G_loss: -9.0814\n",
      "  Batch [20/31] D_loss: -30.2148, G_loss: -20.2780\n",
      "  Batch [30/31] D_loss: -29.8193, G_loss: 45.3750\n",
      "\n",
      "Epoch 62 Summary:\n",
      "  Average D_loss: -14.9622\n",
      "  Average G_loss: 10.5758\n",
      "\n",
      "Epoch [63/200]\n",
      "  Batch [0/31] D_loss: -23.3740, G_loss: 34.9801\n",
      "  Batch [10/31] D_loss: -27.8120, G_loss: -51.3122\n",
      "  Batch [20/31] D_loss: -31.3280, G_loss: 43.1221\n",
      "  Batch [30/31] D_loss: -36.6309, G_loss: -18.3553\n",
      "\n",
      "Epoch 63 Summary:\n",
      "  Average D_loss: -13.0173\n",
      "  Average G_loss: -0.7499\n",
      "\n",
      "Epoch [64/200]\n",
      "  Batch [0/31] D_loss: -32.1562, G_loss: -33.3806\n",
      "  Batch [10/31] D_loss: -28.9896, G_loss: 28.7026\n",
      "  Batch [20/31] D_loss: -25.5592, G_loss: -11.1427\n",
      "  Batch [30/31] D_loss: -26.6881, G_loss: 12.6123\n",
      "\n",
      "Epoch 64 Summary:\n",
      "  Average D_loss: -12.7768\n",
      "  Average G_loss: 4.2716\n",
      "\n",
      "Epoch [65/200]\n",
      "  Batch [0/31] D_loss: -22.9177, G_loss: 8.7150\n",
      "  Batch [10/31] D_loss: -29.9343, G_loss: -10.9229\n",
      "  Batch [20/31] D_loss: -35.5816, G_loss: -11.8962\n",
      "  Batch [30/31] D_loss: -29.0151, G_loss: -13.5078\n",
      "\n",
      "Epoch 65 Summary:\n",
      "  Average D_loss: -13.8454\n",
      "  Average G_loss: 7.4872\n",
      "\n",
      "Epoch [66/200]\n",
      "  Batch [0/31] D_loss: -30.3426, G_loss: -20.7392\n",
      "  Batch [10/31] D_loss: -29.8740, G_loss: 45.5553\n",
      "  Batch [20/31] D_loss: -34.2235, G_loss: -44.0954\n",
      "  Batch [30/31] D_loss: -17.7873, G_loss: 45.6393\n",
      "\n",
      "Epoch 66 Summary:\n",
      "  Average D_loss: -13.0978\n",
      "  Average G_loss: 1.0967\n",
      "\n",
      "Epoch [67/200]\n",
      "  Batch [0/31] D_loss: -22.1210, G_loss: 40.1892\n",
      "  Batch [10/31] D_loss: -36.8073, G_loss: -31.6734\n",
      "  Batch [20/31] D_loss: -35.6773, G_loss: 13.7227\n",
      "  Batch [30/31] D_loss: -24.4043, G_loss: 12.7469\n",
      "\n",
      "Epoch 67 Summary:\n",
      "  Average D_loss: -11.5193\n",
      "  Average G_loss: 5.7885\n",
      "\n",
      "Epoch [68/200]\n",
      "  Batch [0/31] D_loss: -32.6897, G_loss: 28.4997\n",
      "  Batch [10/31] D_loss: -37.7908, G_loss: 21.4567\n",
      "  Batch [20/31] D_loss: -41.3319, G_loss: -9.0729\n",
      "  Batch [30/31] D_loss: -26.4559, G_loss: -7.4437\n",
      "\n",
      "Epoch 68 Summary:\n",
      "  Average D_loss: -14.3658\n",
      "  Average G_loss: 2.7203\n",
      "\n",
      "Epoch [69/200]\n",
      "  Batch [0/31] D_loss: -22.5643, G_loss: -11.9334\n",
      "  Batch [10/31] D_loss: -21.3832, G_loss: 34.2180\n",
      "  Batch [20/31] D_loss: -31.7222, G_loss: -10.8369\n",
      "  Batch [30/31] D_loss: -47.1656, G_loss: 55.9559\n",
      "\n",
      "Epoch 69 Summary:\n",
      "  Average D_loss: -13.7941\n",
      "  Average G_loss: 2.6197\n",
      "\n",
      "Epoch [70/200]\n",
      "  Batch [0/31] D_loss: -33.4874, G_loss: 40.8353\n",
      "  Batch [10/31] D_loss: -32.5194, G_loss: 17.1021\n",
      "  Batch [20/31] D_loss: -27.5614, G_loss: -11.5347\n",
      "  Batch [30/31] D_loss: -25.3889, G_loss: 24.5934\n",
      "\n",
      "Epoch 70 Summary:\n",
      "  Average D_loss: -12.3703\n",
      "  Average G_loss: 5.7465\n",
      "\n",
      "Epoch [71/200]\n",
      "  Batch [0/31] D_loss: -26.4589, G_loss: 24.9958\n",
      "  Batch [10/31] D_loss: -18.8978, G_loss: -13.9390\n",
      "  Batch [20/31] D_loss: -26.8629, G_loss: 9.8936\n",
      "  Batch [30/31] D_loss: -27.1168, G_loss: 17.8468\n",
      "\n",
      "Epoch 71 Summary:\n",
      "  Average D_loss: -13.6388\n",
      "  Average G_loss: 9.0811\n",
      "\n",
      "Epoch [72/200]\n",
      "  Batch [0/31] D_loss: -25.1983, G_loss: 2.8119\n",
      "  Batch [10/31] D_loss: -34.9288, G_loss: 3.0636\n",
      "  Batch [20/31] D_loss: -29.3084, G_loss: -0.6991\n",
      "  Batch [30/31] D_loss: -31.1491, G_loss: 3.6359\n",
      "\n",
      "Epoch 72 Summary:\n",
      "  Average D_loss: -13.3453\n",
      "  Average G_loss: -2.7468\n",
      "\n",
      "Epoch [73/200]\n",
      "  Batch [0/31] D_loss: -24.0296, G_loss: 11.0310\n",
      "  Batch [10/31] D_loss: -30.8722, G_loss: 15.3911\n",
      "  Batch [20/31] D_loss: -30.9135, G_loss: 42.3823\n",
      "  Batch [30/31] D_loss: -52.4731, G_loss: -48.3514\n",
      "\n",
      "Epoch 73 Summary:\n",
      "  Average D_loss: -14.5661\n",
      "  Average G_loss: 8.4648\n",
      "\n",
      "Epoch [74/200]\n",
      "  Batch [0/31] D_loss: -25.4392, G_loss: -53.0188\n",
      "  Batch [10/31] D_loss: -30.7617, G_loss: 36.7414\n",
      "  Batch [20/31] D_loss: -29.0907, G_loss: -3.0903\n",
      "  Batch [30/31] D_loss: -38.5500, G_loss: -7.9013\n",
      "\n",
      "Epoch 74 Summary:\n",
      "  Average D_loss: -11.7487\n",
      "  Average G_loss: 5.3307\n",
      "\n",
      "Epoch [75/200]\n",
      "  Batch [0/31] D_loss: -38.0070, G_loss: -10.6143\n",
      "  Batch [10/31] D_loss: -29.0260, G_loss: 13.6851\n",
      "  Batch [20/31] D_loss: -33.8142, G_loss: 15.8912\n",
      "  Batch [30/31] D_loss: -36.7467, G_loss: 26.5766\n",
      "\n",
      "Epoch 75 Summary:\n",
      "  Average D_loss: -15.5999\n",
      "  Average G_loss: -1.2958\n",
      "\n",
      "Epoch [76/200]\n",
      "  Batch [0/31] D_loss: -39.5942, G_loss: 33.1696\n",
      "  Batch [10/31] D_loss: -35.5048, G_loss: 35.9015\n",
      "  Batch [20/31] D_loss: -40.8043, G_loss: -15.7616\n",
      "  Batch [30/31] D_loss: -44.1139, G_loss: 58.3452\n",
      "\n",
      "Epoch 76 Summary:\n",
      "  Average D_loss: -14.3561\n",
      "  Average G_loss: 8.5322\n",
      "\n",
      "Epoch [77/200]\n",
      "  Batch [0/31] D_loss: -9.5593, G_loss: 30.6399\n",
      "  Batch [10/31] D_loss: -38.5409, G_loss: -46.2539\n",
      "  Batch [20/31] D_loss: -27.7762, G_loss: 29.2536\n",
      "  Batch [30/31] D_loss: -10.5265, G_loss: 9.7236\n",
      "\n",
      "Epoch 77 Summary:\n",
      "  Average D_loss: -11.2101\n",
      "  Average G_loss: 3.1780\n",
      "\n",
      "Epoch [78/200]\n",
      "  Batch [0/31] D_loss: -15.3361, G_loss: 5.8336\n",
      "  Batch [10/31] D_loss: -33.0455, G_loss: -25.6473\n",
      "  Batch [20/31] D_loss: -28.9979, G_loss: 15.3202\n",
      "  Batch [30/31] D_loss: -39.9005, G_loss: 31.0129\n",
      "\n",
      "Epoch 78 Summary:\n",
      "  Average D_loss: -14.6490\n",
      "  Average G_loss: 2.1067\n",
      "\n",
      "Epoch [79/200]\n",
      "  Batch [0/31] D_loss: -16.3216, G_loss: -3.1132\n",
      "  Batch [10/31] D_loss: -38.3302, G_loss: 26.1821\n",
      "  Batch [20/31] D_loss: -32.1111, G_loss: -23.2238\n",
      "  Batch [30/31] D_loss: -35.8858, G_loss: 26.7440\n",
      "\n",
      "Epoch 79 Summary:\n",
      "  Average D_loss: -13.4017\n",
      "  Average G_loss: 6.8205\n",
      "\n",
      "Epoch [80/200]\n",
      "  Batch [0/31] D_loss: -35.0598, G_loss: 6.3493\n",
      "  Batch [10/31] D_loss: -26.1621, G_loss: 23.1788\n",
      "  Batch [20/31] D_loss: -35.5163, G_loss: -0.4136\n",
      "  Batch [30/31] D_loss: -26.7862, G_loss: -22.4486\n",
      "\n",
      "Epoch 80 Summary:\n",
      "  Average D_loss: -13.6769\n",
      "  Average G_loss: 1.1482\n",
      "\n",
      "Epoch [81/200]\n",
      "  Batch [0/31] D_loss: -24.2633, G_loss: -8.9331\n",
      "  Batch [10/31] D_loss: -32.6064, G_loss: -8.1075\n",
      "  Batch [20/31] D_loss: -29.5079, G_loss: 0.5349\n",
      "  Batch [30/31] D_loss: -32.0590, G_loss: 17.1632\n",
      "\n",
      "Epoch 81 Summary:\n",
      "  Average D_loss: -14.7226\n",
      "  Average G_loss: 0.5848\n",
      "\n",
      "Epoch [82/200]\n",
      "  Batch [0/31] D_loss: -28.5249, G_loss: 14.3133\n",
      "  Batch [10/31] D_loss: -25.9008, G_loss: 5.2381\n",
      "  Batch [20/31] D_loss: -25.9212, G_loss: -8.0160\n",
      "  Batch [30/31] D_loss: -26.3514, G_loss: 6.5453\n",
      "\n",
      "Epoch 82 Summary:\n",
      "  Average D_loss: -14.1707\n",
      "  Average G_loss: 2.7817\n",
      "\n",
      "Epoch [83/200]\n",
      "  Batch [0/31] D_loss: -29.9288, G_loss: 29.6100\n",
      "  Batch [10/31] D_loss: -30.5892, G_loss: -8.9466\n",
      "  Batch [20/31] D_loss: -39.2597, G_loss: 1.9159\n",
      "  Batch [30/31] D_loss: -34.7577, G_loss: 58.4119\n",
      "\n",
      "Epoch 83 Summary:\n",
      "  Average D_loss: -15.8704\n",
      "  Average G_loss: 13.8541\n",
      "\n",
      "Epoch [84/200]\n",
      "  Batch [0/31] D_loss: -21.4641, G_loss: 44.0865\n",
      "  Batch [10/31] D_loss: -21.6725, G_loss: -71.6141\n",
      "  Batch [20/31] D_loss: -26.7756, G_loss: 36.2003\n",
      "  Batch [30/31] D_loss: -1.2682, G_loss: 24.3773\n",
      "\n",
      "Epoch 84 Summary:\n",
      "  Average D_loss: -14.5044\n",
      "  Average G_loss: 1.0692\n",
      "\n",
      "Epoch [85/200]\n",
      "  Batch [0/31] D_loss: -3.3626, G_loss: 13.6786\n",
      "  Batch [10/31] D_loss: -33.4888, G_loss: -75.4144\n",
      "  Batch [20/31] D_loss: -11.7388, G_loss: 35.2724\n",
      "  Batch [30/31] D_loss: -28.8033, G_loss: -58.2171\n",
      "\n",
      "Epoch 85 Summary:\n",
      "  Average D_loss: -10.7188\n",
      "  Average G_loss: -9.4972\n",
      "\n",
      "Epoch [86/200]\n",
      "  Batch [0/31] D_loss: -14.2844, G_loss: -52.7800\n",
      "  Batch [10/31] D_loss: -30.0652, G_loss: 42.4084\n",
      "  Batch [20/31] D_loss: -16.8185, G_loss: 33.9053\n",
      "  Batch [30/31] D_loss: -30.6805, G_loss: -58.9702\n",
      "\n",
      "Epoch 86 Summary:\n",
      "  Average D_loss: -11.7736\n",
      "  Average G_loss: 7.7647\n",
      "\n",
      "Epoch [87/200]\n",
      "  Batch [0/31] D_loss: -32.4619, G_loss: -71.9027\n",
      "  Batch [10/31] D_loss: -36.5888, G_loss: 42.1415\n",
      "  Batch [20/31] D_loss: -55.1334, G_loss: 34.2152\n",
      "  Batch [30/31] D_loss: -35.6535, G_loss: 16.8308\n",
      "\n",
      "Epoch 87 Summary:\n",
      "  Average D_loss: -13.2271\n",
      "  Average G_loss: 0.3599\n",
      "\n",
      "Epoch [88/200]\n",
      "  Batch [0/31] D_loss: -44.3398, G_loss: 3.3908\n",
      "  Batch [10/31] D_loss: -32.1724, G_loss: 22.4763\n",
      "  Batch [20/31] D_loss: -19.4220, G_loss: -33.3920\n",
      "  Batch [30/31] D_loss: -48.6330, G_loss: 58.3715\n",
      "\n",
      "Epoch 88 Summary:\n",
      "  Average D_loss: -14.7685\n",
      "  Average G_loss: 4.0717\n",
      "\n",
      "Epoch [89/200]\n",
      "  Batch [0/31] D_loss: -44.7795, G_loss: 67.6379\n",
      "  Batch [10/31] D_loss: -35.8320, G_loss: -8.6035\n",
      "  Batch [20/31] D_loss: -41.4990, G_loss: 23.1612\n",
      "  Batch [30/31] D_loss: -39.9637, G_loss: 25.6235\n",
      "\n",
      "Epoch 89 Summary:\n",
      "  Average D_loss: -13.0679\n",
      "  Average G_loss: 9.8342\n",
      "\n",
      "Epoch [90/200]\n",
      "  Batch [0/31] D_loss: -38.0462, G_loss: 17.9722\n",
      "  Batch [10/31] D_loss: -30.9189, G_loss: 9.1527\n",
      "  Batch [20/31] D_loss: -32.8306, G_loss: 14.4876\n",
      "  Batch [30/31] D_loss: -35.1467, G_loss: 23.9407\n",
      "\n",
      "Epoch 90 Summary:\n",
      "  Average D_loss: -13.6225\n",
      "  Average G_loss: 6.1060\n",
      "\n",
      "Epoch [91/200]\n",
      "  Batch [0/31] D_loss: -32.0770, G_loss: 17.6323\n",
      "  Batch [10/31] D_loss: -30.2671, G_loss: 5.9778\n",
      "  Batch [20/31] D_loss: -38.8591, G_loss: 38.9218\n",
      "  Batch [30/31] D_loss: -30.5828, G_loss: -20.8382\n",
      "\n",
      "Epoch 91 Summary:\n",
      "  Average D_loss: -13.7413\n",
      "  Average G_loss: 2.5311\n",
      "\n",
      "Epoch [92/200]\n",
      "  Batch [0/31] D_loss: -40.9617, G_loss: -39.6095\n",
      "  Batch [10/31] D_loss: -33.7952, G_loss: 29.4821\n",
      "  Batch [20/31] D_loss: -44.3091, G_loss: 1.0758\n",
      "  Batch [30/31] D_loss: -36.4010, G_loss: -4.9630\n",
      "\n",
      "Epoch 92 Summary:\n",
      "  Average D_loss: -15.0264\n",
      "  Average G_loss: -2.7154\n",
      "\n",
      "Epoch [93/200]\n",
      "  Batch [0/31] D_loss: -30.7782, G_loss: 21.3908\n",
      "  Batch [10/31] D_loss: -33.8284, G_loss: -0.7106\n",
      "  Batch [20/31] D_loss: -33.8379, G_loss: -8.4836\n",
      "  Batch [30/31] D_loss: -28.6110, G_loss: -14.6396\n",
      "\n",
      "Epoch 93 Summary:\n",
      "  Average D_loss: -14.5295\n",
      "  Average G_loss: 5.9499\n",
      "\n",
      "Epoch [94/200]\n",
      "  Batch [0/31] D_loss: -37.9607, G_loss: -32.9388\n",
      "  Batch [10/31] D_loss: -30.6212, G_loss: 33.2727\n",
      "  Batch [20/31] D_loss: -30.6727, G_loss: -31.5701\n",
      "  Batch [30/31] D_loss: -39.1537, G_loss: 37.3189\n",
      "\n",
      "Epoch 94 Summary:\n",
      "  Average D_loss: -14.7700\n",
      "  Average G_loss: 9.6912\n",
      "\n",
      "Epoch [95/200]\n",
      "  Batch [0/31] D_loss: -25.7535, G_loss: 19.6081\n",
      "  Batch [10/31] D_loss: -21.0016, G_loss: -25.0559\n",
      "  Batch [20/31] D_loss: -27.7226, G_loss: 11.9418\n",
      "  Batch [30/31] D_loss: -30.8235, G_loss: -21.9373\n",
      "\n",
      "Epoch 95 Summary:\n",
      "  Average D_loss: -15.1977\n",
      "  Average G_loss: -0.9092\n",
      "\n",
      "Epoch [96/200]\n",
      "  Batch [0/31] D_loss: -44.6321, G_loss: -54.1528\n",
      "  Batch [10/31] D_loss: -33.9646, G_loss: 33.7682\n",
      "  Batch [20/31] D_loss: -27.1899, G_loss: 10.3124\n",
      "  Batch [30/31] D_loss: -31.4743, G_loss: 8.5227\n",
      "\n",
      "Epoch 96 Summary:\n",
      "  Average D_loss: -12.5229\n",
      "  Average G_loss: 6.5843\n",
      "\n",
      "Epoch [97/200]\n",
      "  Batch [0/31] D_loss: -28.8893, G_loss: -0.4049\n",
      "  Batch [10/31] D_loss: -35.0294, G_loss: -0.5919\n",
      "  Batch [20/31] D_loss: -30.1345, G_loss: 1.5237\n",
      "  Batch [30/31] D_loss: -33.1359, G_loss: 26.0309\n",
      "\n",
      "Epoch 97 Summary:\n",
      "  Average D_loss: -15.2017\n",
      "  Average G_loss: -1.6923\n",
      "\n",
      "Epoch [98/200]\n",
      "  Batch [0/31] D_loss: -43.7769, G_loss: 43.7806\n",
      "  Batch [10/31] D_loss: -32.8750, G_loss: -12.8808\n",
      "  Batch [20/31] D_loss: -32.6398, G_loss: -7.6667\n",
      "  Batch [30/31] D_loss: -40.0307, G_loss: 51.6069\n",
      "\n",
      "Epoch 98 Summary:\n",
      "  Average D_loss: -14.7891\n",
      "  Average G_loss: 4.9712\n",
      "\n",
      "Epoch [99/200]\n",
      "  Batch [0/31] D_loss: -24.7594, G_loss: 49.1870\n",
      "  Batch [10/31] D_loss: -33.7457, G_loss: 2.3274\n",
      "  Batch [20/31] D_loss: -37.1294, G_loss: 18.0057\n",
      "  Batch [30/31] D_loss: -31.0472, G_loss: -15.8501\n",
      "\n",
      "Epoch 99 Summary:\n",
      "  Average D_loss: -13.4066\n",
      "  Average G_loss: 5.0437\n",
      "\n",
      "Epoch [100/200]\n",
      "  Batch [0/31] D_loss: -43.3337, G_loss: -23.1530\n",
      "  Batch [10/31] D_loss: -43.8886, G_loss: 32.7755\n",
      "  Batch [20/31] D_loss: -25.5433, G_loss: -37.8050\n",
      "  Batch [30/31] D_loss: -36.0525, G_loss: 11.0057\n",
      "\n",
      "Epoch 100 Summary:\n",
      "  Average D_loss: -15.0575\n",
      "  Average G_loss: 2.7209\n",
      "\n",
      "Epoch [101/200]\n",
      "  Batch [0/31] D_loss: -17.4474, G_loss: -11.0940\n",
      "  Batch [10/31] D_loss: -41.9155, G_loss: 43.2462\n",
      "  Batch [20/31] D_loss: -35.0944, G_loss: 13.1708\n",
      "  Batch [30/31] D_loss: -37.8268, G_loss: -14.0924\n",
      "\n",
      "Epoch 101 Summary:\n",
      "  Average D_loss: -13.3405\n",
      "  Average G_loss: 7.1958\n",
      "\n",
      "Epoch [102/200]\n",
      "  Batch [0/31] D_loss: -35.9851, G_loss: -21.3403\n",
      "  Batch [10/31] D_loss: -29.8509, G_loss: -21.7728\n",
      "  Batch [20/31] D_loss: -29.5874, G_loss: -14.7201\n",
      "  Batch [30/31] D_loss: -33.0764, G_loss: 25.1814\n",
      "\n",
      "Epoch 102 Summary:\n",
      "  Average D_loss: -16.0104\n",
      "  Average G_loss: -3.1894\n",
      "\n",
      "Epoch [103/200]\n",
      "  Batch [0/31] D_loss: -29.8215, G_loss: 33.9103\n",
      "  Batch [10/31] D_loss: -35.4970, G_loss: -5.2799\n",
      "  Batch [20/31] D_loss: -46.4146, G_loss: -12.5810\n",
      "  Batch [30/31] D_loss: -34.0973, G_loss: 1.5382\n",
      "\n",
      "Epoch 103 Summary:\n",
      "  Average D_loss: -15.6235\n",
      "  Average G_loss: -4.0729\n",
      "\n",
      "Epoch [104/200]\n",
      "  Batch [0/31] D_loss: -39.0065, G_loss: 16.0083\n",
      "  Batch [10/31] D_loss: -33.5746, G_loss: 20.5347\n",
      "  Batch [20/31] D_loss: -28.9772, G_loss: -28.1199\n",
      "  Batch [30/31] D_loss: -26.3399, G_loss: -4.9362\n",
      "\n",
      "Epoch 104 Summary:\n",
      "  Average D_loss: -14.3054\n",
      "  Average G_loss: 12.0939\n",
      "\n",
      "Epoch [105/200]\n",
      "  Batch [0/31] D_loss: -29.5172, G_loss: -12.4061\n",
      "  Batch [10/31] D_loss: -40.9558, G_loss: 10.7829\n",
      "  Batch [20/31] D_loss: -37.2280, G_loss: -3.1906\n",
      "  Batch [30/31] D_loss: -31.1896, G_loss: 4.2304\n",
      "\n",
      "Epoch 105 Summary:\n",
      "  Average D_loss: -14.4643\n",
      "  Average G_loss: 6.4422\n",
      "\n",
      "Epoch [106/200]\n",
      "  Batch [0/31] D_loss: -31.4055, G_loss: 5.4455\n",
      "  Batch [10/31] D_loss: -29.4370, G_loss: -1.4645\n",
      "  Batch [20/31] D_loss: -28.8128, G_loss: -0.6473\n",
      "  Batch [30/31] D_loss: -37.5560, G_loss: 5.6915\n",
      "\n",
      "Epoch 106 Summary:\n",
      "  Average D_loss: -16.0266\n",
      "  Average G_loss: -2.2646\n",
      "\n",
      "Epoch [107/200]\n",
      "  Batch [0/31] D_loss: -39.7634, G_loss: -2.6387\n",
      "  Batch [10/31] D_loss: -34.3436, G_loss: 24.8799\n",
      "  Batch [20/31] D_loss: -32.2798, G_loss: -49.4629\n",
      "  Batch [30/31] D_loss: -19.9277, G_loss: 14.7861\n",
      "\n",
      "Epoch 107 Summary:\n",
      "  Average D_loss: -15.3020\n",
      "  Average G_loss: 1.4254\n",
      "\n",
      "Epoch [108/200]\n",
      "  Batch [0/31] D_loss: -17.3968, G_loss: -13.1809\n",
      "  Batch [10/31] D_loss: -33.0901, G_loss: 8.0084\n",
      "  Batch [20/31] D_loss: -36.6763, G_loss: -2.6132\n",
      "  Batch [30/31] D_loss: -30.3116, G_loss: 7.0384\n",
      "\n",
      "Epoch 108 Summary:\n",
      "  Average D_loss: -13.8095\n",
      "  Average G_loss: 4.1021\n",
      "\n",
      "Epoch [109/200]\n",
      "  Batch [0/31] D_loss: -36.4563, G_loss: 30.1672\n",
      "  Batch [10/31] D_loss: -32.5388, G_loss: -2.4088\n",
      "  Batch [20/31] D_loss: -33.1118, G_loss: 3.8632\n",
      "  Batch [30/31] D_loss: -33.8315, G_loss: -17.9108\n",
      "\n",
      "Epoch 109 Summary:\n",
      "  Average D_loss: -16.0928\n",
      "  Average G_loss: 2.9553\n",
      "\n",
      "Epoch [110/200]\n",
      "  Batch [0/31] D_loss: -23.2442, G_loss: -15.5415\n",
      "  Batch [10/31] D_loss: -38.4056, G_loss: 26.7596\n",
      "  Batch [20/31] D_loss: -37.1809, G_loss: -8.3148\n",
      "  Batch [30/31] D_loss: -32.7487, G_loss: 29.0805\n",
      "\n",
      "Epoch 110 Summary:\n",
      "  Average D_loss: -14.2294\n",
      "  Average G_loss: 1.3019\n",
      "\n",
      "Epoch [111/200]\n",
      "  Batch [0/31] D_loss: -39.4141, G_loss: 34.4150\n",
      "  Batch [10/31] D_loss: -37.7298, G_loss: 22.0564\n",
      "  Batch [20/31] D_loss: -23.1164, G_loss: -45.0840\n",
      "  Batch [30/31] D_loss: -39.6592, G_loss: 48.4469\n",
      "\n",
      "Epoch 111 Summary:\n",
      "  Average D_loss: -17.0110\n",
      "  Average G_loss: -0.1098\n",
      "\n",
      "Epoch [112/200]\n",
      "  Batch [0/31] D_loss: -43.4861, G_loss: 52.0636\n",
      "  Batch [10/31] D_loss: -33.5706, G_loss: 8.4170\n",
      "  Batch [20/31] D_loss: -28.9292, G_loss: -48.3638\n",
      "  Batch [30/31] D_loss: -32.1184, G_loss: 0.2292\n",
      "\n",
      "Epoch 112 Summary:\n",
      "  Average D_loss: -14.6337\n",
      "  Average G_loss: 4.9394\n",
      "\n",
      "Epoch [113/200]\n",
      "  Batch [0/31] D_loss: -40.7124, G_loss: 19.1993\n",
      "  Batch [10/31] D_loss: -36.7417, G_loss: 5.2632\n",
      "  Batch [20/31] D_loss: -32.4591, G_loss: 27.1741\n",
      "  Batch [30/31] D_loss: -22.2152, G_loss: -16.6994\n",
      "\n",
      "Epoch 113 Summary:\n",
      "  Average D_loss: -14.0672\n",
      "  Average G_loss: 3.9695\n",
      "\n",
      "Epoch [114/200]\n",
      "  Batch [0/31] D_loss: -19.8972, G_loss: -10.2173\n",
      "  Batch [10/31] D_loss: -40.1919, G_loss: 21.3427\n",
      "  Batch [20/31] D_loss: -37.3349, G_loss: -15.2517\n",
      "  Batch [30/31] D_loss: -34.3043, G_loss: 24.2986\n",
      "\n",
      "Epoch 114 Summary:\n",
      "  Average D_loss: -15.3216\n",
      "  Average G_loss: 4.2833\n",
      "\n",
      "Epoch [115/200]\n",
      "  Batch [0/31] D_loss: -34.2193, G_loss: 26.0430\n",
      "  Batch [10/31] D_loss: -39.8560, G_loss: -12.3665\n",
      "  Batch [20/31] D_loss: -29.5139, G_loss: 22.3606\n",
      "  Batch [30/31] D_loss: -28.4486, G_loss: 7.7714\n",
      "\n",
      "Epoch 115 Summary:\n",
      "  Average D_loss: -14.9614\n",
      "  Average G_loss: 0.7863\n",
      "\n",
      "Epoch [116/200]\n",
      "  Batch [0/31] D_loss: -27.4400, G_loss: -20.0145\n",
      "  Batch [10/31] D_loss: -36.9152, G_loss: -8.1845\n",
      "  Batch [20/31] D_loss: -30.3376, G_loss: 11.5474\n",
      "  Batch [30/31] D_loss: -36.3404, G_loss: -16.6927\n",
      "\n",
      "Epoch 116 Summary:\n",
      "  Average D_loss: -14.7879\n",
      "  Average G_loss: 4.5750\n",
      "\n",
      "Epoch [117/200]\n",
      "  Batch [0/31] D_loss: -40.9837, G_loss: -8.4102\n",
      "  Batch [10/31] D_loss: -31.7342, G_loss: -10.6094\n",
      "  Batch [20/31] D_loss: -27.4191, G_loss: 21.6696\n",
      "  Batch [30/31] D_loss: -32.0143, G_loss: -12.5446\n",
      "\n",
      "Epoch 117 Summary:\n",
      "  Average D_loss: -14.9921\n",
      "  Average G_loss: 1.7941\n",
      "\n",
      "Epoch [118/200]\n",
      "  Batch [0/31] D_loss: -40.5621, G_loss: -26.4463\n",
      "  Batch [10/31] D_loss: -36.7724, G_loss: 11.7410\n",
      "  Batch [20/31] D_loss: -37.8493, G_loss: 1.3803\n",
      "  Batch [30/31] D_loss: -19.2396, G_loss: 10.7746\n",
      "\n",
      "Epoch 118 Summary:\n",
      "  Average D_loss: -16.2142\n",
      "  Average G_loss: -3.6829\n",
      "\n",
      "Epoch [119/200]\n",
      "  Batch [0/31] D_loss: -25.0861, G_loss: 17.5200\n",
      "  Batch [10/31] D_loss: -30.4323, G_loss: -3.2167\n",
      "  Batch [20/31] D_loss: -47.7304, G_loss: -36.2599\n",
      "  Batch [30/31] D_loss: -36.7033, G_loss: 48.0863\n",
      "\n",
      "Epoch 119 Summary:\n",
      "  Average D_loss: -16.6156\n",
      "  Average G_loss: 9.5408\n",
      "\n",
      "Epoch [120/200]\n",
      "  Batch [0/31] D_loss: -25.5795, G_loss: 19.8400\n",
      "  Batch [10/31] D_loss: -41.0404, G_loss: 0.0853\n",
      "  Batch [20/31] D_loss: -35.9441, G_loss: 34.0854\n",
      "  Batch [30/31] D_loss: -28.6399, G_loss: -13.8364\n",
      "\n",
      "Epoch 120 Summary:\n",
      "  Average D_loss: -15.7409\n",
      "  Average G_loss: -0.5581\n",
      "\n",
      "Epoch [121/200]\n",
      "  Batch [0/31] D_loss: -26.7440, G_loss: -13.8081\n",
      "  Batch [10/31] D_loss: -35.2094, G_loss: 51.1244\n",
      "  Batch [20/31] D_loss: -29.2704, G_loss: -1.7492\n",
      "  Batch [30/31] D_loss: -48.0715, G_loss: 44.5258\n",
      "\n",
      "Epoch 121 Summary:\n",
      "  Average D_loss: -17.2927\n",
      "  Average G_loss: 3.7998\n",
      "\n",
      "Epoch [122/200]\n",
      "  Batch [0/31] D_loss: -20.2136, G_loss: 9.0669\n",
      "  Batch [10/31] D_loss: -30.7326, G_loss: -9.7428\n",
      "  Batch [20/31] D_loss: -26.5384, G_loss: -18.9743\n",
      "  Batch [30/31] D_loss: -39.4552, G_loss: 42.6695\n",
      "\n",
      "Epoch 122 Summary:\n",
      "  Average D_loss: -15.4288\n",
      "  Average G_loss: 0.4054\n",
      "\n",
      "Epoch [123/200]\n",
      "  Batch [0/31] D_loss: -33.2234, G_loss: 39.0388\n",
      "  Batch [10/31] D_loss: -33.9507, G_loss: 4.0048\n",
      "  Batch [20/31] D_loss: -40.6350, G_loss: -36.1157\n",
      "  Batch [30/31] D_loss: -36.8761, G_loss: 45.4044\n",
      "\n",
      "Epoch 123 Summary:\n",
      "  Average D_loss: -15.6942\n",
      "  Average G_loss: 1.3201\n",
      "\n",
      "Epoch [124/200]\n",
      "  Batch [0/31] D_loss: -42.6812, G_loss: 45.0241\n",
      "  Batch [10/31] D_loss: -34.3559, G_loss: 3.2590\n",
      "  Batch [20/31] D_loss: -30.5484, G_loss: -10.8294\n",
      "  Batch [30/31] D_loss: -51.3587, G_loss: 42.1588\n",
      "\n",
      "Epoch 124 Summary:\n",
      "  Average D_loss: -15.1218\n",
      "  Average G_loss: 6.4632\n",
      "\n",
      "Epoch [125/200]\n",
      "  Batch [0/31] D_loss: -22.8991, G_loss: 23.1956\n",
      "  Batch [10/31] D_loss: -25.3176, G_loss: -11.5368\n",
      "  Batch [20/31] D_loss: -34.1595, G_loss: 26.7338\n",
      "  Batch [30/31] D_loss: -33.7220, G_loss: -8.9152\n",
      "\n",
      "Epoch 125 Summary:\n",
      "  Average D_loss: -13.8626\n",
      "  Average G_loss: 6.3426\n",
      "\n",
      "Epoch [126/200]\n",
      "  Batch [0/31] D_loss: -35.3263, G_loss: -35.6262\n",
      "  Batch [10/31] D_loss: -25.3449, G_loss: 31.4097\n",
      "  Batch [20/31] D_loss: -40.3519, G_loss: -63.7829\n",
      "  Batch [30/31] D_loss: -28.8094, G_loss: -1.4642\n",
      "\n",
      "Epoch 126 Summary:\n",
      "  Average D_loss: -14.9792\n",
      "  Average G_loss: -5.8566\n",
      "\n",
      "Epoch [127/200]\n",
      "  Batch [0/31] D_loss: -27.7103, G_loss: 16.2411\n",
      "  Batch [10/31] D_loss: -35.8894, G_loss: -12.3335\n",
      "  Batch [20/31] D_loss: -39.3959, G_loss: 25.4439\n",
      "  Batch [30/31] D_loss: -44.1567, G_loss: 26.6718\n",
      "\n",
      "Epoch 127 Summary:\n",
      "  Average D_loss: -15.2931\n",
      "  Average G_loss: 6.0937\n",
      "\n",
      "Epoch [128/200]\n",
      "  Batch [0/31] D_loss: -40.2481, G_loss: 7.1074\n",
      "  Batch [10/31] D_loss: -44.8078, G_loss: 22.3212\n",
      "  Batch [20/31] D_loss: -28.8349, G_loss: 20.5747\n",
      "  Batch [30/31] D_loss: -30.8006, G_loss: -22.0543\n",
      "\n",
      "Epoch 128 Summary:\n",
      "  Average D_loss: -14.8446\n",
      "  Average G_loss: 0.1223\n",
      "\n",
      "Epoch [129/200]\n",
      "  Batch [0/31] D_loss: -36.3553, G_loss: -31.6308\n",
      "  Batch [10/31] D_loss: -44.3093, G_loss: 22.7815\n",
      "  Batch [20/31] D_loss: -38.5569, G_loss: -7.5837\n",
      "  Batch [30/31] D_loss: -41.7832, G_loss: 25.3698\n",
      "\n",
      "Epoch 129 Summary:\n",
      "  Average D_loss: -16.7572\n",
      "  Average G_loss: 3.3658\n",
      "\n",
      "Epoch [130/200]\n",
      "  Batch [0/31] D_loss: -32.9765, G_loss: 12.8354\n",
      "  Batch [10/31] D_loss: -41.4269, G_loss: 12.4220\n",
      "  Batch [20/31] D_loss: -36.8640, G_loss: 24.6029\n",
      "  Batch [30/31] D_loss: -27.2894, G_loss: -25.8610\n",
      "\n",
      "Epoch 130 Summary:\n",
      "  Average D_loss: -15.9927\n",
      "  Average G_loss: 7.0607\n",
      "\n",
      "Epoch [131/200]\n",
      "  Batch [0/31] D_loss: -31.2872, G_loss: -18.2188\n",
      "  Batch [10/31] D_loss: -34.5079, G_loss: -29.0958\n",
      "  Batch [20/31] D_loss: -48.1818, G_loss: 46.2584\n",
      "  Batch [30/31] D_loss: -41.9243, G_loss: 12.5534\n",
      "\n",
      "Epoch 131 Summary:\n",
      "  Average D_loss: -17.7554\n",
      "  Average G_loss: 0.2275\n",
      "\n",
      "Epoch [132/200]\n",
      "  Batch [0/31] D_loss: -31.7509, G_loss: 4.4838\n",
      "  Batch [10/31] D_loss: -34.8841, G_loss: 22.7618\n",
      "  Batch [20/31] D_loss: -24.1983, G_loss: -7.3407\n",
      "  Batch [30/31] D_loss: -41.8116, G_loss: -25.3221\n",
      "\n",
      "Epoch 132 Summary:\n",
      "  Average D_loss: -15.5010\n",
      "  Average G_loss: -1.9089\n",
      "\n",
      "Epoch [133/200]\n",
      "  Batch [0/31] D_loss: -29.7254, G_loss: -29.3186\n",
      "  Batch [10/31] D_loss: -24.9812, G_loss: 2.9107\n",
      "  Batch [20/31] D_loss: -32.5440, G_loss: -27.9233\n",
      "  Batch [30/31] D_loss: -31.3874, G_loss: 5.0029\n",
      "\n",
      "Epoch 133 Summary:\n",
      "  Average D_loss: -14.6501\n",
      "  Average G_loss: 2.3726\n",
      "\n",
      "Epoch [134/200]\n",
      "  Batch [0/31] D_loss: -31.7861, G_loss: -0.2156\n",
      "  Batch [10/31] D_loss: -33.3834, G_loss: 27.4644\n",
      "  Batch [20/31] D_loss: -26.6176, G_loss: 5.8511\n",
      "  Batch [30/31] D_loss: -38.4582, G_loss: 22.4703\n",
      "\n",
      "Epoch 134 Summary:\n",
      "  Average D_loss: -14.9100\n",
      "  Average G_loss: 8.5398\n",
      "\n",
      "Epoch [135/200]\n",
      "  Batch [0/31] D_loss: -31.2596, G_loss: 2.8521\n",
      "  Batch [10/31] D_loss: -42.5273, G_loss: -13.0065\n",
      "  Batch [20/31] D_loss: -36.9984, G_loss: -24.7277\n",
      "  Batch [30/31] D_loss: -31.8062, G_loss: 27.6252\n",
      "\n",
      "Epoch 135 Summary:\n",
      "  Average D_loss: -16.6214\n",
      "  Average G_loss: 0.6393\n",
      "\n",
      "Epoch [136/200]\n",
      "  Batch [0/31] D_loss: -20.1493, G_loss: 22.2204\n",
      "  Batch [10/31] D_loss: -37.5845, G_loss: 6.6672\n",
      "  Batch [20/31] D_loss: -34.7411, G_loss: 35.9918\n",
      "  Batch [30/31] D_loss: -30.7719, G_loss: 12.2510\n",
      "\n",
      "Epoch 136 Summary:\n",
      "  Average D_loss: -15.6755\n",
      "  Average G_loss: -3.2255\n",
      "\n",
      "Epoch [137/200]\n",
      "  Batch [0/31] D_loss: -41.0521, G_loss: 25.6289\n",
      "  Batch [10/31] D_loss: -30.8545, G_loss: 14.3559\n",
      "  Batch [20/31] D_loss: -23.5694, G_loss: 10.4080\n",
      "  Batch [30/31] D_loss: -36.2613, G_loss: -15.4293\n",
      "\n",
      "Epoch 137 Summary:\n",
      "  Average D_loss: -15.3984\n",
      "  Average G_loss: 5.1369\n",
      "\n",
      "Epoch [138/200]\n",
      "  Batch [0/31] D_loss: -24.1287, G_loss: 9.8065\n",
      "  Batch [10/31] D_loss: -35.7910, G_loss: -30.3308\n",
      "  Batch [20/31] D_loss: -26.9810, G_loss: 10.0541\n",
      "  Batch [30/31] D_loss: -39.2725, G_loss: 17.5171\n",
      "\n",
      "Epoch 138 Summary:\n",
      "  Average D_loss: -16.0725\n",
      "  Average G_loss: -1.2498\n",
      "\n",
      "Epoch [139/200]\n",
      "  Batch [0/31] D_loss: -30.8862, G_loss: 2.8014\n",
      "  Batch [10/31] D_loss: -41.0713, G_loss: -3.7960\n",
      "  Batch [20/31] D_loss: -31.9854, G_loss: -28.5175\n",
      "  Batch [30/31] D_loss: -32.8524, G_loss: 32.4384\n",
      "\n",
      "Epoch 139 Summary:\n",
      "  Average D_loss: -16.6210\n",
      "  Average G_loss: 4.2264\n",
      "\n",
      "Epoch [140/200]\n",
      "  Batch [0/31] D_loss: -30.5321, G_loss: 2.9440\n",
      "  Batch [10/31] D_loss: -39.3093, G_loss: -6.2673\n",
      "  Batch [20/31] D_loss: -28.4464, G_loss: -4.5584\n",
      "  Batch [30/31] D_loss: -31.2301, G_loss: 24.1727\n",
      "\n",
      "Epoch 140 Summary:\n",
      "  Average D_loss: -15.8754\n",
      "  Average G_loss: 1.5947\n",
      "\n",
      "Epoch [141/200]\n",
      "  Batch [0/31] D_loss: -31.6011, G_loss: 23.0740\n",
      "  Batch [10/31] D_loss: -34.4383, G_loss: -21.0078\n",
      "  Batch [20/31] D_loss: -31.6137, G_loss: -43.3294\n",
      "  Batch [30/31] D_loss: -44.9955, G_loss: 47.2287\n",
      "\n",
      "Epoch 141 Summary:\n",
      "  Average D_loss: -16.3799\n",
      "  Average G_loss: -3.8509\n",
      "\n",
      "Epoch [142/200]\n",
      "  Batch [0/31] D_loss: -22.2583, G_loss: 18.1668\n",
      "  Batch [10/31] D_loss: -30.5095, G_loss: 9.4163\n",
      "  Batch [20/31] D_loss: -36.0580, G_loss: -14.6028\n",
      "  Batch [30/31] D_loss: -31.9805, G_loss: 7.4294\n",
      "\n",
      "Epoch 142 Summary:\n",
      "  Average D_loss: -16.4746\n",
      "  Average G_loss: 6.3995\n",
      "\n",
      "Epoch [143/200]\n",
      "  Batch [0/31] D_loss: -36.1734, G_loss: 39.2247\n",
      "  Batch [10/31] D_loss: -31.5485, G_loss: -5.5799\n",
      "  Batch [20/31] D_loss: -31.4148, G_loss: 5.8517\n",
      "  Batch [30/31] D_loss: -31.7541, G_loss: -0.1427\n",
      "\n",
      "Epoch 143 Summary:\n",
      "  Average D_loss: -15.2763\n",
      "  Average G_loss: 4.3978\n",
      "\n",
      "Epoch [144/200]\n",
      "  Batch [0/31] D_loss: -27.1570, G_loss: 4.1458\n",
      "  Batch [10/31] D_loss: -38.2385, G_loss: 19.5031\n",
      "  Batch [20/31] D_loss: -33.2049, G_loss: -5.7865\n",
      "  Batch [30/31] D_loss: -31.5861, G_loss: -10.9630\n",
      "\n",
      "Epoch 144 Summary:\n",
      "  Average D_loss: -14.8916\n",
      "  Average G_loss: 2.3744\n",
      "\n",
      "Epoch [145/200]\n",
      "  Batch [0/31] D_loss: -35.2535, G_loss: 11.8112\n",
      "  Batch [10/31] D_loss: -28.7021, G_loss: 8.1302\n",
      "  Batch [20/31] D_loss: -26.9216, G_loss: -4.7094\n",
      "  Batch [30/31] D_loss: -33.5729, G_loss: 24.0529\n",
      "\n",
      "Epoch 145 Summary:\n",
      "  Average D_loss: -15.7256\n",
      "  Average G_loss: -1.1044\n",
      "\n",
      "Epoch [146/200]\n",
      "  Batch [0/31] D_loss: -31.0020, G_loss: 5.9407\n",
      "  Batch [10/31] D_loss: -33.5145, G_loss: -9.2510\n",
      "  Batch [20/31] D_loss: -31.4089, G_loss: -10.4912\n",
      "  Batch [30/31] D_loss: -37.1516, G_loss: 21.8202\n",
      "\n",
      "Epoch 146 Summary:\n",
      "  Average D_loss: -15.8306\n",
      "  Average G_loss: 7.2163\n",
      "\n",
      "Epoch [147/200]\n",
      "  Batch [0/31] D_loss: -39.3119, G_loss: 28.4672\n",
      "  Batch [10/31] D_loss: -37.5268, G_loss: -20.2348\n",
      "  Batch [20/31] D_loss: -41.9226, G_loss: -21.5678\n",
      "  Batch [30/31] D_loss: -38.1204, G_loss: 15.1724\n",
      "\n",
      "Epoch 147 Summary:\n",
      "  Average D_loss: -16.2886\n",
      "  Average G_loss: -0.9300\n",
      "\n",
      "Epoch [148/200]\n",
      "  Batch [0/31] D_loss: -23.7541, G_loss: -17.9705\n",
      "  Batch [10/31] D_loss: -43.5602, G_loss: 20.7296\n",
      "  Batch [20/31] D_loss: -32.2047, G_loss: -5.9939\n",
      "  Batch [30/31] D_loss: -39.0951, G_loss: -25.4487\n",
      "\n",
      "Epoch 148 Summary:\n",
      "  Average D_loss: -17.6332\n",
      "  Average G_loss: -2.4640\n",
      "\n",
      "Epoch [149/200]\n",
      "  Batch [0/31] D_loss: -29.0541, G_loss: -30.8164\n",
      "  Batch [10/31] D_loss: -29.9616, G_loss: -2.3972\n",
      "  Batch [20/31] D_loss: -36.2802, G_loss: 10.6994\n",
      "  Batch [30/31] D_loss: -44.5118, G_loss: -7.3632\n",
      "\n",
      "Epoch 149 Summary:\n",
      "  Average D_loss: -16.9843\n",
      "  Average G_loss: 4.2330\n",
      "\n",
      "Epoch [150/200]\n",
      "  Batch [0/31] D_loss: -43.4593, G_loss: -6.8411\n",
      "  Batch [10/31] D_loss: -36.6573, G_loss: 16.8941\n",
      "  Batch [20/31] D_loss: -23.6692, G_loss: 1.7259\n",
      "  Batch [30/31] D_loss: -35.3154, G_loss: 9.3749\n",
      "\n",
      "Epoch 150 Summary:\n",
      "  Average D_loss: -16.9935\n",
      "  Average G_loss: 0.4238\n",
      "\n",
      "Epoch [151/200]\n",
      "  Batch [0/31] D_loss: -44.1669, G_loss: 31.9019\n",
      "  Batch [10/31] D_loss: -36.3002, G_loss: -14.2660\n",
      "  Batch [20/31] D_loss: -27.9389, G_loss: -4.1211\n",
      "  Batch [30/31] D_loss: -42.9061, G_loss: 29.1143\n",
      "\n",
      "Epoch 151 Summary:\n",
      "  Average D_loss: -16.8733\n",
      "  Average G_loss: 2.5638\n",
      "\n",
      "Epoch [152/200]\n",
      "  Batch [0/31] D_loss: -40.8331, G_loss: 36.1713\n",
      "  Batch [10/31] D_loss: -43.5949, G_loss: -12.8011\n",
      "  Batch [20/31] D_loss: -31.5084, G_loss: 21.1105\n",
      "  Batch [30/31] D_loss: -37.6321, G_loss: 14.8873\n",
      "\n",
      "Epoch 152 Summary:\n",
      "  Average D_loss: -16.9956\n",
      "  Average G_loss: 6.2223\n",
      "\n",
      "Epoch [153/200]\n",
      "  Batch [0/31] D_loss: -29.1342, G_loss: 15.0414\n",
      "  Batch [10/31] D_loss: -32.2899, G_loss: -9.8028\n",
      "  Batch [20/31] D_loss: -29.9105, G_loss: 13.9738\n",
      "  Batch [30/31] D_loss: -46.7092, G_loss: -25.0835\n",
      "\n",
      "Epoch 153 Summary:\n",
      "  Average D_loss: -16.0689\n",
      "  Average G_loss: -4.0514\n",
      "\n",
      "Epoch [154/200]\n",
      "  Batch [0/31] D_loss: -35.9796, G_loss: -29.2067\n",
      "  Batch [10/31] D_loss: -31.3692, G_loss: 22.2659\n",
      "  Batch [20/31] D_loss: -17.1688, G_loss: 40.4850\n",
      "  Batch [30/31] D_loss: -36.3500, G_loss: -16.7875\n",
      "\n",
      "Epoch 154 Summary:\n",
      "  Average D_loss: -16.0772\n",
      "  Average G_loss: 2.9732\n",
      "\n",
      "Epoch [155/200]\n",
      "  Batch [0/31] D_loss: -34.5129, G_loss: -23.2815\n",
      "  Batch [10/31] D_loss: -26.6653, G_loss: -6.8382\n",
      "  Batch [20/31] D_loss: -26.6953, G_loss: -16.4772\n",
      "  Batch [30/31] D_loss: -25.3461, G_loss: 26.5872\n",
      "\n",
      "Epoch 155 Summary:\n",
      "  Average D_loss: -15.0334\n",
      "  Average G_loss: -1.4637\n",
      "\n",
      "Epoch [156/200]\n",
      "  Batch [0/31] D_loss: -36.8851, G_loss: 35.5213\n",
      "  Batch [10/31] D_loss: -36.2982, G_loss: 13.5314\n",
      "  Batch [20/31] D_loss: -42.0638, G_loss: -25.6974\n",
      "  Batch [30/31] D_loss: -24.7810, G_loss: 10.4925\n",
      "\n",
      "Epoch 156 Summary:\n",
      "  Average D_loss: -17.0969\n",
      "  Average G_loss: -0.1444\n",
      "\n",
      "Epoch [157/200]\n",
      "  Batch [0/31] D_loss: -31.5963, G_loss: -6.5850\n",
      "  Batch [10/31] D_loss: -32.9285, G_loss: 19.4272\n",
      "  Batch [20/31] D_loss: -30.4027, G_loss: 12.4730\n",
      "  Batch [30/31] D_loss: -38.3594, G_loss: 11.7284\n",
      "\n",
      "Epoch 157 Summary:\n",
      "  Average D_loss: -15.8783\n",
      "  Average G_loss: 5.9369\n",
      "\n",
      "Epoch [158/200]\n",
      "  Batch [0/31] D_loss: -35.6436, G_loss: 17.3578\n",
      "  Batch [10/31] D_loss: -29.1315, G_loss: -12.8264\n",
      "  Batch [20/31] D_loss: -17.0750, G_loss: 10.9863\n",
      "  Batch [30/31] D_loss: -26.2508, G_loss: -13.5955\n",
      "\n",
      "Epoch 158 Summary:\n",
      "  Average D_loss: -13.5813\n",
      "  Average G_loss: 5.3428\n",
      "\n",
      "Epoch [159/200]\n",
      "  Batch [0/31] D_loss: -33.4397, G_loss: 16.1939\n",
      "  Batch [10/31] D_loss: -32.3055, G_loss: 18.2882\n",
      "  Batch [20/31] D_loss: -34.3930, G_loss: 16.0039\n",
      "  Batch [30/31] D_loss: -35.9923, G_loss: -6.8407\n",
      "\n",
      "Epoch 159 Summary:\n",
      "  Average D_loss: -16.0977\n",
      "  Average G_loss: -5.1865\n",
      "\n",
      "Epoch [160/200]\n",
      "  Batch [0/31] D_loss: -39.7757, G_loss: -21.3407\n",
      "  Batch [10/31] D_loss: -48.9867, G_loss: 31.8904\n",
      "  Batch [20/31] D_loss: -39.4618, G_loss: -34.4755\n",
      "  Batch [30/31] D_loss: -39.0332, G_loss: 29.0563\n",
      "\n",
      "Epoch 160 Summary:\n",
      "  Average D_loss: -17.5274\n",
      "  Average G_loss: -0.3733\n",
      "\n",
      "Epoch [161/200]\n",
      "  Batch [0/31] D_loss: -35.7702, G_loss: 11.7839\n",
      "  Batch [10/31] D_loss: -25.9438, G_loss: 8.8274\n",
      "  Batch [20/31] D_loss: -20.3750, G_loss: 1.9119\n",
      "  Batch [30/31] D_loss: -44.9865, G_loss: 39.1135\n",
      "\n",
      "Epoch 161 Summary:\n",
      "  Average D_loss: -15.0841\n",
      "  Average G_loss: 4.7480\n",
      "\n",
      "Epoch [162/200]\n",
      "  Batch [0/31] D_loss: -36.4883, G_loss: 32.8657\n",
      "  Batch [10/31] D_loss: -31.6853, G_loss: 15.5745\n",
      "  Batch [20/31] D_loss: -24.5066, G_loss: -34.1852\n",
      "  Batch [30/31] D_loss: -41.7289, G_loss: 33.5089\n",
      "\n",
      "Epoch 162 Summary:\n",
      "  Average D_loss: -13.9737\n",
      "  Average G_loss: -1.4172\n",
      "\n",
      "Epoch [163/200]\n",
      "  Batch [0/31] D_loss: -38.6468, G_loss: 34.2960\n",
      "  Batch [10/31] D_loss: -35.8173, G_loss: 24.8654\n",
      "  Batch [20/31] D_loss: -24.5658, G_loss: 6.1675\n",
      "  Batch [30/31] D_loss: -40.9522, G_loss: 23.7592\n",
      "\n",
      "Epoch 163 Summary:\n",
      "  Average D_loss: -15.2504\n",
      "  Average G_loss: 6.3001\n",
      "\n",
      "Epoch [164/200]\n",
      "  Batch [0/31] D_loss: -46.5133, G_loss: 31.4856\n",
      "  Batch [10/31] D_loss: -12.0188, G_loss: -33.6609\n",
      "  Batch [20/31] D_loss: -20.8753, G_loss: 32.2495\n",
      "  Batch [30/31] D_loss: -29.8657, G_loss: -5.3485\n",
      "\n",
      "Epoch 164 Summary:\n",
      "  Average D_loss: -14.1713\n",
      "  Average G_loss: -3.6459\n",
      "\n",
      "Epoch [165/200]\n",
      "  Batch [0/31] D_loss: -33.1260, G_loss: -14.1063\n",
      "  Batch [10/31] D_loss: -20.2335, G_loss: 27.6592\n",
      "  Batch [20/31] D_loss: -46.5626, G_loss: -8.6826\n",
      "  Batch [30/31] D_loss: -34.2095, G_loss: -10.6273\n",
      "\n",
      "Epoch 165 Summary:\n",
      "  Average D_loss: -16.7022\n",
      "  Average G_loss: 4.4115\n",
      "\n",
      "Epoch [166/200]\n",
      "  Batch [0/31] D_loss: -27.0846, G_loss: -13.5519\n",
      "  Batch [10/31] D_loss: -35.3410, G_loss: 12.9455\n",
      "  Batch [20/31] D_loss: -30.2980, G_loss: 6.3054\n",
      "  Batch [30/31] D_loss: -40.2564, G_loss: -5.4453\n",
      "\n",
      "Epoch 166 Summary:\n",
      "  Average D_loss: -15.6631\n",
      "  Average G_loss: 1.0801\n",
      "\n",
      "Epoch [167/200]\n",
      "  Batch [0/31] D_loss: -36.7258, G_loss: -5.4888\n",
      "  Batch [10/31] D_loss: -33.5469, G_loss: 21.9528\n",
      "  Batch [20/31] D_loss: -35.5786, G_loss: 6.6821\n",
      "  Batch [30/31] D_loss: -44.7594, G_loss: -18.3497\n",
      "\n",
      "Epoch 167 Summary:\n",
      "  Average D_loss: -16.2982\n",
      "  Average G_loss: 1.0424\n",
      "\n",
      "Epoch [168/200]\n",
      "  Batch [0/31] D_loss: -30.8497, G_loss: -5.6945\n",
      "  Batch [10/31] D_loss: -26.6746, G_loss: -24.1401\n",
      "  Batch [20/31] D_loss: -25.9468, G_loss: 12.1866\n",
      "  Batch [30/31] D_loss: -33.3968, G_loss: 14.7782\n",
      "\n",
      "Epoch 168 Summary:\n",
      "  Average D_loss: -15.0993\n",
      "  Average G_loss: 0.2125\n",
      "\n",
      "Epoch [169/200]\n",
      "  Batch [0/31] D_loss: -36.5984, G_loss: -0.4888\n",
      "  Batch [10/31] D_loss: -26.4091, G_loss: 6.8186\n",
      "  Batch [20/31] D_loss: -37.8419, G_loss: -8.6339\n",
      "  Batch [30/31] D_loss: -30.9133, G_loss: -12.6057\n",
      "\n",
      "Epoch 169 Summary:\n",
      "  Average D_loss: -15.1104\n",
      "  Average G_loss: 0.7596\n",
      "\n",
      "Epoch [170/200]\n",
      "  Batch [0/31] D_loss: -18.8506, G_loss: 4.3775\n",
      "  Batch [10/31] D_loss: -31.5154, G_loss: -10.5442\n",
      "  Batch [20/31] D_loss: -36.1990, G_loss: -11.0181\n",
      "  Batch [30/31] D_loss: -32.4421, G_loss: 11.9827\n",
      "\n",
      "Epoch 170 Summary:\n",
      "  Average D_loss: -16.0338\n",
      "  Average G_loss: 1.6387\n",
      "\n",
      "Epoch [171/200]\n",
      "  Batch [0/31] D_loss: -31.2613, G_loss: 9.5461\n",
      "  Batch [10/31] D_loss: -30.3492, G_loss: -3.2802\n",
      "  Batch [20/31] D_loss: -28.8758, G_loss: 3.2076\n",
      "  Batch [30/31] D_loss: -38.9430, G_loss: -6.3123\n",
      "\n",
      "Epoch 171 Summary:\n",
      "  Average D_loss: -14.5231\n",
      "  Average G_loss: 1.8786\n",
      "\n",
      "Epoch [172/200]\n",
      "  Batch [0/31] D_loss: -37.8705, G_loss: 11.9876\n",
      "  Batch [10/31] D_loss: -31.3806, G_loss: 29.9707\n",
      "  Batch [20/31] D_loss: -32.1742, G_loss: -31.3547\n",
      "  Batch [30/31] D_loss: -44.6419, G_loss: 45.2344\n",
      "\n",
      "Epoch 172 Summary:\n",
      "  Average D_loss: -16.9105\n",
      "  Average G_loss: -3.3967\n",
      "\n",
      "Epoch [173/200]\n",
      "  Batch [0/31] D_loss: -24.8640, G_loss: 15.4817\n",
      "  Batch [10/31] D_loss: -28.2768, G_loss: 4.8695\n",
      "  Batch [20/31] D_loss: -33.3281, G_loss: 17.1208\n",
      "  Batch [30/31] D_loss: -35.4665, G_loss: -19.9279\n",
      "\n",
      "Epoch 173 Summary:\n",
      "  Average D_loss: -14.9323\n",
      "  Average G_loss: 7.6955\n",
      "\n",
      "Epoch [174/200]\n",
      "  Batch [0/31] D_loss: -34.6800, G_loss: -21.8567\n",
      "  Batch [10/31] D_loss: -26.0551, G_loss: 0.0749\n",
      "  Batch [20/31] D_loss: -25.1201, G_loss: 24.6630\n",
      "  Batch [30/31] D_loss: -26.2611, G_loss: 4.0093\n",
      "\n",
      "Epoch 174 Summary:\n",
      "  Average D_loss: -15.4783\n",
      "  Average G_loss: 1.9563\n",
      "\n",
      "Epoch [175/200]\n",
      "  Batch [0/31] D_loss: -30.3620, G_loss: 8.7422\n",
      "  Batch [10/31] D_loss: -32.2812, G_loss: 0.2965\n",
      "  Batch [20/31] D_loss: -31.5392, G_loss: -19.1511\n",
      "  Batch [30/31] D_loss: -42.4610, G_loss: 63.3205\n",
      "\n",
      "Epoch 175 Summary:\n",
      "  Average D_loss: -16.4147\n",
      "  Average G_loss: -1.1324\n",
      "\n",
      "Epoch [176/200]\n",
      "  Batch [0/31] D_loss: -20.3941, G_loss: 23.7331\n",
      "  Batch [10/31] D_loss: -34.3623, G_loss: 9.4010\n",
      "  Batch [20/31] D_loss: -29.5924, G_loss: 4.3370\n",
      "  Batch [30/31] D_loss: -36.1433, G_loss: -18.0967\n",
      "\n",
      "Epoch 176 Summary:\n",
      "  Average D_loss: -14.9273\n",
      "  Average G_loss: 2.6791\n",
      "\n",
      "Epoch [177/200]\n",
      "  Batch [0/31] D_loss: -30.1648, G_loss: -15.9970\n",
      "  Batch [10/31] D_loss: -38.2760, G_loss: -16.9683\n",
      "  Batch [20/31] D_loss: -33.8036, G_loss: -12.4966\n",
      "  Batch [30/31] D_loss: -37.1126, G_loss: 32.1943\n",
      "\n",
      "Epoch 177 Summary:\n",
      "  Average D_loss: -16.0678\n",
      "  Average G_loss: 1.4327\n",
      "\n",
      "Epoch [178/200]\n",
      "  Batch [0/31] D_loss: -27.7766, G_loss: 32.3521\n",
      "  Batch [10/31] D_loss: -37.8697, G_loss: -14.7329\n",
      "  Batch [20/31] D_loss: -32.1917, G_loss: -2.1346\n",
      "  Batch [30/31] D_loss: -33.1430, G_loss: 20.8057\n",
      "\n",
      "Epoch 178 Summary:\n",
      "  Average D_loss: -16.1162\n",
      "  Average G_loss: -0.3963\n",
      "\n",
      "Epoch [179/200]\n",
      "  Batch [0/31] D_loss: -40.8653, G_loss: -28.3157\n",
      "  Batch [10/31] D_loss: -32.3250, G_loss: -13.0053\n",
      "  Batch [20/31] D_loss: -33.2346, G_loss: 0.0163\n",
      "  Batch [30/31] D_loss: -18.8458, G_loss: 9.4225\n",
      "\n",
      "Epoch 179 Summary:\n",
      "  Average D_loss: -16.9091\n",
      "  Average G_loss: 1.9825\n",
      "\n",
      "Epoch [180/200]\n",
      "  Batch [0/31] D_loss: -19.4156, G_loss: -5.0783\n",
      "  Batch [10/31] D_loss: -38.3842, G_loss: -9.6799\n",
      "  Batch [20/31] D_loss: -30.3562, G_loss: 13.4075\n",
      "  Batch [30/31] D_loss: -38.5652, G_loss: 6.7746\n",
      "\n",
      "Epoch 180 Summary:\n",
      "  Average D_loss: -15.4552\n",
      "  Average G_loss: -1.6891\n",
      "\n",
      "Epoch [181/200]\n",
      "  Batch [0/31] D_loss: -34.5905, G_loss: 8.2184\n",
      "  Batch [10/31] D_loss: -33.3304, G_loss: -0.2211\n",
      "  Batch [20/31] D_loss: -24.1361, G_loss: -58.7366\n",
      "  Batch [30/31] D_loss: -29.9938, G_loss: 1.7548\n",
      "\n",
      "Epoch 181 Summary:\n",
      "  Average D_loss: -13.6335\n",
      "  Average G_loss: -2.2744\n",
      "\n",
      "Epoch [182/200]\n",
      "  Batch [0/31] D_loss: -30.8902, G_loss: 28.8613\n",
      "  Batch [10/31] D_loss: -21.7796, G_loss: 20.8314\n",
      "  Batch [20/31] D_loss: -43.9805, G_loss: -37.8659\n",
      "  Batch [30/31] D_loss: -31.9213, G_loss: -8.6551\n",
      "\n",
      "Epoch 182 Summary:\n",
      "  Average D_loss: -13.7954\n",
      "  Average G_loss: 2.9153\n",
      "\n",
      "Epoch [183/200]\n",
      "  Batch [0/31] D_loss: -36.4252, G_loss: 16.2276\n",
      "  Batch [10/31] D_loss: -37.4147, G_loss: 12.9857\n",
      "  Batch [20/31] D_loss: -38.1466, G_loss: -26.8867\n",
      "  Batch [30/31] D_loss: -31.7305, G_loss: 4.0369\n",
      "\n",
      "Epoch 183 Summary:\n",
      "  Average D_loss: -16.2686\n",
      "  Average G_loss: 1.9155\n",
      "\n",
      "Epoch [184/200]\n",
      "  Batch [0/31] D_loss: -39.8134, G_loss: -15.6383\n",
      "  Batch [10/31] D_loss: -37.5003, G_loss: -11.6406\n",
      "  Batch [20/31] D_loss: -39.4877, G_loss: -14.5865\n",
      "  Batch [30/31] D_loss: -24.2057, G_loss: -17.3226\n",
      "\n",
      "Epoch 184 Summary:\n",
      "  Average D_loss: -15.8057\n",
      "  Average G_loss: -0.4768\n",
      "\n",
      "Epoch [185/200]\n",
      "  Batch [0/31] D_loss: -31.5381, G_loss: -21.6562\n",
      "  Batch [10/31] D_loss: -28.1208, G_loss: -16.2839\n",
      "  Batch [20/31] D_loss: -36.5588, G_loss: -13.0708\n",
      "  Batch [30/31] D_loss: -32.7121, G_loss: -18.5767\n",
      "\n",
      "Epoch 185 Summary:\n",
      "  Average D_loss: -15.2958\n",
      "  Average G_loss: 2.7150\n",
      "\n",
      "Epoch [186/200]\n",
      "  Batch [0/31] D_loss: -31.8533, G_loss: 21.0769\n",
      "  Batch [10/31] D_loss: -39.2661, G_loss: -15.4630\n",
      "  Batch [20/31] D_loss: -44.8408, G_loss: -31.9281\n",
      "  Batch [30/31] D_loss: -22.3485, G_loss: -31.5084\n",
      "\n",
      "Epoch 186 Summary:\n",
      "  Average D_loss: -15.2219\n",
      "  Average G_loss: -4.3644\n",
      "\n",
      "Epoch [187/200]\n",
      "  Batch [0/31] D_loss: -31.1204, G_loss: -12.7171\n",
      "  Batch [10/31] D_loss: -27.7240, G_loss: 26.7707\n",
      "  Batch [20/31] D_loss: -35.9771, G_loss: 32.4189\n",
      "  Batch [30/31] D_loss: -41.0697, G_loss: -32.0051\n",
      "\n",
      "Epoch 187 Summary:\n",
      "  Average D_loss: -15.7986\n",
      "  Average G_loss: 14.2380\n",
      "\n",
      "Epoch [188/200]\n",
      "  Batch [0/31] D_loss: -22.8382, G_loss: -31.0505\n",
      "  Batch [10/31] D_loss: -39.0719, G_loss: -19.5026\n",
      "  Batch [20/31] D_loss: -27.2141, G_loss: 9.2779\n",
      "  Batch [30/31] D_loss: -36.0574, G_loss: -5.1210\n",
      "\n",
      "Epoch 188 Summary:\n",
      "  Average D_loss: -15.7464\n",
      "  Average G_loss: 1.1375\n",
      "\n",
      "Epoch [189/200]\n",
      "  Batch [0/31] D_loss: -32.5055, G_loss: -10.8337\n",
      "  Batch [10/31] D_loss: -16.0919, G_loss: -32.5171\n",
      "  Batch [20/31] D_loss: -36.8273, G_loss: 24.6761\n",
      "  Batch [30/31] D_loss: -37.4146, G_loss: -21.4484\n",
      "\n",
      "Epoch 189 Summary:\n",
      "  Average D_loss: -15.8884\n",
      "  Average G_loss: -0.4309\n",
      "\n",
      "Epoch [190/200]\n",
      "  Batch [0/31] D_loss: -31.0141, G_loss: -17.2987\n",
      "  Batch [10/31] D_loss: -23.7036, G_loss: 39.0647\n",
      "  Batch [20/31] D_loss: -29.3887, G_loss: 19.2056\n",
      "  Batch [30/31] D_loss: -32.3618, G_loss: -19.1618\n",
      "\n",
      "Epoch 190 Summary:\n",
      "  Average D_loss: -15.7212\n",
      "  Average G_loss: 7.0890\n",
      "\n",
      "Epoch [191/200]\n",
      "  Batch [0/31] D_loss: -30.5743, G_loss: -20.0448\n",
      "  Batch [10/31] D_loss: -27.1896, G_loss: 5.6924\n",
      "  Batch [20/31] D_loss: -42.0144, G_loss: -27.2290\n",
      "  Batch [30/31] D_loss: -31.6434, G_loss: 42.8940\n",
      "\n",
      "Epoch 191 Summary:\n",
      "  Average D_loss: -16.6894\n",
      "  Average G_loss: -8.9525\n",
      "\n",
      "Epoch [192/200]\n",
      "  Batch [0/31] D_loss: -48.2838, G_loss: 72.1397\n",
      "  Batch [10/31] D_loss: -24.6647, G_loss: -27.6284\n",
      "  Batch [20/31] D_loss: -30.6352, G_loss: 7.9015\n",
      "  Batch [30/31] D_loss: -35.8597, G_loss: 2.0664\n",
      "\n",
      "Epoch 192 Summary:\n",
      "  Average D_loss: -15.0537\n",
      "  Average G_loss: 5.8187\n",
      "\n",
      "Epoch [193/200]\n",
      "  Batch [0/31] D_loss: -29.6386, G_loss: 6.4076\n",
      "  Batch [10/31] D_loss: -29.7685, G_loss: 15.0980\n",
      "  Batch [20/31] D_loss: -38.4125, G_loss: -49.5889\n",
      "  Batch [30/31] D_loss: -44.8639, G_loss: 57.2290\n",
      "\n",
      "Epoch 193 Summary:\n",
      "  Average D_loss: -16.9496\n",
      "  Average G_loss: -1.9052\n",
      "\n",
      "Epoch [194/200]\n",
      "  Batch [0/31] D_loss: -9.6795, G_loss: 27.4116\n",
      "  Batch [10/31] D_loss: -34.7267, G_loss: 16.1526\n",
      "  Batch [20/31] D_loss: -32.5545, G_loss: -9.2578\n",
      "  Batch [30/31] D_loss: -37.0137, G_loss: 8.1606\n",
      "\n",
      "Epoch 194 Summary:\n",
      "  Average D_loss: -14.3567\n",
      "  Average G_loss: 1.9235\n",
      "\n",
      "Epoch [195/200]\n",
      "  Batch [0/31] D_loss: -47.3045, G_loss: 39.4478\n",
      "  Batch [10/31] D_loss: -35.4199, G_loss: 60.2035\n",
      "  Batch [20/31] D_loss: -31.3288, G_loss: -17.7362\n",
      "  Batch [30/31] D_loss: -36.3622, G_loss: -42.6773\n",
      "\n",
      "Epoch 195 Summary:\n",
      "  Average D_loss: -15.4109\n",
      "  Average G_loss: -6.3043\n",
      "\n",
      "Epoch [196/200]\n",
      "  Batch [0/31] D_loss: -23.4411, G_loss: -7.6591\n",
      "  Batch [10/31] D_loss: -27.5616, G_loss: 14.9307\n",
      "  Batch [20/31] D_loss: -29.1730, G_loss: 16.7928\n",
      "  Batch [30/31] D_loss: -31.4482, G_loss: -1.2325\n",
      "\n",
      "Epoch 196 Summary:\n",
      "  Average D_loss: -13.3501\n",
      "  Average G_loss: 10.9676\n",
      "\n",
      "Epoch [197/200]\n",
      "  Batch [0/31] D_loss: -33.1412, G_loss: 10.6518\n",
      "  Batch [10/31] D_loss: -36.4390, G_loss: -18.7006\n",
      "  Batch [20/31] D_loss: -32.2769, G_loss: 28.2373\n",
      "  Batch [30/31] D_loss: -33.2134, G_loss: -21.9574\n",
      "\n",
      "Epoch 197 Summary:\n",
      "  Average D_loss: -16.1180\n",
      "  Average G_loss: 3.4604\n",
      "\n",
      "Epoch [198/200]\n",
      "  Batch [0/31] D_loss: -26.9282, G_loss: -5.2004\n",
      "  Batch [10/31] D_loss: -35.5192, G_loss: -11.6316\n",
      "  Batch [20/31] D_loss: -40.0431, G_loss: 14.7044\n",
      "  Batch [30/31] D_loss: -30.5708, G_loss: 9.7727\n",
      "\n",
      "Epoch 198 Summary:\n",
      "  Average D_loss: -15.4605\n",
      "  Average G_loss: -0.2682\n",
      "\n",
      "Epoch [199/200]\n",
      "  Batch [0/31] D_loss: -36.2454, G_loss: 13.0668\n",
      "  Batch [10/31] D_loss: -40.4842, G_loss: -12.3959\n",
      "  Batch [20/31] D_loss: -27.1736, G_loss: -15.2539\n",
      "  Batch [30/31] D_loss: -32.3721, G_loss: 31.2264\n",
      "\n",
      "Epoch 199 Summary:\n",
      "  Average D_loss: -16.6566\n",
      "  Average G_loss: -5.5924\n",
      "\n",
      "Epoch [200/200]\n",
      "  Batch [0/31] D_loss: -37.8675, G_loss: 51.7117\n",
      "  Batch [10/31] D_loss: -38.4811, G_loss: -34.9050\n",
      "  Batch [20/31] D_loss: -33.6768, G_loss: 11.7507\n",
      "  Batch [30/31] D_loss: -25.4149, G_loss: 11.5232\n",
      "\n",
      "Epoch 200 Summary:\n",
      "  Average D_loss: -14.9179\n",
      "  Average G_loss: 4.6169\n"
     ]
    }
   ],
   "source": [
    "def main(selected_categories=None):\n",
    "    \"\"\"\n",
    "    Train the GAN with selected categorical variables\n",
    "    Args:\n",
    "        selected_categories: List of column names to use as categorical variables.\n",
    "                           If None, uses all columns except 'cell_id'\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'epochs': 200,\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 32,\n",
    "        'nb_layers': 2,\n",
    "        'hdim': 256,\n",
    "        'lr': 5e-4,\n",
    "        'nb_critic': 5\n",
    "    }\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/\"\n",
    "    \n",
    "    # Load expression matrix\n",
    "    with h5py.File(data_path+'combined_scaled_data.h5', 'r') as f:\n",
    "        x_train = f['matrix'][:]\n",
    "    \n",
    "    # Load all categorical variables from single file\n",
    "    cat_data = pd.read_csv(data_path+'combined_metadata.csv', sep=';')\n",
    "    print(\"Categorical data shape:\", cat_data.shape)\n",
    "    print(\"Available categorical variables:\", [col for col in cat_data.columns if col != 'cell_id'])\n",
    "    \n",
    "    # Determine which categories to use\n",
    "    if selected_categories is None:\n",
    "        # Use all columns except cell_id\n",
    "        categories_to_use = [col for col in cat_data.columns if col != 'cell_id']\n",
    "    else:\n",
    "        # Validate selected categories\n",
    "        invalid_categories = [cat for cat in selected_categories if cat not in cat_data.columns]\n",
    "        if invalid_categories:\n",
    "            raise ValueError(f\"Invalid categories: {invalid_categories}\")\n",
    "        categories_to_use = selected_categories\n",
    "    \n",
    "    print(f\"\\nUsing categorical variables: {categories_to_use}\")\n",
    "    \n",
    "    # Create dictionaries and inverse mappings for categorical variables\n",
    "    cat_dicts = []\n",
    "    encoded_covs = []\n",
    "    \n",
    "    # Process each selected column as a categorical variable\n",
    "    for column in categories_to_use:\n",
    "        # Get the column data\n",
    "        cat_vec = cat_data[column]\n",
    "        print(f\"\\nProcessing categorical variable: {column}\")\n",
    "        \n",
    "        # Create list of unique category names, sorted\n",
    "        dict_inv = np.array(list(sorted(set(cat_vec.values))))\n",
    "        dict_map = {t: i for i, t in enumerate(dict_inv)}\n",
    "        cat_dicts.append(dict_inv)\n",
    "        \n",
    "        # Convert categorical variables to integers\n",
    "        encoded = np.vectorize(lambda t: dict_map[t])(cat_vec)\n",
    "        encoded = encoded.reshape(-1, 1)  # Reshape to column vector\n",
    "        encoded_covs.append(encoded)\n",
    "        \n",
    "        print(f\"Categories in {column}:\", dict_inv)\n",
    "        print(f\"Number of categories:\", len(dict_inv))\n",
    "    \n",
    "    # Combine all categorical covariates\n",
    "    cat_covs = np.hstack(encoded_covs)\n",
    "    print(\"\\nCombined categorical covariates shape:\", cat_covs.shape)\n",
    "    \n",
    "    # Load numerical covariates (currently empty)\n",
    "    num_covs = np.zeros((x_train.shape[0], 0))\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to device\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)  # Keep on CPU for DataLoader\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(x_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    vocab_sizes = [len(c) for c in cat_dicts]\n",
    "    print(\"\\nVocabulary sizes for categorical variables:\", vocab_sizes)\n",
    "    nb_numeric = num_covs.shape[-1]\n",
    "    x_dim = x_train.shape[-1]\n",
    "    \n",
    "    generator = Generator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "        z_dim=CONFIG['latent_dim']\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize wandb with unique run name\n",
    "    run_name = f\"run_{int(time.time())}\"  # Uses timestamp for unique name\n",
    "    wandb.init(\n",
    "        project='adversarial_gene_expr',\n",
    "        config=CONFIG,\n",
    "        name=run_name,\n",
    "        reinit=True  # Ensures new run each time\n",
    "    )\n",
    "    \n",
    "    # Add selected categories to wandb config\n",
    "    wandb.config.update({'selected_categories': categories_to_use})\n",
    "    \n",
    "    # Train model\n",
    "    train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=train_loader,\n",
    "        cat_covs=cat_covs,\n",
    "        num_covs=num_covs,\n",
    "        config=CONFIG,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    # Use specific categories:\n",
    "    main(selected_categories=['dataset'])\n",
    "    \n",
    "    # Or use all available categories:\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'epochs': 500,\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 32,\n",
    "        'nb_layers': 2,\n",
    "        'hdim': 256,\n",
    "        'lr': 5e-4,\n",
    "        'nb_critic': 5\n",
    "    }\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/\"\n",
    "    \n",
    "    # Load expression matrix\n",
    "    with h5py.File(data_path+'train_data_1dataset.h5', 'r') as f:\n",
    "        x_train = f['matrix'][:]\n",
    "    \n",
    "    # Load cluster info\n",
    "    cluster_vec = pd.read_csv(data_path+'train_data_1dataset_cluster.csv').T\n",
    "    \n",
    "    # Load numerical covariates (currently empty)\n",
    "    num_covs = np.zeros((x_train.shape[0], 0))  # Changed to create empty numpy array\n",
    "    \n",
    "    # Create dictionaries and inverse mappings for categorical variables\n",
    "    cat_dicts = []\n",
    "    \n",
    "    # Create list of unique cluster names, sorted\n",
    "    cluster_dict_inv = np.array(list(sorted(set(cluster_vec.values.flatten()))))\n",
    "    cluster_dict = {t: i for i, t in enumerate(cluster_dict_inv)}\n",
    "    cat_dicts.append(cluster_dict_inv)\n",
    "    \n",
    "    # Convert categorical variables to integers\n",
    "    clusters_encoded = np.vectorize(lambda t: cluster_dict[t])(cluster_vec)\n",
    "    \n",
    "    # Assign categorical covariates\n",
    "    cat_covs = clusters_encoded\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to device\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)  # Keep on CPU for DataLoader\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(x_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    vocab_sizes = [len(c) for c in cat_dicts]\n",
    "    nb_numeric = num_covs.shape[-1]\n",
    "    x_dim = x_train.shape[-1]\n",
    "    \n",
    "    generator = Generator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "        z_dim=CONFIG['latent_dim']\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        h_dims=[CONFIG['hdim']] * CONFIG['nb_layers']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(project='adversarial_gene_expr', config=CONFIG)\n",
    "    \n",
    "    # Train model\n",
    "    train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=train_loader,\n",
    "        cat_covs=cat_covs,\n",
    "        num_covs=num_covs,\n",
    "        config=CONFIG,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
