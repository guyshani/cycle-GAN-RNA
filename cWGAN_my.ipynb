{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import anndata as ad\n",
    "import wandb\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, z_dim, library_size=10000):\n",
    "        \"\"\"\n",
    "        Generator network for conditional GAN with fixed architecture and library size normalization\n",
    "        Args:\n",
    "            x_dim: Dimension of output data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            z_dim: Dimension of latent noise vector\n",
    "            library_size: Target sum for the generated expression values (default: 10000)\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Store library size\n",
    "        self.library_size = library_size\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size)) \n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is latent dim + embedding dim + numeric covariates\n",
    "        input_dim = z_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Fixed architecture with 3 layers: 256 -> 512 -> 1024\n",
    "        self.network = nn.Sequential(\n",
    "            # First layer: input_dim -> 256\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Second layer: 256 -> 512\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Third layer: 512 -> 1024\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Output layer: 1024 -> x_dim\n",
    "            nn.Linear(1024, x_dim),\n",
    "            # No activation here as we'll normalize in forward pass\n",
    "        )\n",
    "\n",
    "    def normalize_to_library_size(self, x):\n",
    "        \"\"\"\n",
    "        Normalize the output tensor so the sum equals the target library size\n",
    "        while ensuring all values are non-negative\n",
    "        \"\"\"\n",
    "        # Apply ReLU to ensure non-negative values\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Add small epsilon to avoid division by zero\n",
    "        epsilon = 1e-10\n",
    "        \n",
    "        # Calculate current sum for each sample\n",
    "        current_sums = x.sum(dim=1, keepdim=True) + epsilon\n",
    "        \n",
    "        # Scale to target library size\n",
    "        normalized = x * (self.library_size / current_sums)\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "    def forward(self, z, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Generate output through network\n",
    "        output = self.network(gen_input)\n",
    "        \n",
    "        # Normalize to library size\n",
    "        normalized_output = self.normalize_to_library_size(output)\n",
    "        \n",
    "        return normalized_output\n",
    "\n",
    "    def get_negative_penalty(self, generated_data):\n",
    "        \"\"\"Calculate penalty for negative values\"\"\"\n",
    "        negative_mask = (generated_data < 0).float()\n",
    "        negative_proportion = negative_mask.mean()\n",
    "        negative_magnitude = (generated_data * negative_mask).abs().mean()\n",
    "        return negative_magnitude, negative_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, use_neg_detector=False):\n",
    "        \"\"\"\n",
    "        Discriminator network with fixed architecture: 1024 -> 512 -> 256 -> 1\n",
    "        Args:\n",
    "            x_dim: Dimension of input data\n",
    "            vocab_sizes: List of vocabulary sizes for each categorical variable\n",
    "            nb_numeric: Number of numeric covariates\n",
    "            use_neg_detector: Whether to use negative value detection\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Store use_neg_detector flag\n",
    "        self.use_neg_detector = use_neg_detector\n",
    "        \n",
    "        # Embedding layers for categorical variables\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, min(50, vocab_size))\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate total embedding dimension\n",
    "        embedding_dim = sum(min(50, vocab_size) for vocab_size in vocab_sizes)\n",
    "        \n",
    "        # Input dimension is data dim + embedding dim + numeric covariates\n",
    "        input_dim = x_dim + embedding_dim + nb_numeric\n",
    "        \n",
    "        # Fixed discriminator architecture\n",
    "        self.main_network = nn.Sequential(\n",
    "            # First layer: input_dim -> 1024\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Second layer: 1024 -> 512\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Third layer: 512 -> 256\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Output layer: 256 -> 1\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Add negative value detection branch if enabled\n",
    "        if use_neg_detector:\n",
    "            self.negative_detector = nn.Sequential(\n",
    "                nn.Linear(x_dim, 1024),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(1024, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x, cat_covs, num_covs):\n",
    "        # Process categorical covariates through embeddings\n",
    "        embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Concatenate inputs for main discrimination\n",
    "        disc_input = torch.cat([x, embedded, num_covs], dim=1)\n",
    "        \n",
    "        # Main discrimination score\n",
    "        validity = self.main_network(disc_input)\n",
    "        \n",
    "        # Add negative value detection if enabled\n",
    "        if self.use_neg_detector:\n",
    "            neg_score = self.negative_detector(torch.relu(-x))  # Only pass negative values\n",
    "            return validity - 0.1 * neg_score  # Penalize negative values\n",
    "        \n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, cat_covs, num_covs, device):\n",
    "    \"\"\"\n",
    "    Calculate gradient penalty for WGAN-GP\n",
    "    \"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand((real_samples.size(0), 1), device=device)\n",
    "    \n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    \n",
    "    # Calculate discriminator output for interpolated samples\n",
    "    d_interpolates = discriminator(interpolates, cat_covs, num_covs)\n",
    "    \n",
    "    # Get gradients w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # Calculate gradient penalty\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, cat_covs, num_covs, \n",
    "              config, device, score_fn=None, save_fn=None):\n",
    "    \"\"\"\n",
    "    Train the conditional GAN with progress tracking and proper device handling\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    '''\n",
    "    g_optimizer = optim.RMSprop(generator.parameters(), lr=config['lr'])\n",
    "    d_optimizer = optim.RMSprop(discriminator.parameters(), lr=config['lr'])\n",
    "    '''\n",
    "    # New optimizers with AMSGrad:\n",
    "    g_optimizer = optim.Adam(\n",
    "        generator.parameters(),\n",
    "        lr=config['lr'],\n",
    "        betas=(config.get('beta1', 0.5), config.get('beta2', 0.9)),\n",
    "        eps=config.get('eps', 1e-8),\n",
    "        amsgrad=True)\n",
    "\n",
    "    d_optimizer = optim.Adam(\n",
    "        discriminator.parameters(),\n",
    "        lr=config['lr'],\n",
    "        betas=(config.get('beta1', 0.5), config.get('beta2', 0.9)),\n",
    "        eps=config.get('eps', 1e-8),\n",
    "        amsgrad=True)\n",
    "    \n",
    "    # Training parameters\n",
    "    lambda_gp = config.get('lambda_gp', 10)\n",
    "    grad_clip_value = config.get('grad_clip_value', 1.0)\n",
    "    neg_penalty_start = config.get('negative_penalty_start_epoch', 50)\n",
    "    neg_penalty_ramp = config.get('negative_penalty_ramp_epochs', 50)\n",
    "    max_neg_penalty = config.get('max_negative_penalty', 10.0)\n",
    "    \n",
    "    # Convert covariates to tensors and move to device\n",
    "    cat_covs = torch.tensor(cat_covs, dtype=torch.long).to(device)\n",
    "    num_covs = torch.tensor(num_covs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "    print(f\"Total batches per epoch: {total_batches}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Using negative detector: {discriminator.use_neg_detector}\")\n",
    "    print(f\"Negative penalty starts at epoch: {neg_penalty_start}\")\n",
    "    \n",
    "    def get_negative_penalty_weight(epoch):\n",
    "        \"\"\"Calculate curriculum learning weight for negative penalty\"\"\"\n",
    "        if epoch < neg_penalty_start:\n",
    "            return 0.0\n",
    "        \n",
    "        ramp_progress = (epoch - neg_penalty_start) / neg_penalty_ramp\n",
    "        ramp_progress = min(1.0, max(0.0, ramp_progress))\n",
    "        return max_neg_penalty * ramp_progress\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        g_losses_main = []  # Track main generator loss without penalty\n",
    "        neg_metrics = []\n",
    "        \n",
    "        print(f\"\\nEpoch [{epoch+1}/{config['epochs']}]\")\n",
    "        curr_neg_weight = get_negative_penalty_weight(epoch)\n",
    "        \n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Move real data to device\n",
    "            real_data = real_data.to(device)\n",
    "            \n",
    "            # Get random batch of categorical and numerical covariates\n",
    "            batch_indices = torch.randint(0, cat_covs.size(0), (batch_size,))\n",
    "            batch_cat_covs = cat_covs[batch_indices]\n",
    "            batch_num_covs = num_covs[batch_indices]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(config['nb_critic']):\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "                fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate discriminator outputs\n",
    "                real_validity = discriminator(real_data, batch_cat_covs, batch_num_covs)\n",
    "                fake_validity = discriminator(fake_data.detach(), batch_cat_covs, batch_num_covs)\n",
    "                \n",
    "                # Calculate gradient penalty\n",
    "                gp = compute_gradient_penalty(\n",
    "                    discriminator,\n",
    "                    real_data,\n",
    "                    fake_data.detach(),\n",
    "                    batch_cat_covs,\n",
    "                    batch_num_covs,\n",
    "                    device)\n",
    "                \n",
    "                # Calculate discriminator loss with gradient penalty\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
    "                \n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, config['latent_dim']).to(device)\n",
    "            fake_data = generator(z, batch_cat_covs, batch_num_covs)\n",
    "            \n",
    "            # Calculate standard generator loss\n",
    "            fake_validity = discriminator(fake_data, batch_cat_covs, batch_num_covs)\n",
    "            g_loss_main = -torch.mean(fake_validity)\n",
    "            \n",
    "            # Calculate negative penalty\n",
    "            neg_magnitude, neg_proportion = generator.get_negative_penalty(fake_data)\n",
    "            g_loss = g_loss_main + curr_neg_weight * neg_magnitude\n",
    "            \n",
    "            g_loss.backward()\n",
    "            \n",
    "            g_optimizer.step()\n",
    "            \n",
    "            # Track losses and metrics\n",
    "            g_losses.append(g_loss.item())\n",
    "            g_losses_main.append(g_loss_main.item())\n",
    "            neg_metrics.append({\n",
    "                'proportion': neg_proportion.item(),\n",
    "                'magnitude': neg_magnitude.item()\n",
    "            })\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                progress_msg = (\n",
    "                    f\"  Batch [{batch_idx}/{total_batches}] \"\n",
    "                    f\"D_loss: {d_loss.item():.4f}, \"\n",
    "                    f\"G_loss: {g_loss.item():.4f}, \"\n",
    "                    f\"G_main: {g_loss_main.item():.4f}, \"\n",
    "                    f\"Neg_prop: {neg_proportion.item():.3f}, \"\n",
    "                    f\"Neg_mag: {neg_magnitude.item():.3f}, \"\n",
    "                    f\"Neg_weight: {curr_neg_weight:.3f}\"\n",
    "                )\n",
    "                print(progress_msg)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        avg_g_main = np.mean(g_losses_main)\n",
    "        avg_neg_prop = np.mean([m['proportion'] for m in neg_metrics])\n",
    "        avg_neg_mag = np.mean([m['magnitude'] for m in neg_metrics])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average D_loss: {avg_d_loss:.4f}\")\n",
    "        print(f\"  Average G_loss: {avg_g_loss:.4f}\")\n",
    "        print(f\"  Average G_main: {avg_g_main:.4f}\")\n",
    "        print(f\"  Average Neg_prop: {avg_neg_prop:.4f}\")\n",
    "        print(f\"  Average Neg_mag: {avg_neg_mag:.4f}\")\n",
    "        print(f\"  Negative Penalty Weight: {curr_neg_weight:.4f}\")\n",
    "        \n",
    "        # Log metrics\n",
    "        if wandb.run is not None:\n",
    "            metrics = {\n",
    "                'epoch': epoch,\n",
    "                'd_loss': avg_d_loss,\n",
    "                'g_loss': avg_g_loss,\n",
    "                'g_loss_main': avg_g_main,\n",
    "                'negative_proportion': avg_neg_prop,\n",
    "                'negative_magnitude': avg_neg_mag,\n",
    "                'negative_penalty_weight': curr_neg_weight\n",
    "            }\n",
    "            \n",
    "            if discriminator.use_neg_detector:\n",
    "                metrics.update({\n",
    "                    'discriminator_neg_proportion': avg_neg_prop,\n",
    "                    'discriminator_neg_magnitude': avg_neg_mag\n",
    "                })\n",
    "            \n",
    "            wandb.log(metrics)\n",
    "        \n",
    "        # Evaluate and save model if needed\n",
    "        if score_fn is not None and epoch % 10 == 0:\n",
    "            score = score_fn(generator)\n",
    "            print(f'Epoch {epoch}: Score = {score:.4f}')\n",
    "        \n",
    "        if save_fn is not None and epoch % 20 == 0:\n",
    "            save_fn(generator, discriminator, epoch)\n",
    "        \n",
    "        # Save model from last epoch\n",
    "        if epoch == config['epochs'] - 1 and save_fn is not None:\n",
    "            save_fn(generator, discriminator, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(selected_categories=None):\n",
    "    \"\"\"\n",
    "    Train the GAN with selected categorical variables\n",
    "    Args:\n",
    "        selected_categories: List of column names to use as categorical variables.\n",
    "                           If None, uses all columns except 'cell_id'\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'epochs': 300,\n",
    "        'latent_dim': 64,\n",
    "        'batch_size': 32,\n",
    "        'lr': 1e-4,\n",
    "        'beta1': 0.5,      # First moment coefficient\n",
    "        'beta2': 0.9,    # Second moment coefficient\n",
    "        'eps': 1e-8,       # Small constant for numerical stability\n",
    "        'nb_critic': 5,\n",
    "        'lambda_gp': 10,\n",
    "        'negative_penalty_start_epoch': 50,\n",
    "        'negative_penalty_ramp_epochs': 50,\n",
    "        'max_negative_penalty': 5.0,\n",
    "        'library_size': 10000 \n",
    "    }\n",
    "    neg_detector = False # True - use negative value detector in the discriminator\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"/Users/guyshani/Documents/PHD/Aim_2/10x_data_mouse/20_1_2025__normalized/\"\n",
    "    # data_path = \"/content/drive/MyDrive/Colab_Notebooks/data/5000_genes/\"\n",
    "    \n",
    "    # Load expression matrix\n",
    "    # matrix with cells as columns and genes as rows\n",
    "    '''\n",
    "    with h5py.File(data_path+'combined_normalized_data.h5', 'r') as f:\n",
    "        x_train = f['matrix'][:]\n",
    "    with h5py.File(data_path+'full_matrix_top5000.h5', 'r') as f:\n",
    "        x_train = np.array(f['matrix/data'])\n",
    "    '''\n",
    "\n",
    "    # Load expression matrix\n",
    "    # Load the saved AnnData object\n",
    "    adata = ad.read_h5ad(os.path.join(data_path, \"log_normalized_PBMC_mouse.h5ad\"))\n",
    "    # Load expression matrix\n",
    "    x_train = np.array(adata.X)\n",
    "    # To access the metadata columns\n",
    "    metadata = adata.obs\n",
    "    # Print some info about the loaded data\n",
    "    #print(f\"Loaded data with {adata.n_obs} cells and {adata.n_vars} genes\")\n",
    "    #print(f\"Available metadata columns: {list(adata.obs.columns)}\")\n",
    "\n",
    "\n",
    "    # Load all categorical variables from single file\n",
    "    cat_data = pd.DataFrame(adata.obs, index=adata.obs_names)\n",
    "    print(\"Categorical data shape:\", cat_data.shape)\n",
    "    print(\"Available categorical variables:\", [col for col in cat_data.columns if col != 'cell_id'])\n",
    "    \n",
    "    # Determine which categories to use\n",
    "    if selected_categories is None:\n",
    "        # Use all columns except cell_id\n",
    "        categories_to_use = [col for col in cat_data.columns if col != 'cell_id']\n",
    "    else:\n",
    "        # Validate selected categories\n",
    "        invalid_categories = [cat for cat in selected_categories if cat not in cat_data.columns]\n",
    "        if invalid_categories:\n",
    "            raise ValueError(f\"Invalid categories: {invalid_categories}\")\n",
    "        categories_to_use = selected_categories\n",
    "    \n",
    "    print(f\"\\nUsing categorical variables: {categories_to_use}\")\n",
    "    \n",
    "    # Create dictionaries and inverse mappings for categorical variables\n",
    "    cat_dicts = []\n",
    "    encoded_covs = []\n",
    "    \n",
    "    # Process each selected column as a categorical variable\n",
    "    for column in categories_to_use:\n",
    "        # Get the column data\n",
    "        cat_vec = cat_data[column]\n",
    "        print(f\"\\nProcessing categorical variable: {column}\")\n",
    "        \n",
    "        # Create list of unique category names, sorted\n",
    "        dict_inv = np.array(list(sorted(set(cat_vec.values))))\n",
    "        dict_map = {t: i for i, t in enumerate(dict_inv)}\n",
    "        cat_dicts.append(dict_inv)\n",
    "        \n",
    "        # Convert categorical variables to integers\n",
    "        encoded = np.vectorize(lambda t: dict_map[t])(cat_vec)\n",
    "        encoded = encoded.reshape(-1, 1)  # Reshape to column vector\n",
    "        encoded_covs.append(encoded)\n",
    "        \n",
    "        print(f\"Categories in {column}:\", dict_inv)\n",
    "        print(f\"Number of categories:\", len(dict_inv))\n",
    "    \n",
    "    # Combine all categorical covariates\n",
    "    cat_covs = np.hstack(encoded_covs)\n",
    "    print(\"\\nCombined categorical covariates shape:\", cat_covs.shape)\n",
    "    \n",
    "    # Load numerical covariates (currently empty)\n",
    "    num_covs = np.zeros((x_train.shape[0], 0))\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to device\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)  # Keep on CPU for DataLoader\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(x_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    ############# Initialize models\n",
    "    # Generator\n",
    "    vocab_sizes = [len(c) for c in cat_dicts]\n",
    "    print(\"\\nVocabulary sizes for categorical variables:\", vocab_sizes)\n",
    "    nb_numeric = num_covs.shape[-1]\n",
    "    x_dim = x_train.shape[-1]\n",
    "    \n",
    "    generator = Generator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        z_dim=CONFIG['latent_dim'],\n",
    "        library_size=CONFIG['library_size']).to(device)\n",
    "    \n",
    "    # Discriminator\n",
    "    discriminator = Discriminator(\n",
    "        x_dim=x_dim,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        nb_numeric=nb_numeric,\n",
    "        use_neg_detector=False).to(device)\n",
    "    \n",
    "    # Define save function\n",
    "    def save_models(generator, discriminator, epoch):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # create save directory\n",
    "        categories_str = \"+\".join(categories_to_use)\n",
    "        save_dir = os.path.join(data_path, \"saved_models\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Create run folder\n",
    "        run_dir = os.path.join(save_dir, f\"run_{timestamp}_{categories_str}\")\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "        # Save model initialization parameters\n",
    "        model_config = {\n",
    "            'x_dim': x_dim,\n",
    "            'vocab_sizes': vocab_sizes,\n",
    "            'nb_numeric': nb_numeric,\n",
    "            'h_dims': [CONFIG['hdim']] * CONFIG['nb_layers'],\n",
    "            'z_dim': CONFIG['latent_dim'],\n",
    "            'categories': categories_to_use,\n",
    "            'training_config': CONFIG}\n",
    "        config_path = os.path.join(run_dir, 'model_config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(model_config, f, indent=4)\n",
    "        \n",
    "        # Save generator\n",
    "        generator_path = os.path.join(run_dir, f\"generator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(generator.state_dict(), generator_path)\n",
    "        \n",
    "        \n",
    "        # Save discriminator\n",
    "        discriminator_path = os.path.join(run_dir, f\"discriminator_{timestamp}_{categories_str}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(discriminator.state_dict(), discriminator_path)\n",
    "        \n",
    "        print(f\"\\nModels saved at epoch {epoch + 1}:\")\n",
    "        print(f\"Generator: {generator_path}\")\n",
    "        print(f\"Discriminator: {discriminator_path}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        if wandb.run is not None:\n",
    "            wandb.save(generator_path)\n",
    "            wandb.save(discriminator_path)\n",
    "\n",
    "    # Initialize wandb with unique run name\n",
    "    run_name = f\"run_{int(time.time())}\"  # Uses timestamp for unique name\n",
    "    wandb.init(\n",
    "        project='adversarial_gene_expr',\n",
    "        config=CONFIG,\n",
    "        name=run_name,\n",
    "        reinit=True  # Ensures new run each time\n",
    "    )\n",
    "    \n",
    "    # Add selected categories to wandb config\n",
    "    wandb.config.update({'selected_categories': categories_to_use})\n",
    "    \n",
    "    # Train model\n",
    "    train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=train_loader,\n",
    "        cat_covs=cat_covs,\n",
    "        num_covs=num_covs,\n",
    "        config=CONFIG,\n",
    "        device=device,\n",
    "        save_fn=save_models\n",
    "        #save_fn=None\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    # Use specific categories:\n",
    "    main(selected_categories=['dataset','myannotations'])\n",
    "    \n",
    "    # Or use all available categories:\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for data gneration\n",
    "def inspect_generator_dims(generator):\n",
    "    \"\"\"\n",
    "    Inspect the generator's dimensions and architecture\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Generator model\n",
    "    \n",
    "    Returns:\n",
    "        dict containing dimension information\n",
    "    \"\"\"\n",
    "    # Get embedding dimensions\n",
    "    embedding_dims = [emb.embedding_dim for emb in generator.embeddings]\n",
    "    total_embedding_dim = sum(embedding_dims)\n",
    "    \n",
    "    # Get first layer dimension\n",
    "    first_layer_in_dim = generator.network[0].in_features\n",
    "    \n",
    "    return {\n",
    "        'embedding_dims': embedding_dims,\n",
    "        'total_embedding_dim': total_embedding_dim,\n",
    "        'first_layer_in_dim': first_layer_in_dim,\n",
    "        'recommended_latent_dim': first_layer_in_dim - total_embedding_dim\n",
    "    }\n",
    "\n",
    "def generate_expression_profiles(generator, n_samples, dataset_category, cell_type_category, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate gene expression profiles using the trained cWGAN generator\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained Generator model\n",
    "        n_samples: Number of profiles to generate\n",
    "        dataset_category: Integer indicating which dataset category to generate\n",
    "        cell_type_category: Integer indicating which cell type to generate\n",
    "        device: Device to run generation on ('cuda', 'mps', or 'cpu')\n",
    "        debug: If True, print debugging information\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of generated expression profiles with shape (n_samples, n_genes)\n",
    "    \"\"\"\n",
    "    # Set generator to eval mode\n",
    "    generator.eval()\n",
    "    \n",
    "    # Inspect dimensions\n",
    "    dims = inspect_generator_dims(generator)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Generator dimensions:\")\n",
    "        for k, v in dims.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    \n",
    "    # Create latent vectors\n",
    "    latent_dim = dims['recommended_latent_dim']\n",
    "    z = torch.randn(n_samples, latent_dim, device=device)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nLatent vector shape: {z.shape}\")\n",
    "    \n",
    "    # Create categorical condition tensor with dataset and cell type\n",
    "    num_embeddings = len(generator.embeddings)\n",
    "    cat_covs = torch.zeros((n_samples, num_embeddings), dtype=torch.long, device=device)\n",
    "    cat_covs[:, 0] = dataset_category  # Set dataset category\n",
    "    cat_covs[:, 1] = cell_type_category  # Set cell type category\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Categorical covariates shape: {cat_covs.shape}\")\n",
    "        print(f\"Number of embedding layers: {num_embeddings}\")\n",
    "    \n",
    "    # Create empty numeric covariates tensor\n",
    "    num_covs = torch.zeros((n_samples, 0), device=device)\n",
    "    \n",
    "    # Generate samples\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings\n",
    "            embeddings = [emb(cat_covs[:, i]) for i, emb in enumerate(generator.embeddings)]\n",
    "            embedded = torch.cat(embeddings, dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Embedded shape: {embedded.shape}\")\n",
    "            \n",
    "            # Concatenate inputs\n",
    "            gen_input = torch.cat([z, embedded, num_covs], dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Generator input shape: {gen_input.shape}\")\n",
    "                print(f\"First layer input dim: {generator.network[0].in_features}\")\n",
    "                print(f\"First layer weight shape: {generator.network[0].weight.shape}\")\n",
    "            \n",
    "            # Generate samples\n",
    "            fake_samples = generator.network(gen_input)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(\"\\nError during generation:\")\n",
    "        print(e)\n",
    "        print(\"\\nGenerator architecture:\")\n",
    "        print(generator)\n",
    "        raise\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    return fake_samples.cpu().numpy()\n",
    "\n",
    "def generate_and_save_profiles(generator, samples_per_combination, save_path, cell_type_names, device='mps', debug=False):\n",
    "    \"\"\"\n",
    "    Generate expression profiles using the trained generator and save as h5ad format\n",
    "    \n",
    "    Parameters:\n",
    "        generator: Trained generator model\n",
    "        samples_per_combination: Dictionary with (dataset_num, cell_type_num) keys and number of samples as values\n",
    "        save_path: Where to save the generated data\n",
    "        cell_type_names: Dictionary mapping cell type indices to their names\n",
    "        device: Device to use for generation\n",
    "        debug: Whether to print debug information\n",
    "    \n",
    "    Returns:\n",
    "        AnnData object containing the generated data and metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    all_samples = []\n",
    "    all_dataset_labels = []\n",
    "    all_annotation_labels = []\n",
    "    \n",
    "    # Generate samples for specified combinations\n",
    "    for (dataset_num, cell_type_num), n_samples in samples_per_combination.items():\n",
    "        dataset_category = dataset_num - 1  # Convert dataset number (1-7) to category index (0-6)\n",
    "        cell_type_category = cell_type_num\n",
    "        cell_type_name = cell_type_names[cell_type_num]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nGenerating {n_samples} samples for dataset{dataset_num}, {cell_type_name}\")\n",
    "        \n",
    "        samples = generate_expression_profiles(\n",
    "            generator, \n",
    "            n_samples, \n",
    "            dataset_category,\n",
    "            cell_type_category,\n",
    "            device,\n",
    "            debug=debug\n",
    "        )\n",
    "        all_samples.append(samples)\n",
    "        \n",
    "        # Store dataset and annotation labels separately\n",
    "        dataset_label = f\"dataset{dataset_num}\"\n",
    "        all_dataset_labels.extend([dataset_label] * n_samples)\n",
    "        all_annotation_labels.extend([cell_type_name] * n_samples)\n",
    "\n",
    "    # Combine all samples\n",
    "    all_samples = np.vstack(all_samples)\n",
    "    \n",
    "    print(f\"Save location: {save_path}\")\n",
    "\n",
    "    # Create metadata DataFrame\n",
    "    obs = pd.DataFrame({\n",
    "        'dataset': all_dataset_labels,\n",
    "        'myannotations': all_annotation_labels\n",
    "    })\n",
    "    \n",
    "    # Create an AnnData object\n",
    "    adata = ad.AnnData(\n",
    "        X=all_samples,\n",
    "        obs=obs\n",
    "    )\n",
    "    \n",
    "    # Save as h5ad file\n",
    "    adata.write_h5ad(f\"{save_path}.h5ad\")\n",
    "    \n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATA:\n",
      "Inspecting file: /Users/guyshani/Documents/PHD/Aim_2/PBMC_data/mouse/log_normalized_PBMC_mouse.h5ad\n",
      "AnnData object with n_obs × n_vars = 47413 × 18445\n",
      "\n",
      "Metadata columns:\n",
      "  - dataset: category\n",
      "  - myannotations: category\n",
      "\n",
      "Sample of metadata:\n",
      "                     dataset myannotations\n",
      "AAACCGTGTATCAGTA-1  dataset1        B cell\n",
      "AAACCGTGTATGCGCA-1  dataset1   Erythrocyte\n",
      "AAACCGTGTCGTCTTA-1  dataset1        B cell\n",
      "AAACCGTGTGCCACTT-1  dataset1        B cell\n",
      "AAACGCTGTCATCTTG-1  dataset1        B cell\n",
      "\n",
      "Cell type distribution by dataset:\n",
      "myannotations  B cell  CD4 T cell  CD8 T cell    DC  Granulocyte  Macrophage  \\\n",
      "dataset                                                                        \n",
      "dataset1         2153         483         151     8           88           0   \n",
      "dataset2         2334         517         181     4          102           0   \n",
      "dataset3         1894         718         378   139          118         332   \n",
      "dataset4           19         544         267     3            0           0   \n",
      "dataset5          358         214          53     0           88           1   \n",
      "dataset6         4861         716         245  1390          131        2078   \n",
      "dataset7         4351        1735         532   151          121         207   \n",
      "dataset8         4294         887         357   366          163        1368   \n",
      "dataset9         4059         638         230   697          117        2034   \n",
      "\n",
      "myannotations  Monocyte  NK T cell  NK cell  Plasma cell  pDC  Erythrocyte  \n",
      "dataset                                                                     \n",
      "dataset1            398        110      232          100    5           43  \n",
      "dataset2            413        100      285          127    5           75  \n",
      "dataset3            180          4      114           11    8            2  \n",
      "dataset4              8         35        2            1    0            0  \n",
      "dataset5             95         32       51           10    0            1  \n",
      "dataset6            220          9       95          157   26          155  \n",
      "dataset7            132          1      137           71   34            6  \n",
      "dataset8             84          2       82          140   28          181  \n",
      "dataset9            182          5       86          115   21          122  \n",
      "\n",
      "Expression matrix statistics:\n",
      "  Min value: 0.0\n",
      "  Max value: 8.900602340698242\n",
      "  Mean value: 0.133357435464859\n",
      "  Non-zero values: 10.00%\n",
      "  Negative values: 0 (0.0000%)\n",
      "\n",
      "==================================================\n",
      "\n",
      "GENERATED DATA:\n",
      "Inspecting file: /Users/guyshani/Documents/PHD/Aim_2/test_models/full_data/run_20250226_143044_dataset+myannotations/generated_data.h5ad\n",
      "AnnData object with n_obs × n_vars = 47413 × 18445\n",
      "\n",
      "Metadata columns:\n",
      "  - dataset: category\n",
      "  - myannotations: category\n",
      "\n",
      "Sample of metadata:\n",
      "    dataset myannotations\n",
      "0  dataset1        B cell\n",
      "1  dataset1        B cell\n",
      "2  dataset1        B cell\n",
      "3  dataset1        B cell\n",
      "4  dataset1        B cell\n",
      "\n",
      "Cell type distribution by dataset:\n",
      "myannotations  B cell  CD4 T cell  CD8 T cell    DC  Erythrocyte  Granulocyte  \\\n",
      "dataset                                                                         \n",
      "dataset1         2153         483         151     8           43           88   \n",
      "dataset2         2334         517         181     4           75          102   \n",
      "dataset3         1894         718         378   139            2          118   \n",
      "dataset4           19         544         267     3            0            0   \n",
      "dataset5          358         214          53     0            1           88   \n",
      "dataset6         4861         716         245  1390          155          131   \n",
      "dataset7         4351        1735         532   151            6          121   \n",
      "dataset8         4294         887         357   366          181          163   \n",
      "dataset9         4059         638         230   697          122          117   \n",
      "\n",
      "myannotations  Macrophage  Monocyte  NK T cell  NK cell  Plasma cell  pDC  \n",
      "dataset                                                                    \n",
      "dataset1                0       398        110      232          100    5  \n",
      "dataset2                0       413        100      285          127    5  \n",
      "dataset3              332       180          4      114           11    8  \n",
      "dataset4                0         8         35        2            1    0  \n",
      "dataset5                1        95         32       51           10    0  \n",
      "dataset6             2078       220          9       95          157   26  \n",
      "dataset7              207       132          1      137           71   34  \n",
      "dataset8             1368        84          2       82          140   28  \n",
      "dataset9             2034       182          5       86          115   21  \n",
      "\n",
      "Expression matrix statistics:\n",
      "  Min value: -4.7472991943359375\n",
      "  Max value: 6.444520473480225\n",
      "  Mean value: -0.39874786138534546\n",
      "  Non-zero values: 18.74%\n",
      "  Negative values: 710670706 (81.2629%)\n"
     ]
    }
   ],
   "source": [
    "def inspect_h5ad(file_path):\n",
    "    \"\"\"\n",
    "    Inspect an h5ad file to ensure it has the correct structure\n",
    "    \n",
    "    Parameters:\n",
    "        file_path: Path to the h5ad file\n",
    "    \"\"\"\n",
    "    print(f\"Inspecting file: {file_path}\")\n",
    "    \n",
    "    # Load the file\n",
    "    adata = ad.read_h5ad(file_path)\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"AnnData object with n_obs × n_vars = {adata.n_obs} × {adata.n_vars}\")\n",
    "    \n",
    "    # Check metadata columns\n",
    "    print(\"\\nMetadata columns:\")\n",
    "    for col in adata.obs.columns:\n",
    "        print(f\"  - {col}: {adata.obs[col].dtype}\")\n",
    "    \n",
    "    # Show sample of metadata\n",
    "    print(\"\\nSample of metadata:\")\n",
    "    print(adata.obs.head())\n",
    "    \n",
    "    # Show distribution of cell types by dataset\n",
    "    print(\"\\nCell type distribution by dataset:\")\n",
    "    dist_df = pd.crosstab(adata.obs['dataset'], adata.obs['myannotations'])\n",
    "    print(dist_df)\n",
    "    \n",
    "    # Expression matrix stats\n",
    "    print(\"\\nExpression matrix statistics:\")\n",
    "    print(f\"  Min value: {adata.X.min()}\")\n",
    "    print(f\"  Max value: {adata.X.max()}\")\n",
    "    print(f\"  Mean value: {adata.X.mean()}\")\n",
    "    print(f\"  Non-zero values: {(adata.X > 0).sum() / (adata.X.size) * 100:.2f}%\")\n",
    "    \n",
    "    # Check for negative values (shouldn't exist in normalized data)\n",
    "    neg_values = (adata.X < 0).sum()\n",
    "    print(f\"  Negative values: {neg_values} ({neg_values / adata.X.size * 100:.4f}%)\")\n",
    "    \n",
    "    return adata\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/test_models/full_data/run_20250226_143044_dataset+myannotations/\"\n",
    "    generated_file = os.path.join(run_dir, \"generated_data.h5ad\")\n",
    "    \n",
    "    # Compare with original file\n",
    "    original_file = \"/Users/guyshani/Documents/PHD/Aim_2/PBMC_data/mouse/log_normalized_PBMC_mouse.h5ad\"\n",
    "    \n",
    "    print(\"ORIGINAL DATA:\")\n",
    "    original_adata = inspect_h5ad(original_file)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(\"GENERATED DATA:\")\n",
    "    generated_adata = inspect_h5ad(generated_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loaded data with 11854 cells and 18445 genes\n",
      "Available metadata columns: ['dataset', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'doublet_score', 'predicted_doublet', '_scvi_batch', '_scvi_labels', 'leiden', 'size_factor', 'cellassign_predictions', 'cluster_celltype', 'myannotations', 'size_factors', 'is_synthetic', 'batch', 'split']\n",
      "Detected cell types:\n",
      "B cell: 0\n",
      "CD4 T cell: 1\n",
      "CD8 T cell: 2\n",
      "DC: 3\n",
      "Erythrocyte: 4\n",
      "Granulocyte: 5\n",
      "Macrophage: 6\n",
      "Monocyte: 7\n",
      "NK T cell: 8\n",
      "NK cell: 9\n",
      "Plasma cell: 10\n",
      "pDC: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g_/c_f20pc566n31j9p36lcdc0w0000gn/T/ipykernel_28435/848756348.py:39: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  counts = meta.groupby(['dataset', 'myannotations']).size().to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save location: /Users/guyshani/Documents/PHD/Aim_2/test_models/full_data_counts/run_20250302_091205_dataset+myannotations/generated_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guyshani/Documents/PHD/Aim_2/cycle_GAN/.venv/lib/python3.11/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 11854 cells with 18445 genes\n",
      "Dataset distribution: dataset\n",
      "dataset6    2495\n",
      "dataset9    2113\n",
      "dataset8    2045\n",
      "dataset7    1827\n",
      "dataset3     999\n",
      "dataset2     990\n",
      "dataset1     948\n",
      "dataset5     230\n",
      "dataset4     207\n",
      "Name: count, dtype: int64\n",
      "Cell type distribution: myannotations\n",
      "B cell         6081\n",
      "CD4 T cell     1613\n",
      "Macrophage     1505\n",
      "DC              690\n",
      "CD8 T cell      599\n",
      "Monocyte        428\n",
      "NK cell         271\n",
      "Granulocyte     232\n",
      "Plasma cell     183\n",
      "Erythrocyte     146\n",
      "NK T cell        74\n",
      "pDC              32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set directories\n",
    "data_path = \"/Users/guyshani/Documents/PHD/Aim_2/PBMC_data/mouse/\"\n",
    "run_dir = \"/Users/guyshani/Documents/PHD/Aim_2/test_models/full_data_counts/run_20250302_091205_dataset+myannotations/\"\n",
    "generator_model = \"generator_20250302_091205_dataset+myannotations_epoch_281.pt\"\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(run_dir, 'model_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "# Initialize generator with saved configuration\n",
    "generator = Generator(\n",
    "    x_dim=model_config['x_dim'],\n",
    "    vocab_sizes=model_config['vocab_sizes'],\n",
    "    nb_numeric=model_config['nb_numeric'],\n",
    "    z_dim=model_config['z_dim']\n",
    ").to(device)\n",
    "\n",
    "# Load model weights\n",
    "generator_path = os.path.join(run_dir, generator_model)\n",
    "generator.load_state_dict(torch.load(generator_path, map_location=device, weights_only=True))\n",
    "\n",
    "# Load original data metadata to get distribution of cell types and datasets\n",
    "adata = ad.read_h5ad(os.path.join(data_path, \"test_data_library_counts_PBMC.h5ad\"))\n",
    "meta = adata.obs\n",
    "print(f\"Loaded data with {adata.n_obs} cells and {adata.n_vars} genes\")\n",
    "print(f\"Available metadata columns: {list(adata.obs.columns)}\")\n",
    "\n",
    "# Get counts of each dataset-celltype combination\n",
    "counts = meta.groupby(['dataset', 'myannotations']).size().to_dict()\n",
    "# Filter out all entries with value 0\n",
    "counts = {k: v for k, v in counts.items() if v != 0}\n",
    "\n",
    "# Get unique cell types from the data\n",
    "unique_cell_types = sorted(meta['myannotations'].unique())\n",
    "\n",
    "# Create cell type mapping automatically\n",
    "cell_type_map = {cell_type: idx for idx, cell_type in enumerate(unique_cell_types)}\n",
    "\n",
    "print(\"Detected cell types:\")\n",
    "for cell_type, idx in cell_type_map.items():\n",
    "    print(f\"{cell_type}: {idx}\")\n",
    "\n",
    "# Convert the dict keys from tuple of strings to tuple of numbers\n",
    "samples_dict = {}\n",
    "for (dataset, cell_type), count in counts.items():\n",
    "    # Extract dataset number\n",
    "    dataset_num = int(dataset.replace('dataset', ''))\n",
    "    # Get cell type number from our automatic mapping\n",
    "    cell_type_num = cell_type_map[cell_type]\n",
    "    # Add to new dictionary with numerical tuple as key\n",
    "    samples_dict[(dataset_num, cell_type_num)] = count\n",
    "\n",
    "# Get the reverse mapping for cell type names\n",
    "cell_type_names = {v: k for k, v in cell_type_map.items()}\n",
    "\n",
    "# Generate and save as h5ad\n",
    "generated_adata = generate_and_save_profiles(\n",
    "    generator,\n",
    "    samples_per_combination=samples_dict,\n",
    "    save_path=os.path.join(run_dir, 'generated_data'),\n",
    "    cell_type_names=cell_type_names,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(f\"Generated {generated_adata.n_obs} cells with {generated_adata.n_vars} genes\")\n",
    "print(f\"Dataset distribution: {generated_adata.obs['dataset'].value_counts()}\")\n",
    "print(f\"Cell type distribution: {generated_adata.obs['myannotations'].value_counts()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
