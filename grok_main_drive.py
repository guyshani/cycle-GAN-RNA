# -*- coding: utf-8 -*-
"""grok_main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UJiFMPU1YRTDH5TLKmyG7SExcgKK2Py9
"""

#!pip install anndata umap-learn scikit-learn scanpy

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Import required libraries
import os
import torch
import anndata as ad
import scipy.sparse
import scanpy as sc
import numpy as np
import umap
from datetime import datetime
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import json
#sys.path.append('/content/drive/MyDrive/Colab_Notebooks/translation/')
os.chdir('/content/drive/MyDrive/Colab_Notebooks/translation/')
from grok_functions import *

# Optional: Import wandb for experiment tracking
try:
    import wandb
    has_wandb = True
except ImportError:
    has_wandb = False
    print("wandb not installed. Logging to wandb will be disabled.")

# Set plot style
plt.style.use('seaborn-v0_8')
# %matplotlib inline

# Default Configuration
def create_default_config():
    return {
        'epochs': 800,
        'batch_size': 64,
        'lr': 2e-4,
        'hidden_dims': [2048, 1024, 512],
        'latent_dim': 256,
        'num_attention_heads': 4,
        'residual_blocks': 4,
        'nb_critic': 4,
        'lambda_adv': 1,
        'lambda_cycle': 1,
        'lambda_celltype': 0.2,
        'lambda_gp': 13,
        'lambda_fm': 0.3,
        'lambda_kl': 0.1,
        'use_self_attention': False,
        'use_sparse_attention': False,
        'sparsity_target': 0.9,
        'temperature': 0.5,
        'library_size': 10000,
        'use_lr_decay': True,
        'decay_start_epoch': 100,
        'decay_type': 'cosine',
        'decay_factor': 0.1,
        'use_wandb': has_wandb,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'preprocess_type': 'counts',
        # log_normalized
        'cycle_loss_type': 'poisson'
    }

config = create_default_config()
device = torch.device(config['device'])

mouse_adata = ad.read_h5ad("train_data_library_counts_PBMC.h5ad")
human_adata = ad.read_h5ad("train_data_library_counts_PBMC_human.h5ad")

mouse_tensor = preprocess_data(mouse_adata, config['preprocess_type'], config['library_size'])
human_tensor = preprocess_data(human_adata, config['preprocess_type'], config['library_size'])

'''
# Get a dense version of the first 5 cells and 5 genes
print(human_adata.X[:5, :5].toarray())
# counts
print(human_adata.layers["counts"][:5, :5].toarray())
# lognorm
print(human_adata.layers["log_normalized"][:5, :5].toarray())
'''

#mouse_tensor = torch.tensor(mouse_data.toarray() if scipy.sparse.issparse(mouse_data) else mouse_data, dtype=torch.float32)
#human_tensor = torch.tensor(human_data.toarray() if scipy.sparse.issparse(human_data) else human_data, dtype=torch.float32)

# Initialize and fit scaler on human data
scaler = StandardScaler()
human_data_np = human_tensor.numpy()
scaler.fit(human_data_np)
human_tensor_scaled = torch.tensor(scaler.transform(human_data_np), dtype=torch.float32)

# Categorical covariates
unique_cell_types = sorted(set(mouse_adata.obs['myannotations']) | set(human_adata.obs['myannotations']))
value_to_idx = {val: idx for idx, val in enumerate(unique_cell_types)}
idx_to_cell_type = {idx: val for val, idx in value_to_idx.items()}
mouse_cat = np.array([value_to_idx[val] for val in mouse_adata.obs['myannotations'].values])
human_cat = np.array([value_to_idx[val] for val in human_adata.obs['myannotations'].values])
cat_tensor = np.concatenate([mouse_cat, human_cat])[:, None]

# Precompute PCA and UMAP on scaled human data
pca_model = PCA(n_components=50)
human_pca = pca_model.fit_transform(scaler.transform(human_data_np))  # Fit PCA on scaled data
umap_model = umap.UMAP(n_components=2, random_state=42)
human_umap_coords = umap_model.fit_transform(human_pca)  # Fit UMAP on scaled PCA

# Define color dictionary for plotting
color_dict = {ct: plt.cm.tab20(i) for i, ct in enumerate(unique_cell_types)}

# Initialize models
vocab_sizes = [len(unique_cell_types)]
g_m2h = GeneExpressionGenerator(mouse_tensor.shape[1], human_tensor.shape[1], vocab_sizes, config).to(device)
g_h2m = GeneExpressionGenerator(human_tensor.shape[1], mouse_tensor.shape[1], vocab_sizes, config).to(device)
d_m = GeneExpressionDiscriminator(mouse_tensor.shape[1], vocab_sizes, config).to(device)
d_h = GeneExpressionDiscriminator(human_tensor.shape[1], vocab_sizes, config).to(device)

# Train with test figures
metrics, output_dir = train_enhanced_cycle_gan(g_m2h, g_h2m, d_m, d_h, mouse_adata, human_adata,
                                              mouse_tensor, human_tensor, cat_tensor, human_umap_coords,
                                              umap_model, idx_to_cell_type, config, device,
                                              pca_model=pca_model, scaler=scaler)
plot_metrics(metrics, output_dir)
print(f"Training completed. Results saved in {output_dir}")