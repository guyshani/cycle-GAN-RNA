{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims=None, z_dim=10):\n",
    "        super().__init__()\n",
    "        if h_dims is None:\n",
    "            h_dims = [256, 256]\n",
    "            \n",
    "        self.z_dim = z_dim\n",
    "        self.nb_categoric = len(vocab_sizes)\n",
    "        \n",
    "        # Embeddings for categorical variables\n",
    "        self.embeddings = nn.ModuleList()\n",
    "        total_emb_dim = 0\n",
    "        for vs in vocab_sizes:\n",
    "            emb_dim = int(vs ** 0.5) + 1\n",
    "            self.embeddings.append(nn.Embedding(vs, emb_dim))\n",
    "            total_emb_dim += emb_dim\n",
    "        total_emb_dim += nb_numeric\n",
    "        \n",
    "        # Generator layers\n",
    "        layers = []\n",
    "        prev_dim = z_dim + total_emb_dim\n",
    "        for dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, x_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, z, cat, num):\n",
    "        # Process categorical variables\n",
    "        emb = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            emb.append(embedding(cat[:, i]))\n",
    "        if self.nb_categoric > 0:\n",
    "            emb = torch.cat(emb, dim=1)\n",
    "            h = torch.cat([z, num, emb], dim=1)\n",
    "        else:\n",
    "            h = torch.cat([z, num], dim=1)\n",
    "        \n",
    "        return self.layers(h)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims=None):\n",
    "        super().__init__()\n",
    "        if h_dims is None:\n",
    "            h_dims = [256, 256]\n",
    "            \n",
    "        self.nb_categoric = len(vocab_sizes)\n",
    "        \n",
    "        # Embeddings for categorical variables\n",
    "        self.embeddings = nn.ModuleList()\n",
    "        total_emb_dim = 0\n",
    "        for vs in vocab_sizes:\n",
    "            emb_dim = int(vs ** 0.5) + 1\n",
    "            self.embeddings.append(nn.Embedding(vs, emb_dim))\n",
    "            total_emb_dim += emb_dim\n",
    "            \n",
    "        # Discriminator layers\n",
    "        layers = []\n",
    "        prev_dim = x_dim + nb_numeric + total_emb_dim\n",
    "        for dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x, cat, num):\n",
    "        # Process categorical variables\n",
    "        emb = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            emb.append(embedding(cat[:, i]))\n",
    "        if self.nb_categoric > 0:\n",
    "            emb = torch.cat(emb, dim=1)\n",
    "            h = torch.cat([x, num, emb], dim=1)\n",
    "        else:\n",
    "            h = torch.cat([x, num], dim=1)\n",
    "            \n",
    "        return self.layers(h)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_dim, h_dims=None, z_dim=10):\n",
    "        super().__init__()\n",
    "        if h_dims is None:\n",
    "            h_dims = [256, 256]\n",
    "            \n",
    "        layers = []\n",
    "        prev_dim = x_dim\n",
    "        for dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, z_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, cat, num):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, device=real_samples.device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates, cat, num)\n",
    "    \n",
    "    grad_outputs = torch.ones_like(d_interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def train(dataset, cat_covs, num_covs, z_dim, epochs, batch_size,\n",
    "          generator, discriminator, encoder, score_fn, save_fn,\n",
    "          device='cuda', lr=5e-4, nb_critic=5, cycle_weight=10,\n",
    "          gradient_penalty_weight=10, checkpoint_dir='./checkpoints',\n",
    "          log_dir='./logs', patience=10):\n",
    "    \n",
    "    # Convert data to tensors\n",
    "    dataset = torch.FloatTensor(dataset).to(device)\n",
    "    cat_covs = torch.LongTensor(cat_covs).to(device)\n",
    "    num_covs = torch.FloatTensor(num_covs).to(device)\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(dataset, cat_covs, num_covs)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Optimizers\n",
    "    gen_optimizer = optim.RMSprop(generator.parameters(), lr=lr)\n",
    "    disc_optimizer = optim.RMSprop(discriminator.parameters(), lr=lr)\n",
    "    encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr)\n",
    "    \n",
    "    # Logger\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    patience_counter = patience\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        gen_losses = []\n",
    "        disc_losses = []\n",
    "        \n",
    "        for batch_idx, (real_data, cat, num) in enumerate(train_loader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(nb_critic):\n",
    "                disc_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, z_dim, device=device)\n",
    "                fake_data = generator(z, cat, num)\n",
    "                \n",
    "                # Cycle consistency\n",
    "                z_rec = encoder(fake_data)\n",
    "                cycled_data = generator(z_rec, cat, num)\n",
    "                \n",
    "                # Compute discriminator outputs\n",
    "                real_validity = discriminator(real_data, cat, num)\n",
    "                fake_validity = discriminator(fake_data.detach(), cat, num)\n",
    "                \n",
    "                # Compute losses\n",
    "                gradient_penalty = compute_gradient_penalty(\n",
    "                    discriminator, real_data, fake_data.detach(), cat, num)\n",
    "                cycle_loss = torch.mean(torch.abs(real_data - cycled_data))\n",
    "                \n",
    "                disc_loss = (-torch.mean(real_validity) + torch.mean(fake_validity)\n",
    "                           + gradient_penalty_weight * gradient_penalty\n",
    "                           + cycle_weight * cycle_loss)\n",
    "                \n",
    "                disc_loss.backward()\n",
    "                disc_optimizer.step()\n",
    "                disc_losses.append(disc_loss.item())\n",
    "            \n",
    "            # Train Generator and Encoder\n",
    "            gen_optimizer.zero_grad()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, z_dim, device=device)\n",
    "            fake_data = generator(z, cat, num)\n",
    "            fake_validity = discriminator(fake_data, cat, num)\n",
    "            \n",
    "            z_rec = encoder(fake_data)\n",
    "            cycled_data = generator(z_rec, cat, num)\n",
    "            cycle_loss = torch.mean(torch.abs(fake_data - cycled_data))\n",
    "            \n",
    "            gen_loss = -torch.mean(fake_validity) + cycle_weight * cycle_loss\n",
    "            gen_loss.backward()\n",
    "            \n",
    "            gen_optimizer.step()\n",
    "            encoder_optimizer.step()\n",
    "            gen_losses.append(gen_loss.item())\n",
    "        \n",
    "        # Logging\n",
    "        writer.add_scalar('Generator Loss', np.mean(gen_losses), epoch)\n",
    "        writer.add_scalar('Discriminator Loss', np.mean(disc_losses), epoch)\n",
    "        \n",
    "        # Model evaluation and checkpointing\n",
    "        if epoch % 5 == 0:\n",
    "            score = score_fn(generator)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                save_fn()\n",
    "                patience_counter = patience\n",
    "            else:\n",
    "                patience_counter -= 1\n",
    "                \n",
    "            if patience_counter == 0:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "                \n",
    "        print(f'Epoch {epoch}/{epochs} - '\n",
    "              f'Gen Loss: {np.mean(gen_losses):.4f} - '\n",
    "              f'Disc Loss: {np.mean(disc_losses):.4f}')\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(x_dim, vocab_sizes, nb_numeric).to(device)\n",
    "discriminator = Discriminator(x_dim, vocab_sizes, nb_numeric).to(device)\n",
    "encoder = Encoder(x_dim, z_dim=z_dim).to(device)\n",
    "\n",
    "train(dataset, cat_covs, num_covs, z_dim, epochs, batch_size,\n",
    "      generator, discriminator, encoder, score_fn, save_fn)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
