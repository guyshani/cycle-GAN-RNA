{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import datetime\n",
    "from rnaseqdb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set device\n",
    "def get_device():\n",
    "    \"\"\"Automatically select the device based on availability\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading RNASeqDB dataset\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'RNAseqDB/data/normalized/_expr.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m expr_df, info_df \u001b[38;5;241m=\u001b[39m \u001b[43mrnaseqdb_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#x = expr_df.values.T\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#symbols = expr_df.index.levels[0].values\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#sampl_ids = expr_df.columns.values\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#tissues = info_df['TISSUE_GTEX'].values\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#datasets = info_df['DATASET'].values\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PHD/Aim_2/cycle_GAN/rnaseqdb.py:220\u001b[0m, in \u001b[0;36mrnaseqdb_load\u001b[0;34m(expr_file, info_file)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03mLoads RNASeqDB\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m:param expr_file: expressions file name\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m:param info_file: sample information file name\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... loading RNASeqDB dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 220\u001b[0m expr_df \u001b[38;5;241m=\u001b[39m \u001b[43mrnaseqdb_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m info_df \u001b[38;5;241m=\u001b[39m rnaseqdb_info_df(info_file)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m expr_df, info_df\n",
      "File \u001b[0;32m~/Documents/PHD/Aim_2/cycle_GAN/rnaseqdb.py:40\u001b[0m, in \u001b[0;36mrnaseqdb_df\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrnaseqdb_df\u001b[39m(file\u001b[38;5;241m=\u001b[39mRNAS_EXPR_FILE):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Loads RNASeqDB expression dataset\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    :param file: RNASeqDB expression file name\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    :return: Pandas dataframe with RNA-Seq values (rows: genes, cols: samples)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# df = df + eps\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# df = df.apply(np.log2)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/Documents/PHD/Aim_2/cycle_GAN/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PHD/Aim_2/cycle_GAN/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Documents/PHD/Aim_2/cycle_GAN/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PHD/Aim_2/cycle_GAN/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Documents/PHD/Aim_2/cycle_GAN/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RNAseqDB/data/normalized/_expr.csv'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "# Load dataset\n",
    "expr_df, info_df = rnaseqdb_load()\n",
    "#x = expr_df.values.T\n",
    "#symbols = expr_df.index.levels[0].values\n",
    "#sampl_ids = expr_df.columns.values\n",
    "#tissues = info_df['TISSUE_GTEX'].values\n",
    "#datasets = info_df['DATASET'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims=None, z_dim=10):\n",
    "        super().__init__()\n",
    "        if h_dims is None:\n",
    "            h_dims = [256, 256]\n",
    "            \n",
    "        self.z_dim = z_dim\n",
    "        self.nb_categoric = len(vocab_sizes)\n",
    "        \n",
    "        self.embeddings = nn.ModuleList()\n",
    "        total_emb_dim = 0\n",
    "        for vs in vocab_sizes:\n",
    "            emb_dim = int(vs ** 0.5) + 1\n",
    "            self.embeddings.append(nn.Embedding(vs, emb_dim))\n",
    "            total_emb_dim += emb_dim\n",
    "        total_emb_dim += nb_numeric\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = z_dim + total_emb_dim\n",
    "        for dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, x_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, z, cat, num):\n",
    "        emb = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            emb.append(embedding(cat[:, i]))\n",
    "        if self.nb_categoric > 0:\n",
    "            emb = torch.cat(emb, dim=1)\n",
    "            h = torch.cat([z, num, emb], dim=1)\n",
    "        else:\n",
    "            h = torch.cat([z, num], dim=1)\n",
    "        \n",
    "        return self.layers(h)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, vocab_sizes, nb_numeric, h_dims=None):\n",
    "        super().__init__()\n",
    "        if h_dims is None:\n",
    "            h_dims = [256, 256]\n",
    "            \n",
    "        self.nb_categoric = len(vocab_sizes)\n",
    "        \n",
    "        self.embeddings = nn.ModuleList()\n",
    "        total_emb_dim = 0\n",
    "        for vs in vocab_sizes:\n",
    "            emb_dim = int(vs ** 0.5) + 1\n",
    "            self.embeddings.append(nn.Embedding(vs, emb_dim))\n",
    "            total_emb_dim += emb_dim\n",
    "            \n",
    "        layers = []\n",
    "        prev_dim = x_dim + nb_numeric + total_emb_dim\n",
    "        for dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x, cat, num):\n",
    "        emb = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            emb.append(embedding(cat[:, i]))\n",
    "        if self.nb_categoric > 0:\n",
    "            emb = torch.cat(emb, dim=1)\n",
    "            h = torch.cat([x, num, emb], dim=1)\n",
    "        else:\n",
    "            h = torch.cat([x, num], dim=1)\n",
    "            \n",
    "        return self.layers(h)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_dim, h_dims=None, z_dim=10):\n",
    "        super().__init__()\n",
    "        if h_dims is None:\n",
    "            h_dims = [256, 256]\n",
    "            \n",
    "        layers = []\n",
    "        prev_dim = x_dim\n",
    "        for dim in h_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, z_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, cat, num):\n",
    "    device = real_samples.device\n",
    "    alpha = torch.rand(real_samples.size(0), 1, device=device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates, cat, num)\n",
    "    \n",
    "    grad_outputs = torch.ones_like(d_interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def train(dataset, cat_covs, num_covs, z_dim, epochs, batch_size,\n",
    "          generator, discriminator, encoder, score_fn, save_fn,\n",
    "          lr=5e-4, nb_critic=5, cycle_weight=10,\n",
    "          gradient_penalty_weight=10, checkpoint_dir='./checkpoints',\n",
    "          log_dir='./logs', patience=10):\n",
    "    \n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move models to device\n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "    \n",
    "    # Convert data to tensors\n",
    "    dataset = torch.FloatTensor(dataset).to(device)\n",
    "    cat_covs = torch.LongTensor(cat_covs).to(device)\n",
    "    num_covs = torch.FloatTensor(num_covs).to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(dataset, cat_covs, num_covs)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    gen_optimizer = optim.RMSprop(generator.parameters(), lr=lr)\n",
    "    disc_optimizer = optim.RMSprop(discriminator.parameters(), lr=lr)\n",
    "    encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    patience_counter = patience\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        gen_losses = []\n",
    "        disc_losses = []\n",
    "        \n",
    "        for batch_idx, (real_data, cat, num) in enumerate(train_loader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            for _ in range(nb_critic):\n",
    "                disc_optimizer.zero_grad()\n",
    "                \n",
    "                z = torch.randn(batch_size, z_dim, device=device)\n",
    "                fake_data = generator(z, cat, num)\n",
    "                \n",
    "                z_rec = encoder(fake_data)\n",
    "                cycled_data = generator(z_rec, cat, num)\n",
    "                \n",
    "                real_validity = discriminator(real_data, cat, num)\n",
    "                fake_validity = discriminator(fake_data.detach(), cat, num)\n",
    "                \n",
    "                gradient_penalty = compute_gradient_penalty(\n",
    "                    discriminator, real_data, fake_data.detach(), cat, num)\n",
    "                cycle_loss = torch.mean(torch.abs(real_data - cycled_data))\n",
    "                \n",
    "                disc_loss = (-torch.mean(real_validity) + torch.mean(fake_validity)\n",
    "                           + gradient_penalty_weight * gradient_penalty\n",
    "                           + cycle_weight * cycle_loss)\n",
    "                \n",
    "                disc_loss.backward()\n",
    "                disc_optimizer.step()\n",
    "                disc_losses.append(disc_loss.item())\n",
    "            \n",
    "            gen_optimizer.zero_grad()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, z_dim, device=device)\n",
    "            fake_data = generator(z, cat, num)\n",
    "            fake_validity = discriminator(fake_data, cat, num)\n",
    "            \n",
    "            z_rec = encoder(fake_data)\n",
    "            cycled_data = generator(z_rec, cat, num)\n",
    "            cycle_loss = torch.mean(torch.abs(fake_data - cycled_data))\n",
    "            \n",
    "            gen_loss = -torch.mean(fake_validity) + cycle_weight * cycle_loss\n",
    "            gen_loss.backward()\n",
    "            \n",
    "            gen_optimizer.step()\n",
    "            encoder_optimizer.step()\n",
    "            gen_losses.append(gen_loss.item())\n",
    "        \n",
    "        writer.add_scalar('Generator Loss', np.mean(gen_losses), epoch)\n",
    "        writer.add_scalar('Discriminator Loss', np.mean(disc_losses), epoch)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            score = score_fn(generator)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                save_fn()\n",
    "                patience_counter = patience\n",
    "            else:\n",
    "                patience_counter -= 1\n",
    "                \n",
    "            if patience_counter == 0:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "                \n",
    "        print(f'Epoch {epoch}/{epochs} - '\n",
    "              f'Gen Loss: {np.mean(gen_losses):.4f} - '\n",
    "              f'Disc Loss: {np.mean(disc_losses):.4f}')\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(cat_covs, num_covs, generator, z=None):\n",
    "    \"\"\"\n",
    "    Generate samples using the trained generator\n",
    "    \"\"\"\n",
    "    device = next(generator.parameters()).device\n",
    "    generator.eval()\n",
    "    \n",
    "    cat_covs = torch.LongTensor(cat_covs).to(device)\n",
    "    num_covs = torch.FloatTensor(num_covs).to(device)\n",
    "    \n",
    "    if z is None:\n",
    "        z = torch.randn(cat_covs.size(0), generator.z_dim, device=device)\n",
    "    elif not isinstance(z, torch.Tensor):\n",
    "        z = torch.FloatTensor(z).to(device)\n",
    "        \n",
    "    samples = generator(z, cat_covs, num_covs)\n",
    "    return samples.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(x_dim, vocab_sizes, nb_numeric)\n",
    "discriminator = Discriminator(x_dim, vocab_sizes, nb_numeric)\n",
    "encoder = Encoder(x_dim, z_dim=z_dim)\n",
    "\n",
    "train(dataset, cat_covs, num_covs, z_dim, epochs, batch_size,\n",
    "      generator, discriminator, encoder, score_fn, save_fn)\n",
    "\n",
    "\n",
    "samples = predict(cat_covs, num_covs, generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
